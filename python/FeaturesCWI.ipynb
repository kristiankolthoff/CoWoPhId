{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.wsd import lesk\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_ENGLISH_WIKIPEDIA = \"../cwishareddataset/traindevset/\" + \\\n",
    "                           \"english/Wikipedia_Train.tsv\"\n",
    "df = pd.read_csv(TRAIN_ENGLISH_WIKIPEDIA, sep = \"\\t\")\n",
    "df.columns = ['id', 'sentence', \"start\", \"end\", \"target\", \n",
    "              \"nat\", \"non_nat\", \"nat_marked\", \"non_nat_marked\", \"binary\", \"prob\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'sentence', 'start', 'end', 'target', 'nat', 'non_nat',\n",
       "       'nat_marked', 'non_nat_marked', 'binary', 'prob'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.index\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>binary</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2493.0</td>\n",
       "      <td>2493.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.211372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.228076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       binary         prob\n",
       "count  2493.0  2493.000000\n",
       "mean      1.0     0.211372\n",
       "std       0.0     0.228076\n",
       "min       1.0     0.050000\n",
       "25%       1.0     0.050000\n",
       "50%       1.0     0.100000\n",
       "75%       1.0     0.250000\n",
       "max       1.0     1.000000"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.binary == 1, ['target', 'binary', 'prob']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>binary</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3057.0</td>\n",
       "      <td>6.357213</td>\n",
       "      <td>3.558134</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2493.0</td>\n",
       "      <td>10.203771</td>\n",
       "      <td>5.206972</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         count       mean       std  min  25%  50%   75%   max\n",
       "binary                                                        \n",
       "0       3057.0   6.357213  3.558134  2.0  4.0  6.0   7.0  49.0\n",
       "1       2493.0  10.203771  5.206972  2.0  7.0  9.0  12.0  49.0"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['length'] = df.target.apply(len)\n",
    "df.groupby('binary')['length'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet Features\n",
    "Here we implement all the relevant features based on WordNet and SentiWordNet. For example, the number of synsets the target word is contained in or the average length of the lemmas of all the synsets the target word is contained in. Note that all features that are computed in the following exploit neither the POS-Tag of the target word nor Word Sense Disambiguation by e.g. UKB-Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\k_kol\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2889: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\k_kol\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: RuntimeWarning: Mean of empty slice\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\k_kol\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: RuntimeWarning: Mean of empty slice\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "def wn_synset_freq(target):\n",
    "    return len(wn.synsets(target))\n",
    "\n",
    "def wn_synset_avg_lemma_freq(target):\n",
    "    return np.nan_to_num(np.mean([len(synset.lemmas()) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_lemma_len(target):\n",
    "    return np.nan_to_num(np.nanmean([len(lemma.name()) \n",
    "            for synset in wn.synsets(target) \n",
    "            for lemma in synset.lemmas()]))\n",
    "\n",
    "def wn_synset_avg_hypernyms(target):\n",
    "    return np.nan_to_num(np.nanmean([len(synset.hypernyms()) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_hyponyms(target):\n",
    "    return np.nan_to_num(np.mean([len(synset.hyponyms()) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_sum_hypernyms(target):\n",
    "    return np.sum(([len(synset.hypernyms()) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_definition_len(target):\n",
    "    return np.nan_to_num(np.mean([len(str(synset.definition())) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_hyptree_depth(target):\n",
    "    return np.nan_to_num(np.mean([synset.max_depth() \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_num_distinct_pos(target):\n",
    "    return len(set([synset.pos() for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_num_relations(target):\n",
    "    return np.nan_to_num(np.mean([np.sum([len(synset.hypernyms()), len(synset.hyponyms()), \n",
    "             len(synset.instance_hypernyms()), len(synset.instance_hyponyms()),\n",
    "             len(synset.member_holonyms()), len(synset.substance_holonyms()),\n",
    "             len(synset.part_holonyms()), len(synset.member_meronyms()),\n",
    "             len(synset.substance_meronyms()), len(synset.part_meronyms())]) \n",
    "             for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_freq_pos(target, pos):\n",
    "    return len(wn.synsets(target, pos = pos))\n",
    "\n",
    "def wn_synset_sense_entropy_uniform(target):\n",
    "    num_senses = len(wn.synsets(target))\n",
    "    return -np.sum([((1 / num_senses) * np.log2(1 / num_senses)) \n",
    "                     for index in range(0, num_senses)])\n",
    "\n",
    "def wn_synset_sense_entropy_pos_uniform(target):\n",
    "    num_senses = len(wn.synsets(target))\n",
    "    pos_distribution = [len(wn.synsets(target, pos = wn.NOUN)),\n",
    "                        len(wn.synsets(target, pos = wn.VERB)),\n",
    "                        len(wn.synsets(target, pos = wn.ADJ)),\n",
    "                        len(wn.synsets(target, pos = wn.ADV))]\n",
    "    return -np.sum([(np.nan_to_num((count / num_senses) * np.log2(count / num_senses))) \n",
    "            for count in pos_distribution]) if num_senses != 0 else 0\n",
    "\n",
    "def wn_synsets_sense_entropy_pos_central(target, pos):\n",
    "    num_senses_pos = len(wn.synsets(target, pos = pos))\n",
    "    return -np.sum([((1 / num_senses_pos) * np.log2(1 / num_senses_pos))\n",
    "                     for index in range(0, num_senses_pos)])\n",
    "     \n",
    "\n",
    "def swn_avg_objective_score(target):\n",
    "    return np.nan_to_num(np.mean([senti_synset.obj_score() \n",
    "                for senti_synset in swn.senti_synsets(target)]))\n",
    "\n",
    "def pos_tag(sentence, target):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    wordPOSPairs = [token for token in nltk.pos_tag(tokens) if token[0] == target]\n",
    "    return wordPOSPairs[0][1] if len(wordPOSPairs) > 0 else None\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    if not tag:\n",
    "        return None\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    "    if tag.startswith('V'):\n",
    "        return 'v'\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    "    if tag.startswith('R'):\n",
    "        return 'r'\n",
    "    return None\n",
    "\n",
    "df['wn_synset_freq'] = df.target.apply(wn_synset_freq)\n",
    "df['wn_synset_avg_lemma_freq'] = df.target.apply(wn_synset_avg_lemma_freq)\n",
    "df['wn_synset_avg_lemma_len'] = df.target.apply(wn_synset_avg_lemma_len)\n",
    "\n",
    "df['wn_synset_diff_len_avg_lemma_len'] = df.wn_synset_avg_lemma_len - df.length\n",
    "df['wn_synset_avg_hypernyms'] = df.target.apply(wn_synset_avg_hypernyms)\n",
    "df['wn_synset_sum_hypernyms'] = df.target.apply(wn_synset_sum_hypernyms)\n",
    "df['wn_synset_avg_hyponyms'] = df.target.apply(wn_synset_avg_hyponyms)\n",
    "\n",
    "df['wn_synset_avg_definition_len'] = df.target.apply(wn_synset_avg_definition_len)\n",
    "df['wn_synset_avg_hyptree_depth'] = df.target.apply(wn_synset_avg_hyptree_depth)\n",
    "df['wn_synset_num_distinct_pos'] = df.target.apply(wn_synset_num_distinct_pos)\n",
    "df['wn_synset_avg_num_relations'] = df.target.apply(wn_synset_avg_num_relations)\n",
    "\n",
    "# Synset sizes of the target word for the four different POS-Tags in WordNet\n",
    "df['wn_synset_avg_freq_pos_noun'] = df.target.apply(lambda target : wn_synset_avg_freq_pos(target, wn.NOUN))\n",
    "df['wn_synset_avg_freq_pos_verb'] = df.target.apply(lambda target : wn_synset_avg_freq_pos(target, wn.VERB))\n",
    "df['wn_synset_avg_freq_pos_adj'] = df.target.apply(lambda target : wn_synset_avg_freq_pos(target, wn.ADJ))\n",
    "df['wn_synset_avg_freq_pos_adv'] = df.target.apply(lambda target : wn_synset_avg_freq_pos(target, wn.ADV))\n",
    "\n",
    "# Normalized POS-Tag synset sizes\n",
    "df['wn_synset_avg_freq_pos_noun_norm'] = np.nan_to_num(df.wn_synset_avg_freq_pos_noun / df.wn_synset_freq)\n",
    "df['wn_synset_avg_freq_pos_verb_norm'] = np.nan_to_num(df.wn_synset_avg_freq_pos_verb / df.wn_synset_freq)\n",
    "df['wn_synset_avg_freq_pos_adj_norm'] = np.nan_to_num(df.wn_synset_avg_freq_pos_adj / df.wn_synset_freq)\n",
    "df['wn_synset_avg_freq_pos_adv_norm'] = np.nan_to_num(df.wn_synset_avg_freq_pos_adv / df.wn_synset_freq)\n",
    "\n",
    "df['pos_tag'] = df[['sentence', 'target']].apply(lambda vals : pos_tag(*vals), axis = 1)\n",
    "df['wn_synset_sense_entropy_uniform'] = df.target.apply(wn_synset_sense_entropy_uniform)\n",
    "df['wn_synset_sense_entropy_pos_uniform'] = df.target.apply(wn_synset_sense_entropy_pos_uniform)\n",
    "df['wn_synsets_sense_entropy_pos_central'] = df[['target', 'pos_tag']].apply(\n",
    "    lambda vals : wn_synsets_sense_entropy_pos_central(vals[0], penn_to_wn(vals[1])), axis = 1)\n",
    "\n",
    "df['swn_avg_objective_score'] = df.target.apply(swn_avg_objective_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>length</th>\n",
       "      <th>wn_synset_freq</th>\n",
       "      <th>wn_synset_sense_entropy_uniform</th>\n",
       "      <th>wn_synset_sense_entropy_pos_uniform</th>\n",
       "      <th>wn_synsets_sense_entropy_pos_central</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>passed</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>4.643856</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>4.643856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>land</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>4.169925</td>\n",
       "      <td>0.964079</td>\n",
       "      <td>3.459432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>future</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>2.807355</td>\n",
       "      <td>0.985228</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>future generations</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>generations</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>2.807355</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>2.807355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               target  length  wn_synset_freq  \\\n",
       "0              passed       6              25   \n",
       "1                land       4              18   \n",
       "2              future       6               7   \n",
       "3  future generations      18               0   \n",
       "4         generations      11               7   \n",
       "\n",
       "   wn_synset_sense_entropy_uniform  wn_synset_sense_entropy_pos_uniform  \\\n",
       "0                         4.643856                            -0.000000   \n",
       "1                         4.169925                             0.964079   \n",
       "2                         2.807355                             0.985228   \n",
       "3                        -0.000000                             0.000000   \n",
       "4                         2.807355                            -0.000000   \n",
       "\n",
       "   wn_synsets_sense_entropy_pos_central  \n",
       "0                              4.643856  \n",
       "1                              3.459432  \n",
       "2                              2.000000  \n",
       "3                             -0.000000  \n",
       "4                              2.807355  "
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:,['target', 'length','wn_synset_freq', 'wn_synset_sense_entropy_uniform', 'wn_synset_sense_entropy_pos_uniform', 'wn_synsets_sense_entropy_pos_central']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PorterStemmer and StanfordNLP Features\n",
    "Here we implement features based on the PorterStemmer library from nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<DependencyGraph with 8 nodes>]"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "from nltk.stem.wordnet import *\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "\n",
    "java_path = \"C:/Program Files (x86)/Java/jdk1.8.0_144/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "path_to_jar = 'resources/stanford-dependency-parser/stanford-parser.jar'\n",
    "path_to_models_jar = 'resources/stanford-dependency-parser/stanford-parser-3.9.1-models.jar'\n",
    "\n",
    "porterStemmer = PorterStemmer()\n",
    "wordNetLemmatizer = WordNetLemmatizer()\n",
    "nerTagger = StanfordNERTagger('resources/stanford-ner-tagger/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "               'resources/stanford-ner-tagger/stanford-ner.jar',\n",
    "               encoding='utf-8')\n",
    "dependencyParser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "\n",
    "def porter_stem_len(target):\n",
    "    return len(str(porterStemmer.stem(target)))\n",
    "\n",
    "def porter_stemmer_num_steps(target):\n",
    "    stem = target.lower()\n",
    "    applied_steps = 0\n",
    "    if porterStemmer.mode == porterStemmer.NLTK_EXTENSIONS and target in porterStemmer.pool:\n",
    "            return applied_steps\n",
    "    if porterStemmer.mode != porterStemmer.ORIGINAL_ALGORITHM and len(target) <= 2:\n",
    "            return applied_steps\n",
    "    step_funcs = [porterStemmer._step1a, porterStemmer._step1b, porterStemmer._step1c,\n",
    "                  porterStemmer._step2, porterStemmer._step3, porterStemmer._step3,\n",
    "                  porterStemmer._step4, porterStemmer._step5a, porterStemmer._step5b]\n",
    "    for step_func in step_funcs:\n",
    "        stem_step = step_func(stem)\n",
    "        if stem_step != stem:\n",
    "            stem = stem_step\n",
    "            applied_steps += 1\n",
    "    return applied_steps\n",
    "\n",
    "def is_named_entity(sentence, target):\n",
    "    tokenized_sent = word_tokenize(sentence)\n",
    "    tagged_sent = nerTagger.tag(tokenized_sent)\n",
    "    for token, tag in tagged_sent:\n",
    "        if token == target and tag != 'O':\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def named_entity_type(sentence, target):\n",
    "    tokenized_sent = word_tokenize(sentence)\n",
    "    tagged_sent = nerTagger.tag(tokenized_sent)\n",
    "    return [tag for token, tag in tagged_sent if token == target][0]\n",
    "\n",
    "# TODO consider using stanford lemmatizer and compute word similarity metric\n",
    "# to orignal target\n",
    "def wordnet_lemma_len(target):\n",
    "    return len(wordNetLemmatizer.lemmatize(target))\n",
    "\n",
    "\n",
    "# Porter stemmer stem length, number of applied steps,\n",
    "# difference of stem length to target and reduction ratio\n",
    "df['porter_stem_len'] = df.target.apply(porter_stem_len)\n",
    "df['porter_stemmer_num_steps'] = df.target.apply(porter_stemmer_num_steps)\n",
    "df['diff_len_stem_len'] = df.length - df.num_porter_stem_len\n",
    "df['reduction_stem_len'] = 1 - df.porter_stem_len / df.length\n",
    "\n",
    "# WordNet lemma length, differnce of lemma length to target\n",
    "# length and reduction ratio for lemmatization\n",
    "df['wordnet_lemma_len'] = df.target.apply(wordnet_lemma_len)\n",
    "df['diff_len_wordnet_lemma_len'] = df.length - df.wordnet_lemma_len\n",
    "df['reduction_lemma_len'] = 1 - df.wordnet_lemma_len / df.length\n",
    "\n",
    "# StanfordNLP features CAUTION: The tagger is slow\n",
    "#df['is_named_entity'] = df[['sentence', 'target']].apply(lambda vals : is_named_entity(*vals), axis = 1)\n",
    "#df['named_entity_type'] = df[['sentence', 'target']].apply(lambda vals : named_entity_type(*vals), axis = 1)\n",
    "result = dependencyParser.raw_parse('I shot an elephant in my sleep')\n",
    "list(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[:, ['target', 'length', 'wordnet_lemma_len', 'diff_len_wordnet_lemma_len', 'reduction_lemma_len']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orthographic Features\n",
    "Here we compute orthographic features for the target word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relative position of the target word based on tokens\n",
    "df['relative_position'] = df[['sentence', 'target']].apply(lambda vals : \n",
    "            (nltk.word_tokenize(vals[0]).index(vals[1].split()[0])) / len((nltk.word_tokenize(vals[0]))), axis = 1)\n",
    "# Relative positions of the target word based on character counting\n",
    "df['relative_position_left'] = df[['sentence', 'start']].apply(lambda vals : vals[1] / len(vals[0]), axis = 1)\n",
    "df['relative_position_centered'] = df[['sentence', 'start', 'end']].apply(lambda vals : \n",
    "            ((vals[1] + vals[2]) / 2) / len(vals[0]), axis = 1)\n",
    "df['relative_position_right'] = df[['sentence', 'end']].apply(lambda vals : vals[1] / len(vals[0]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.loc[:, ['sentence', 'target', 'start', 'relative_position', 'relative_position_left', 'relative_position_centered', 'relative_position_right']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context-Aware Features\n",
    "Here we compute not only the context extraction/definition in the first place but also the corresponding context features afterwards. Also we need to implement proper strategies to cope with the target occuring multiple times in the sentence. To avoid mistakes, we should use the actual start and end tags from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "from nltk.parse.stanford import StanfordNeuralDependencyParser\n",
    "import os\n",
    "\n",
    "java_path = \"C:/Program Files (x86)/Java/jdk1.8.0_144/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "path_to_jar = 'resources/stanford-dependency-parser/stanford-parser.jar'\n",
    "path_to_models_jar = 'resources/stanford-dependency-parser/stanford-parser-3.9.1-models.jar'\n",
    "\n",
    "dependencyParser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "\n",
    "def post_process_ctx(context):\n",
    "    return [token for token in context if token not in (\",\", \"'\", \"'s\")]\n",
    "    \n",
    "def ctx_extraction_all(context, target):\n",
    "    return word_tokenize(context)\n",
    "\n",
    "def ctx_extraction_all_filtered(context, target):\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    return post_process_ctx(ctx_tokens)\n",
    "\n",
    "def ctx_extraction_window_pre_n(context, target, n = 3):\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    post_ctx_tokens = post_process_ctx(ctx_tokens)\n",
    "    target_index = post_ctx_tokens.index(target)\n",
    "    start_index = (target_index - n) if (target_index - n) > 0 else 0\n",
    "    return post_ctx_tokens[start_index:target_index]\n",
    "\n",
    "def ctx_extraction_window_suc_n(context, target, n = 3):\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    post_ctx_tokens = post_process_ctx(ctx_tokens)\n",
    "    target_index = post_ctx_tokens.index(target)\n",
    "    end_index = (target_index + 1 + n) if (target_index + 1 + n) \\\n",
    "                < len(post_ctx_tokens) else len(post_ctx_tokens)\n",
    "    return post_ctx_tokens[target_index+1:end_index]\n",
    "\n",
    "def ctx_extraction_window_pre_suc_n(context, target, n = 3):\n",
    "    ctx_tokens_pre = ctx_extraction_window_pre_n(context, target, n)\n",
    "    ctx_tokens_suc = ctx_extraction_window_suc_n(context, target, n)\n",
    "    ctx_tokens_pre.extend(ctx_tokens_suc)\n",
    "    return ctx_tokens_pre\n",
    "\n",
    "def ctx_extraction_dep_in(context, target):\n",
    "    return [triple[0][0] for parse in dependencyParser.raw_parse(context)\n",
    "            for triple in list(parse.triples()) if triple[2][0] == target]\n",
    "\n",
    "def ctx_extraction_dep_out(context, target):\n",
    "    return [triple[2][0] for parse in dependencyParser.raw_parse(context)\n",
    "            for triple in list(parse.triples()) if triple[0][0] == target]\n",
    "\n",
    "def ctx_extraction_dep_in_out(context, target):\n",
    "    ctx_tokens_in = ctx_extraction_dep_in(context, target)\n",
    "    ctx_tokens_out = ctx_extraction_dep_out(context, target)\n",
    "    ctx_tokens_in.extend(ctx_tokens_out)\n",
    "    return ctx_tokens_in\n",
    "\n",
    "def ctx_extraction_dep_recu_in_n_steps(context, target, n = 2):\n",
    "    deps = [triple for parse in dependencyParser.raw_parse(context)\n",
    "            for triple in parse.triples()]\n",
    "    result_tokens = []\n",
    "    curr_target = [target]\n",
    "    for step in range(0, n):\n",
    "        step_result = [triple[0][0] for triple in deps \n",
    "                       if triple[2][0] in curr_target]\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "    return list(set(result_tokens))\n",
    "\n",
    "def ctx_extraction_dep_recu_out_n_steps(context, target, n = 2):\n",
    "    deps = [triple for parse in dependencyParser.raw_parse(context)\n",
    "            for triple in parse.triples()]\n",
    "    result_tokens = []\n",
    "    curr_target = [target]\n",
    "    for step in range(0, n):\n",
    "        step_result = [triple[2][0] for triple in deps \n",
    "                       if triple[0][0] in curr_target]\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "    return list(set(result_tokens))\n",
    "\n",
    "def ctx_extraction_dep_recu_in_out_n_steps(context, target, n = 2):\n",
    "    deps = [triple for parse in dependencyParser.raw_parse(context)\n",
    "            for triple in parse.triples()]\n",
    "    result_tokens = []\n",
    "    curr_target = [target]\n",
    "    for step in range(0, n):\n",
    "        step_result = [triple[2][0] for triple in deps \n",
    "                       if triple[0][0] in curr_target]\n",
    "        step_result_out = [triple[0][0] for triple in deps \n",
    "                       if triple[2][0] in curr_target]\n",
    "        step_result.extend(step_result_out)\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "    return list(set(result_tokens))\n",
    "\n",
    "def ctx_extraction_dep_recu_in_cover(context, target, cover = 0.1):\n",
    "    deps = [triple for parse in dependencyParser.raw_parse(context)\n",
    "            for triple in parse.triples()]\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    ctx_tokens_post = post_process_ctx(ctx_tokens)\n",
    "    result_tokens = []\n",
    "    curr_target = [target]\n",
    "    curr_cover = 0\n",
    "    while curr_cover < cover:\n",
    "        step_result = [triple[0][0] for triple in deps \n",
    "                       if triple[2][0] in curr_target]\n",
    "        if set(step_result) == set(curr_target):\n",
    "                break\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "        curr_cover = len(result_tokens) / len(ctx_tokens_post)\n",
    "    return list(set(result_tokens))\n",
    "\n",
    "def ctx_extraction_dep_recu_out_cover(context, target, cover = 0.1):\n",
    "    deps = [triple for parse in dependencyParser.raw_parse(context)\n",
    "            for triple in parse.triples()]\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    ctx_tokens_post = post_process_ctx(ctx_tokens)\n",
    "    result_tokens = []\n",
    "    curr_target = [target]\n",
    "    curr_cover = 0\n",
    "    while curr_cover < cover:\n",
    "        step_result = [triple[2][0] for triple in deps \n",
    "                       if triple[0][0] in curr_target]\n",
    "        if set(step_result) == set(curr_target):\n",
    "                break\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "        curr_cover = len(result_tokens) / len(ctx_tokens_post)\n",
    "    return list(set(result_tokens))\n",
    "\n",
    "def ctx_extraction_dep_recu_in_out_cover(context, target, cover = 0.1):\n",
    "    deps = [triple for parse in dependencyParser.raw_parse(context)\n",
    "            for triple in parse.triples()]\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    ctx_tokens_post = post_process_ctx(ctx_tokens)\n",
    "    result_tokens = []\n",
    "    curr_target = [target]\n",
    "    curr_cover = 0\n",
    "    while curr_cover < cover:\n",
    "        step_result = [triple[2][0] for triple in deps \n",
    "                       if triple[0][0] in curr_target]\n",
    "        step_result_out = [triple[0][0] for triple in deps \n",
    "                       if triple[2][0] in curr_target]\n",
    "        step_result.extend(step_result_out)\n",
    "        if set(step_result) == set(curr_target):\n",
    "                break\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "        curr_cover = len(result_tokens) / len(ctx_tokens_post)\n",
    "    print(curr_cover)\n",
    "    return list(set(result_tokens))\n",
    "\n",
    "def agg_feat_num_average(tokens, func_feature):\n",
    "    return np.mean([func_feature(token) for token in tokens])\n",
    "\n",
    "def agg_feat_num_median(tokens, func_feature):\n",
    "    return np.median([func_feature(token) for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Normally, the land will be passed down by future generations in a way \" + \\\n",
    "             \"that recognizes the community's traditional connection to that country \"\n",
    "target = 'passed'\n",
    "\n",
    "print('ctx_etraction_all:')\n",
    "print(ctx_extraction_all_filtered(sentence, target))\n",
    "\n",
    "print('ctx_extraction_window_pre_n:')\n",
    "print(ctx_extraction_window_pre_n(sentence, \"Normally\"))\n",
    "print(ctx_extraction_window_pre_n(sentence, \"the\"))\n",
    "print(ctx_extraction_window_pre_n(sentence, \"land\"))\n",
    "print(ctx_extraction_window_pre_n(sentence, target, n = 5))\n",
    "\n",
    "print('ctx_extraction_window_suc_n:')\n",
    "print(ctx_extraction_window_suc_n(sentence, \"country\"))\n",
    "print(ctx_extraction_window_suc_n(sentence, \"to\"))\n",
    "print(ctx_extraction_window_suc_n(sentence, \"connection\"))\n",
    "print(ctx_extraction_window_suc_n(sentence, \"community\", n = 5))\n",
    "\n",
    "print('ctx_extraction_window_pre_suc_n:')\n",
    "print(ctx_extraction_window_pre_suc_n(sentence, \"passed\"))\n",
    "print(ctx_extraction_window_pre_suc_n(sentence, \"the\"))\n",
    "print(ctx_extraction_window_pre_suc_n(sentence, \"to\"))\n",
    "\n",
    "print('ctx_extraction_dep_in:')\n",
    "print(ctx_extraction_dep_in(sentence, \"land\"))\n",
    "\n",
    "print('ctx_extraction_dep_out:')\n",
    "print(ctx_extraction_dep_out(sentence, target))\n",
    "print(ctx_extraction_dep_out(sentence, \"land\"))\n",
    "\n",
    "print('ctx_extraction_dep_in_out:')\n",
    "print(ctx_extraction_dep_in_out(sentence, \"land\"))\n",
    "\n",
    "print('ctx_extraction_dep_recu_in_n_steps:')\n",
    "print(ctx_extraction_dep_recu_in_n_steps(sentence, \"the\", n = 3))\n",
    "\n",
    "print('ctx_extraction_dep_recu_out_n_steps:')\n",
    "print(ctx_extraction_dep_recu_out_n_steps(sentence, \"the\"))\n",
    "\n",
    "print('ctx_extraction_dep_recu_in_out_n_steps:')\n",
    "print(ctx_extraction_dep_recu_in_out_n_steps(sentence, \"the\"))\n",
    "\n",
    "print('ctx_extraction_dep_recu_in_cover:')\n",
    "print(ctx_extraction_dep_recu_in_cover(sentence, \"the\", cover=0.1))\n",
    "\n",
    "print('ctx_extraction_dep_recu_out_cover:')\n",
    "print(ctx_extraction_dep_recu_out_cover(sentence, \"the\", cover=0.1))\n",
    "\n",
    "print('ctx_extraction_dep_recu_in_out_cover:')\n",
    "print(ctx_extraction_dep_recu_in_out_cover(sentence, \"the\", cover=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we defined all the context extraction approaches, we can apply them on the actual dataset. To do so, we first extract all the distinct sentences from the actual training set and create a new dataframe containing only the sentence ids, the sentence, the target and all the computed contexts. This also makes it easier to integrate context extraction functions implemented in other languages. Afterwards we can compute the context features and join them back with the target features dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_context = df.loc[:, ['id', 'sentence', 'target', 'start', 'end']].head()\n",
    "df_context['ctx_avg_word_length'] = \\\n",
    "    df_context[['sentence', 'target']].apply(lambda vals : \n",
    "                               ctx_extraction_window_pre_suc_n(vals[0], vals[1]), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "Here we compute individual feature importance based on different metrics. For example, we implement and compute the F-Score, providing an idea of the discrimination power the feature has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18792713906124658"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.groupby('binary')['porter_stemmer_num_steps'].describe()\n",
    "df.groupby('target').size().sort_values()\n",
    "\n",
    "def feat_importance_f_score(dataframe, feat_name, label_name):\n",
    "    mean_feat = np.mean(dataframe.loc[:, [feat_name]])[0]\n",
    "    means = df.loc[: , [feat_name, label_name]].groupby(label_name).mean().reset_index()\n",
    "    mean_negativ = means.loc[means[label_name] == 0, [feat_name]][feat_name][0]\n",
    "    mean_positiv = means.loc[means[label_name] == 1, [feat_name]][feat_name][1]\n",
    "    # Compute the sum of deviations of the class mean from the overall mean\n",
    "    class_mean_devs = (mean_positiv - mean_feat)**2 + (mean_negativ - mean_feat)**2\n",
    "    # Compute neagtive instance based values\n",
    "    neg_inst = dataframe.loc[dataframe[label_name] == 0, [feat_name]]\n",
    "    std_dev_neg = (np.sum((neg_inst - mean_negativ)**2) / (len(neg_inst) - 1))[feat_name]\n",
    "    #Compute positive instance based values\n",
    "    pos_inst = dataframe.loc[dataframe[label_name] == 1, [feat_name]]\n",
    "    std_dev_pos = (np.sum((pos_inst - mean_positiv)**2) / (len(pos_inst) - 1))[feat_name]\n",
    "    return class_mean_devs / (std_dev_neg + std_dev_pos)\n",
    "    \n",
    "feat_importance_f_score(df, 'length', 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
