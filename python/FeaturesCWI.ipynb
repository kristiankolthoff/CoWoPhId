{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "First, we load all the data we need into pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.wsd import lesk\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_ENGLISH_WIKIPEDIA = \"../cwishareddataset/traindevset/\" + \\\n",
    "                           \"english/Wikipedia_Train.tsv\"\n",
    "df = pd.read_csv(TRAIN_ENGLISH_WIKIPEDIA, sep = \"\\t\")\n",
    "df.columns = ['id', 'sentence', \"start\", \"end\", \"target\", \n",
    "              \"nat\", \"non_nat\", \"nat_marked\", \"non_nat_marked\", \"binary\", \"prob\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 200\n",
    "df_trans = df.loc[df.length < 5, [\"sentence\", \"target\", \"binary\"]]\n",
    "df_trans.loc[df_trans.binary == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation\n",
    "Since many labels are multi-word expression, we first of all define some aggregation functions that aggregate feature values over multiple tokens. Implementing this seperately allows to easily exchange the used aggregation function and keeps the feature computation functions clean. These feature computation functions should only compute features for a single target word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def agg_feat_num_average(target, func_feature, *args):\n",
    "    return np.mean([func_feature(token, *args) for token in word_tokenize(target)])\n",
    "\n",
    "def agg_feat_num_median(target, func_feature, *args):\n",
    "    return np.median([func_feature(token, *args) for token in word_tokenize(target)])\n",
    "\n",
    "def agg_feat_num_max(target, func_feature, *args):\n",
    "    return np.max([func_feature(token, *args) for token in word_tokenize(target)])\n",
    "\n",
    "def agg_feat_num_min(target, func_feature, *args):\n",
    "    return np.min([func_feature(token, *args) for token in word_tokenize(target)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orthographic features\n",
    "Here we start computing simple features like the length of the target word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def ratio_cap_letters(target):\n",
    "    return np.sum([1 for letter in target if target.isupper()]) / len(target)\n",
    "\n",
    "df['length'] = df.target.apply(lambda target : agg_feat_num_average(target, len))\n",
    "df['num_words'] = df.target.apply(lambda target : len(word_tokenize(target)))\n",
    "#Relative position of the target word based on tokens\n",
    "df['relative_position'] = df[['sentence', 'target']].apply(lambda vals : \n",
    "            (nltk.word_tokenize(vals[0]).index(vals[1].split()[0])) / len((nltk.word_tokenize(vals[0]))), axis = 1)\n",
    "# Relative positions of the target word based on character counting\n",
    "df['relative_position_left'] = df[['sentence', 'start']].apply(lambda vals : vals[1] / len(vals[0]), axis = 1)\n",
    "df['relative_position_centered'] = df[['sentence', 'start', 'end']].apply(lambda vals : \n",
    "            ((vals[1] + vals[2]) / 2) / len(vals[0]), axis = 1)\n",
    "df['relative_position_right'] = df[['sentence', 'end']].apply(lambda vals : vals[1] / len(vals[0]), axis = 1)\n",
    "df['ratio_cap_letters'] = df.target.apply(lambda target : agg_feat_num_average(target, ratio_cap_letters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linguistic Features\n",
    "Here we compute linguistic word features like the number of vowels the word has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import cmudict\n",
    "import numpy as np\n",
    "d = cmudict.dict()\n",
    "\n",
    "def num_syllables_rule_based(target):\n",
    "    vowels = \"aeiouy\"\n",
    "    numVowels = 0\n",
    "    lastWasVowel = False\n",
    "    for wc in target:\n",
    "        foundVowel = False\n",
    "        for v in vowels:\n",
    "            if v == wc:\n",
    "                if not lastWasVowel: numVowels+=1  \n",
    "                foundVowel = lastWasVowel = True\n",
    "                break\n",
    "        if not foundVowel:  \n",
    "            lastWasVowel = False\n",
    "    if len(target) > 2 and target[-2:] == \"es\":\n",
    "        numVowels-=1\n",
    "    elif len(target) > 1 and target[-1:] == \"e\":\n",
    "        numVowels-=1\n",
    "    return numVowels\n",
    "\n",
    "def num_syllables(target):\n",
    "    if target in d:\n",
    "        return np.mean([len(list(y for y in x if y[-1].isdigit())) for x in d[target.lower()]])\n",
    "    else:\n",
    "        return num_syllables_rule_based(target)\n",
    "\n",
    "def num_vowels(target):\n",
    "    return np.sum([target.lower().count(vowel) for vowel in 'aeiouy'])\n",
    "\n",
    "df['num_syllables'] = df.target.apply(lambda target : agg_feat_num_average(target, num_syllables))\n",
    "df['num_vowels'] = df.target.apply(lambda target : agg_feat_num_average(target, num_vowels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import *\n",
    "\n",
    "wordNetLemmatizer = WordNetLemmatizer()\n",
    "\n",
    "word_freq_wiki = {}\n",
    "with open(\"resources/word-freq-dumps/enwiki-20150602-words-frequency.txt\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        word, freq = line.partition(\" \")[::2]\n",
    "        word_freq_wiki[word.strip()] = int(freq)\n",
    "\n",
    "word_pknown_nobs_prev_freqZipf = {}\n",
    "with open(\"resources/word-freq-dumps/word_prevelance.csv\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        word, p_known, nobs, prevelance, freqZipf = line.split(\";\")\n",
    "        word_pknown_nobs_prev_freqZipf[word.strip()] = (float(p_known.replace(',','.')), \n",
    "                                                        float(nobs.replace(',','.')), \n",
    "                                                        float(prevelance.replace(',','.')), \n",
    "                                                        float(freqZipf.replace(',','.')))\n",
    "\n",
    "def get_dict_count(target, freqs):\n",
    "    return freqs.get(target.strip().lower(), 0)\n",
    "\n",
    "def perc_known(target):\n",
    "    stats = word_pknown_nobs_prev_freqZipf.get(target)\n",
    "    return stats[0] if stats else 0\n",
    "\n",
    "def nobs(target):\n",
    "    stats = word_pknown_nobs_prev_freqZipf.get(target)\n",
    "    return stats[1] if stats else 0\n",
    "\n",
    "def prevelance(target):\n",
    "    stats = word_pknown_nobs_prev_freqZipf.get(target)\n",
    "    return stats[2] if stats else 0\n",
    "\n",
    "def freqZipf(target):\n",
    "    stats = word_pknown_nobs_prev_freqZipf.get(target)\n",
    "    return stats[3] if stats else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['freq_wiki'] = df.target.apply(lambda target : agg_feat_num_average(target, get_dict_count, word_freq_wiki))\n",
    "df['perc_known'] = df.target.apply(lambda target : agg_feat_num_average(target, perc_known))\n",
    "df['nobs'] = df.target.apply(lambda target : agg_feat_num_average(target, nobs))\n",
    "df['prevelance'] = df.target.apply(lambda target : agg_feat_num_average(target, prevelance))\n",
    "df['freqZipf'] = df.target.apply(lambda target : agg_feat_num_average(target, freqZipf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet Features\n",
    "Here we implement all the relevant features based on WordNet and SentiWordNet. For example, the number of synsets the target word is contained in or the average length of the lemmas of all the synsets the target word is contained in. Note that all features that are computed in the following exploit neither the POS-Tag of the target word nor Word Sense Disambiguation by e.g. UKB-Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "def wn_synset_freq(target):\n",
    "    return len(wn.synsets(target))\n",
    "\n",
    "def wn_synset_avg_lemma_freq(target):\n",
    "    return np.nan_to_num(np.mean([len(synset.lemmas()) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_lemma_len(target):\n",
    "    return np.nan_to_num(np.nanmean([len(lemma.name()) \n",
    "            for synset in wn.synsets(target) \n",
    "            for lemma in synset.lemmas()]))\n",
    "\n",
    "def wn_synset_avg_hypernyms(target):\n",
    "    return np.nan_to_num(np.nanmean([len(synset.hypernyms()) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_hyponyms(target):\n",
    "    return np.nan_to_num(np.mean([len(synset.hyponyms()) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_sum_hypernyms(target):\n",
    "    return np.sum(([len(synset.hypernyms()) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_definition_len(target):\n",
    "    return np.nan_to_num(np.mean([len(str(synset.definition())) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_hyptree_depth(target):\n",
    "    return np.nan_to_num(np.mean([synset.max_depth() \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_num_distinct_pos(target):\n",
    "    return len(set([synset.pos() for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_num_relations(target):\n",
    "    return np.nan_to_num(np.mean([np.sum([len(synset.hypernyms()), len(synset.hyponyms()), \n",
    "             len(synset.instance_hypernyms()), len(synset.instance_hyponyms()),\n",
    "             len(synset.member_holonyms()), len(synset.substance_holonyms()),\n",
    "             len(synset.part_holonyms()), len(synset.member_meronyms()),\n",
    "             len(synset.substance_meronyms()), len(synset.part_meronyms())]) \n",
    "             for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_freq_pos(target, pos):\n",
    "    return len(wn.synsets(target, pos = pos))\n",
    "\n",
    "def wn_synset_sense_entropy_uniform(target):\n",
    "    num_senses = len(wn.synsets(target))\n",
    "    return -np.sum([((1 / num_senses) * np.log2(1 / num_senses)) \n",
    "                     for index in range(0, num_senses)])\n",
    "\n",
    "def wn_synset_sense_entropy_pos_uniform(target):\n",
    "    num_senses = len(wn.synsets(target))\n",
    "    pos_distribution = [len(wn.synsets(target, pos = wn.NOUN)),\n",
    "                        len(wn.synsets(target, pos = wn.VERB)),\n",
    "                        len(wn.synsets(target, pos = wn.ADJ)),\n",
    "                        len(wn.synsets(target, pos = wn.ADV))]\n",
    "    return -np.sum([(np.nan_to_num((count / num_senses) * np.log2(count / num_senses))) \n",
    "            for count in pos_distribution]) if num_senses != 0 else 0\n",
    "\n",
    "def wn_synsets_sense_entropy_pos_central(target, pos):\n",
    "    num_senses_pos = len(wn.synsets(target, pos = pos))\n",
    "    return -np.sum([((1 / num_senses_pos) * np.log2(1 / num_senses_pos))\n",
    "                     for index in range(0, num_senses_pos)])\n",
    "\n",
    "def wn_synsets_freq_ratio_to_max_agg_min(target, freqs_func, freqs):\n",
    "    lemmas = [lemma.name().split('_') for synset in wn.synsets(target) \n",
    "                  for lemma in synset.lemmas()]\n",
    "    if not lemmas:\n",
    "        return 1\n",
    "    freqis = [np.min([freqs_func(lemma, freqs) for lemma in lemmata]) \n",
    "              for lemmata in lemmas]\n",
    "    target_freq = freqs_func(target, freqs)\n",
    "    max_freq = np.max(freqis)\n",
    "    return target_freq / max_freq\n",
    "\n",
    "def wn_synsets_freq_ratio_to_max_agg_mean(target, freqs_func, freqs):\n",
    "    lemmas = [lemma.name().split('_') for synset in wn.synsets(target) \n",
    "                  for lemma in synset.lemmas()]\n",
    "    if not lemmas:\n",
    "        return 1\n",
    "    freqis = [np.mean([freqs_func(lemma, freqs) for lemma in lemmata]) \n",
    "              for lemmata in lemmas]\n",
    "    target_freq = freqs_func(target, freqs)\n",
    "    max_freq = np.max(freqis)\n",
    "    return target_freq / max_freq\n",
    "\n",
    "def wn_synsets_freq_ratio_to_max_agg_median(target, freqs_func, freqs):\n",
    "    lemmas = [lemma.name().split('_') for synset in wn.synsets(target) \n",
    "                  for lemma in synset.lemmas()]\n",
    "    if not lemmas:\n",
    "        return 1\n",
    "    freqis = [np.median([freqs_func(lemma, freqs) for lemma in lemmata]) \n",
    "              for lemmata in lemmas]\n",
    "    target_freq = freqs_func(target, freqs)\n",
    "    max_freq = np.max(freqis)\n",
    "    return target_freq / max_freq\n",
    "    \n",
    "def swn_avg_objective_score(target):\n",
    "    return np.nan_to_num(np.mean([senti_synset.obj_score() \n",
    "                for senti_synset in swn.senti_synsets(target)]))\n",
    "\n",
    "def pos_tag(sentence, target):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    wordPOSPairs = [token for token in nltk.pos_tag(tokens) if token[0] == target]\n",
    "    return wordPOSPairs[0][1] if len(wordPOSPairs) > 0 else None\n",
    "\n",
    "# TODO consider using stanford lemmatizer and compute word similarity metric\n",
    "# to orignal target\n",
    "def wordnet_lemma_len(target):\n",
    "    return len(wordNetLemmatizer.lemmatize(target))\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    if not tag:\n",
    "        return None\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    "    if tag.startswith('V'):\n",
    "        return 'v'\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    "    if tag.startswith('R'):\n",
    "        return 'r'\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\k_kol\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2889: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\k_kol\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: RuntimeWarning: Mean of empty slice\n",
      "  \n",
      "C:\\Users\\k_kol\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: RuntimeWarning: Mean of empty slice\n"
     ]
    }
   ],
   "source": [
    "df['wn_synset_freq'] = df.target.apply(lambda target : agg_feat_num_average(target, wn_synset_freq))\n",
    "df['wn_synset_avg_lemma_freq'] = df.target.apply(lambda target : agg_feat_num_average(target, wn_synset_avg_lemma_freq))\n",
    "df['wn_synset_avg_lemma_len'] = df.target.apply(lambda target : agg_feat_num_average(target, wn_synset_avg_lemma_len))\n",
    "\n",
    "df['wn_synset_diff_len_avg_lemma_len'] = df.wn_synset_avg_lemma_len - df.length\n",
    "df['wn_synset_avg_hypernyms'] = df.target.apply(lambda target : agg_feat_num_average(target, wn_synset_avg_hypernyms))\n",
    "df['wn_synset_sum_hypernyms'] = df.target.apply(lambda target : agg_feat_num_average(target, wn_synset_sum_hypernyms))\n",
    "df['wn_synset_avg_hyponyms'] = df.target.apply(lambda target : agg_feat_num_average(target, wn_synset_avg_hyponyms))\n",
    "\n",
    "df['wn_synset_avg_definition_len'] = df.target.apply(lambda target : \n",
    "                                                     agg_feat_num_average(target, wn_synset_avg_definition_len))\n",
    "df['wn_synset_avg_hyptree_depth'] = df.target.apply(lambda target :\n",
    "                                                     agg_feat_num_average(target, wn_synset_avg_hyptree_depth))\n",
    "df['wn_synset_num_distinct_pos'] = df.target.apply(lambda target : \n",
    "                                                     agg_feat_num_average(target, wn_synset_num_distinct_pos))\n",
    "df['wn_synset_avg_num_relations'] = df.target.apply(lambda target : \n",
    "                                                     agg_feat_num_average(target, wn_synset_avg_num_relations))\n",
    "\n",
    "df['wn_synset_avg_freq_pos_noun'] = df.target.apply(lambda target : \n",
    "                                                    agg_feat_num_average(target, wn_synset_avg_freq_pos, wn.NOUN))\n",
    "df['wn_synset_avg_freq_pos_verb'] = df.target.apply(lambda target : \n",
    "                                                    agg_feat_num_average(target, wn_synset_avg_freq_pos, wn.VERB))\n",
    "df['wn_synset_avg_freq_pos_adj'] = df.target.apply(lambda target : \n",
    "                                                   agg_feat_num_average(target, wn_synset_avg_freq_pos, wn.ADJ))\n",
    "df['wn_synset_avg_freq_pos_adv'] = df.target.apply(lambda target : \n",
    "                                                   agg_feat_num_average(target, wn_synset_avg_freq_pos, wn.ADV))\n",
    "\n",
    "df['wn_synset_avg_freq_pos_noun_norm'] = np.nan_to_num(df.wn_synset_avg_freq_pos_noun / df.wn_synset_freq)\n",
    "df['wn_synset_avg_freq_pos_verb_norm'] = np.nan_to_num(df.wn_synset_avg_freq_pos_verb / df.wn_synset_freq)\n",
    "df['wn_synset_avg_freq_pos_adj_norm'] = np.nan_to_num(df.wn_synset_avg_freq_pos_adj / df.wn_synset_freq)\n",
    "df['wn_synset_avg_freq_pos_adv_norm'] = np.nan_to_num(df.wn_synset_avg_freq_pos_adv / df.wn_synset_freq)\n",
    "\n",
    "df['pos_tag'] = df[['sentence', 'target']].apply(lambda vals : pos_tag(*vals), axis = 1)\n",
    "df['wn_synset_sense_entropy_uniform'] = df.target.apply(lambda target : \n",
    "                                                        agg_feat_num_average(target, wn_synset_sense_entropy_uniform))\n",
    "df['wn_synset_sense_entropy_pos_uniform'] = df.target.apply(lambda target :\n",
    "                                                        agg_feat_num_average(target, wn_synset_sense_entropy_pos_uniform))\n",
    "df['wn_synsets_sense_entropy_pos_central'] = df[['target', 'pos_tag']].apply(\n",
    "    lambda vals : wn_synsets_sense_entropy_pos_central(vals[0], penn_to_wn(vals[1])), axis = 1)\n",
    "\n",
    "df['swn_avg_objective_score'] = df.target.apply(lambda target : agg_feat_num_average(target, swn_avg_objective_score))\n",
    "\n",
    "df['wordnet_lemma_len'] = df.target.apply(lambda target : agg_feat_num_average(target, wordnet_lemma_len))\n",
    "df['diff_len_wordnet_lemma_len'] = df.length - df.wordnet_lemma_len\n",
    "df['reduction_lemma_len'] = 1 - df.wordnet_lemma_len / df.length\n",
    "\n",
    "df['wn_synsets_freq_ratio_to_max_agg_min'] = df.target.apply(lambda target : \\\n",
    "                                                agg_feat_num_average(target, wn_synsets_freq_ratio_to_max_agg_min, \\\n",
    "                                                                     get_dict_count, word_freq_wiki))\n",
    "df['wn_synsets_freq_ratio_to_max_agg_mean'] = df.target.apply(lambda target : \\\n",
    "                                                agg_feat_num_average(target, wn_synsets_freq_ratio_to_max_agg_mean, \\\n",
    "                                                                     get_dict_count, word_freq_wiki))\n",
    "df['wn_synsets_freq_ratio_to_max_agg_median'] = df.target.apply(lambda target : \\\n",
    "                                                agg_feat_num_average(target, wn_synsets_freq_ratio_to_max_agg_median, \\\n",
    "                                                                     get_dict_count, word_freq_wiki))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>length</th>\n",
       "      <th>wordnet_lemma_len</th>\n",
       "      <th>diff_len_wordnet_lemma_len</th>\n",
       "      <th>reduction_lemma_len</th>\n",
       "      <th>wn_synsets_freq_ratio_to_max_agg_min</th>\n",
       "      <th>wn_synsets_freq_ratio_to_max_agg_mean</th>\n",
       "      <th>wn_synsets_freq_ratio_to_max_agg_median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>passed</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333867</td>\n",
       "      <td>0.004366</td>\n",
       "      <td>0.025479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>land</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347925</td>\n",
       "      <td>0.347925</td>\n",
       "      <td>0.347925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>future</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.512238</td>\n",
       "      <td>0.019643</td>\n",
       "      <td>0.119332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>future generations</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.415089</td>\n",
       "      <td>0.168791</td>\n",
       "      <td>0.218636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>generations</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.317939</td>\n",
       "      <td>0.317939</td>\n",
       "      <td>0.317939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>recognizes</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028839</td>\n",
       "      <td>0.014078</td>\n",
       "      <td>0.014078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>community</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.028216</td>\n",
       "      <td>0.919014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>traditional</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>traditional connection to that country</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.713637</td>\n",
       "      <td>0.713637</td>\n",
       "      <td>0.713637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>country</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.360355</td>\n",
       "      <td>0.360355</td>\n",
       "      <td>0.360355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>connection</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.207832</td>\n",
       "      <td>0.207832</td>\n",
       "      <td>0.207832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Aboriginal</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103653</td>\n",
       "      <td>0.092333</td>\n",
       "      <td>0.092333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>passing</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160432</td>\n",
       "      <td>0.002098</td>\n",
       "      <td>0.012243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Aboriginal land rights legislaton</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>7.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.554222</td>\n",
       "      <td>0.534061</td>\n",
       "      <td>0.534061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>land rights legislaton</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.704412</td>\n",
       "      <td>0.681304</td>\n",
       "      <td>0.681304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>land</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347925</td>\n",
       "      <td>0.347925</td>\n",
       "      <td>0.347925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>legislaton</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rights</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.765310</td>\n",
       "      <td>0.695987</td>\n",
       "      <td>0.695987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>preceded</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058221</td>\n",
       "      <td>0.026997</td>\n",
       "      <td>0.026997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Australia</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.018887</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Aboriginal</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103653</td>\n",
       "      <td>0.092333</td>\n",
       "      <td>0.092333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>number</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>important</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.013422</td>\n",
       "      <td>0.013422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Aboriginal protests</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.464640</td>\n",
       "      <td>0.458980</td>\n",
       "      <td>0.458980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Stockmen</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009217</td>\n",
       "      <td>0.004396</td>\n",
       "      <td>0.004396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>protests</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.825626</td>\n",
       "      <td>0.825626</td>\n",
       "      <td>0.825626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>including</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.890660</td>\n",
       "      <td>0.051250</td>\n",
       "      <td>0.051250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Aboriginal</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103653</td>\n",
       "      <td>0.092333</td>\n",
       "      <td>0.092333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Strike</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.169803</td>\n",
       "      <td>0.004209</td>\n",
       "      <td>0.004209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Yolngu</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5520</th>\n",
       "      <td>develops</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071359</td>\n",
       "      <td>0.023946</td>\n",
       "      <td>0.023946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5521</th>\n",
       "      <td>yellow</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5522</th>\n",
       "      <td>tail horn</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.515770</td>\n",
       "      <td>0.061322</td>\n",
       "      <td>0.061322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5523</th>\n",
       "      <td>tail</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031540</td>\n",
       "      <td>0.020577</td>\n",
       "      <td>0.020577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5524</th>\n",
       "      <td>horn</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.102067</td>\n",
       "      <td>0.102067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5525</th>\n",
       "      <td>turns</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.065743</td>\n",
       "      <td>0.003625</td>\n",
       "      <td>0.010240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5526</th>\n",
       "      <td>black</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5527</th>\n",
       "      <td>yellow</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5528</th>\n",
       "      <td>larva</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5529</th>\n",
       "      <td>pupates</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.293443</td>\n",
       "      <td>0.293443</td>\n",
       "      <td>0.293443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5530</th>\n",
       "      <td>grows</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.127308</td>\n",
       "      <td>0.042720</td>\n",
       "      <td>0.042720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5531</th>\n",
       "      <td>mm</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5532</th>\n",
       "      <td>underground</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.689584</td>\n",
       "      <td>0.181725</td>\n",
       "      <td>0.181725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5533</th>\n",
       "      <td>underground chamber</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.844792</td>\n",
       "      <td>0.468761</td>\n",
       "      <td>0.468761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5534</th>\n",
       "      <td>chamber</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.755797</td>\n",
       "      <td>0.755797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5535</th>\n",
       "      <td>terminology</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035795</td>\n",
       "      <td>0.035795</td>\n",
       "      <td>0.035795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5536</th>\n",
       "      <td>actor</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.364411</td>\n",
       "      <td>0.364411</td>\n",
       "      <td>0.364411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5537</th>\n",
       "      <td>actress</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5538</th>\n",
       "      <td>female</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5539</th>\n",
       "      <td>dramatic</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5540</th>\n",
       "      <td>person</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.976458</td>\n",
       "      <td>0.976458</td>\n",
       "      <td>0.976458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5541</th>\n",
       "      <td>acts</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.100407</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.003717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5542</th>\n",
       "      <td>comic</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5543</th>\n",
       "      <td>production</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5544</th>\n",
       "      <td>capacity</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5545</th>\n",
       "      <td>works</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.431929</td>\n",
       "      <td>0.023464</td>\n",
       "      <td>0.062571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5546</th>\n",
       "      <td>film</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5547</th>\n",
       "      <td>television</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.763270</td>\n",
       "      <td>0.763270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5548</th>\n",
       "      <td>theater</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.123264</td>\n",
       "      <td>0.005613</td>\n",
       "      <td>0.123264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5549</th>\n",
       "      <td>radio</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.826068</td>\n",
       "      <td>0.826068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5550 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      target     length  wordnet_lemma_len  \\\n",
       "0                                     passed   6.000000           6.000000   \n",
       "1                                       land   4.000000           4.000000   \n",
       "2                                     future   6.000000           6.000000   \n",
       "3                         future generations   8.500000           8.000000   \n",
       "4                                generations  11.000000          10.000000   \n",
       "5                                 recognizes  10.000000          10.000000   \n",
       "6                                  community   9.000000           9.000000   \n",
       "7                                traditional  11.000000          11.000000   \n",
       "8     traditional connection to that country   6.800000           6.800000   \n",
       "9                                    country   7.000000           7.000000   \n",
       "10                                connection  10.000000          10.000000   \n",
       "11                                Aboriginal  10.000000          10.000000   \n",
       "12                                   passing   7.000000           7.000000   \n",
       "13         Aboriginal land rights legislaton   7.500000           7.250000   \n",
       "14                    land rights legislaton   6.666667           6.333333   \n",
       "15                                      land   4.000000           4.000000   \n",
       "16                                legislaton  10.000000          10.000000   \n",
       "17                                    rights   6.000000           5.000000   \n",
       "18                                  preceded   8.000000           8.000000   \n",
       "19                                 Australia   9.000000           9.000000   \n",
       "20                                Aboriginal  10.000000          10.000000   \n",
       "21                                    number   6.000000           6.000000   \n",
       "22                                 important   9.000000           9.000000   \n",
       "23                       Aboriginal protests   9.000000           8.500000   \n",
       "24                                  Stockmen   8.000000           8.000000   \n",
       "25                                  protests   8.000000           7.000000   \n",
       "26                                 including   9.000000           9.000000   \n",
       "27                                Aboriginal  10.000000          10.000000   \n",
       "28                                    Strike   6.000000           6.000000   \n",
       "29                                    Yolngu   6.000000           6.000000   \n",
       "...                                      ...        ...                ...   \n",
       "5520                                develops   8.000000           8.000000   \n",
       "5521                                  yellow   6.000000           6.000000   \n",
       "5522                               tail horn   4.000000           4.000000   \n",
       "5523                                    tail   4.000000           4.000000   \n",
       "5524                                    horn   4.000000           4.000000   \n",
       "5525                                   turns   5.000000           4.000000   \n",
       "5526                                   black   5.000000           5.000000   \n",
       "5527                                  yellow   6.000000           6.000000   \n",
       "5528                                   larva   5.000000           5.000000   \n",
       "5529                                 pupates   7.000000           7.000000   \n",
       "5530                                   grows   5.000000           5.000000   \n",
       "5531                                      mm   2.000000           2.000000   \n",
       "5532                             underground  11.000000          11.000000   \n",
       "5533                     underground chamber   9.000000           9.000000   \n",
       "5534                                 chamber   7.000000           7.000000   \n",
       "5535                             terminology  11.000000          11.000000   \n",
       "5536                                   actor   5.000000           5.000000   \n",
       "5537                                 actress   7.000000           7.000000   \n",
       "5538                                  female   6.000000           6.000000   \n",
       "5539                                dramatic   8.000000           8.000000   \n",
       "5540                                  person   6.000000           6.000000   \n",
       "5541                                    acts   4.000000           3.000000   \n",
       "5542                                   comic   5.000000           5.000000   \n",
       "5543                              production  10.000000          10.000000   \n",
       "5544                                capacity   8.000000           8.000000   \n",
       "5545                                   works   5.000000           4.000000   \n",
       "5546                                    film   4.000000           4.000000   \n",
       "5547                              television  10.000000          10.000000   \n",
       "5548                                 theater   7.000000           7.000000   \n",
       "5549                                   radio   5.000000           5.000000   \n",
       "\n",
       "      diff_len_wordnet_lemma_len  reduction_lemma_len  \\\n",
       "0                       0.000000             0.000000   \n",
       "1                       0.000000             0.000000   \n",
       "2                       0.000000             0.000000   \n",
       "3                       0.500000             0.058824   \n",
       "4                       1.000000             0.090909   \n",
       "5                       0.000000             0.000000   \n",
       "6                       0.000000             0.000000   \n",
       "7                       0.000000             0.000000   \n",
       "8                       0.000000             0.000000   \n",
       "9                       0.000000             0.000000   \n",
       "10                      0.000000             0.000000   \n",
       "11                      0.000000             0.000000   \n",
       "12                      0.000000             0.000000   \n",
       "13                      0.250000             0.033333   \n",
       "14                      0.333333             0.050000   \n",
       "15                      0.000000             0.000000   \n",
       "16                      0.000000             0.000000   \n",
       "17                      1.000000             0.166667   \n",
       "18                      0.000000             0.000000   \n",
       "19                      0.000000             0.000000   \n",
       "20                      0.000000             0.000000   \n",
       "21                      0.000000             0.000000   \n",
       "22                      0.000000             0.000000   \n",
       "23                      0.500000             0.055556   \n",
       "24                      0.000000             0.000000   \n",
       "25                      1.000000             0.125000   \n",
       "26                      0.000000             0.000000   \n",
       "27                      0.000000             0.000000   \n",
       "28                      0.000000             0.000000   \n",
       "29                      0.000000             0.000000   \n",
       "...                          ...                  ...   \n",
       "5520                    0.000000             0.000000   \n",
       "5521                    0.000000             0.000000   \n",
       "5522                    0.000000             0.000000   \n",
       "5523                    0.000000             0.000000   \n",
       "5524                    0.000000             0.000000   \n",
       "5525                    1.000000             0.200000   \n",
       "5526                    0.000000             0.000000   \n",
       "5527                    0.000000             0.000000   \n",
       "5528                    0.000000             0.000000   \n",
       "5529                    0.000000             0.000000   \n",
       "5530                    0.000000             0.000000   \n",
       "5531                    0.000000             0.000000   \n",
       "5532                    0.000000             0.000000   \n",
       "5533                    0.000000             0.000000   \n",
       "5534                    0.000000             0.000000   \n",
       "5535                    0.000000             0.000000   \n",
       "5536                    0.000000             0.000000   \n",
       "5537                    0.000000             0.000000   \n",
       "5538                    0.000000             0.000000   \n",
       "5539                    0.000000             0.000000   \n",
       "5540                    0.000000             0.000000   \n",
       "5541                    1.000000             0.250000   \n",
       "5542                    0.000000             0.000000   \n",
       "5543                    0.000000             0.000000   \n",
       "5544                    0.000000             0.000000   \n",
       "5545                    1.000000             0.200000   \n",
       "5546                    0.000000             0.000000   \n",
       "5547                    0.000000             0.000000   \n",
       "5548                    0.000000             0.000000   \n",
       "5549                    0.000000             0.000000   \n",
       "\n",
       "      wn_synsets_freq_ratio_to_max_agg_min  \\\n",
       "0                                 0.333867   \n",
       "1                                 0.347925   \n",
       "2                                 0.512238   \n",
       "3                                 0.415089   \n",
       "4                                 0.317939   \n",
       "5                                 0.028839   \n",
       "6                                 1.000000   \n",
       "7                                 1.000000   \n",
       "8                                 0.713637   \n",
       "9                                 0.360355   \n",
       "10                                0.207832   \n",
       "11                                0.103653   \n",
       "12                                0.160432   \n",
       "13                                0.554222   \n",
       "14                                0.704412   \n",
       "15                                0.347925   \n",
       "16                                1.000000   \n",
       "17                                0.765310   \n",
       "18                                0.058221   \n",
       "19                                1.000000   \n",
       "20                                0.103653   \n",
       "21                                1.000000   \n",
       "22                                1.000000   \n",
       "23                                0.464640   \n",
       "24                                0.009217   \n",
       "25                                0.825626   \n",
       "26                                1.890660   \n",
       "27                                0.103653   \n",
       "28                                0.169803   \n",
       "29                                1.000000   \n",
       "...                                    ...   \n",
       "5520                              0.071359   \n",
       "5521                              1.000000   \n",
       "5522                              0.515770   \n",
       "5523                              0.031540   \n",
       "5524                              1.000000   \n",
       "5525                              0.065743   \n",
       "5526                              1.000000   \n",
       "5527                              1.000000   \n",
       "5528                              1.000000   \n",
       "5529                              0.293443   \n",
       "5530                              0.127308   \n",
       "5531                              1.000000   \n",
       "5532                              0.689584   \n",
       "5533                              0.844792   \n",
       "5534                              1.000000   \n",
       "5535                              0.035795   \n",
       "5536                              0.364411   \n",
       "5537                              1.000000   \n",
       "5538                              1.000000   \n",
       "5539                              1.000000   \n",
       "5540                              0.976458   \n",
       "5541                              0.100407   \n",
       "5542                              1.000000   \n",
       "5543                              1.000000   \n",
       "5544                              1.000000   \n",
       "5545                              0.431929   \n",
       "5546                              1.000000   \n",
       "5547                              1.000000   \n",
       "5548                              0.123264   \n",
       "5549                              1.000000   \n",
       "\n",
       "      wn_synsets_freq_ratio_to_max_agg_mean  \\\n",
       "0                                  0.004366   \n",
       "1                                  0.347925   \n",
       "2                                  0.019643   \n",
       "3                                  0.168791   \n",
       "4                                  0.317939   \n",
       "5                                  0.014078   \n",
       "6                                  0.028216   \n",
       "7                                  1.000000   \n",
       "8                                  0.713637   \n",
       "9                                  0.360355   \n",
       "10                                 0.207832   \n",
       "11                                 0.092333   \n",
       "12                                 0.002098   \n",
       "13                                 0.534061   \n",
       "14                                 0.681304   \n",
       "15                                 0.347925   \n",
       "16                                 1.000000   \n",
       "17                                 0.695987   \n",
       "18                                 0.026997   \n",
       "19                                 0.018887   \n",
       "20                                 0.092333   \n",
       "21                                 1.000000   \n",
       "22                                 0.013422   \n",
       "23                                 0.458980   \n",
       "24                                 0.004396   \n",
       "25                                 0.825626   \n",
       "26                                 0.051250   \n",
       "27                                 0.092333   \n",
       "28                                 0.004209   \n",
       "29                                 1.000000   \n",
       "...                                     ...   \n",
       "5520                               0.023946   \n",
       "5521                               1.000000   \n",
       "5522                               0.061322   \n",
       "5523                               0.020577   \n",
       "5524                               0.102067   \n",
       "5525                               0.003625   \n",
       "5526                               1.000000   \n",
       "5527                               1.000000   \n",
       "5528                               1.000000   \n",
       "5529                               0.293443   \n",
       "5530                               0.042720   \n",
       "5531                               1.000000   \n",
       "5532                               0.181725   \n",
       "5533                               0.468761   \n",
       "5534                               0.755797   \n",
       "5535                               0.035795   \n",
       "5536                               0.364411   \n",
       "5537                               1.000000   \n",
       "5538                               1.000000   \n",
       "5539                               1.000000   \n",
       "5540                               0.976458   \n",
       "5541                               0.002400   \n",
       "5542                               1.000000   \n",
       "5543                               1.000000   \n",
       "5544                               1.000000   \n",
       "5545                               0.023464   \n",
       "5546                               1.000000   \n",
       "5547                               0.763270   \n",
       "5548                               0.005613   \n",
       "5549                               0.826068   \n",
       "\n",
       "      wn_synsets_freq_ratio_to_max_agg_median  \n",
       "0                                    0.025479  \n",
       "1                                    0.347925  \n",
       "2                                    0.119332  \n",
       "3                                    0.218636  \n",
       "4                                    0.317939  \n",
       "5                                    0.014078  \n",
       "6                                    0.919014  \n",
       "7                                    1.000000  \n",
       "8                                    0.713637  \n",
       "9                                    0.360355  \n",
       "10                                   0.207832  \n",
       "11                                   0.092333  \n",
       "12                                   0.012243  \n",
       "13                                   0.534061  \n",
       "14                                   0.681304  \n",
       "15                                   0.347925  \n",
       "16                                   1.000000  \n",
       "17                                   0.695987  \n",
       "18                                   0.026997  \n",
       "19                                   1.000000  \n",
       "20                                   0.092333  \n",
       "21                                   1.000000  \n",
       "22                                   0.013422  \n",
       "23                                   0.458980  \n",
       "24                                   0.004396  \n",
       "25                                   0.825626  \n",
       "26                                   0.051250  \n",
       "27                                   0.092333  \n",
       "28                                   0.004209  \n",
       "29                                   1.000000  \n",
       "...                                       ...  \n",
       "5520                                 0.023946  \n",
       "5521                                 1.000000  \n",
       "5522                                 0.061322  \n",
       "5523                                 0.020577  \n",
       "5524                                 0.102067  \n",
       "5525                                 0.010240  \n",
       "5526                                 1.000000  \n",
       "5527                                 1.000000  \n",
       "5528                                 1.000000  \n",
       "5529                                 0.293443  \n",
       "5530                                 0.042720  \n",
       "5531                                 1.000000  \n",
       "5532                                 0.181725  \n",
       "5533                                 0.468761  \n",
       "5534                                 0.755797  \n",
       "5535                                 0.035795  \n",
       "5536                                 0.364411  \n",
       "5537                                 1.000000  \n",
       "5538                                 1.000000  \n",
       "5539                                 1.000000  \n",
       "5540                                 0.976458  \n",
       "5541                                 0.003717  \n",
       "5542                                 1.000000  \n",
       "5543                                 1.000000  \n",
       "5544                                 1.000000  \n",
       "5545                                 0.062571  \n",
       "5546                                 1.000000  \n",
       "5547                                 0.763270  \n",
       "5548                                 0.123264  \n",
       "5549                                 0.826068  \n",
       "\n",
       "[5550 rows x 8 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:,['target', 'length', 'wordnet_lemma_len', 'diff_len_wordnet_lemma_len', 'reduction_lemma_len', \\\n",
    "          'wn_synsets_freq_ratio_to_max_agg_min', 'wn_synsets_freq_ratio_to_max_agg_mean', 'wn_synsets_freq_ratio_to_max_agg_median']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PorterStemmer, StanfordNLP and Dependency Tree Features\n",
    "Here we implement features based on the PorterStemmer library from nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "from nltk.stem.wordnet import *\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "\n",
    "java_path = \"C:/Program Files (x86)/Java/jdk1.8.0_144/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "path_to_jar = 'resources/stanford-dependency-parser/stanford-parser.jar'\n",
    "path_to_models_jar = 'resources/stanford-dependency-parser/stanford-parser-3.9.1-models.jar'\n",
    "\n",
    "porterStemmer = PorterStemmer()\n",
    "wordNetLemmatizer = WordNetLemmatizer()\n",
    "nerTagger = StanfordNERTagger('resources/stanford-ner-tagger/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "               'resources/stanford-ner-tagger/stanford-ner.jar',\n",
    "               encoding='utf-8')\n",
    "dependencyParser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "\n",
    "def porter_stem_len(target):\n",
    "    return len(str(porterStemmer.stem(target)))\n",
    "\n",
    "def porter_stemmer_num_steps(target):\n",
    "    stem = target.lower()\n",
    "    applied_steps = 0\n",
    "    if porterStemmer.mode == porterStemmer.NLTK_EXTENSIONS and target in porterStemmer.pool:\n",
    "            return applied_steps\n",
    "    if porterStemmer.mode != porterStemmer.ORIGINAL_ALGORITHM and len(target) <= 2:\n",
    "            return applied_steps\n",
    "    step_funcs = [porterStemmer._step1a, porterStemmer._step1b, porterStemmer._step1c,\n",
    "                  porterStemmer._step2, porterStemmer._step3, porterStemmer._step3,\n",
    "                  porterStemmer._step4, porterStemmer._step5a, porterStemmer._step5b]\n",
    "    for step_func in step_funcs:\n",
    "        stem_step = step_func(stem)\n",
    "        if stem_step != stem:\n",
    "            stem = stem_step\n",
    "            applied_steps += 1\n",
    "    return applied_steps\n",
    "\n",
    "def is_named_entity(sentence, target):\n",
    "    tokenized_sent = word_tokenize(sentence)\n",
    "    tagged_sent = nerTagger.tag(tokenized_sent)\n",
    "    for token, tag in tagged_sent:\n",
    "        if token == target and tag != 'O':\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def named_entity_type(sentence, target):\n",
    "    tokenized_sent = word_tokenize(sentence)\n",
    "    tagged_sent = nerTagger.tag(tokenized_sent)\n",
    "    return [tag for token, tag in tagged_sent if token == target][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porter stemmer stem length, number of applied steps,\n",
    "# difference of stem length to target and reduction ratio\n",
    "df['porter_stem_len'] = df.target.apply(lambda target : agg_feat_num_average(target, porter_stem_len))\n",
    "df['porter_stemmer_num_steps'] = df.target.apply(lambda target : agg_feat_num_average(target, porter_stemmer_num_steps))\n",
    "df['diff_len_stem_len'] = df.length - df.porter_stem_len\n",
    "df['reduction_stem_len'] = 1 - df.porter_stem_len / df.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>length</th>\n",
       "      <th>num_syllables</th>\n",
       "      <th>num_vowels</th>\n",
       "      <th>porter_stemmer_num_steps</th>\n",
       "      <th>diff_len_stem_len</th>\n",
       "      <th>reduction_stem_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>passed</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>land</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>future</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>future generations</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.411765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>generations</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>recognizes</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>community</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>traditional</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.454545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>traditional connection to that country</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.80</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>country</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>connection</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Aboriginal</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>passing</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Aboriginal land rights legislaton</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>land rights legislaton</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>land</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>legislaton</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rights</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>preceded</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Australia</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Aboriginal</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>number</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>important</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Aboriginal protests</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.50</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Stockmen</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>protests</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>including</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Aboriginal</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Strike</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Yolngu</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5520</th>\n",
       "      <td>develops</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5521</th>\n",
       "      <td>yellow</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5522</th>\n",
       "      <td>tail horn</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5523</th>\n",
       "      <td>tail</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5524</th>\n",
       "      <td>horn</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5525</th>\n",
       "      <td>turns</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5526</th>\n",
       "      <td>black</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5527</th>\n",
       "      <td>yellow</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5528</th>\n",
       "      <td>larva</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5529</th>\n",
       "      <td>pupates</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5530</th>\n",
       "      <td>grows</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5531</th>\n",
       "      <td>mm</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5532</th>\n",
       "      <td>underground</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5533</th>\n",
       "      <td>underground chamber</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5534</th>\n",
       "      <td>chamber</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5535</th>\n",
       "      <td>terminology</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5536</th>\n",
       "      <td>actor</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5537</th>\n",
       "      <td>actress</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5538</th>\n",
       "      <td>female</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5539</th>\n",
       "      <td>dramatic</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5540</th>\n",
       "      <td>person</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5541</th>\n",
       "      <td>acts</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5542</th>\n",
       "      <td>comic</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5543</th>\n",
       "      <td>production</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5544</th>\n",
       "      <td>capacity</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5545</th>\n",
       "      <td>works</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5546</th>\n",
       "      <td>film</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5547</th>\n",
       "      <td>television</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5548</th>\n",
       "      <td>theater</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5549</th>\n",
       "      <td>radio</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5550 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      target     length  num_syllables  \\\n",
       "0                                     passed   6.000000            1.0   \n",
       "1                                       land   4.000000            1.0   \n",
       "2                                     future   6.000000            2.0   \n",
       "3                         future generations   8.500000            3.0   \n",
       "4                                generations  11.000000            4.0   \n",
       "5                                 recognizes  10.000000            4.0   \n",
       "6                                  community   9.000000            4.0   \n",
       "7                                traditional  11.000000            4.0   \n",
       "8     traditional connection to that country   6.800000            2.2   \n",
       "9                                    country   7.000000            2.0   \n",
       "10                                connection  10.000000            3.0   \n",
       "11                                Aboriginal  10.000000            4.0   \n",
       "12                                   passing   7.000000            2.0   \n",
       "13         Aboriginal land rights legislaton   7.500000            2.5   \n",
       "14                    land rights legislaton   6.666667            2.0   \n",
       "15                                      land   4.000000            1.0   \n",
       "16                                legislaton  10.000000            4.0   \n",
       "17                                    rights   6.000000            1.0   \n",
       "18                                  preceded   8.000000            3.0   \n",
       "19                                 Australia   9.000000            3.0   \n",
       "20                                Aboriginal  10.000000            4.0   \n",
       "21                                    number   6.000000            2.0   \n",
       "22                                 important   9.000000            3.0   \n",
       "23                       Aboriginal protests   9.000000            3.0   \n",
       "24                                  Stockmen   8.000000            2.0   \n",
       "25                                  protests   8.000000            2.0   \n",
       "26                                 including   9.000000            3.0   \n",
       "27                                Aboriginal  10.000000            4.0   \n",
       "28                                    Strike   6.000000            1.0   \n",
       "29                                    Yolngu   6.000000            2.0   \n",
       "...                                      ...        ...            ...   \n",
       "5520                                develops   8.000000            3.0   \n",
       "5521                                  yellow   6.000000            2.0   \n",
       "5522                               tail horn   4.000000            1.0   \n",
       "5523                                    tail   4.000000            1.0   \n",
       "5524                                    horn   4.000000            1.0   \n",
       "5525                                   turns   5.000000            1.0   \n",
       "5526                                   black   5.000000            1.0   \n",
       "5527                                  yellow   6.000000            2.0   \n",
       "5528                                   larva   5.000000            2.0   \n",
       "5529                                 pupates   7.000000            2.0   \n",
       "5530                                   grows   5.000000            1.0   \n",
       "5531                                      mm   2.000000            0.0   \n",
       "5532                             underground  11.000000            3.0   \n",
       "5533                     underground chamber   9.000000            2.5   \n",
       "5534                                 chamber   7.000000            2.0   \n",
       "5535                             terminology  11.000000            5.0   \n",
       "5536                                   actor   5.000000            2.0   \n",
       "5537                                 actress   7.000000            2.0   \n",
       "5538                                  female   6.000000            2.0   \n",
       "5539                                dramatic   8.000000            3.0   \n",
       "5540                                  person   6.000000            2.0   \n",
       "5541                                    acts   4.000000            1.0   \n",
       "5542                                   comic   5.000000            2.0   \n",
       "5543                              production  10.000000            3.0   \n",
       "5544                                capacity   8.000000            4.0   \n",
       "5545                                   works   5.000000            1.0   \n",
       "5546                                    film   4.000000            1.0   \n",
       "5547                              television  10.000000            4.0   \n",
       "5548                                 theater   7.000000            3.0   \n",
       "5549                                   radio   5.000000            3.0   \n",
       "\n",
       "      num_vowels  porter_stemmer_num_steps  diff_len_stem_len  \\\n",
       "0           2.00                  1.000000           2.000000   \n",
       "1           1.00                  0.000000           0.000000   \n",
       "2           3.00                  1.000000           1.000000   \n",
       "3           4.00                  2.000000           3.500000   \n",
       "4           5.00                  3.000000           6.000000   \n",
       "5           4.00                  2.000000           4.000000   \n",
       "6           4.00                  2.000000           3.000000   \n",
       "7           5.00                  2.000000           5.000000   \n",
       "8           2.80                  0.800000           1.600000   \n",
       "9           3.00                  1.000000           0.000000   \n",
       "10          4.00                  1.000000           3.000000   \n",
       "11          5.00                  1.000000           2.000000   \n",
       "12          2.00                  1.000000           3.000000   \n",
       "13          2.75                  0.500000           0.750000   \n",
       "14          2.00                  0.333333           0.333333   \n",
       "15          1.00                  0.000000           0.000000   \n",
       "16          4.00                  0.000000           0.000000   \n",
       "17          1.00                  1.000000           1.000000   \n",
       "18          3.00                  1.000000           2.000000   \n",
       "19          5.00                  0.000000           0.000000   \n",
       "20          5.00                  1.000000           2.000000   \n",
       "21          2.00                  0.000000           0.000000   \n",
       "22          3.00                  1.000000           3.000000   \n",
       "23          3.50                  1.000000           1.500000   \n",
       "24          2.00                  0.000000           0.000000   \n",
       "25          2.00                  1.000000           1.000000   \n",
       "26          3.00                  1.000000           3.000000   \n",
       "27          5.00                  1.000000           2.000000   \n",
       "28          2.00                  0.000000           0.000000   \n",
       "29          3.00                  0.000000           0.000000   \n",
       "...          ...                       ...                ...   \n",
       "5520        3.00                  1.000000           1.000000   \n",
       "5521        3.00                  0.000000           0.000000   \n",
       "5522        1.50                  0.000000           0.000000   \n",
       "5523        2.00                  0.000000           0.000000   \n",
       "5524        1.00                  0.000000           0.000000   \n",
       "5525        1.00                  1.000000           1.000000   \n",
       "5526        1.00                  0.000000           0.000000   \n",
       "5527        3.00                  0.000000           0.000000   \n",
       "5528        2.00                  0.000000           0.000000   \n",
       "5529        3.00                  2.000000           2.000000   \n",
       "5530        1.00                  1.000000           1.000000   \n",
       "5531        0.00                  0.000000           0.000000   \n",
       "5532        4.00                  0.000000           0.000000   \n",
       "5533        3.00                  0.000000           0.000000   \n",
       "5534        2.00                  0.000000           0.000000   \n",
       "5535        5.00                  2.000000           1.000000   \n",
       "5536        2.00                  0.000000           0.000000   \n",
       "5537        2.00                  0.000000           0.000000   \n",
       "5538        3.00                  1.000000           1.000000   \n",
       "5539        3.00                  1.000000           2.000000   \n",
       "5540        2.00                  0.000000           0.000000   \n",
       "5541        1.00                  1.000000           1.000000   \n",
       "5542        2.00                  0.000000           0.000000   \n",
       "5543        4.00                  1.000000           3.000000   \n",
       "5544        4.00                  2.000000           3.000000   \n",
       "5545        1.00                  1.000000           1.000000   \n",
       "5546        1.00                  0.000000           0.000000   \n",
       "5547        5.00                  1.000000           3.000000   \n",
       "5548        3.00                  0.000000           0.000000   \n",
       "5549        3.00                  0.000000           0.000000   \n",
       "\n",
       "      reduction_stem_len  \n",
       "0               0.333333  \n",
       "1               0.000000  \n",
       "2               0.166667  \n",
       "3               0.411765  \n",
       "4               0.545455  \n",
       "5               0.400000  \n",
       "6               0.333333  \n",
       "7               0.454545  \n",
       "8               0.235294  \n",
       "9               0.000000  \n",
       "10              0.300000  \n",
       "11              0.200000  \n",
       "12              0.428571  \n",
       "13              0.100000  \n",
       "14              0.050000  \n",
       "15              0.000000  \n",
       "16              0.000000  \n",
       "17              0.166667  \n",
       "18              0.250000  \n",
       "19              0.000000  \n",
       "20              0.200000  \n",
       "21              0.000000  \n",
       "22              0.333333  \n",
       "23              0.166667  \n",
       "24              0.000000  \n",
       "25              0.125000  \n",
       "26              0.333333  \n",
       "27              0.200000  \n",
       "28              0.000000  \n",
       "29              0.000000  \n",
       "...                  ...  \n",
       "5520            0.125000  \n",
       "5521            0.000000  \n",
       "5522            0.000000  \n",
       "5523            0.000000  \n",
       "5524            0.000000  \n",
       "5525            0.200000  \n",
       "5526            0.000000  \n",
       "5527            0.000000  \n",
       "5528            0.000000  \n",
       "5529            0.285714  \n",
       "5530            0.200000  \n",
       "5531            0.000000  \n",
       "5532            0.000000  \n",
       "5533            0.000000  \n",
       "5534            0.000000  \n",
       "5535            0.090909  \n",
       "5536            0.000000  \n",
       "5537            0.000000  \n",
       "5538            0.166667  \n",
       "5539            0.250000  \n",
       "5540            0.000000  \n",
       "5541            0.250000  \n",
       "5542            0.000000  \n",
       "5543            0.300000  \n",
       "5544            0.375000  \n",
       "5545            0.200000  \n",
       "5546            0.000000  \n",
       "5547            0.300000  \n",
       "5548            0.000000  \n",
       "5549            0.000000  \n",
       "\n",
       "[5550 rows x 7 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, ['target', 'length', 'num_syllables', 'num_vowels', 'porter_stemmer_num_steps', 'diff_len_stem_len', 'reduction_stem_len']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textatistic\n",
    "\n",
    "df['dict_dale_chall'] = df.target.apply(lambda target : agg_feat_num_average(target, textatistic.notdalechall_count))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context-Aware Features\n",
    "Here we compute not only the context extraction/definition in the first place but also the corresponding context features afterwards. Also we need to implement proper strategies to cope with the target occuring multiple times in the sentence. To avoid mistakes, we should use the actual start and end tags from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Context-Token Aggregation\n",
    "First we define how feature values of multiple context-tokens should be aggreagated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def agg_ctx_feat_num_average(tokens, func_feature, *args):\n",
    "    return np.mean([func_feature(token, *args) for token in tokens])\n",
    "\n",
    "def agg_ctx_feat_num_median(tokens, func_feature, *args):\n",
    "    return np.median([func_feature(token, *args) for token in tokens])\n",
    "\n",
    "def agg_ctx_feat_num_max(tokens, func_feature, *args):\n",
    "    return np.max([func_feature(token, *args) for token in tokens])\n",
    "\n",
    "def agg_ctx_feat_num_min(tokens, func_feature, *args):\n",
    "    return np.min([func_feature(token, *args) for token in tokens])\n",
    "\n",
    "def agg_ctx_feat_num_sum(tokens, func_feature, *args):\n",
    "    return np.sum([func_feature(token, *args) for token in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Context Definition\n",
    "Here we compute different kinds of context definitions. For example, as a baseline we extract all tokens from the sentence except the target. A second approach is to use a n preceeding or n succeding tokens, or a combined window apporach were we extract n tokens preceeding and succeding of the target. A more sophisticated apporach involves dependency parsing of the sentence and applying different extraction heuristics. Finally we also implement a context extraction approach exploting FrameNet semantic parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def mult_target(sentence, target):\n",
    "    counter = Counter(word_tokenize(sentence))\n",
    "    targets = word_tokenize(target)\n",
    "    return np.sum([counter[target] for target in targets]) / len(targets)\n",
    "\n",
    "df['mult_target'] = df[['sentence', 'target']].apply(lambda vals : mult_target(*vals), axis = 1)\n",
    "df[df.mult_target == 4].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "from nltk.parse.stanford import StanfordNeuralDependencyParser\n",
    "import os\n",
    "\n",
    "java_path = \"C:/Program Files (x86)/Java/jdk1.8.0_144/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "path_to_jar = 'resources/stanford-dependency-parser/stanford-parser.jar'\n",
    "path_to_models_jar = 'resources/stanford-dependency-parser/stanford-parser-3.9.1-models.jar'\n",
    "\n",
    "dependencyParser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "\n",
    "def post_process_ctx(context):\n",
    "    return [token for token in context if token.isalnum()]\n",
    "\n",
    "def preprocess_target(target):\n",
    "    return target.strip()\n",
    "\n",
    "def target_index_char_based(start, end, ctx_tokens):\n",
    "    size = np.sum([len(token) for token in ctx_tokens]) + len(ctx_tokens)\n",
    "    target_pos = (start + end) / 2\n",
    "    target_pos_rel = target_pos / size\n",
    "    return int(target_pos_rel * len(post_process_ctx(ctx_tokens)))\n",
    "\n",
    "def targets_with_index(start, end, context):\n",
    "    curr_pos = 0\n",
    "    targets = []\n",
    "    for index, token in enumerate(word_tokenize(context), 1):\n",
    "        targets.append((token, index, curr_pos, (curr_pos + len(token))))\n",
    "        curr_pos += len(token) + 1\n",
    "    print(targets)\n",
    "    return [(target[0], target[1]) for target in targets \\\n",
    "            if target[2] >= start and target[3] <= end]\n",
    "\n",
    "def dependency_parse(sentence):\n",
    "    dependency_parser = dependencyParser.raw_parse(sentence)\n",
    "    dependencies = []\n",
    "    parsetree = list(dependency_parser)[0]\n",
    "    for index, node in parsetree.nodes.items():\n",
    "        for relation, dependant in parsetree.nodes[index]['deps'].items():\n",
    "            triple = ((node['word'], index), relation, \\\n",
    "                      (parsetree.nodes[dependant[0]]['word'], dependant[0]))\n",
    "            if relation != 'root': dependencies.append(triple)\n",
    "    return dependencies\n",
    "\n",
    "def ctx_extraction_all(context, target):\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    if target in ctx_tokens:\n",
    "        ctx_tokens.remove(target)\n",
    "    return ctx_tokens\n",
    "\n",
    "def ctx_extraction_all_filtered(context, target):\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    post_ctx_tokens = post_process_ctx(ctx_tokens)\n",
    "    if target in ctx_tokens:\n",
    "        ctx_tokens.remove(target)\n",
    "    return post_process_ctx\n",
    "\n",
    "def ctx_extraction_window_pre_n(context, target, start, end, n = 3):\n",
    "    target = preprocess_target(target)\n",
    "    ctx_tokens = word_tokenize(context[:start])\n",
    "    post_ctx_tokens = post_process_ctx(ctx_tokens)\n",
    "    return post_ctx_tokens[-n:]\n",
    "\n",
    "def ctx_extraction_window_suc_n(context, target, start, end, n = 3):\n",
    "    target = preprocess_target(target)\n",
    "    ctx_tokens = word_tokenize(context[end:])\n",
    "    post_ctx_tokens = post_process_ctx(ctx_tokens)\n",
    "    return post_ctx_tokens[:n]\n",
    "\n",
    "def ctx_extraction_window_pre_suc_n(context, target, start, end, n = 3):\n",
    "    ctx_tokens_pre = ctx_extraction_window_pre_n(context, target, start, end, n)\n",
    "    ctx_tokens_suc = ctx_extraction_window_suc_n(context, target, start, end, n)\n",
    "    ctx_tokens_pre.extend(ctx_tokens_suc)\n",
    "    return ctx_tokens_pre\n",
    "\n",
    "def ctx_extraction_dep_in(context, target, start, end):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    return list(set([triple[0][0] for triple in triples if triple[2] in targets]))\n",
    "\n",
    "def ctx_extraction_dep_out(context, target, start, end):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    return list(set([triple[2][0] for triple in triples if triple[0] in targets]))\n",
    "\n",
    "def ctx_extraction_dep_in_out(context, target, start, end):\n",
    "    ctx_tokens_in = ctx_extraction_dep_in(context, target, start, end)\n",
    "    ctx_tokens_out = ctx_extraction_dep_out(context, target, start, end)\n",
    "    ctx_tokens_in.extend(ctx_tokens_out)\n",
    "    return list(set(ctx_tokens_in))\n",
    "\n",
    "def ctx_extraction_dep_recu_in_n_steps(context, target, start, end, n = 2):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    result_tokens = []\n",
    "    curr_target = targets\n",
    "    for step in range(0, n):\n",
    "        step_result = [triple[0] for triple in triples \n",
    "                       if triple[2] in curr_target]\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "    return list(set([token[0] for token in result_tokens]))\n",
    "\n",
    "def ctx_extraction_dep_recu_out_n_steps(context, target, start, end, n = 2):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    result_tokens = []\n",
    "    curr_target = targets\n",
    "    for step in range(0, n):\n",
    "        step_result = [triple[2] for triple in triples \n",
    "                       if triple[0] in curr_target]\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "    return list(set([token[0] for token in result_tokens]))\n",
    "\n",
    "def ctx_extraction_dep_recu_in_out_n_steps(context, target, start, end, n = 2):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    result_tokens = []\n",
    "    curr_target = targets\n",
    "    for step in range(0, n):\n",
    "        step_result = [triple[2] for triple in triples \n",
    "                       if triple[0] in curr_target]\n",
    "        step_result_out = [triple[0] for triple in triples \n",
    "                       if triple[2] in curr_target]\n",
    "        step_result.extend(step_result_out)\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "    return list(set([token[0] for token in result_tokens]))\n",
    "\n",
    "def ctx_extraction_dep_recu_in_cover(context, target, start, end, cover = 0.1):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    ctx_tokens_post = post_process_ctx(ctx_tokens)\n",
    "    result_tokens = []\n",
    "    curr_target = targets\n",
    "    curr_cover = 0\n",
    "    while curr_cover < cover:\n",
    "        step_result = [triple[0] for triple in triples \n",
    "                       if triple[2] in curr_target]\n",
    "        if set(step_result) == set(curr_target):\n",
    "                break\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "        curr_cover = len(result_tokens) / len(ctx_tokens_post)\n",
    "    return list(set([token[0] for token in result_tokens]))\n",
    "\n",
    "def ctx_extraction_dep_recu_out_cover(context, target, start, end, cover = 0.1):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    ctx_tokens_post = post_process_ctx(ctx_tokens)\n",
    "    result_tokens = []\n",
    "    curr_target = targets\n",
    "    curr_cover = 0\n",
    "    while curr_cover < cover:\n",
    "        step_result = [triple[2] for triple in triples \n",
    "                       if triple[0] in curr_target]\n",
    "        if set(step_result) == set(curr_target):\n",
    "                break\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "        curr_cover = len(result_tokens) / len(ctx_tokens_post)\n",
    "    return list(set([token[0] for token in result_tokens]))\n",
    "\n",
    "def ctx_extraction_dep_recu_in_out_cover(context, target, start, end, cover = 0.1):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    ctx_tokens_post = post_process_ctx(ctx_tokens)\n",
    "    result_tokens = []\n",
    "    curr_target = targets\n",
    "    curr_cover = 0\n",
    "    while curr_cover < cover:\n",
    "        step_result = [triple[2] for triple in triples \n",
    "                       if triple[0] in curr_target]\n",
    "        step_result_out = [triple[0] for triple in triples \n",
    "                       if triple[2] in curr_target]\n",
    "        step_result.extend(step_result_out)\n",
    "        if set(step_result) == set(curr_target):\n",
    "                break\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "        curr_cover = len(result_tokens) / len(ctx_tokens_post)\n",
    "    return list(set([token[0] for token in result_tokens]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Normally, the land will be passed down by future generations in a way \" + \\\n",
    "             \"that recognizes the community's traditional connection to that country \"\n",
    "target = 'passed'\n",
    "\n",
    "print('ctx_etraction_all:')\n",
    "print(ctx_extraction_all_filtered(sentence, target))\n",
    "\n",
    "print('ctx_extraction_window_pre_n:')\n",
    "print(ctx_extraction_window_pre_n(sentence, \"Normally\"))\n",
    "print(ctx_extraction_window_pre_n(sentence, \"the\"))\n",
    "print(ctx_extraction_window_pre_n(sentence, \"land\"))\n",
    "print(ctx_extraction_window_pre_n(sentence, target, n = 5))\n",
    "\n",
    "print('ctx_extraction_window_suc_n:')\n",
    "print(ctx_extraction_window_suc_n(sentence, \"country\"))\n",
    "print(ctx_extraction_window_suc_n(sentence, \"to\"))\n",
    "print(ctx_extraction_window_suc_n(sentence, \"connection\"))\n",
    "print(ctx_extraction_window_suc_n(sentence, \"community\", n = 5))\n",
    "\n",
    "print('ctx_extraction_window_pre_suc_n:')\n",
    "print(ctx_extraction_window_pre_suc_n(sentence, \"passed\"))\n",
    "print(ctx_extraction_window_pre_suc_n(sentence, \"the\"))\n",
    "print(ctx_extraction_window_pre_suc_n(sentence, \"to\"))\n",
    "\n",
    "print('ctx_extraction_dep_in:')\n",
    "print(ctx_extraction_dep_in(sentence, \"land\"))\n",
    "\n",
    "print('ctx_extraction_dep_out:')\n",
    "print(ctx_extraction_dep_out(sentence, target))\n",
    "print(ctx_extraction_dep_out(sentence, \"land\"))\n",
    "\n",
    "print('ctx_extraction_dep_in_out:')\n",
    "print(ctx_extraction_dep_in_out(sentence, \"land\"))\n",
    "\n",
    "print('ctx_extraction_dep_recu_in_n_steps:')\n",
    "print(ctx_extraction_dep_recu_in_n_steps(sentence, \"the\", n = 3))\n",
    "\n",
    "print('ctx_extraction_dep_recu_out_n_steps:')\n",
    "print(ctx_extraction_dep_recu_out_n_steps(sentence, \"the\"))\n",
    "\n",
    "print('ctx_extraction_dep_recu_in_out_n_steps:')\n",
    "print(ctx_extraction_dep_recu_in_out_n_steps(sentence, \"the\"))\n",
    "\n",
    "print('ctx_extraction_dep_recu_in_cover:')\n",
    "print(ctx_extraction_dep_recu_in_cover(sentence, \"the\", cover=0.1))\n",
    "\n",
    "print('ctx_extraction_dep_recu_out_cover:')\n",
    "print(ctx_extraction_dep_recu_out_cover(sentence, \"the\", cover=0.1))\n",
    "\n",
    "print('ctx_extraction_dep_recu_in_out_cover:')\n",
    "print(ctx_extraction_dep_recu_in_out_cover(sentence, \"the\", cover=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Context Extraction\n",
    "\n",
    "After we defined all the context extraction approaches, we can apply them on the actual dataset. To do so, we first extract all the distinct sentences from the actual training set and create a new dataframe containing only the sentence ids, the sentence, the target and all the computed contexts. This also makes it easier to integrate context extraction functions implemented in other languages. Afterwards we can compute the context features and join them back with the target features dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df_context = df.loc[0:1, ['id', 'sentence', 'target', 'start', 'end']]\n",
    "\n",
    "df_context['ctx_extraction_window_pre_n'] = df_context.apply(lambda columns : \n",
    "                                        ctx_extraction_window_pre_n(columns['sentence'], columns['target'], \\\n",
    "                                                                   columns['start'], columns['end']), axis = 1)\n",
    "\n",
    "df_context['ctx_extraction_window_suc_n'] = df_context.apply(lambda columns : \n",
    "                                        ctx_extraction_window_suc_n(columns['sentence'], columns['target'], \\\n",
    "                                                                   columns['start'], columns['end']), axis = 1)\n",
    "\n",
    "df_context['ctx_extraction_window_pre_suc_n'] = df_context.apply(lambda columns : \n",
    "                                        ctx_extraction_window_pre_suc_n(columns['sentence'], columns['target'], \\\n",
    "                                                                   columns['start'], columns['end']), axis = 1)\n",
    "\n",
    "df_context['ctx_extraction_dep_in'] = df_context.apply(lambda columns : \n",
    "                                        ctx_extraction_dep_in(columns['sentence'], columns['target'], \\\n",
    "                                                                   columns['start'], columns['end']), axis = 1)\n",
    "\n",
    "df_context['ctx_extraction_dep_out'] = df_context.apply(lambda columns : \n",
    "                                        ctx_extraction_dep_out(columns['sentence'], columns['target'], \\\n",
    "                                                                   columns['start'], columns['end']), axis = 1)\n",
    "# 1. Compute dep_in_out using defined function\n",
    "df_context['ctx_extraction_dep_in_out'] = df_context.apply(lambda columns : \n",
    "                                        ctx_extraction_dep_in_out(columns['sentence'], columns['target'], \\\n",
    "                                                                   columns['start'], columns['end']), axis = 1)\n",
    "\n",
    "# 2. Compute dep_in_out by combining precomputed results\n",
    "df_context['ctx_extraction_dep_in_out_dir'] = df_context[['ctx_extraction_dep_in', \\\n",
    "                                                      'ctx_extraction_dep_out']].apply(lambda vals : vals[0]+vals[1], axis=1)\n",
    "\n",
    "\n",
    "df_context['ctx_extraction_dep_recu_in_n_steps']  = df_context.apply(lambda columns : \n",
    "                                        ctx_extraction_dep_recu_in_n_steps(columns['sentence'], columns['target'], \\\n",
    "                                        columns['start'], columns['end'], n=2), axis = 1)\n",
    "\n",
    "df_context['ctx_extraction_dep_recu_out_n_steps']  = df_context.apply(lambda columns : \n",
    "                                        ctx_extraction_dep_recu_out_n_steps(columns['sentence'], columns['target'], \\\n",
    "                                        columns['start'], columns['end'], n=2), axis = 1)\n",
    "\n",
    "\n",
    "df_context['ctx_extraction_dep_recu_in_out_n_steps']  = df_context.apply(lambda columns : \n",
    "                                        ctx_extraction_dep_recu_in_out_n_steps(columns['sentence'], columns['target'], \\\n",
    "                                        columns['start'], columns['end'], n=2), axis = 1)\n",
    "\n",
    "\n",
    "df_context['ctx_extraction_dep_recu_in_cover']  = df_context.apply(lambda columns : \n",
    "                                        ctx_extraction_dep_recu_in_cover(columns['sentence'], columns['target'], \\\n",
    "                                        columns['start'], columns['end'], cover=0.2), axis = 1)\n",
    "\n",
    "\n",
    "df_context['ctx_extraction_dep_recu_out_cover']  = df_context.apply(lambda columns : \n",
    "                                        ctx_extraction_dep_recu_out_cover(columns['sentence'], columns['target'], \\\n",
    "                                        columns['start'], columns['end'], cover=0.2), axis = 1)\n",
    "\n",
    "\n",
    "df_context['ctx_extraction_dep_recu_in_out_cover']  = df_context.apply(lambda columns : \n",
    "                                        ctx_extraction_dep_recu_in_out_cover(columns['sentence'], columns['target'], \\\n",
    "                                        columns['start'], columns['end'], cover=0.2), axis = 1)\n",
    "\n",
    "df_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Context Features\n",
    "After defining all the context definitions and extracting the different kinds of contexts from the sentence, we compute features on the context words. Therefore we first define which of the precomputed contexts to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Readability Measures\n",
    "Here we implement some of the most popular and well-known historical readability measures. Most of them need multiple sentences to compute them properly, however, we will apply them on the extracted context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readability_flesch_kincaid(ctx_len, ctx_sum_syllables):\n",
    "    return 206.835 - (1.015 * ctx_len) - (84.6 * (ctx_sum_syllables / ctx_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_context['context'] = df_context['ctx_extraction_window_pre_suc_n']\n",
    "\n",
    "df_context['ctx_num_tokens'] = df_context.context.apply(lambda context : len(context))\n",
    "df_context['ctx_avg_length'] = df_context.context.apply(lambda context : agg_ctx_feat_num_average(context, len))\n",
    "df_context['ctx_sum_syllables'] = df_context.context.apply(lambda context : agg_ctx_feat_num_sum(context, num_syllables))\n",
    "df_context['ctx_avg_word_freq_wiki'] = df_context.context.apply(lambda context : \\\n",
    "                                                    agg_feat_num_average(context, get_dict_count, word_freq_wiki))\n",
    "df_context.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>rb_dalechall_score</th>\n",
       "      <th>rb_flesch_score</th>\n",
       "      <th>rb_fleschkincaid_score</th>\n",
       "      <th>rb_gunningfog_score</th>\n",
       "      <th>rb_polysyblword_count</th>\n",
       "      <th>rb_smog_score</th>\n",
       "      <th>rb_sybl_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>passed</td>\n",
       "      <td>[land, will, be, down, to, future]</td>\n",
       "      <td>6.565767</td>\n",
       "      <td>102.045</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0</td>\n",
       "      <td>3.1291</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>land</td>\n",
       "      <td>[Normally, the, will, be, passed]</td>\n",
       "      <td>7.042500</td>\n",
       "      <td>100.240</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.1291</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                             context  rb_dalechall_score  \\\n",
       "0  passed  [land, will, be, down, to, future]            6.565767   \n",
       "1    land   [Normally, the, will, be, passed]            7.042500   \n",
       "\n",
       "   rb_flesch_score  rb_fleschkincaid_score  rb_gunningfog_score  \\\n",
       "0          102.045                0.516667                  2.4   \n",
       "1          100.240                0.520000                  2.0   \n",
       "\n",
       "   rb_polysyblword_count  rb_smog_score  rb_sybl_count  \n",
       "0                      0         3.1291              7  \n",
       "1                      0         3.1291              6  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textatistic import Textatistic\n",
    "\n",
    "df_context['readability_flesch_kincaid'] = df_context[['num_ctx_tokens', 'sum_ctx_syllables']] \\\n",
    "                            .apply(lambda vals : readability_flesch_kincaid(vals[0], vals[1]), axis = 1)\n",
    "df_context['rb_dalechall_score'] = df_context.context.apply(lambda context : \\\n",
    "                                                Textatistic(' '.join(context) + '.').dalechall_score)\n",
    "df_context['rb_flesch_score'] = df_context.context.apply(lambda context : \\\n",
    "                                                Textatistic(' '.join(context) + '.').flesch_score)\n",
    "df_context['rb_fleschkincaid_score'] = df_context.context.apply(lambda context : \\\n",
    "                                                Textatistic(' '.join(context) + '.').fleschkincaid_score)\n",
    "df_context['rb_gunningfog_score'] = df_context.context.apply(lambda context : \\\n",
    "                                                Textatistic(' '.join(context) + '.').gunningfog_score)\n",
    "df_context['rb_polysyblword_count'] = df_context.context.apply(lambda context : \\\n",
    "                                                Textatistic(' '.join(context) + '.').polysyblword_count)\n",
    "df_context['rb_smog_score'] = df_context.context.apply(lambda context : \\\n",
    "                                                Textatistic(' '.join(context) + '.').smog_score)\n",
    "df_context['rb_sybl_count'] = df_context.context.apply(lambda context : \\\n",
    "                                                Textatistic(' '.join(context) + '.').sybl_count)\n",
    "\n",
    "df_context[['target', 'context', 'rb_dalechall_score', 'rb_flesch_score', 'rb_fleschkincaid_score', \\\n",
    "            'rb_gunningfog_score', 'rb_polysyblword_count', 'rb_smog_score', 'rb_sybl_count']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "Here we compute individual feature importance based on different metrics. For example, we implement and compute the F-Score, providing an idea of the discrimination power the feature has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_importance_f_score(dataframe, feat_name, label_name):\n",
    "    df = dataframe.copy()\n",
    "    mean_feat = np.mean(df.loc[:, [feat_name]])[0]\n",
    "    means = df.loc[: , [feat_name, label_name]].groupby(label_name).mean().reset_index()\n",
    "    mean_negativ = means.loc[means[label_name] == 0, [feat_name]][feat_name][0]\n",
    "    mean_positiv = means.loc[means[label_name] == 1, [feat_name]][feat_name][1]\n",
    "    # Compute the sum of deviations of the class mean from the overall mean\n",
    "    class_mean_devs = (mean_positiv - mean_feat)**2 + (mean_negativ - mean_feat)**2\n",
    "    # Compute neagtive instance based values\n",
    "    neg_inst = df.loc[df[label_name] == 0, [feat_name]]\n",
    "    std_dev_neg = (np.sum((neg_inst - mean_negativ)**2) / (len(neg_inst) - 1))[feat_name]\n",
    "    #Compute positive instance based values\n",
    "    pos_inst = df.loc[df[label_name] == 1, [feat_name]]\n",
    "    std_dev_pos = (np.sum((pos_inst - mean_positiv)**2) / (len(pos_inst) - 1))[feat_name]\n",
    "    return class_mean_devs / (std_dev_neg + std_dev_pos)\n",
    "\n",
    "def compute_all_feat_importance_metrics(dataframe, label_name):\n",
    "    pass\n",
    "    \n",
    "\n",
    "df_feat = df.drop(['id', 'sentence', 'target', 'nat', 'non_nat', \n",
    "                   'nat_marked', 'non_nat_marked', 'prob'], axis = 1)\n",
    "print(df_feat.mean())\n",
    "print(df_feat.groupby('binary').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'id', 'sentence', \"start\", \"end\", \"target\", \n",
    "              \"nat\", \"non_nat\", \"nat_marked\", \"non_nat_marked\", \"binary\", \"prob\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
