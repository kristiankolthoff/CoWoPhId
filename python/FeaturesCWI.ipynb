{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.wsd import lesk\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_ENGLISH_WIKIPEDIA = \"../cwishareddataset/traindevset/\" + \\\n",
    "                           \"english/Wikipedia_Train.tsv\"\n",
    "df = pd.read_csv(TRAIN_ENGLISH_WIKIPEDIA, sep = \"\\t\")\n",
    "df.columns = ['id', 'sentence', \"start\", \"end\", \"target\", \n",
    "              \"nat\", \"non_nat\", \"nat_marked\", \"non_nat_marked\", \"binary\", \"prob\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'sentence', 'start', 'end', 'target', 'nat', 'non_nat',\n",
       "       'nat_marked', 'non_nat_marked', 'binary', 'prob'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.index\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>binary</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2493.0</td>\n",
       "      <td>2493.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.211372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.228076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       binary         prob\n",
       "count  2493.0  2493.000000\n",
       "mean      1.0     0.211372\n",
       "std       0.0     0.228076\n",
       "min       1.0     0.050000\n",
       "25%       1.0     0.050000\n",
       "50%       1.0     0.100000\n",
       "75%       1.0     0.250000\n",
       "max       1.0     1.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.binary == 1, ['target', 'binary', 'prob']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>target</th>\n",
       "      <th>nat</th>\n",
       "      <th>non_nat</th>\n",
       "      <th>nat_marked</th>\n",
       "      <th>non_nat_marked</th>\n",
       "      <th>binary</th>\n",
       "      <th>prob</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to fut...</td>\n",
       "      <td>28</td>\n",
       "      <td>34</td>\n",
       "      <td>passed</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to fut...</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>land</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to fut...</td>\n",
       "      <td>43</td>\n",
       "      <td>49</td>\n",
       "      <td>future</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to fut...</td>\n",
       "      <td>43</td>\n",
       "      <td>61</td>\n",
       "      <td>future generations</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.15</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to fut...</td>\n",
       "      <td>50</td>\n",
       "      <td>61</td>\n",
       "      <td>generations</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to fut...</td>\n",
       "      <td>76</td>\n",
       "      <td>86</td>\n",
       "      <td>recognizes</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.30</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to fut...</td>\n",
       "      <td>91</td>\n",
       "      <td>100</td>\n",
       "      <td>community</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to fut...</td>\n",
       "      <td>104</td>\n",
       "      <td>115</td>\n",
       "      <td>traditional</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.20</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to fut...</td>\n",
       "      <td>104</td>\n",
       "      <td>142</td>\n",
       "      <td>traditional connection to that country</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to fut...</td>\n",
       "      <td>135</td>\n",
       "      <td>142</td>\n",
       "      <td>country</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to fut...</td>\n",
       "      <td>116</td>\n",
       "      <td>126</td>\n",
       "      <td>connection</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>15</td>\n",
       "      <td>25</td>\n",
       "      <td>Aboriginal</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.55</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>passing</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>15</td>\n",
       "      <td>48</td>\n",
       "      <td>Aboriginal land rights legislaton</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>26</td>\n",
       "      <td>48</td>\n",
       "      <td>land rights legislaton</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>26</td>\n",
       "      <td>30</td>\n",
       "      <td>land</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>38</td>\n",
       "      <td>48</td>\n",
       "      <td>legislaton</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>31</td>\n",
       "      <td>37</td>\n",
       "      <td>rights</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>66</td>\n",
       "      <td>74</td>\n",
       "      <td>preceded</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.70</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>52</td>\n",
       "      <td>61</td>\n",
       "      <td>Australia</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>100</td>\n",
       "      <td>110</td>\n",
       "      <td>Aboriginal</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>80</td>\n",
       "      <td>86</td>\n",
       "      <td>number</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>90</td>\n",
       "      <td>99</td>\n",
       "      <td>important</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>100</td>\n",
       "      <td>119</td>\n",
       "      <td>Aboriginal protests</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>152</td>\n",
       "      <td>160</td>\n",
       "      <td>Stockmen</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>111</td>\n",
       "      <td>119</td>\n",
       "      <td>protests</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>122</td>\n",
       "      <td>131</td>\n",
       "      <td>including</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>141</td>\n",
       "      <td>151</td>\n",
       "      <td>Aboriginal</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>164</td>\n",
       "      <td>170</td>\n",
       "      <td>Strike</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>182</td>\n",
       "      <td>188</td>\n",
       "      <td>Yolngu</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5520</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>In the third instar , purple or blue edging de...</td>\n",
       "      <td>44</td>\n",
       "      <td>52</td>\n",
       "      <td>develops</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5521</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>In the third instar , purple or blue edging de...</td>\n",
       "      <td>60</td>\n",
       "      <td>66</td>\n",
       "      <td>yellow</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5522</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>In the third instar , purple or blue edging de...</td>\n",
       "      <td>83</td>\n",
       "      <td>92</td>\n",
       "      <td>tail horn</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5523</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>In the third instar , purple or blue edging de...</td>\n",
       "      <td>83</td>\n",
       "      <td>87</td>\n",
       "      <td>tail</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5524</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>In the third instar , purple or blue edging de...</td>\n",
       "      <td>88</td>\n",
       "      <td>92</td>\n",
       "      <td>horn</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5525</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>In the third instar , purple or blue edging de...</td>\n",
       "      <td>93</td>\n",
       "      <td>98</td>\n",
       "      <td>turns</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5526</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>In the third instar , purple or blue edging de...</td>\n",
       "      <td>104</td>\n",
       "      <td>109</td>\n",
       "      <td>black</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5527</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>In the third instar , purple or blue edging de...</td>\n",
       "      <td>113</td>\n",
       "      <td>119</td>\n",
       "      <td>yellow</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5528</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>The larva grows to about 120-130 mm , and pupa...</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>larva</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.15</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5529</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>The larva grows to about 120-130 mm , and pupa...</td>\n",
       "      <td>42</td>\n",
       "      <td>49</td>\n",
       "      <td>pupates</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5530</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>The larva grows to about 120-130 mm , and pupa...</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>grows</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5531</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>The larva grows to about 120-130 mm , and pupa...</td>\n",
       "      <td>33</td>\n",
       "      <td>35</td>\n",
       "      <td>mm</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5532</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>The larva grows to about 120-130 mm , and pupa...</td>\n",
       "      <td>56</td>\n",
       "      <td>67</td>\n",
       "      <td>underground</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5533</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>The larva grows to about 120-130 mm , and pupa...</td>\n",
       "      <td>56</td>\n",
       "      <td>75</td>\n",
       "      <td>underground chamber</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5534</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>The larva grows to about 120-130 mm , and pupa...</td>\n",
       "      <td>68</td>\n",
       "      <td>75</td>\n",
       "      <td>chamber</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5535</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>46</td>\n",
       "      <td>57</td>\n",
       "      <td>terminology</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5536</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>actor</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5537</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>21</td>\n",
       "      <td>28</td>\n",
       "      <td>actress</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5538</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>33</td>\n",
       "      <td>39</td>\n",
       "      <td>female</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5539</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>86</td>\n",
       "      <td>94</td>\n",
       "      <td>dramatic</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.30</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5540</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>65</td>\n",
       "      <td>71</td>\n",
       "      <td>person</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5541</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>76</td>\n",
       "      <td>80</td>\n",
       "      <td>acts</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5542</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>98</td>\n",
       "      <td>103</td>\n",
       "      <td>comic</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5543</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>104</td>\n",
       "      <td>114</td>\n",
       "      <td>production</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5544</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>179</td>\n",
       "      <td>187</td>\n",
       "      <td>capacity</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.20</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5545</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>123</td>\n",
       "      <td>128</td>\n",
       "      <td>works</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5546</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>132</td>\n",
       "      <td>136</td>\n",
       "      <td>film</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5547</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>139</td>\n",
       "      <td>149</td>\n",
       "      <td>television</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5548</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>152</td>\n",
       "      <td>159</td>\n",
       "      <td>theater</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5549</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>165</td>\n",
       "      <td>170</td>\n",
       "      <td>radio</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5550 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id  \\\n",
       "0     3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "1     3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "2     3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "3     3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "4     3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "5     3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "6     3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "7     3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "8     3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "9     3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "10    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "11    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "12    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "13    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "14    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "15    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "16    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "17    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "18    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "19    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "20    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "21    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "22    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "23    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "24    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "25    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "26    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "27    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "28    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "29    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "...                              ...   \n",
       "5520  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5521  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5522  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5523  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5524  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5525  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5526  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5527  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5528  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5529  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5530  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5531  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5532  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5533  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5534  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5535  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5536  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5537  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5538  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5539  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5540  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5541  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5542  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5543  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5544  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5545  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5546  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5547  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5548  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5549  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "\n",
       "                                               sentence  start  end  \\\n",
       "0     Normally , the land will be passed down to fut...     28   34   \n",
       "1     Normally , the land will be passed down to fut...     15   19   \n",
       "2     Normally , the land will be passed down to fut...     43   49   \n",
       "3     Normally , the land will be passed down to fut...     43   61   \n",
       "4     Normally , the land will be passed down to fut...     50   61   \n",
       "5     Normally , the land will be passed down to fut...     76   86   \n",
       "6     Normally , the land will be passed down to fut...     91  100   \n",
       "7     Normally , the land will be passed down to fut...    104  115   \n",
       "8     Normally , the land will be passed down to fut...    104  142   \n",
       "9     Normally , the land will be passed down to fut...    135  142   \n",
       "10    Normally , the land will be passed down to fut...    116  126   \n",
       "11    The passing of Aboriginal land rights legislat...     15   25   \n",
       "12    The passing of Aboriginal land rights legislat...      4   11   \n",
       "13    The passing of Aboriginal land rights legislat...     15   48   \n",
       "14    The passing of Aboriginal land rights legislat...     26   48   \n",
       "15    The passing of Aboriginal land rights legislat...     26   30   \n",
       "16    The passing of Aboriginal land rights legislat...     38   48   \n",
       "17    The passing of Aboriginal land rights legislat...     31   37   \n",
       "18    The passing of Aboriginal land rights legislat...     66   74   \n",
       "19    The passing of Aboriginal land rights legislat...     52   61   \n",
       "20    The passing of Aboriginal land rights legislat...    100  110   \n",
       "21    The passing of Aboriginal land rights legislat...     80   86   \n",
       "22    The passing of Aboriginal land rights legislat...     90   99   \n",
       "23    The passing of Aboriginal land rights legislat...    100  119   \n",
       "24    The passing of Aboriginal land rights legislat...    152  160   \n",
       "25    The passing of Aboriginal land rights legislat...    111  119   \n",
       "26    The passing of Aboriginal land rights legislat...    122  131   \n",
       "27    The passing of Aboriginal land rights legislat...    141  151   \n",
       "28    The passing of Aboriginal land rights legislat...    164  170   \n",
       "29    The passing of Aboriginal land rights legislat...    182  188   \n",
       "...                                                 ...    ...  ...   \n",
       "5520  In the third instar , purple or blue edging de...     44   52   \n",
       "5521  In the third instar , purple or blue edging de...     60   66   \n",
       "5522  In the third instar , purple or blue edging de...     83   92   \n",
       "5523  In the third instar , purple or blue edging de...     83   87   \n",
       "5524  In the third instar , purple or blue edging de...     88   92   \n",
       "5525  In the third instar , purple or blue edging de...     93   98   \n",
       "5526  In the third instar , purple or blue edging de...    104  109   \n",
       "5527  In the third instar , purple or blue edging de...    113  119   \n",
       "5528  The larva grows to about 120-130 mm , and pupa...      4    9   \n",
       "5529  The larva grows to about 120-130 mm , and pupa...     42   49   \n",
       "5530  The larva grows to about 120-130 mm , and pupa...     10   15   \n",
       "5531  The larva grows to about 120-130 mm , and pupa...     33   35   \n",
       "5532  The larva grows to about 120-130 mm , and pupa...     56   67   \n",
       "5533  The larva grows to about 120-130 mm , and pupa...     56   75   \n",
       "5534  The larva grows to about 120-130 mm , and pupa...     68   75   \n",
       "5535  An actor ( sometimes actress for female ; see ...     46   57   \n",
       "5536  An actor ( sometimes actress for female ; see ...      3    8   \n",
       "5537  An actor ( sometimes actress for female ; see ...     21   28   \n",
       "5538  An actor ( sometimes actress for female ; see ...     33   39   \n",
       "5539  An actor ( sometimes actress for female ; see ...     86   94   \n",
       "5540  An actor ( sometimes actress for female ; see ...     65   71   \n",
       "5541  An actor ( sometimes actress for female ; see ...     76   80   \n",
       "5542  An actor ( sometimes actress for female ; see ...     98  103   \n",
       "5543  An actor ( sometimes actress for female ; see ...    104  114   \n",
       "5544  An actor ( sometimes actress for female ; see ...    179  187   \n",
       "5545  An actor ( sometimes actress for female ; see ...    123  128   \n",
       "5546  An actor ( sometimes actress for female ; see ...    132  136   \n",
       "5547  An actor ( sometimes actress for female ; see ...    139  149   \n",
       "5548  An actor ( sometimes actress for female ; see ...    152  159   \n",
       "5549  An actor ( sometimes actress for female ; see ...    165  170   \n",
       "\n",
       "                                      target  nat  non_nat  nat_marked  \\\n",
       "0                                     passed   10       10           0   \n",
       "1                                       land   10       10           0   \n",
       "2                                     future   10       10           1   \n",
       "3                         future generations   10       10           1   \n",
       "4                                generations   10       10           3   \n",
       "5                                 recognizes   10       10           2   \n",
       "6                                  community   10       10           0   \n",
       "7                                traditional   10       10           1   \n",
       "8     traditional connection to that country   10       10           0   \n",
       "9                                    country   10       10           0   \n",
       "10                                connection   10       10           0   \n",
       "11                                Aboriginal   10       10           6   \n",
       "12                                   passing   10       10           0   \n",
       "13         Aboriginal land rights legislaton   10       10           0   \n",
       "14                    land rights legislaton   10       10           1   \n",
       "15                                      land   10       10           0   \n",
       "16                                legislaton   10       10          10   \n",
       "17                                    rights   10       10           0   \n",
       "18                                  preceded   10       10           8   \n",
       "19                                 Australia   10       10           0   \n",
       "20                                Aboriginal   10       10           1   \n",
       "21                                    number   10       10           0   \n",
       "22                                 important   10       10           0   \n",
       "23                       Aboriginal protests   10       10           0   \n",
       "24                                  Stockmen   10       10           0   \n",
       "25                                  protests   10       10           0   \n",
       "26                                 including   10       10           0   \n",
       "27                                Aboriginal   10       10           0   \n",
       "28                                    Strike   10       10           0   \n",
       "29                                    Yolngu   10       10           0   \n",
       "...                                      ...  ...      ...         ...   \n",
       "5520                                develops   10       10           0   \n",
       "5521                                  yellow   10       10           0   \n",
       "5522                               tail horn   10       10           0   \n",
       "5523                                    tail   10       10           0   \n",
       "5524                                    horn   10       10           0   \n",
       "5525                                   turns   10       10           0   \n",
       "5526                                   black   10       10           0   \n",
       "5527                                  yellow   10       10           0   \n",
       "5528                                   larva   10       10           3   \n",
       "5529                                 pupates   10       10           7   \n",
       "5530                                   grows   10       10           0   \n",
       "5531                                      mm   10       10           0   \n",
       "5532                             underground   10       10           0   \n",
       "5533                     underground chamber   10       10           0   \n",
       "5534                                 chamber   10       10           1   \n",
       "5535                             terminology   10       10           7   \n",
       "5536                                   actor   10       10           0   \n",
       "5537                                 actress   10       10           0   \n",
       "5538                                  female   10       10           0   \n",
       "5539                                dramatic   10       10           4   \n",
       "5540                                  person   10       10           0   \n",
       "5541                                    acts   10       10           0   \n",
       "5542                                   comic   10       10           2   \n",
       "5543                              production   10       10           0   \n",
       "5544                                capacity   10       10           4   \n",
       "5545                                   works   10       10           0   \n",
       "5546                                    film   10       10           0   \n",
       "5547                              television   10       10           0   \n",
       "5548                                 theater   10       10           0   \n",
       "5549                                   radio   10       10           0   \n",
       "\n",
       "      non_nat_marked  binary  prob  length  \n",
       "0                  1       1  0.05       6  \n",
       "1                  0       0  0.00       4  \n",
       "2                  0       1  0.05       6  \n",
       "3                  2       1  0.15      18  \n",
       "4                  2       1  0.25      11  \n",
       "5                  4       1  0.30      10  \n",
       "6                  0       0  0.00       9  \n",
       "7                  3       1  0.20      11  \n",
       "8                  0       0  0.00      38  \n",
       "9                  1       1  0.05       7  \n",
       "10                 0       0  0.00      10  \n",
       "11                 5       1  0.55      10  \n",
       "12                 0       0  0.00       7  \n",
       "13                 1       1  0.05      33  \n",
       "14                 0       1  0.05      22  \n",
       "15                 0       0  0.00       4  \n",
       "16                 5       1  0.75      10  \n",
       "17                 0       0  0.00       6  \n",
       "18                 6       1  0.70       8  \n",
       "19                 0       0  0.00       9  \n",
       "20                 1       1  0.10      10  \n",
       "21                 0       0  0.00       6  \n",
       "22                 0       0  0.00       9  \n",
       "23                 1       1  0.05      19  \n",
       "24                 0       0  0.00       8  \n",
       "25                 0       0  0.00       8  \n",
       "26                 0       0  0.00       9  \n",
       "27                 0       0  0.00      10  \n",
       "28                 1       1  0.05       6  \n",
       "29                 0       0  0.00       6  \n",
       "...              ...     ...   ...     ...  \n",
       "5520               0       0  0.00       8  \n",
       "5521               0       0  0.00       6  \n",
       "5522               2       1  0.10       9  \n",
       "5523               0       0  0.00       4  \n",
       "5524               1       1  0.05       4  \n",
       "5525               0       0  0.00       5  \n",
       "5526               0       0  0.00       5  \n",
       "5527               0       0  0.00       6  \n",
       "5528               0       1  0.15       5  \n",
       "5529               9       1  0.80       7  \n",
       "5530               0       0  0.00       5  \n",
       "5531               0       0  0.00       2  \n",
       "5532               1       1  0.05      11  \n",
       "5533               1       1  0.05      19  \n",
       "5534               1       1  0.10       7  \n",
       "5535               3       1  0.50      11  \n",
       "5536               0       0  0.00       5  \n",
       "5537               0       0  0.00       7  \n",
       "5538               0       0  0.00       6  \n",
       "5539               2       1  0.30       8  \n",
       "5540               0       0  0.00       6  \n",
       "5541               0       0  0.00       4  \n",
       "5542               0       1  0.10       5  \n",
       "5543               1       1  0.05      10  \n",
       "5544               0       1  0.20       8  \n",
       "5545               0       0  0.00       5  \n",
       "5546               0       0  0.00       4  \n",
       "5547               0       0  0.00      10  \n",
       "5548               0       0  0.00       7  \n",
       "5549               0       0  0.00       5  \n",
       "\n",
       "[5550 rows x 12 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['length'] = df.target.str.len()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>binary</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3057.0</td>\n",
       "      <td>6.357213</td>\n",
       "      <td>3.558134</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2493.0</td>\n",
       "      <td>10.203771</td>\n",
       "      <td>5.206972</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         count       mean       std  min  25%  50%   75%   max\n",
       "binary                                                        \n",
       "0       3057.0   6.357213  3.558134  2.0  4.0  6.0   7.0  49.0\n",
       "1       2493.0  10.203771  5.206972  2.0  7.0  9.0  12.0  49.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('binary')['length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('bad.s.14.bad'), Lemma('bad.s.14.defective')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synset_bads14 = wn.synsets('bad')[14]\n",
    "synset_bads14.lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('real_property.n.01')]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synsets_dog = wn.synsets('land')\n",
    "synsets_dog\n",
    "synset_dog_noun = synsets_dog[0]\n",
    "synset_dog_noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('bad.a.01'),\n",
       " Synset('bad.s.02'),\n",
       " Synset('bad.s.03'),\n",
       " Synset('bad.s.04'),\n",
       " Synset('regretful.a.01'),\n",
       " Synset('bad.s.06'),\n",
       " Synset('bad.s.07'),\n",
       " Synset('bad.s.08'),\n",
       " Synset('bad.s.09'),\n",
       " Synset('bad.s.10'),\n",
       " Synset('bad.s.11'),\n",
       " Synset('bad.s.12'),\n",
       " Synset('bad.s.13'),\n",
       " Synset('bad.s.14')]"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synsets_bad = wn.synsets('bad')\n",
    "synsets_bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bad', 'badness']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lemma.name() for lemma in synsets_bad[0].lemmas()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'them'"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordNetLemmatizer.lemmatize('them')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet Features\n",
    "Here we implement all the relevant features based on WordNet and SentiWordNet. For example, the number of synsets the target word is contained in or the average length of the lemmas of all the synsets the target word is contained in. Note that all features that are computed in the following exploit neither the POS-Tag of the target word nor Word Sense Disambiguation by e.g. UKB-Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\k_kol\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2889: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\k_kol\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: RuntimeWarning: Mean of empty slice\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\k_kol\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: RuntimeWarning: Mean of empty slice\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "def wn_synset_freq(target):\n",
    "    return len(wn.synsets(target))\n",
    "\n",
    "def wn_synset_avg_lemma_freq(target):\n",
    "    return np.nan_to_num(np.mean([len(synset.lemmas()) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_lemma_len(target):\n",
    "    return np.nan_to_num(np.nanmean([len(lemma.name()) \n",
    "            for synset in wn.synsets(target) \n",
    "            for lemma in synset.lemmas()]))\n",
    "\n",
    "def wn_synset_avg_hypernyms(target):\n",
    "    return np.nan_to_num(np.nanmean([len(synset.hypernyms()) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_hyponyms(target):\n",
    "    return np.nan_to_num(np.mean([len(synset.hyponyms()) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_sum_hypernyms(target):\n",
    "    return np.sum(([len(synset.hypernyms()) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_definition_len(target):\n",
    "    return np.nan_to_num(np.mean([len(str(synset.definition())) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_hyptree_depth(target):\n",
    "    return np.nan_to_num(np.mean([synset.max_depth() \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_num_distinct_pos(target):\n",
    "    return len(set([synset.pos() for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_num_relations(target):\n",
    "    return np.nan_to_num(np.mean([np.sum([len(synset.hypernyms()), len(synset.hyponyms()), \n",
    "             len(synset.instance_hypernyms()), len(synset.instance_hyponyms()),\n",
    "             len(synset.member_holonyms()), len(synset.substance_holonyms()),\n",
    "             len(synset.part_holonyms()), len(synset.member_meronyms()),\n",
    "             len(synset.substance_meronyms()), len(synset.part_meronyms())]) \n",
    "             for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_freq_pos(target, pos):\n",
    "    return len(wn.synsets(target, pos = pos))\n",
    "\n",
    "def wn_synset_sense_entropy_uniform(target):\n",
    "    num_senses = len(wn.synsets(target))\n",
    "    return -np.sum([((1 / num_senses) * np.log2(1 / num_senses)) \n",
    "                     for index in range(0, num_senses)])\n",
    "\n",
    "def wn_synset_sense_entropy_pos_uniform(target):\n",
    "    num_senses = len(wn.synsets(target))\n",
    "    pos_distribution = [len(wn.synsets(target, pos = wn.NOUN)),\n",
    "                        len(wn.synsets(target, pos = wn.VERB)),\n",
    "                        len(wn.synsets(target, pos = wn.ADJ)),\n",
    "                        len(wn.synsets(target, pos = wn.ADV))]\n",
    "    return -np.sum([(np.nan_to_num((count / num_senses) * np.log2(count / num_senses))) \n",
    "            for count in pos_distribution]) if num_senses != 0 else 0\n",
    "\n",
    "def wn_synsets_sense_entropy_pos_central(target, pos):\n",
    "    num_senses_pos = len(wn.synsets(target, pos = pos))\n",
    "    return -np.sum([((1 / num_senses_pos) * np.log2(1 / num_senses_pos))\n",
    "                     for index in range(0, num_senses_pos)])\n",
    "     \n",
    "\n",
    "def swn_avg_objective_score(target):\n",
    "    return np.nan_to_num(np.mean([senti_synset.obj_score() \n",
    "                for senti_synset in swn.senti_synsets(target)]))\n",
    "\n",
    "def pos_tag(sentence, target):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    wordPOSPairs = [token for token in nltk.pos_tag(tokens) if token[0] == target]\n",
    "    return wordPOSPairs[0][1] if len(wordPOSPairs) > 0 else None\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    if not tag:\n",
    "        return None\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    "    if tag.startswith('V'):\n",
    "        return 'v'\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    "    if tag.startswith('R'):\n",
    "        return 'r'\n",
    "    return None\n",
    "\n",
    "df['wn_synset_freq'] = df.target.apply(wn_synset_freq)\n",
    "df['wn_synset_avg_lemma_freq'] = df.target.apply(wn_synset_avg_lemma_freq)\n",
    "df['wn_synset_avg_lemma_len'] = df.target.apply(wn_synset_avg_lemma_len)\n",
    "\n",
    "df['wn_synset_diff_len_avg_lemma_len'] = df.wn_synset_avg_lemma_len - df.length\n",
    "df['wn_synset_avg_hypernyms'] = df.target.apply(wn_synset_avg_hypernyms)\n",
    "df['wn_synset_sum_hypernyms'] = df.target.apply(wn_synset_sum_hypernyms)\n",
    "df['wn_synset_avg_hyponyms'] = df.target.apply(wn_synset_avg_hyponyms)\n",
    "\n",
    "df['wn_synset_avg_definition_len'] = df.target.apply(wn_synset_avg_definition_len)\n",
    "df['wn_synset_avg_hyptree_depth'] = df.target.apply(wn_synset_avg_hyptree_depth)\n",
    "df['wn_synset_num_distinct_pos'] = df.target.apply(wn_synset_num_distinct_pos)\n",
    "df['wn_synset_avg_num_relations'] = df.target.apply(wn_synset_avg_num_relations)\n",
    "\n",
    "# Synset sizes of the target word for the four different POS-Tags in WordNet\n",
    "df['wn_synset_avg_freq_pos_noun'] = df.target.apply(lambda target : wn_synset_avg_freq_pos(target, wn.NOUN))\n",
    "df['wn_synset_avg_freq_pos_verb'] = df.target.apply(lambda target : wn_synset_avg_freq_pos(target, wn.VERB))\n",
    "df['wn_synset_avg_freq_pos_adj'] = df.target.apply(lambda target : wn_synset_avg_freq_pos(target, wn.ADJ))\n",
    "df['wn_synset_avg_freq_pos_adv'] = df.target.apply(lambda target : wn_synset_avg_freq_pos(target, wn.ADV))\n",
    "\n",
    "# Normalized POS-Tag synset sizes\n",
    "df['wn_synset_avg_freq_pos_noun_norm'] = np.nan_to_num(df.wn_synset_avg_freq_pos_noun / df.wn_synset_freq)\n",
    "df['wn_synset_avg_freq_pos_verb_norm'] = np.nan_to_num(df.wn_synset_avg_freq_pos_verb / df.wn_synset_freq)\n",
    "df['wn_synset_avg_freq_pos_adj_norm'] = np.nan_to_num(df.wn_synset_avg_freq_pos_adj / df.wn_synset_freq)\n",
    "df['wn_synset_avg_freq_pos_adv_norm'] = np.nan_to_num(df.wn_synset_avg_freq_pos_adv / df.wn_synset_freq)\n",
    "\n",
    "df['pos_tag'] = df[['sentence', 'target']].apply(lambda vals : pos_tag(*vals), axis = 1)\n",
    "df['wn_synset_sense_entropy_uniform'] = df.target.apply(wn_synset_sense_entropy_uniform)\n",
    "df['wn_synset_sense_entropy_pos_uniform'] = df.target.apply(wn_synset_sense_entropy_pos_uniform)\n",
    "df['wn_synsets_sense_entropy_pos_central'] = df[['target', 'pos_tag']].apply(\n",
    "    lambda vals : wn_synsets_sense_entropy_pos_central(vals[0], penn_to_wn(vals[1])), axis = 1)\n",
    "\n",
    "df['swn_avg_objective_score'] = df.target.apply(swn_avg_objective_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>length</th>\n",
       "      <th>wn_synset_freq</th>\n",
       "      <th>wn_synset_sense_entropy_uniform</th>\n",
       "      <th>wn_synset_sense_entropy_pos_uniform</th>\n",
       "      <th>wn_synsets_sense_entropy_pos_central</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>passed</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>4.643856</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>4.643856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>land</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>4.169925</td>\n",
       "      <td>0.964079</td>\n",
       "      <td>3.459432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>future</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>2.807355</td>\n",
       "      <td>0.985228</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>future generations</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>generations</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>2.807355</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>2.807355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               target  length  wn_synset_freq  \\\n",
       "0              passed       6              25   \n",
       "1                land       4              18   \n",
       "2              future       6               7   \n",
       "3  future generations      18               0   \n",
       "4         generations      11               7   \n",
       "\n",
       "   wn_synset_sense_entropy_uniform  wn_synset_sense_entropy_pos_uniform  \\\n",
       "0                         4.643856                            -0.000000   \n",
       "1                         4.169925                             0.964079   \n",
       "2                         2.807355                             0.985228   \n",
       "3                        -0.000000                             0.000000   \n",
       "4                         2.807355                            -0.000000   \n",
       "\n",
       "   wn_synsets_sense_entropy_pos_central  \n",
       "0                              4.643856  \n",
       "1                              3.459432  \n",
       "2                              2.000000  \n",
       "3                             -0.000000  \n",
       "4                              2.807355  "
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:,['target', 'length','wn_synset_freq', 'wn_synset_sense_entropy_uniform', 'wn_synset_sense_entropy_pos_uniform', 'wn_synsets_sense_entropy_pos_central']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PorterStemmer and StanfordNLP Features\n",
    "Here we implement features based on the PorterStemmer library from nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<DependencyGraph with 8 nodes>]"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "from nltk.stem.wordnet import *\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "\n",
    "java_path = \"C:/Program Files (x86)/Java/jdk1.8.0_144/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "path_to_jar = 'resources/stanford-dependency-parser/stanford-parser.jar'\n",
    "path_to_models_jar = 'resources/stanford-dependency-parser/stanford-parser-3.9.1-models.jar'\n",
    "\n",
    "porterStemmer = PorterStemmer()\n",
    "wordNetLemmatizer = WordNetLemmatizer()\n",
    "nerTagger = StanfordNERTagger('resources/stanford-ner-tagger/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "               'resources/stanford-ner-tagger/stanford-ner.jar',\n",
    "               encoding='utf-8')\n",
    "dependencyParser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "\n",
    "def porter_stem_len(target):\n",
    "    return len(str(porterStemmer.stem(target)))\n",
    "\n",
    "def porter_stemmer_num_steps(target):\n",
    "    stem = target.lower()\n",
    "    applied_steps = 0\n",
    "    if porterStemmer.mode == porterStemmer.NLTK_EXTENSIONS and target in porterStemmer.pool:\n",
    "            return applied_steps\n",
    "    if porterStemmer.mode != porterStemmer.ORIGINAL_ALGORITHM and len(target) <= 2:\n",
    "            return applied_steps\n",
    "    step_funcs = [porterStemmer._step1a, porterStemmer._step1b, porterStemmer._step1c,\n",
    "                  porterStemmer._step2, porterStemmer._step3, porterStemmer._step3,\n",
    "                  porterStemmer._step4, porterStemmer._step5a, porterStemmer._step5b]\n",
    "    for step_func in step_funcs:\n",
    "        stem_step = step_func(stem)\n",
    "        if stem_step != stem:\n",
    "            stem = stem_step\n",
    "            applied_steps += 1\n",
    "    return applied_steps\n",
    "\n",
    "def is_named_entity(sentence, target):\n",
    "    tokenized_sent = word_tokenize(sentence)\n",
    "    tagged_sent = nerTagger.tag(tokenized_sent)\n",
    "    for token, tag in tagged_sent:\n",
    "        if token == target and tag != 'O':\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def named_entity_type(sentence, target):\n",
    "    tokenized_sent = word_tokenize(sentence)\n",
    "    tagged_sent = nerTagger.tag(tokenized_sent)\n",
    "    return [tag for token, tag in tagged_sent if token == target][0]\n",
    "\n",
    "# TODO consider using stanford lemmatizer and compute word similarity metric\n",
    "# to orignal target\n",
    "def wordnet_lemma_len(target):\n",
    "    return len(wordNetLemmatizer.lemmatize(target))\n",
    "\n",
    "\n",
    "# Porter stemmer stem length, number of applied steps,\n",
    "# difference of stem length to target and reduction ratio\n",
    "df['porter_stem_len'] = df.target.apply(porter_stem_len)\n",
    "df['porter_stemmer_num_steps'] = df.target.apply(porter_stemmer_num_steps)\n",
    "df['diff_len_stem_len'] = df.length - df.num_porter_stem_len\n",
    "df['reduction_stem_len'] = 1 - df.porter_stem_len / df.length\n",
    "\n",
    "# WordNet lemma length, differnce of lemma length to target\n",
    "# length and reduction ratio for lemmatization\n",
    "df['wordnet_lemma_len'] = df.target.apply(wordnet_lemma_len)\n",
    "df['diff_len_wordnet_lemma_len'] = df.length - df.wordnet_lemma_len\n",
    "df['reduction_lemma_len'] = 1 - df.wordnet_lemma_len / df.length\n",
    "\n",
    "# StanfordNLP features CAUTION: The tagger is slow\n",
    "#df['is_named_entity'] = df[['sentence', 'target']].apply(lambda vals : is_named_entity(*vals), axis = 1)\n",
    "#df['named_entity_type'] = df[['sentence', 'target']].apply(lambda vals : named_entity_type(*vals), axis = 1)\n",
    "result = dependencyParser.raw_parse('I shot an elephant in my sleep')\n",
    "list(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[:, ['target', 'length', 'wordnet_lemma_len', 'diff_len_wordnet_lemma_len', 'reduction_lemma_len']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orthographic Features\n",
    "Here we compute orthographic features for the target word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Relative position of the target word based on tokens\n",
    "df['relative_position'] = df[['sentence', 'target']].apply(lambda vals : \n",
    "            (nltk.word_tokenize(vals[0]).index(vals[1].split()[0])) / len((nltk.word_tokenize(vals[0]))), axis = 1)\n",
    "# Relative positions of the target word based on character counting\n",
    "df['relative_position_left'] = df[['sentence', 'start']].apply(lambda vals : vals[1] / len(vals[0]), axis = 1)\n",
    "df['relative_position_centered'] = df[['sentence', 'start', 'end']].apply(lambda vals : \n",
    "            ((vals[1] + vals[2]) / 2) / len(vals[0]), axis = 1)\n",
    "df['relative_position_right'] = df[['sentence', 'end']].apply(lambda vals : vals[1] / len(vals[0]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.loc[:, ['sentence', 'target', 'start', 'relative_position', 'relative_position_left', 'relative_position_centered', 'relative_position_right']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context-Aware Features\n",
    "Here we compute not only the context extraction/definition in the first place but also the corresponding context features afterwards. Also we need to implement proper strategies to cope with the target occuring multiple times in the sentence. To avoid mistakes, we should use the actual start and end tags from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ctx_etraction_all:\n",
      "['Normally', 'the', 'land', 'will', 'be', 'passed', 'down', 'by', 'future', 'generations', 'in', 'a', 'way', 'that', 'recognizes', 'the', 'community', 'traditional', 'connection', 'to', 'that', 'country']\n",
      "ctx_extraction_window_pre_n:\n",
      "[]\n",
      "['Normally']\n",
      "['Normally', 'the']\n",
      "['Normally', 'the', 'land', 'will', 'be']\n",
      "ctx_extraction_window_suc_n:\n",
      "[]\n",
      "['that', 'country']\n",
      "['to', 'that', 'country']\n",
      "['traditional', 'connection', 'to', 'that', 'country']\n",
      "ctx_extraction_window_pre_suc_n:\n",
      "['land', 'will', 'be', 'down', 'by', 'future']\n",
      "['Normally', 'land', 'will', 'be']\n",
      "['community', 'traditional', 'connection', 'that', 'country']\n",
      "ctx_extraction_dep_recu_in_n_steps:\n",
      "['land', 'connection', 'recognizes', 'passed', 'community']\n",
      "ctx_extraction_dep_recu_out_n_steps:\n",
      "[]\n",
      "ctx_extraction_dep_recu_in_out_n_steps:\n",
      "['land', 'connection', 'the', \"'s\", 'passed', 'community']\n",
      "ctx_extraction_dep_recu_in_cover:\n",
      "['land', 'connection', 'passed', 'community']\n",
      "ctx_extraction_dep_recu_out_cover:\n",
      "[]\n",
      "ctx_extraction_dep_recu_in_out_cover:\n",
      "0.3181818181818182\n",
      "['land', 'connection', 'the', \"'s\", 'passed', 'community']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "from nltk.parse.stanford import StanfordNeuralDependencyParser\n",
    "import os\n",
    "\n",
    "java_path = \"C:/Program Files (x86)/Java/jdk1.8.0_144/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "path_to_jar = 'resources/stanford-dependency-parser/stanford-parser.jar'\n",
    "path_to_models_jar = 'resources/stanford-dependency-parser/stanford-parser-3.9.1-models.jar'\n",
    "\n",
    "dependencyParser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "\n",
    "def post_process_ctx(context):\n",
    "    return [token for token in context if token not in (\",\", \"'\", \"'s\")]\n",
    "    \n",
    "def ctx_extraction_all(context, target):\n",
    "    return word_tokenize(context)\n",
    "\n",
    "def ctx_extraction_all_filtered(context, target):\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    return post_process_ctx(ctx_tokens)\n",
    "\n",
    "def ctx_extraction_window_pre_n(context, target, n = 3):\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    post_ctx_tokens = post_process_ctx(ctx_tokens)\n",
    "    target_index = post_ctx_tokens.index(target)\n",
    "    start_index = (target_index - n) if (target_index - n) > 0 else 0\n",
    "    return post_ctx_tokens[start_index:target_index]\n",
    "\n",
    "def ctx_extraction_window_suc_n(context, target, n = 3):\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    post_ctx_tokens = post_process_ctx(ctx_tokens)\n",
    "    target_index = post_ctx_tokens.index(target)\n",
    "    end_index = (target_index + 1 + n) if (target_index + 1 + n) \\\n",
    "                < len(post_ctx_tokens) else len(post_ctx_tokens)\n",
    "    return post_ctx_tokens[target_index+1:end_index]\n",
    "\n",
    "def ctx_extraction_window_pre_suc_n(context, target, n = 3):\n",
    "    ctx_tokens_pre = ctx_extraction_window_pre_n(context, target, n)\n",
    "    ctx_tokens_suc = ctx_extraction_window_suc_n(context, target, n)\n",
    "    ctx_tokens_pre.extend(ctx_tokens_suc)\n",
    "    return ctx_tokens_pre\n",
    "\n",
    "def ctx_extraction_dep_in(context, target):\n",
    "    return [triple[0][0] for parse in dependencyParser.raw_parse(context)\n",
    "            for triple in list(parse.triples()) if triple[2][0] == target]\n",
    "\n",
    "def ctx_extraction_dep_out(context, target):\n",
    "    return [triple[2][0] for parse in dependencyParser.raw_parse(context)\n",
    "            for triple in list(parse.triples()) if triple[0][0] == target]\n",
    "\n",
    "def ctx_extraction_dep_in_out(context, target):\n",
    "    ctx_tokens_in = ctx_extraction_dep_in(context, target)\n",
    "    ctx_tokens_out = ctx_extraction_dep_out(context, target)\n",
    "    ctx_tokens_in.extend(ctx_tokens_out)\n",
    "    return ctx_tokens_in\n",
    "\n",
    "def ctx_extraction_dep_recu_in_n_steps(context, target, n = 2):\n",
    "    deps = [triple for parse in dependencyParser.raw_parse(context)\n",
    "            for triple in parse.triples()]\n",
    "    result_tokens = []\n",
    "    curr_target = [target]\n",
    "    for step in range(0, n):\n",
    "        step_result = [triple[0][0] for triple in deps \n",
    "                       if triple[2][0] in curr_target]\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "    return list(set(result_tokens))\n",
    "\n",
    "def ctx_extraction_dep_recu_out_n_steps(context, target, n = 2):\n",
    "    deps = [triple for parse in dependencyParser.raw_parse(context)\n",
    "            for triple in parse.triples()]\n",
    "    result_tokens = []\n",
    "    curr_target = [target]\n",
    "    for step in range(0, n):\n",
    "        step_result = [triple[2][0] for triple in deps \n",
    "                       if triple[0][0] in curr_target]\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "    return list(set(result_tokens))\n",
    "\n",
    "def ctx_extraction_dep_recu_in_out_n_steps(context, target, n = 2):\n",
    "    deps = [triple for parse in dependencyParser.raw_parse(context)\n",
    "            for triple in parse.triples()]\n",
    "    result_tokens = []\n",
    "    curr_target = [target]\n",
    "    for step in range(0, n):\n",
    "        step_result = [triple[2][0] for triple in deps \n",
    "                       if triple[0][0] in curr_target]\n",
    "        step_result_out = [triple[0][0] for triple in deps \n",
    "                       if triple[2][0] in curr_target]\n",
    "        step_result.extend(step_result_out)\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "    return list(set(result_tokens))\n",
    "\n",
    "def ctx_extraction_dep_recu_in_cover(context, target, cover = 0.1):\n",
    "    deps = [triple for parse in dependencyParser.raw_parse(context)\n",
    "            for triple in parse.triples()]\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    ctx_tokens_post = post_process_ctx(ctx_tokens)\n",
    "    result_tokens = []\n",
    "    curr_target = [target]\n",
    "    curr_cover = 0\n",
    "    while curr_cover < cover:\n",
    "        step_result = [triple[0][0] for triple in deps \n",
    "                       if triple[2][0] in curr_target]\n",
    "        if set(step_result) == set(curr_target):\n",
    "                break\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "        curr_cover = len(result_tokens) / len(ctx_tokens_post)\n",
    "    return list(set(result_tokens))\n",
    "\n",
    "def ctx_extraction_dep_recu_out_cover(context, target, cover = 0.1):\n",
    "    deps = [triple for parse in dependencyParser.raw_parse(context)\n",
    "            for triple in parse.triples()]\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    ctx_tokens_post = post_process_ctx(ctx_tokens)\n",
    "    result_tokens = []\n",
    "    curr_target = [target]\n",
    "    curr_cover = 0\n",
    "    while curr_cover < cover:\n",
    "        step_result = [triple[2][0] for triple in deps \n",
    "                       if triple[0][0] in curr_target]\n",
    "        if set(step_result) == set(curr_target):\n",
    "                break\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "        curr_cover = len(result_tokens) / len(ctx_tokens_post)\n",
    "    return list(set(result_tokens))\n",
    "\n",
    "def ctx_extraction_dep_recu_in_out_cover(context, target, cover = 0.1):\n",
    "    deps = [triple for parse in dependencyParser.raw_parse(context)\n",
    "            for triple in parse.triples()]\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    ctx_tokens_post = post_process_ctx(ctx_tokens)\n",
    "    result_tokens = []\n",
    "    curr_target = [target]\n",
    "    curr_cover = 0\n",
    "    while curr_cover < cover:\n",
    "        step_result = [triple[2][0] for triple in deps \n",
    "                       if triple[0][0] in curr_target]\n",
    "        step_result_out = [triple[0][0] for triple in deps \n",
    "                       if triple[2][0] in curr_target]\n",
    "        step_result.extend(step_result_out)\n",
    "        if set(step_result) == set(curr_target):\n",
    "                break\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "        curr_cover = len(result_tokens) / len(ctx_tokens_post)\n",
    "    print(curr_cover)\n",
    "    return list(set(result_tokens))\n",
    "\n",
    "sentence = \"Normally, the land will be passed down by future generations in a way \" + \\\n",
    "             \"that recognizes the community's traditional connection to that country \"\n",
    "target = 'passed'\n",
    "\n",
    "\n",
    "print('ctx_etraction_all:')\n",
    "print(ctx_extraction_all_filtered(sentence, target))\n",
    "\n",
    "print('ctx_extraction_window_pre_n:')\n",
    "print(ctx_extraction_window_pre_n(sentence, \"Normally\"))\n",
    "print(ctx_extraction_window_pre_n(sentence, \"the\"))\n",
    "print(ctx_extraction_window_pre_n(sentence, \"land\"))\n",
    "print(ctx_extraction_window_pre_n(sentence, target, n = 5))\n",
    "\n",
    "print('ctx_extraction_window_suc_n:')\n",
    "print(ctx_extraction_window_suc_n(sentence, \"country\"))\n",
    "print(ctx_extraction_window_suc_n(sentence, \"to\"))\n",
    "print(ctx_extraction_window_suc_n(sentence, \"connection\"))\n",
    "print(ctx_extraction_window_suc_n(sentence, \"community\", n = 5))\n",
    "\n",
    "print('ctx_extraction_window_pre_suc_n:')\n",
    "print(ctx_extraction_window_pre_suc_n(sentence, \"passed\"))\n",
    "print(ctx_extraction_window_pre_suc_n(sentence, \"the\"))\n",
    "print(ctx_extraction_window_pre_suc_n(sentence, \"to\"))\n",
    "\n",
    "#print('ctx_extraction_dep_in:')\n",
    "#print(ctx_extraction_dep_in(sentence, \"land\"))\n",
    "\n",
    "#print('ctx_extraction_dep_out:')\n",
    "#print(ctx_extraction_dep_out(sentence, target))\n",
    "#print(ctx_extraction_dep_out(sentence, \"land\"))\n",
    "\n",
    "#print('ctx_extraction_dep_in_out:')\n",
    "#print(ctx_extraction_dep_in_out(sentence, \"land\"))\n",
    "\n",
    "print('ctx_extraction_dep_recu_in_n_steps:')\n",
    "print(ctx_extraction_dep_recu_in_n_steps(sentence, \"the\", n = 3))\n",
    "\n",
    "print('ctx_extraction_dep_recu_out_n_steps:')\n",
    "print(ctx_extraction_dep_recu_out_n_steps(sentence, \"the\"))\n",
    "\n",
    "print('ctx_extraction_dep_recu_in_out_n_steps:')\n",
    "print(ctx_extraction_dep_recu_in_out_n_steps(sentence, \"the\"))\n",
    "\n",
    "print('ctx_extraction_dep_recu_in_cover:')\n",
    "print(ctx_extraction_dep_recu_in_cover(sentence, \"the\", cover=0.1))\n",
    "\n",
    "print('ctx_extraction_dep_recu_out_cover:')\n",
    "print(ctx_extraction_dep_recu_out_cover(sentence, \"the\", cover=0.1))\n",
    "\n",
    "print('ctx_extraction_dep_recu_in_out_cover:')\n",
    "print(ctx_extraction_dep_recu_in_out_cover(sentence, \"the\", cover=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we defined all the context extraction approaches, we can apply them on the actual dataset. To do so, we first extract all the distinct sentences from the actual training set and create a new dataframe containing only the sentence ids, the sentence, the target and all the computed contexts. This also makes it easier to integrate context extraction functions implemented in other languages. Afterwards we can compute the context features and join them back with the target features dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>binary</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3057.0</td>\n",
       "      <td>0.451750</td>\n",
       "      <td>0.604611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2493.0</td>\n",
       "      <td>0.870437</td>\n",
       "      <td>0.751977</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         count      mean       std  min  25%  50%  75%  max\n",
       "binary                                                     \n",
       "0       3057.0  0.451750  0.604611  0.0  0.0  0.0  1.0  3.0\n",
       "1       2493.0  0.870437  0.751977  0.0  0.0  1.0  1.0  4.0"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('binary')['porter_stemmer_num_steps'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>binary</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3057.0</td>\n",
       "      <td>7.397121</td>\n",
       "      <td>8.903936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2493.0</td>\n",
       "      <td>3.193341</td>\n",
       "      <td>4.569106</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         count      mean       std  min  25%  50%   75%   max\n",
       "binary                                                       \n",
       "0       3057.0  7.397121  8.903936  0.0  2.0  5.0  10.0  72.0\n",
       "1       2493.0  3.193341  4.569106  0.0  0.0  2.0   4.0  52.0"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('binary')['wn_synset_freq'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>binary</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3057.0</td>\n",
       "      <td>6.167117</td>\n",
       "      <td>3.311438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.25</td>\n",
       "      <td>6.47619</td>\n",
       "      <td>7.882353</td>\n",
       "      <td>22.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2493.0</td>\n",
       "      <td>5.403105</td>\n",
       "      <td>4.189762</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.50000</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>17.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         count      mean       std  min   25%      50%       75%        max\n",
       "binary                                                                     \n",
       "0       3057.0  6.167117  3.311438  0.0  5.25  6.47619  7.882353  22.666667\n",
       "1       2493.0  5.403105  4.189762  0.0  0.00  6.50000  8.500000  17.666667"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('binary')['wn_synset_avg_lemma_len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>binary</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3057.0</td>\n",
       "      <td>5.834805</td>\n",
       "      <td>7.875869</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2493.0</td>\n",
       "      <td>2.536302</td>\n",
       "      <td>4.028887</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         count      mean       std  min  25%  50%  75%   max\n",
       "binary                                                      \n",
       "0       3057.0  5.834805  7.875869  0.0  0.0  3.0  8.0  60.0\n",
       "1       2493.0  2.536302  4.028887  0.0  0.0  1.0  4.0  48.0"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('binary')['wn_synset_avg_hypernyms'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>binary</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3057.0</td>\n",
       "      <td>0.795982</td>\n",
       "      <td>0.349295</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2493.0</td>\n",
       "      <td>0.606709</td>\n",
       "      <td>0.446212</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         count      mean       std  min       25%       50%  75%  max\n",
       "binary                                                               \n",
       "0       3057.0  0.795982  0.349295  0.0  0.833333  0.958333  1.0  1.0\n",
       "1       2493.0  0.606709  0.446212  0.0  0.000000  0.875000  1.0  1.0"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('binary')['swn_avg_objective_score'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>nat</th>\n",
       "      <th>non_nat</th>\n",
       "      <th>nat_marked</th>\n",
       "      <th>non_nat_marked</th>\n",
       "      <th>binary</th>\n",
       "      <th>prob</th>\n",
       "      <th>length</th>\n",
       "      <th>wn_synset_size</th>\n",
       "      <th>...</th>\n",
       "      <th>reduction_stem_len</th>\n",
       "      <th>porter_stemmer_num_steps</th>\n",
       "      <th>wordnet_lemma_len</th>\n",
       "      <th>diff_len_wordnet_lemma_len</th>\n",
       "      <th>reduction_lemma_len</th>\n",
       "      <th>wn_synset_avg_num_relations</th>\n",
       "      <th>wn_synset_avg_freq_pos_noun</th>\n",
       "      <th>wn_synset_avg_freq_pos_verb</th>\n",
       "      <th>wn_synset_avg_freq_pos_adj</th>\n",
       "      <th>wn_synset_avg_freq_pos_adv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5550.000000</td>\n",
       "      <td>5550.000000</td>\n",
       "      <td>5550.0</td>\n",
       "      <td>5550.0</td>\n",
       "      <td>5550.000000</td>\n",
       "      <td>5550.000000</td>\n",
       "      <td>5550.000000</td>\n",
       "      <td>5550.000000</td>\n",
       "      <td>5550.000000</td>\n",
       "      <td>5550.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5550.000000</td>\n",
       "      <td>5550.00000</td>\n",
       "      <td>5550.000000</td>\n",
       "      <td>5550.000000</td>\n",
       "      <td>5550.000000</td>\n",
       "      <td>5550.000000</td>\n",
       "      <td>5550.000000</td>\n",
       "      <td>5550.000000</td>\n",
       "      <td>5550.000000</td>\n",
       "      <td>5550.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>84.332252</td>\n",
       "      <td>92.417297</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.999099</td>\n",
       "      <td>0.899820</td>\n",
       "      <td>0.449189</td>\n",
       "      <td>0.094946</td>\n",
       "      <td>8.085045</td>\n",
       "      <td>5.508829</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112989</td>\n",
       "      <td>0.63982</td>\n",
       "      <td>7.987027</td>\n",
       "      <td>0.098018</td>\n",
       "      <td>0.014580</td>\n",
       "      <td>5.103052</td>\n",
       "      <td>2.474775</td>\n",
       "      <td>2.227027</td>\n",
       "      <td>0.711532</td>\n",
       "      <td>0.095495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>67.924777</td>\n",
       "      <td>68.101992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.040101</td>\n",
       "      <td>1.898976</td>\n",
       "      <td>0.497456</td>\n",
       "      <td>0.185519</td>\n",
       "      <td>4.775966</td>\n",
       "      <td>7.576974</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142027</td>\n",
       "      <td>0.70615</td>\n",
       "      <td>4.803605</td>\n",
       "      <td>0.335522</td>\n",
       "      <td>0.050693</td>\n",
       "      <td>16.333337</td>\n",
       "      <td>3.508863</td>\n",
       "      <td>5.482322</td>\n",
       "      <td>2.154476</td>\n",
       "      <td>0.668174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>34.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>72.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.071429</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>118.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>584.000000</td>\n",
       "      <td>590.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>225.666667</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             start          end     nat  non_nat   nat_marked  non_nat_marked  \\\n",
       "count  5550.000000  5550.000000  5550.0   5550.0  5550.000000     5550.000000   \n",
       "mean     84.332252    92.417297    10.0     10.0     0.999099        0.899820   \n",
       "std      67.924777    68.101992     0.0      0.0     2.040101        1.898976   \n",
       "min       0.000000     2.000000    10.0     10.0     0.000000        0.000000   \n",
       "25%      34.000000    42.000000    10.0     10.0     0.000000        0.000000   \n",
       "50%      72.000000    80.000000    10.0     10.0     0.000000        0.000000   \n",
       "75%     118.000000   126.000000    10.0     10.0     1.000000        1.000000   \n",
       "max     584.000000   590.000000    10.0     10.0    10.000000       10.000000   \n",
       "\n",
       "            binary         prob       length  wn_synset_size  \\\n",
       "count  5550.000000  5550.000000  5550.000000     5550.000000   \n",
       "mean      0.449189     0.094946     8.085045        5.508829   \n",
       "std       0.497456     0.185519     4.775966        7.576974   \n",
       "min       0.000000     0.000000     2.000000        0.000000   \n",
       "25%       0.000000     0.000000     5.000000        1.000000   \n",
       "50%       0.000000     0.000000     7.000000        3.000000   \n",
       "75%       1.000000     0.100000     9.000000        7.000000   \n",
       "max       1.000000     1.000000    49.000000       72.000000   \n",
       "\n",
       "                  ...              reduction_stem_len  \\\n",
       "count             ...                     5550.000000   \n",
       "mean              ...                        0.112989   \n",
       "std               ...                        0.142027   \n",
       "min               ...                        0.000000   \n",
       "25%               ...                        0.000000   \n",
       "50%               ...                        0.000000   \n",
       "75%               ...                        0.200000   \n",
       "max               ...                        0.700000   \n",
       "\n",
       "       porter_stemmer_num_steps  wordnet_lemma_len  \\\n",
       "count                5550.00000        5550.000000   \n",
       "mean                    0.63982           7.987027   \n",
       "std                     0.70615           4.803605   \n",
       "min                     0.00000           2.000000   \n",
       "25%                     0.00000           5.000000   \n",
       "50%                     1.00000           7.000000   \n",
       "75%                     1.00000           9.000000   \n",
       "max                     4.00000          49.000000   \n",
       "\n",
       "       diff_len_wordnet_lemma_len  reduction_lemma_len  \\\n",
       "count                 5550.000000          5550.000000   \n",
       "mean                     0.098018             0.014580   \n",
       "std                      0.335522             0.050693   \n",
       "min                     -1.000000            -0.200000   \n",
       "25%                      0.000000             0.000000   \n",
       "50%                      0.000000             0.000000   \n",
       "75%                      0.000000             0.000000   \n",
       "max                      5.000000             0.625000   \n",
       "\n",
       "       wn_synset_avg_num_relations  wn_synset_avg_freq_pos_noun  \\\n",
       "count                  5550.000000                  5550.000000   \n",
       "mean                      5.103052                     2.474775   \n",
       "std                      16.333337                     3.508863   \n",
       "min                       0.000000                     0.000000   \n",
       "25%                       0.000000                     0.000000   \n",
       "50%                       2.071429                     1.000000   \n",
       "75%                       5.000000                     4.000000   \n",
       "max                     225.666667                    33.000000   \n",
       "\n",
       "       wn_synset_avg_freq_pos_verb  wn_synset_avg_freq_pos_adj  \\\n",
       "count                  5550.000000                 5550.000000   \n",
       "mean                      2.227027                    0.711532   \n",
       "std                       5.482322                    2.154476   \n",
       "min                       0.000000                    0.000000   \n",
       "25%                       0.000000                    0.000000   \n",
       "50%                       0.000000                    0.000000   \n",
       "75%                       2.000000                    0.000000   \n",
       "max                      59.000000                   27.000000   \n",
       "\n",
       "       wn_synset_avg_freq_pos_adv  \n",
       "count                 5550.000000  \n",
       "mean                     0.095495  \n",
       "std                      0.668174  \n",
       "min                      0.000000  \n",
       "25%                      0.000000  \n",
       "50%                      0.000000  \n",
       "75%                      0.000000  \n",
       "max                     16.000000  \n",
       "\n",
       "[8 rows x 36 columns]"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       " liquors                   1\n",
       "hull                       1\n",
       "humans                     1\n",
       "humor                      1\n",
       "hydrogen-burning           1\n",
       "hydrogen-burning limit     1\n",
       "hypothesize                1\n",
       "iPod                       1\n",
       "iPods                      1\n",
       "iTunes                     1\n",
       "ice hockey                 1\n",
       "ice hockey player          1\n",
       "idea                       1\n",
       "illustrations              1\n",
       "immigration                1\n",
       "impact                     1\n",
       "implement                  1\n",
       "importance                 1\n",
       "improperly                 1\n",
       "in doubt                   1\n",
       "inaugural race             1\n",
       "incisors                   1\n",
       "hui                        1\n",
       "incomplete                 1\n",
       "hub                        1\n",
       "hosts                      1\n",
       "high humidity              1\n",
       "higher proportion          1\n",
       "highly                     1\n",
       "hill                       1\n",
       "                          ..\n",
       "Australia                  7\n",
       "May                        7\n",
       "Saint                      7\n",
       "rights                     7\n",
       "Dushku                     7\n",
       "opened                     8\n",
       "moved                      8\n",
       "including                  8\n",
       "located                    8\n",
       "Ain                        8\n",
       "world                      8\n",
       "made                       8\n",
       "land                       9\n",
       "book                       9\n",
       "brown                      9\n",
       "won                        9\n",
       "Al                         9\n",
       "city                      10\n",
       "played                    10\n",
       "year                      11\n",
       "art                       11\n",
       "compounds                 11\n",
       "number                    11\n",
       "called                    12\n",
       "series                    12\n",
       "Mucha                     12\n",
       "film                      13\n",
       "blocked                   13\n",
       "major                     13\n",
       "season                    13\n",
       "Length: 3745, dtype: int64"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('target').size().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
