{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "First, we load all the data we need into pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.wsd import lesk\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_ENGLISH_WIKIPEDIA = \"../cwishareddataset/traindevset/\" + \\\n",
    "                           \"english/Wikipedia_Train.tsv\"\n",
    "df = pd.read_csv(TRAIN_ENGLISH_WIKIPEDIA, sep = \"\\t\")\n",
    "df.columns = ['id', 'sentence', \"start\", \"end\", \"target\", \n",
    "              \"nat\", \"non_nat\", \"nat_marked\", \"non_nat_marked\", \"binary\", \"prob\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation\n",
    "Since many labels are multi-word expression, we first of all define some aggregation functions that aggregate feature values over multiple tokens. Implementing this seperately allows to easily exchange the used aggregation function and keeps the feature computation functions clean. These feature computation functions should only compute features for a single target word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def agg_feat_num_average(target, func_feature, *args):\n",
    "    return np.mean([func_feature(token, *args) for token in word_tokenize(target)])\n",
    "\n",
    "def agg_feat_num_median(target, func_feature, *args):\n",
    "    return np.median([func_feature(token, *args) for token in word_tokenize(target)])\n",
    "\n",
    "def agg_feat_num_max(target, func_feature, *args):\n",
    "    return np.max([func_feature(token, *args) for token in word_tokenize(target)])\n",
    "\n",
    "def agg_feat_num_min(target, func_feature, *args):\n",
    "    return np.min([func_feature(token, *args) for token in word_tokenize(target)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orthographic features\n",
    "Here we start computing simple features like the length of the target word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['length'] = df.target.apply(lambda target : agg_feat_num_average(target, len))\n",
    "#Relative position of the target word based on tokens\n",
    "df['relative_position'] = df[['sentence', 'target']].apply(lambda vals : \n",
    "            (nltk.word_tokenize(vals[0]).index(vals[1].split()[0])) / len((nltk.word_tokenize(vals[0]))), axis = 1)\n",
    "# Relative positions of the target word based on character counting\n",
    "df['relative_position_left'] = df[['sentence', 'start']].apply(lambda vals : vals[1] / len(vals[0]), axis = 1)\n",
    "df['relative_position_centered'] = df[['sentence', 'start', 'end']].apply(lambda vals : \n",
    "            ((vals[1] + vals[2]) / 2) / len(vals[0]), axis = 1)\n",
    "df['relative_position_right'] = df[['sentence', 'end']].apply(lambda vals : vals[1] / len(vals[0]), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet Features\n",
    "Here we implement all the relevant features based on WordNet and SentiWordNet. For example, the number of synsets the target word is contained in or the average length of the lemmas of all the synsets the target word is contained in. Note that all features that are computed in the following exploit neither the POS-Tag of the target word nor Word Sense Disambiguation by e.g. UKB-Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "def wn_synset_freq(target):\n",
    "    return len(wn.synsets(target))\n",
    "\n",
    "def wn_synset_avg_lemma_freq(target):\n",
    "    return np.nan_to_num(np.mean([len(synset.lemmas()) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_lemma_len(target):\n",
    "    return np.nan_to_num(np.nanmean([len(lemma.name()) \n",
    "            for synset in wn.synsets(target) \n",
    "            for lemma in synset.lemmas()]))\n",
    "\n",
    "def wn_synset_avg_hypernyms(target):\n",
    "    return np.nan_to_num(np.nanmean([len(synset.hypernyms()) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_hyponyms(target):\n",
    "    return np.nan_to_num(np.mean([len(synset.hyponyms()) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_sum_hypernyms(target):\n",
    "    return np.sum(([len(synset.hypernyms()) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_definition_len(target):\n",
    "    return np.nan_to_num(np.mean([len(str(synset.definition())) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_hyptree_depth(target):\n",
    "    return np.nan_to_num(np.mean([synset.max_depth() \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_num_distinct_pos(target):\n",
    "    return len(set([synset.pos() for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_num_relations(target):\n",
    "    return np.nan_to_num(np.mean([np.sum([len(synset.hypernyms()), len(synset.hyponyms()), \n",
    "             len(synset.instance_hypernyms()), len(synset.instance_hyponyms()),\n",
    "             len(synset.member_holonyms()), len(synset.substance_holonyms()),\n",
    "             len(synset.part_holonyms()), len(synset.member_meronyms()),\n",
    "             len(synset.substance_meronyms()), len(synset.part_meronyms())]) \n",
    "             for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_freq_pos(target, pos):\n",
    "    return len(wn.synsets(target, pos = pos))\n",
    "\n",
    "def wn_synset_sense_entropy_uniform(target):\n",
    "    num_senses = len(wn.synsets(target))\n",
    "    return -np.sum([((1 / num_senses) * np.log2(1 / num_senses)) \n",
    "                     for index in range(0, num_senses)])\n",
    "\n",
    "def wn_synset_sense_entropy_pos_uniform(target):\n",
    "    num_senses = len(wn.synsets(target))\n",
    "    pos_distribution = [len(wn.synsets(target, pos = wn.NOUN)),\n",
    "                        len(wn.synsets(target, pos = wn.VERB)),\n",
    "                        len(wn.synsets(target, pos = wn.ADJ)),\n",
    "                        len(wn.synsets(target, pos = wn.ADV))]\n",
    "    return -np.sum([(np.nan_to_num((count / num_senses) * np.log2(count / num_senses))) \n",
    "            for count in pos_distribution]) if num_senses != 0 else 0\n",
    "\n",
    "def wn_synsets_sense_entropy_pos_central(target, pos):\n",
    "    num_senses_pos = len(wn.synsets(target, pos = pos))\n",
    "    return -np.sum([((1 / num_senses_pos) * np.log2(1 / num_senses_pos))\n",
    "                     for index in range(0, num_senses_pos)])\n",
    "     \n",
    "\n",
    "def swn_avg_objective_score(target):\n",
    "    return np.nan_to_num(np.mean([senti_synset.obj_score() \n",
    "                for senti_synset in swn.senti_synsets(target)]))\n",
    "\n",
    "def pos_tag(sentence, target):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    wordPOSPairs = [token for token in nltk.pos_tag(tokens) if token[0] == target]\n",
    "    return wordPOSPairs[0][1] if len(wordPOSPairs) > 0 else None\n",
    "\n",
    "# TODO consider using stanford lemmatizer and compute word similarity metric\n",
    "# to orignal target\n",
    "def wordnet_lemma_len(target):\n",
    "    return len(wordNetLemmatizer.lemmatize(target))\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    if not tag:\n",
    "        return None\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    "    if tag.startswith('V'):\n",
    "        return 'v'\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    "    if tag.startswith('R'):\n",
    "        return 'r'\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\k_kol\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2889: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\k_kol\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: RuntimeWarning: Mean of empty slice\n",
      "  \n",
      "C:\\Users\\k_kol\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: RuntimeWarning: Mean of empty slice\n"
     ]
    }
   ],
   "source": [
    "df['wn_synset_freq'] = df.target.apply(lambda target : agg_feat_num_average(target, wn_synset_freq))\n",
    "df['wn_synset_avg_lemma_freq'] = df.target.apply(lambda target : agg_feat_num_average(target, wn_synset_avg_lemma_freq))\n",
    "df['wn_synset_avg_lemma_len'] = df.target.apply(lambda target : agg_feat_num_average(target, wn_synset_avg_lemma_len))\n",
    "\n",
    "df['wn_synset_diff_len_avg_lemma_len'] = df.wn_synset_avg_lemma_len - df.length\n",
    "df['wn_synset_avg_hypernyms'] = df.target.apply(lambda target : agg_feat_num_average(target, wn_synset_avg_hypernyms))\n",
    "df['wn_synset_sum_hypernyms'] = df.target.apply(lambda target : agg_feat_num_average(target, wn_synset_sum_hypernyms))\n",
    "df['wn_synset_avg_hyponyms'] = df.target.apply(lambda target : agg_feat_num_average(target, wn_synset_avg_hyponyms))\n",
    "\n",
    "df['wn_synset_avg_definition_len'] = df.target.apply(lambda target : \n",
    "                                                     agg_feat_num_average(target, wn_synset_avg_definition_len))\n",
    "df['wn_synset_avg_hyptree_depth'] = df.target.apply(lambda target :\n",
    "                                                     agg_feat_num_average(target, wn_synset_avg_hyptree_depth))\n",
    "df['wn_synset_num_distinct_pos'] = df.target.apply(lambda target : \n",
    "                                                     agg_feat_num_average(target, wn_synset_num_distinct_pos))\n",
    "df['wn_synset_avg_num_relations'] = df.target.apply(lambda target : \n",
    "                                                     agg_feat_num_average(target, wn_synset_avg_num_relations))\n",
    "\n",
    "df['wn_synset_avg_freq_pos_noun'] = df.target.apply(lambda target : \n",
    "                                                    agg_feat_num_average(target, wn_synset_avg_freq_pos, wn.NOUN))\n",
    "df['wn_synset_avg_freq_pos_verb'] = df.target.apply(lambda target : \n",
    "                                                    agg_feat_num_average(target, wn_synset_avg_freq_pos, wn.VERB))\n",
    "df['wn_synset_avg_freq_pos_adj'] = df.target.apply(lambda target : \n",
    "                                                   agg_feat_num_average(target, wn_synset_avg_freq_pos, wn.ADJ))\n",
    "df['wn_synset_avg_freq_pos_adv'] = df.target.apply(lambda target : \n",
    "                                                   agg_feat_num_average(target, wn_synset_avg_freq_pos, wn.ADV))\n",
    "\n",
    "df['wn_synset_avg_freq_pos_noun_norm'] = np.nan_to_num(df.wn_synset_avg_freq_pos_noun / df.wn_synset_freq)\n",
    "df['wn_synset_avg_freq_pos_verb_norm'] = np.nan_to_num(df.wn_synset_avg_freq_pos_verb / df.wn_synset_freq)\n",
    "df['wn_synset_avg_freq_pos_adj_norm'] = np.nan_to_num(df.wn_synset_avg_freq_pos_adj / df.wn_synset_freq)\n",
    "df['wn_synset_avg_freq_pos_adv_norm'] = np.nan_to_num(df.wn_synset_avg_freq_pos_adv / df.wn_synset_freq)\n",
    "\n",
    "df['pos_tag'] = df[['sentence', 'target']].apply(lambda vals : pos_tag(*vals), axis = 1)\n",
    "df['wn_synset_sense_entropy_uniform'] = df.target.apply(lambda target : \n",
    "                                                        agg_feat_num_average(target, wn_synset_sense_entropy_uniform))\n",
    "df['wn_synset_sense_entropy_pos_uniform'] = df.target.apply(lambda target :\n",
    "                                                        agg_feat_num_average(target, wn_synset_sense_entropy_pos_uniform))\n",
    "df['wn_synsets_sense_entropy_pos_central'] = df[['target', 'pos_tag']].apply(\n",
    "    lambda vals : wn_synsets_sense_entropy_pos_central(vals[0], penn_to_wn(vals[1])), axis = 1)\n",
    "\n",
    "df['swn_avg_objective_score'] = df.target.apply(lambda target : agg_feat_num_average(target, swn_avg_objective_score))\n",
    "\n",
    "df['wordnet_lemma_len'] = df.target.apply(lambda target : agg_feat_num_average(target, wordnet_lemma_len))\n",
    "df['diff_len_wordnet_lemma_len'] = df.length - df.wordnet_lemma_len\n",
    "df['reduction_lemma_len'] = 1 - df.wordnet_lemma_len / df.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[:,['target', 'length', 'wordnet_lemma_len', 'diff_len_wordnet_lemma_len', 'reduction_lemma_len']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PorterStemmer, StanfordNLP and Dependency Tree Features\n",
    "Here we implement features based on the PorterStemmer library from nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "from nltk.stem.wordnet import *\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "\n",
    "java_path = \"C:/Program Files (x86)/Java/jdk1.8.0_144/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "path_to_jar = 'resources/stanford-dependency-parser/stanford-parser.jar'\n",
    "path_to_models_jar = 'resources/stanford-dependency-parser/stanford-parser-3.9.1-models.jar'\n",
    "\n",
    "porterStemmer = PorterStemmer()\n",
    "wordNetLemmatizer = WordNetLemmatizer()\n",
    "nerTagger = StanfordNERTagger('resources/stanford-ner-tagger/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "               'resources/stanford-ner-tagger/stanford-ner.jar',\n",
    "               encoding='utf-8')\n",
    "dependencyParser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "\n",
    "def porter_stem_len(target):\n",
    "    return len(str(porterStemmer.stem(target)))\n",
    "\n",
    "def porter_stemmer_num_steps(target):\n",
    "    stem = target.lower()\n",
    "    applied_steps = 0\n",
    "    if porterStemmer.mode == porterStemmer.NLTK_EXTENSIONS and target in porterStemmer.pool:\n",
    "            return applied_steps\n",
    "    if porterStemmer.mode != porterStemmer.ORIGINAL_ALGORITHM and len(target) <= 2:\n",
    "            return applied_steps\n",
    "    step_funcs = [porterStemmer._step1a, porterStemmer._step1b, porterStemmer._step1c,\n",
    "                  porterStemmer._step2, porterStemmer._step3, porterStemmer._step3,\n",
    "                  porterStemmer._step4, porterStemmer._step5a, porterStemmer._step5b]\n",
    "    for step_func in step_funcs:\n",
    "        stem_step = step_func(stem)\n",
    "        if stem_step != stem:\n",
    "            stem = stem_step\n",
    "            applied_steps += 1\n",
    "    return applied_steps\n",
    "\n",
    "def is_named_entity(sentence, target):\n",
    "    tokenized_sent = word_tokenize(sentence)\n",
    "    tagged_sent = nerTagger.tag(tokenized_sent)\n",
    "    for token, tag in tagged_sent:\n",
    "        if token == target and tag != 'O':\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def named_entity_type(sentence, target):\n",
    "    tokenized_sent = word_tokenize(sentence)\n",
    "    tagged_sent = nerTagger.tag(tokenized_sent)\n",
    "    return [tag for token, tag in tagged_sent if token == target][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porter stemmer stem length, number of applied steps,\n",
    "# difference of stem length to target and reduction ratio\n",
    "df['porter_stem_len'] = df.target.apply(lambda target : agg_feat_num_average(target, porter_stem_len))\n",
    "df['porter_stemmer_num_steps'] = df.target.apply(lambda target : agg_feat_num_average(target, porter_stemmer_num_steps))\n",
    "df['diff_len_stem_len'] = df.length - df.porter_stem_len\n",
    "df['reduction_stem_len'] = 1 - df.porter_stem_len / df.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>length</th>\n",
       "      <th>porter_stem_len</th>\n",
       "      <th>porter_stemmer_num_steps</th>\n",
       "      <th>diff_len_stem_len</th>\n",
       "      <th>reduction_stem_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>passed</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>land</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>future</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>future generations</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.411765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>generations</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>recognizes</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>community</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>traditional</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.454545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>traditional connection to that country</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>country</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>connection</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Aboriginal</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>passing</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Aboriginal land rights legislaton</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>6.750000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>land rights legislaton</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>land</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>legislaton</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rights</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>preceded</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Australia</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Aboriginal</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>number</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>important</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Aboriginal protests</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Stockmen</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>protests</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>including</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Aboriginal</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Strike</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Yolngu</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5520</th>\n",
       "      <td>develops</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5521</th>\n",
       "      <td>yellow</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5522</th>\n",
       "      <td>tail horn</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5523</th>\n",
       "      <td>tail</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5524</th>\n",
       "      <td>horn</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5525</th>\n",
       "      <td>turns</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5526</th>\n",
       "      <td>black</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5527</th>\n",
       "      <td>yellow</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5528</th>\n",
       "      <td>larva</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5529</th>\n",
       "      <td>pupates</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5530</th>\n",
       "      <td>grows</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5531</th>\n",
       "      <td>mm</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5532</th>\n",
       "      <td>underground</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5533</th>\n",
       "      <td>underground chamber</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5534</th>\n",
       "      <td>chamber</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5535</th>\n",
       "      <td>terminology</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5536</th>\n",
       "      <td>actor</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5537</th>\n",
       "      <td>actress</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5538</th>\n",
       "      <td>female</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5539</th>\n",
       "      <td>dramatic</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5540</th>\n",
       "      <td>person</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5541</th>\n",
       "      <td>acts</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5542</th>\n",
       "      <td>comic</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5543</th>\n",
       "      <td>production</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5544</th>\n",
       "      <td>capacity</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5545</th>\n",
       "      <td>works</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5546</th>\n",
       "      <td>film</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5547</th>\n",
       "      <td>television</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5548</th>\n",
       "      <td>theater</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5549</th>\n",
       "      <td>radio</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5550 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      target     length  porter_stem_len  \\\n",
       "0                                     passed   6.000000         4.000000   \n",
       "1                                       land   4.000000         4.000000   \n",
       "2                                     future   6.000000         5.000000   \n",
       "3                         future generations   8.500000         5.000000   \n",
       "4                                generations  11.000000         5.000000   \n",
       "5                                 recognizes  10.000000         6.000000   \n",
       "6                                  community   9.000000         6.000000   \n",
       "7                                traditional  11.000000         6.000000   \n",
       "8     traditional connection to that country   6.800000         5.200000   \n",
       "9                                    country   7.000000         7.000000   \n",
       "10                                connection  10.000000         7.000000   \n",
       "11                                Aboriginal  10.000000         8.000000   \n",
       "12                                   passing   7.000000         4.000000   \n",
       "13         Aboriginal land rights legislaton   7.500000         6.750000   \n",
       "14                    land rights legislaton   6.666667         6.333333   \n",
       "15                                      land   4.000000         4.000000   \n",
       "16                                legislaton  10.000000        10.000000   \n",
       "17                                    rights   6.000000         5.000000   \n",
       "18                                  preceded   8.000000         6.000000   \n",
       "19                                 Australia   9.000000         9.000000   \n",
       "20                                Aboriginal  10.000000         8.000000   \n",
       "21                                    number   6.000000         6.000000   \n",
       "22                                 important   9.000000         6.000000   \n",
       "23                       Aboriginal protests   9.000000         7.500000   \n",
       "24                                  Stockmen   8.000000         8.000000   \n",
       "25                                  protests   8.000000         7.000000   \n",
       "26                                 including   9.000000         6.000000   \n",
       "27                                Aboriginal  10.000000         8.000000   \n",
       "28                                    Strike   6.000000         6.000000   \n",
       "29                                    Yolngu   6.000000         6.000000   \n",
       "...                                      ...        ...              ...   \n",
       "5520                                develops   8.000000         7.000000   \n",
       "5521                                  yellow   6.000000         6.000000   \n",
       "5522                               tail horn   4.000000         4.000000   \n",
       "5523                                    tail   4.000000         4.000000   \n",
       "5524                                    horn   4.000000         4.000000   \n",
       "5525                                   turns   5.000000         4.000000   \n",
       "5526                                   black   5.000000         5.000000   \n",
       "5527                                  yellow   6.000000         6.000000   \n",
       "5528                                   larva   5.000000         5.000000   \n",
       "5529                                 pupates   7.000000         5.000000   \n",
       "5530                                   grows   5.000000         4.000000   \n",
       "5531                                      mm   2.000000         2.000000   \n",
       "5532                             underground  11.000000        11.000000   \n",
       "5533                     underground chamber   9.000000         9.000000   \n",
       "5534                                 chamber   7.000000         7.000000   \n",
       "5535                             terminology  11.000000        10.000000   \n",
       "5536                                   actor   5.000000         5.000000   \n",
       "5537                                 actress   7.000000         7.000000   \n",
       "5538                                  female   6.000000         5.000000   \n",
       "5539                                dramatic   8.000000         6.000000   \n",
       "5540                                  person   6.000000         6.000000   \n",
       "5541                                    acts   4.000000         3.000000   \n",
       "5542                                   comic   5.000000         5.000000   \n",
       "5543                              production  10.000000         7.000000   \n",
       "5544                                capacity   8.000000         5.000000   \n",
       "5545                                   works   5.000000         4.000000   \n",
       "5546                                    film   4.000000         4.000000   \n",
       "5547                              television  10.000000         7.000000   \n",
       "5548                                 theater   7.000000         7.000000   \n",
       "5549                                   radio   5.000000         5.000000   \n",
       "\n",
       "      porter_stemmer_num_steps  diff_len_stem_len  reduction_stem_len  \n",
       "0                     1.000000           2.000000            0.333333  \n",
       "1                     0.000000           0.000000            0.000000  \n",
       "2                     1.000000           1.000000            0.166667  \n",
       "3                     2.000000           3.500000            0.411765  \n",
       "4                     3.000000           6.000000            0.545455  \n",
       "5                     2.000000           4.000000            0.400000  \n",
       "6                     2.000000           3.000000            0.333333  \n",
       "7                     2.000000           5.000000            0.454545  \n",
       "8                     0.800000           1.600000            0.235294  \n",
       "9                     1.000000           0.000000            0.000000  \n",
       "10                    1.000000           3.000000            0.300000  \n",
       "11                    1.000000           2.000000            0.200000  \n",
       "12                    1.000000           3.000000            0.428571  \n",
       "13                    0.500000           0.750000            0.100000  \n",
       "14                    0.333333           0.333333            0.050000  \n",
       "15                    0.000000           0.000000            0.000000  \n",
       "16                    0.000000           0.000000            0.000000  \n",
       "17                    1.000000           1.000000            0.166667  \n",
       "18                    1.000000           2.000000            0.250000  \n",
       "19                    0.000000           0.000000            0.000000  \n",
       "20                    1.000000           2.000000            0.200000  \n",
       "21                    0.000000           0.000000            0.000000  \n",
       "22                    1.000000           3.000000            0.333333  \n",
       "23                    1.000000           1.500000            0.166667  \n",
       "24                    0.000000           0.000000            0.000000  \n",
       "25                    1.000000           1.000000            0.125000  \n",
       "26                    1.000000           3.000000            0.333333  \n",
       "27                    1.000000           2.000000            0.200000  \n",
       "28                    0.000000           0.000000            0.000000  \n",
       "29                    0.000000           0.000000            0.000000  \n",
       "...                        ...                ...                 ...  \n",
       "5520                  1.000000           1.000000            0.125000  \n",
       "5521                  0.000000           0.000000            0.000000  \n",
       "5522                  0.000000           0.000000            0.000000  \n",
       "5523                  0.000000           0.000000            0.000000  \n",
       "5524                  0.000000           0.000000            0.000000  \n",
       "5525                  1.000000           1.000000            0.200000  \n",
       "5526                  0.000000           0.000000            0.000000  \n",
       "5527                  0.000000           0.000000            0.000000  \n",
       "5528                  0.000000           0.000000            0.000000  \n",
       "5529                  2.000000           2.000000            0.285714  \n",
       "5530                  1.000000           1.000000            0.200000  \n",
       "5531                  0.000000           0.000000            0.000000  \n",
       "5532                  0.000000           0.000000            0.000000  \n",
       "5533                  0.000000           0.000000            0.000000  \n",
       "5534                  0.000000           0.000000            0.000000  \n",
       "5535                  2.000000           1.000000            0.090909  \n",
       "5536                  0.000000           0.000000            0.000000  \n",
       "5537                  0.000000           0.000000            0.000000  \n",
       "5538                  1.000000           1.000000            0.166667  \n",
       "5539                  1.000000           2.000000            0.250000  \n",
       "5540                  0.000000           0.000000            0.000000  \n",
       "5541                  1.000000           1.000000            0.250000  \n",
       "5542                  0.000000           0.000000            0.000000  \n",
       "5543                  1.000000           3.000000            0.300000  \n",
       "5544                  2.000000           3.000000            0.375000  \n",
       "5545                  1.000000           1.000000            0.200000  \n",
       "5546                  0.000000           0.000000            0.000000  \n",
       "5547                  1.000000           3.000000            0.300000  \n",
       "5548                  0.000000           0.000000            0.000000  \n",
       "5549                  0.000000           0.000000            0.000000  \n",
       "\n",
       "[5550 rows x 6 columns]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, ['target', 'length', 'porter_stem_len', 'porter_stemmer_num_steps', 'diff_len_stem_len', 'reduction_stem_len']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context-Aware Features\n",
    "Here we compute not only the context extraction/definition in the first place but also the corresponding context features afterwards. Also we need to implement proper strategies to cope with the target occuring multiple times in the sentence. To avoid mistakes, we should use the actual start and end tags from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Context-Token Aggregation\n",
    "First we define how feature values of multiple context-tokens should be aggreagated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def agg_feat_num_average(tokens, func_feature, *args):\n",
    "    return np.mean([func_feature(token, *args) for token in tokens])\n",
    "\n",
    "def agg_feat_num_median(tokens, func_feature, *args):\n",
    "    return np.median([func_feature(token, *args) for token in tokens])\n",
    "\n",
    "def agg_feat_num_max(tokens, func_feature, *args):\n",
    "    return np.max([func_feature(token, *args) for token in tokens])\n",
    "\n",
    "def agg_feat_num_min(tokens, func_feature, *args):\n",
    "    return np.min([func_feature(token, *args) for token in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Context Definition\n",
    "Here we compute different kinds of context definitions. For example, as a baseline we extract all tokens from the sentence except the target. A second approach is to use a n preceeding or n succeding tokens, or a combined window apporach were we extract n tokens preceeding and succeding of the target. A more sophisticated apporach involves dependency parsing of the sentence and applying different extraction heuristics. Finally we also implement a context extraction approach exploting FrameNet semantic parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "from nltk.parse.stanford import StanfordNeuralDependencyParser\n",
    "import os\n",
    "\n",
    "java_path = \"C:/Program Files (x86)/Java/jdk1.8.0_144/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "path_to_jar = 'resources/stanford-dependency-parser/stanford-parser.jar'\n",
    "path_to_models_jar = 'resources/stanford-dependency-parser/stanford-parser-3.9.1-models.jar'\n",
    "\n",
    "dependencyParser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "\n",
    "def post_process_ctx(context):\n",
    "    return [token for token in context if token not in (\",\", \"'\", \"'s\")]\n",
    "\n",
    "def preprocess_target(target):\n",
    "    return target.strip()\n",
    "    \n",
    "def ctx_extraction_all(context, target):\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    if target in ctx_tokens:\n",
    "        ctx_tokens.remove(target)\n",
    "    return ctx_tokens\n",
    "\n",
    "def ctx_extraction_all_filtered(context, target):\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    return post_process_ctx(ctx_tokens)\n",
    "\n",
    "def ctx_extraction_window_pre_n(context, target, start, end, n = 3):\n",
    "    target = preprocess_target(target)\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    post_ctx_tokens = post_process_ctx(ctx_tokens)\n",
    "    target_tokens = word_tokenize(target)\n",
    "    target_index = post_ctx_tokens.index(target) if len(target_tokens) == 1 else post_ctx_tokens.index(target_tokens[0])\n",
    "    start_index = (target_index - n) if (target_index - n) > 0 else 0\n",
    "    return post_ctx_tokens[start_index:target_index]\n",
    "\n",
    "def ctx_extraction_window_suc_n(context, target, start, end, n = 3):\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    post_ctx_tokens = post_process_ctx(ctx_tokens)\n",
    "    target_index = post_ctx_tokens.index(target)\n",
    "    end_index = (target_index + 1 + n) if (target_index + 1 + n) \\\n",
    "                < len(post_ctx_tokens) else len(post_ctx_tokens)\n",
    "    return post_ctx_tokens[target_index+1:end_index]\n",
    "\n",
    "def ctx_extraction_window_pre_suc_n(context, target, start, end, n = 3):\n",
    "    ctx_tokens_pre = ctx_extraction_window_pre_n(context, target, start, end, n)\n",
    "    ctx_tokens_suc = ctx_extraction_window_suc_n(context, target, start, end, n)\n",
    "    ctx_tokens_pre.extend(ctx_tokens_suc)\n",
    "    return ctx_tokens_pre\n",
    "\n",
    "def ctx_extraction_dep_in(context, target):\n",
    "    return [triple[0][0] for parse in dependencyParser.raw_parse(context)\n",
    "            for triple in list(parse.triples()) if triple[2][0] == target]\n",
    "\n",
    "def ctx_extraction_dep_out(context, target):\n",
    "    return [triple[2][0] for parse in dependencyParser.raw_parse(context)\n",
    "            for triple in list(parse.triples()) if triple[0][0] == target]\n",
    "\n",
    "def ctx_extraction_dep_in_out(context, target):\n",
    "    ctx_tokens_in = ctx_extraction_dep_in(context, target)\n",
    "    ctx_tokens_out = ctx_extraction_dep_out(context, target)\n",
    "    ctx_tokens_in.extend(ctx_tokens_out)\n",
    "    return ctx_tokens_in\n",
    "\n",
    "def ctx_extraction_dep_recu_in_n_steps(context, target, n = 2):\n",
    "    deps = [triple for parse in dependencyParser.raw_parse(context)\n",
    "            for triple in parse.triples()]\n",
    "    result_tokens = []\n",
    "    curr_target = [target]\n",
    "    for step in range(0, n):\n",
    "        step_result = [triple[0][0] for triple in deps \n",
    "                       if triple[2][0] in curr_target]\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "    return list(set(result_tokens))\n",
    "\n",
    "def ctx_extraction_dep_recu_out_n_steps(context, target, n = 2):\n",
    "    deps = [triple for parse in dependencyParser.raw_parse(context)\n",
    "            for triple in parse.triples()]\n",
    "    result_tokens = []\n",
    "    curr_target = [target]\n",
    "    for step in range(0, n):\n",
    "        step_result = [triple[2][0] for triple in deps \n",
    "                       if triple[0][0] in curr_target]\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "    return list(set(result_tokens))\n",
    "\n",
    "def ctx_extraction_dep_recu_in_out_n_steps(context, target, n = 2):\n",
    "    deps = [triple for parse in dependencyParser.raw_parse(context)\n",
    "            for triple in parse.triples()]\n",
    "    result_tokens = []\n",
    "    curr_target = [target]\n",
    "    for step in range(0, n):\n",
    "        step_result = [triple[2][0] for triple in deps \n",
    "                       if triple[0][0] in curr_target]\n",
    "        step_result_out = [triple[0][0] for triple in deps \n",
    "                       if triple[2][0] in curr_target]\n",
    "        step_result.extend(step_result_out)\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "    return list(set(result_tokens))\n",
    "\n",
    "def ctx_extraction_dep_recu_in_cover(context, target, cover = 0.1):\n",
    "    deps = [triple for parse in dependencyParser.raw_parse(context)\n",
    "            for triple in parse.triples()]\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    ctx_tokens_post = post_process_ctx(ctx_tokens)\n",
    "    result_tokens = []\n",
    "    curr_target = [target]\n",
    "    curr_cover = 0\n",
    "    while curr_cover < cover:\n",
    "        step_result = [triple[0][0] for triple in deps \n",
    "                       if triple[2][0] in curr_target]\n",
    "        if set(step_result) == set(curr_target):\n",
    "                break\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "        curr_cover = len(result_tokens) / len(ctx_tokens_post)\n",
    "    return list(set(result_tokens))\n",
    "\n",
    "def ctx_extraction_dep_recu_out_cover(context, target, cover = 0.1):\n",
    "    deps = [triple for parse in dependencyParser.raw_parse(context)\n",
    "            for triple in parse.triples()]\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    ctx_tokens_post = post_process_ctx(ctx_tokens)\n",
    "    result_tokens = []\n",
    "    curr_target = [target]\n",
    "    curr_cover = 0\n",
    "    while curr_cover < cover:\n",
    "        step_result = [triple[2][0] for triple in deps \n",
    "                       if triple[0][0] in curr_target]\n",
    "        if set(step_result) == set(curr_target):\n",
    "                break\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "        curr_cover = len(result_tokens) / len(ctx_tokens_post)\n",
    "    return list(set(result_tokens))\n",
    "\n",
    "def ctx_extraction_dep_recu_in_out_cover(context, target, cover = 0.1):\n",
    "    deps = [triple for parse in dependencyParser.raw_parse(context)\n",
    "            for triple in parse.triples()]\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    ctx_tokens_post = post_process_ctx(ctx_tokens)\n",
    "    result_tokens = []\n",
    "    curr_target = [target]\n",
    "    curr_cover = 0\n",
    "    while curr_cover < cover:\n",
    "        step_result = [triple[2][0] for triple in deps \n",
    "                       if triple[0][0] in curr_target]\n",
    "        step_result_out = [triple[0][0] for triple in deps \n",
    "                       if triple[2][0] in curr_target]\n",
    "        step_result.extend(step_result_out)\n",
    "        if set(step_result) == set(curr_target):\n",
    "                break\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "        curr_cover = len(result_tokens) / len(ctx_tokens_post)\n",
    "    print(curr_cover)\n",
    "    return list(set(result_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Normally, the land will be passed down by future generations in a way \" + \\\n",
    "             \"that recognizes the community's traditional connection to that country \"\n",
    "target = 'passed'\n",
    "\n",
    "print('ctx_etraction_all:')\n",
    "print(ctx_extraction_all_filtered(sentence, target))\n",
    "\n",
    "print('ctx_extraction_window_pre_n:')\n",
    "print(ctx_extraction_window_pre_n(sentence, \"Normally\"))\n",
    "print(ctx_extraction_window_pre_n(sentence, \"the\"))\n",
    "print(ctx_extraction_window_pre_n(sentence, \"land\"))\n",
    "print(ctx_extraction_window_pre_n(sentence, target, n = 5))\n",
    "\n",
    "print('ctx_extraction_window_suc_n:')\n",
    "print(ctx_extraction_window_suc_n(sentence, \"country\"))\n",
    "print(ctx_extraction_window_suc_n(sentence, \"to\"))\n",
    "print(ctx_extraction_window_suc_n(sentence, \"connection\"))\n",
    "print(ctx_extraction_window_suc_n(sentence, \"community\", n = 5))\n",
    "\n",
    "print('ctx_extraction_window_pre_suc_n:')\n",
    "print(ctx_extraction_window_pre_suc_n(sentence, \"passed\"))\n",
    "print(ctx_extraction_window_pre_suc_n(sentence, \"the\"))\n",
    "print(ctx_extraction_window_pre_suc_n(sentence, \"to\"))\n",
    "\n",
    "print('ctx_extraction_dep_in:')\n",
    "print(ctx_extraction_dep_in(sentence, \"land\"))\n",
    "\n",
    "print('ctx_extraction_dep_out:')\n",
    "print(ctx_extraction_dep_out(sentence, target))\n",
    "print(ctx_extraction_dep_out(sentence, \"land\"))\n",
    "\n",
    "print('ctx_extraction_dep_in_out:')\n",
    "print(ctx_extraction_dep_in_out(sentence, \"land\"))\n",
    "\n",
    "print('ctx_extraction_dep_recu_in_n_steps:')\n",
    "print(ctx_extraction_dep_recu_in_n_steps(sentence, \"the\", n = 3))\n",
    "\n",
    "print('ctx_extraction_dep_recu_out_n_steps:')\n",
    "print(ctx_extraction_dep_recu_out_n_steps(sentence, \"the\"))\n",
    "\n",
    "print('ctx_extraction_dep_recu_in_out_n_steps:')\n",
    "print(ctx_extraction_dep_recu_in_out_n_steps(sentence, \"the\"))\n",
    "\n",
    "print('ctx_extraction_dep_recu_in_cover:')\n",
    "print(ctx_extraction_dep_recu_in_cover(sentence, \"the\", cover=0.1))\n",
    "\n",
    "print('ctx_extraction_dep_recu_out_cover:')\n",
    "print(ctx_extraction_dep_recu_out_cover(sentence, \"the\", cover=0.1))\n",
    "\n",
    "print('ctx_extraction_dep_recu_in_out_cover:')\n",
    "print(ctx_extraction_dep_recu_in_out_cover(sentence, \"the\", cover=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Context Extraction\n",
    "\n",
    "After we defined all the context extraction approaches, we can apply them on the actual dataset. To do so, we first extract all the distinct sentences from the actual training set and create a new dataframe containing only the sentence ids, the sentence, the target and all the computed contexts. This also makes it easier to integrate context extraction functions implemented in other languages. Afterwards we can compute the context features and join them back with the target features dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>target</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>ctx_extraction_window_pre_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to fut...</td>\n",
       "      <td>passed</td>\n",
       "      <td>28</td>\n",
       "      <td>34</td>\n",
       "      <td>[land, will, be]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to fut...</td>\n",
       "      <td>land</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>[Normally, the]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to fut...</td>\n",
       "      <td>future</td>\n",
       "      <td>43</td>\n",
       "      <td>49</td>\n",
       "      <td>[passed, down, to]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to fut...</td>\n",
       "      <td>future generations</td>\n",
       "      <td>43</td>\n",
       "      <td>61</td>\n",
       "      <td>[passed, down, to]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to fut...</td>\n",
       "      <td>generations</td>\n",
       "      <td>50</td>\n",
       "      <td>61</td>\n",
       "      <td>[down, to, future]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               id  \\\n",
       "0  3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "1  3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "2  3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "3  3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "4  3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "\n",
       "                                            sentence              target  \\\n",
       "0  Normally , the land will be passed down to fut...              passed   \n",
       "1  Normally , the land will be passed down to fut...                land   \n",
       "2  Normally , the land will be passed down to fut...              future   \n",
       "3  Normally , the land will be passed down to fut...  future generations   \n",
       "4  Normally , the land will be passed down to fut...         generations   \n",
       "\n",
       "   start  end ctx_extraction_window_pre_n  \n",
       "0     28   34            [land, will, be]  \n",
       "1     15   19             [Normally, the]  \n",
       "2     43   49          [passed, down, to]  \n",
       "3     43   61          [passed, down, to]  \n",
       "4     50   61          [down, to, future]  "
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_context = df.loc[:, ['id', 'sentence', 'target', 'start', 'end']]\n",
    "#df_context['ctx_avg_word_length'] = \\\n",
    "#    df_context[['sentence', 'target']].apply(lambda vals : \n",
    "#                               ctx_extraction_window_pre_suc_n(vals[0], vals[1]), axis = 1)\n",
    "df_context['ctx_extraction_window_pre_n'] = df.apply(lambda columns : \n",
    "                                        ctx_extraction_window_pre_n(columns['sentence'], columns['target'], \\\n",
    "                                                                   columns['start'], columns['end']), axis = 1)\n",
    "df_context.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "Here we compute individual feature importance based on different metrics. For example, we implement and compute the F-Score, providing an idea of the discrimination power the feature has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_importance_f_score(dataframe, feat_name, label_name):\n",
    "    df = dataframe.copy()\n",
    "    mean_feat = np.mean(df.loc[:, [feat_name]])[0]\n",
    "    means = df.loc[: , [feat_name, label_name]].groupby(label_name).mean().reset_index()\n",
    "    mean_negativ = means.loc[means[label_name] == 0, [feat_name]][feat_name][0]\n",
    "    mean_positiv = means.loc[means[label_name] == 1, [feat_name]][feat_name][1]\n",
    "    # Compute the sum of deviations of the class mean from the overall mean\n",
    "    class_mean_devs = (mean_positiv - mean_feat)**2 + (mean_negativ - mean_feat)**2\n",
    "    # Compute neagtive instance based values\n",
    "    neg_inst = df.loc[df[label_name] == 0, [feat_name]]\n",
    "    std_dev_neg = (np.sum((neg_inst - mean_negativ)**2) / (len(neg_inst) - 1))[feat_name]\n",
    "    #Compute positive instance based values\n",
    "    pos_inst = df.loc[df[label_name] == 1, [feat_name]]\n",
    "    std_dev_pos = (np.sum((pos_inst - mean_positiv)**2) / (len(pos_inst) - 1))[feat_name]\n",
    "    return class_mean_devs / (std_dev_neg + std_dev_pos)\n",
    "\n",
    "def compute_all_feat_importance_metrics(dataframe, label_name):\n",
    "    pass\n",
    "    \n",
    "\n",
    "df_feat = df.drop(['id', 'sentence', 'target', 'nat', 'non_nat', \n",
    "                   'nat_marked', 'non_nat_marked', 'prob'], axis = 1)\n",
    "print(df_feat.mean())\n",
    "print(df_feat.groupby('binary').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'id', 'sentence', \"start\", \"end\", \"target\", \n",
    "              \"nat\", \"non_nat\", \"nat_marked\", \"non_nat_marked\", \"binary\", \"prob\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
