{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "First, we load all the data we need into pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.wsd import lesk\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_ENGLISH_WIKIPEDIA = \"../cwishareddataset/traindevset/\" + \\\n",
    "                           \"english/Wikipedia_Train.tsv\"\n",
    "df = pd.read_csv(TRAIN_ENGLISH_WIKIPEDIA, sep = \"\\t\")\n",
    "df.columns = ['id', 'sentence', \"start\", \"end\", \"target\", \n",
    "              \"nat\", \"non_nat\", \"nat_marked\", \"non_nat_marked\", \"binary\", \"prob\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation\n",
    "Since many labels are multi-word expression, we first of all define some aggregation functions that aggregate feature values over multiple tokens. Implementing this seperately allows to easily exchange the used aggregation function and keeps the feature computation functions clean. These feature computation functions should only compute features for a single target word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def agg_feat_num_average(target, func_feature, *args):\n",
    "    return np.mean([func_feature(token, *args) for token in word_tokenize(target)])\n",
    "\n",
    "def agg_feat_num_median(target, func_feature, *args):\n",
    "    return np.median([func_feature(token, *args) for token in word_tokenize(target)])\n",
    "\n",
    "def agg_feat_num_max(target, func_feature, *args):\n",
    "    return np.max([func_feature(token, *args) for token in word_tokenize(target)])\n",
    "\n",
    "def agg_feat_num_min(target, func_feature, *args):\n",
    "    return np.min([func_feature(token, *args) for token in word_tokenize(target)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orthographic features\n",
    "Here we start computing simple features like the length of the target word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "df['length'] = df.target.apply(lambda target : agg_feat_num_average(target, len))\n",
    "df['num_words'] = df.target.apply(lambda target : len(word_tokenize(target)))\n",
    "#Relative position of the target word based on tokens\n",
    "df['relative_position'] = df[['sentence', 'target']].apply(lambda vals : \n",
    "            (nltk.word_tokenize(vals[0]).index(vals[1].split()[0])) / len((nltk.word_tokenize(vals[0]))), axis = 1)\n",
    "# Relative positions of the target word based on character counting\n",
    "df['relative_position_left'] = df[['sentence', 'start']].apply(lambda vals : vals[1] / len(vals[0]), axis = 1)\n",
    "df['relative_position_centered'] = df[['sentence', 'start', 'end']].apply(lambda vals : \n",
    "            ((vals[1] + vals[2]) / 2) / len(vals[0]), axis = 1)\n",
    "df['relative_position_right'] = df[['sentence', 'end']].apply(lambda vals : vals[1] / len(vals[0]), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet Features\n",
    "Here we implement all the relevant features based on WordNet and SentiWordNet. For example, the number of synsets the target word is contained in or the average length of the lemmas of all the synsets the target word is contained in. Note that all features that are computed in the following exploit neither the POS-Tag of the target word nor Word Sense Disambiguation by e.g. UKB-Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "def wn_synset_freq(target):\n",
    "    return len(wn.synsets(target))\n",
    "\n",
    "def wn_synset_avg_lemma_freq(target):\n",
    "    return np.nan_to_num(np.mean([len(synset.lemmas()) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_lemma_len(target):\n",
    "    return np.nan_to_num(np.nanmean([len(lemma.name()) \n",
    "            for synset in wn.synsets(target) \n",
    "            for lemma in synset.lemmas()]))\n",
    "\n",
    "def wn_synset_avg_hypernyms(target):\n",
    "    return np.nan_to_num(np.nanmean([len(synset.hypernyms()) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_hyponyms(target):\n",
    "    return np.nan_to_num(np.mean([len(synset.hyponyms()) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_sum_hypernyms(target):\n",
    "    return np.sum(([len(synset.hypernyms()) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_definition_len(target):\n",
    "    return np.nan_to_num(np.mean([len(str(synset.definition())) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_hyptree_depth(target):\n",
    "    return np.nan_to_num(np.mean([synset.max_depth() \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_num_distinct_pos(target):\n",
    "    return len(set([synset.pos() for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_num_relations(target):\n",
    "    return np.nan_to_num(np.mean([np.sum([len(synset.hypernyms()), len(synset.hyponyms()), \n",
    "             len(synset.instance_hypernyms()), len(synset.instance_hyponyms()),\n",
    "             len(synset.member_holonyms()), len(synset.substance_holonyms()),\n",
    "             len(synset.part_holonyms()), len(synset.member_meronyms()),\n",
    "             len(synset.substance_meronyms()), len(synset.part_meronyms())]) \n",
    "             for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_freq_pos(target, pos):\n",
    "    return len(wn.synsets(target, pos = pos))\n",
    "\n",
    "def wn_synset_sense_entropy_uniform(target):\n",
    "    num_senses = len(wn.synsets(target))\n",
    "    return -np.sum([((1 / num_senses) * np.log2(1 / num_senses)) \n",
    "                     for index in range(0, num_senses)])\n",
    "\n",
    "def wn_synset_sense_entropy_pos_uniform(target):\n",
    "    num_senses = len(wn.synsets(target))\n",
    "    pos_distribution = [len(wn.synsets(target, pos = wn.NOUN)),\n",
    "                        len(wn.synsets(target, pos = wn.VERB)),\n",
    "                        len(wn.synsets(target, pos = wn.ADJ)),\n",
    "                        len(wn.synsets(target, pos = wn.ADV))]\n",
    "    return -np.sum([(np.nan_to_num((count / num_senses) * np.log2(count / num_senses))) \n",
    "            for count in pos_distribution]) if num_senses != 0 else 0\n",
    "\n",
    "def wn_synsets_sense_entropy_pos_central(target, pos):\n",
    "    num_senses_pos = len(wn.synsets(target, pos = pos))\n",
    "    return -np.sum([((1 / num_senses_pos) * np.log2(1 / num_senses_pos))\n",
    "                     for index in range(0, num_senses_pos)])\n",
    "     \n",
    "\n",
    "def swn_avg_objective_score(target):\n",
    "    return np.nan_to_num(np.mean([senti_synset.obj_score() \n",
    "                for senti_synset in swn.senti_synsets(target)]))\n",
    "\n",
    "def pos_tag(sentence, target):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    wordPOSPairs = [token for token in nltk.pos_tag(tokens) if token[0] == target]\n",
    "    return wordPOSPairs[0][1] if len(wordPOSPairs) > 0 else None\n",
    "\n",
    "# TODO consider using stanford lemmatizer and compute word similarity metric\n",
    "# to orignal target\n",
    "def wordnet_lemma_len(target):\n",
    "    return len(wordNetLemmatizer.lemmatize(target))\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    if not tag:\n",
    "        return None\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    "    if tag.startswith('V'):\n",
    "        return 'v'\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    "    if tag.startswith('R'):\n",
    "        return 'r'\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\k_kol\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2889: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\k_kol\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: RuntimeWarning: Mean of empty slice\n",
      "  \n",
      "C:\\Users\\k_kol\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: RuntimeWarning: Mean of empty slice\n"
     ]
    }
   ],
   "source": [
    "df['wn_synset_freq'] = df.target.apply(lambda target : agg_feat_num_average(target, wn_synset_freq))\n",
    "df['wn_synset_avg_lemma_freq'] = df.target.apply(lambda target : agg_feat_num_average(target, wn_synset_avg_lemma_freq))\n",
    "df['wn_synset_avg_lemma_len'] = df.target.apply(lambda target : agg_feat_num_average(target, wn_synset_avg_lemma_len))\n",
    "\n",
    "df['wn_synset_diff_len_avg_lemma_len'] = df.wn_synset_avg_lemma_len - df.length\n",
    "df['wn_synset_avg_hypernyms'] = df.target.apply(lambda target : agg_feat_num_average(target, wn_synset_avg_hypernyms))\n",
    "df['wn_synset_sum_hypernyms'] = df.target.apply(lambda target : agg_feat_num_average(target, wn_synset_sum_hypernyms))\n",
    "df['wn_synset_avg_hyponyms'] = df.target.apply(lambda target : agg_feat_num_average(target, wn_synset_avg_hyponyms))\n",
    "\n",
    "df['wn_synset_avg_definition_len'] = df.target.apply(lambda target : \n",
    "                                                     agg_feat_num_average(target, wn_synset_avg_definition_len))\n",
    "df['wn_synset_avg_hyptree_depth'] = df.target.apply(lambda target :\n",
    "                                                     agg_feat_num_average(target, wn_synset_avg_hyptree_depth))\n",
    "df['wn_synset_num_distinct_pos'] = df.target.apply(lambda target : \n",
    "                                                     agg_feat_num_average(target, wn_synset_num_distinct_pos))\n",
    "df['wn_synset_avg_num_relations'] = df.target.apply(lambda target : \n",
    "                                                     agg_feat_num_average(target, wn_synset_avg_num_relations))\n",
    "\n",
    "df['wn_synset_avg_freq_pos_noun'] = df.target.apply(lambda target : \n",
    "                                                    agg_feat_num_average(target, wn_synset_avg_freq_pos, wn.NOUN))\n",
    "df['wn_synset_avg_freq_pos_verb'] = df.target.apply(lambda target : \n",
    "                                                    agg_feat_num_average(target, wn_synset_avg_freq_pos, wn.VERB))\n",
    "df['wn_synset_avg_freq_pos_adj'] = df.target.apply(lambda target : \n",
    "                                                   agg_feat_num_average(target, wn_synset_avg_freq_pos, wn.ADJ))\n",
    "df['wn_synset_avg_freq_pos_adv'] = df.target.apply(lambda target : \n",
    "                                                   agg_feat_num_average(target, wn_synset_avg_freq_pos, wn.ADV))\n",
    "\n",
    "df['wn_synset_avg_freq_pos_noun_norm'] = np.nan_to_num(df.wn_synset_avg_freq_pos_noun / df.wn_synset_freq)\n",
    "df['wn_synset_avg_freq_pos_verb_norm'] = np.nan_to_num(df.wn_synset_avg_freq_pos_verb / df.wn_synset_freq)\n",
    "df['wn_synset_avg_freq_pos_adj_norm'] = np.nan_to_num(df.wn_synset_avg_freq_pos_adj / df.wn_synset_freq)\n",
    "df['wn_synset_avg_freq_pos_adv_norm'] = np.nan_to_num(df.wn_synset_avg_freq_pos_adv / df.wn_synset_freq)\n",
    "\n",
    "df['pos_tag'] = df[['sentence', 'target']].apply(lambda vals : pos_tag(*vals), axis = 1)\n",
    "df['wn_synset_sense_entropy_uniform'] = df.target.apply(lambda target : \n",
    "                                                        agg_feat_num_average(target, wn_synset_sense_entropy_uniform))\n",
    "df['wn_synset_sense_entropy_pos_uniform'] = df.target.apply(lambda target :\n",
    "                                                        agg_feat_num_average(target, wn_synset_sense_entropy_pos_uniform))\n",
    "df['wn_synsets_sense_entropy_pos_central'] = df[['target', 'pos_tag']].apply(\n",
    "    lambda vals : wn_synsets_sense_entropy_pos_central(vals[0], penn_to_wn(vals[1])), axis = 1)\n",
    "\n",
    "df['swn_avg_objective_score'] = df.target.apply(lambda target : agg_feat_num_average(target, swn_avg_objective_score))\n",
    "\n",
    "df['wordnet_lemma_len'] = df.target.apply(lambda target : agg_feat_num_average(target, wordnet_lemma_len))\n",
    "df['diff_len_wordnet_lemma_len'] = df.length - df.wordnet_lemma_len\n",
    "df['reduction_lemma_len'] = 1 - df.wordnet_lemma_len / df.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[:,['target', 'length', 'wordnet_lemma_len', 'diff_len_wordnet_lemma_len', 'reduction_lemma_len']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PorterStemmer, StanfordNLP and Dependency Tree Features\n",
    "Here we implement features based on the PorterStemmer library from nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "from nltk.stem.wordnet import *\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "\n",
    "java_path = \"C:/Program Files (x86)/Java/jdk1.8.0_144/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "path_to_jar = 'resources/stanford-dependency-parser/stanford-parser.jar'\n",
    "path_to_models_jar = 'resources/stanford-dependency-parser/stanford-parser-3.9.1-models.jar'\n",
    "\n",
    "porterStemmer = PorterStemmer()\n",
    "wordNetLemmatizer = WordNetLemmatizer()\n",
    "nerTagger = StanfordNERTagger('resources/stanford-ner-tagger/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "               'resources/stanford-ner-tagger/stanford-ner.jar',\n",
    "               encoding='utf-8')\n",
    "dependencyParser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "\n",
    "def porter_stem_len(target):\n",
    "    return len(str(porterStemmer.stem(target)))\n",
    "\n",
    "def porter_stemmer_num_steps(target):\n",
    "    stem = target.lower()\n",
    "    applied_steps = 0\n",
    "    if porterStemmer.mode == porterStemmer.NLTK_EXTENSIONS and target in porterStemmer.pool:\n",
    "            return applied_steps\n",
    "    if porterStemmer.mode != porterStemmer.ORIGINAL_ALGORITHM and len(target) <= 2:\n",
    "            return applied_steps\n",
    "    step_funcs = [porterStemmer._step1a, porterStemmer._step1b, porterStemmer._step1c,\n",
    "                  porterStemmer._step2, porterStemmer._step3, porterStemmer._step3,\n",
    "                  porterStemmer._step4, porterStemmer._step5a, porterStemmer._step5b]\n",
    "    for step_func in step_funcs:\n",
    "        stem_step = step_func(stem)\n",
    "        if stem_step != stem:\n",
    "            stem = stem_step\n",
    "            applied_steps += 1\n",
    "    return applied_steps\n",
    "\n",
    "def is_named_entity(sentence, target):\n",
    "    tokenized_sent = word_tokenize(sentence)\n",
    "    tagged_sent = nerTagger.tag(tokenized_sent)\n",
    "    for token, tag in tagged_sent:\n",
    "        if token == target and tag != 'O':\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def named_entity_type(sentence, target):\n",
    "    tokenized_sent = word_tokenize(sentence)\n",
    "    tagged_sent = nerTagger.tag(tokenized_sent)\n",
    "    return [tag for token, tag in tagged_sent if token == target][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porter stemmer stem length, number of applied steps,\n",
    "# difference of stem length to target and reduction ratio\n",
    "df['porter_stem_len'] = df.target.apply(lambda target : agg_feat_num_average(target, porter_stem_len))\n",
    "df['porter_stemmer_num_steps'] = df.target.apply(lambda target : agg_feat_num_average(target, porter_stemmer_num_steps))\n",
    "df['diff_len_stem_len'] = df.length - df.porter_stem_len\n",
    "df['reduction_stem_len'] = 1 - df.porter_stem_len / df.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\k_kol\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\k_kol\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1367: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>porter_stemmer_num_steps</th>\n",
       "      <th>diff_len_stem_len</th>\n",
       "      <th>reduction_stem_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>passed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>land</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>future</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>future generations</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>generations</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>recognizes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>community</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>traditional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>traditional connection to that country</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>country</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>connection</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Aboriginal</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>passing</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Aboriginal land rights legislaton</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>land rights legislaton</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>land</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>legislaton</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rights</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>preceded</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Australia</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Aboriginal</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>number</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>important</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Aboriginal protests</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Stockmen</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>protests</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>including</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Aboriginal</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Strike</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Yolngu</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5520</th>\n",
       "      <td>develops</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5521</th>\n",
       "      <td>yellow</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5522</th>\n",
       "      <td>tail horn</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5523</th>\n",
       "      <td>tail</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5524</th>\n",
       "      <td>horn</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5525</th>\n",
       "      <td>turns</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5526</th>\n",
       "      <td>black</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5527</th>\n",
       "      <td>yellow</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5528</th>\n",
       "      <td>larva</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5529</th>\n",
       "      <td>pupates</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5530</th>\n",
       "      <td>grows</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5531</th>\n",
       "      <td>mm</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5532</th>\n",
       "      <td>underground</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5533</th>\n",
       "      <td>underground chamber</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5534</th>\n",
       "      <td>chamber</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5535</th>\n",
       "      <td>terminology</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5536</th>\n",
       "      <td>actor</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5537</th>\n",
       "      <td>actress</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5538</th>\n",
       "      <td>female</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5539</th>\n",
       "      <td>dramatic</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5540</th>\n",
       "      <td>person</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5541</th>\n",
       "      <td>acts</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5542</th>\n",
       "      <td>comic</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5543</th>\n",
       "      <td>production</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5544</th>\n",
       "      <td>capacity</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5545</th>\n",
       "      <td>works</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5546</th>\n",
       "      <td>film</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5547</th>\n",
       "      <td>television</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5548</th>\n",
       "      <td>theater</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5549</th>\n",
       "      <td>radio</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5550 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      target  length  num_words  \\\n",
       "0                                     passed     1.0          1   \n",
       "1                                       land     1.0          1   \n",
       "2                                     future     1.0          1   \n",
       "3                         future generations     1.0          2   \n",
       "4                                generations     1.0          1   \n",
       "5                                 recognizes     1.0          1   \n",
       "6                                  community     1.0          1   \n",
       "7                                traditional     1.0          1   \n",
       "8     traditional connection to that country     1.0          5   \n",
       "9                                    country     1.0          1   \n",
       "10                                connection     1.0          1   \n",
       "11                                Aboriginal     1.0          1   \n",
       "12                                   passing     1.0          1   \n",
       "13         Aboriginal land rights legislaton     1.0          4   \n",
       "14                    land rights legislaton     1.0          3   \n",
       "15                                      land     1.0          1   \n",
       "16                                legislaton     1.0          1   \n",
       "17                                    rights     1.0          1   \n",
       "18                                  preceded     1.0          1   \n",
       "19                                 Australia     1.0          1   \n",
       "20                                Aboriginal     1.0          1   \n",
       "21                                    number     1.0          1   \n",
       "22                                 important     1.0          1   \n",
       "23                       Aboriginal protests     1.0          2   \n",
       "24                                  Stockmen     1.0          1   \n",
       "25                                  protests     1.0          1   \n",
       "26                                 including     1.0          1   \n",
       "27                                Aboriginal     1.0          1   \n",
       "28                                    Strike     1.0          1   \n",
       "29                                    Yolngu     1.0          1   \n",
       "...                                      ...     ...        ...   \n",
       "5520                                develops     1.0          1   \n",
       "5521                                  yellow     1.0          1   \n",
       "5522                               tail horn     1.0          2   \n",
       "5523                                    tail     1.0          1   \n",
       "5524                                    horn     1.0          1   \n",
       "5525                                   turns     1.0          1   \n",
       "5526                                   black     1.0          1   \n",
       "5527                                  yellow     1.0          1   \n",
       "5528                                   larva     1.0          1   \n",
       "5529                                 pupates     1.0          1   \n",
       "5530                                   grows     1.0          1   \n",
       "5531                                      mm     1.0          1   \n",
       "5532                             underground     1.0          1   \n",
       "5533                     underground chamber     1.0          2   \n",
       "5534                                 chamber     1.0          1   \n",
       "5535                             terminology     1.0          1   \n",
       "5536                                   actor     1.0          1   \n",
       "5537                                 actress     1.0          1   \n",
       "5538                                  female     1.0          1   \n",
       "5539                                dramatic     1.0          1   \n",
       "5540                                  person     1.0          1   \n",
       "5541                                    acts     1.0          1   \n",
       "5542                                   comic     1.0          1   \n",
       "5543                              production     1.0          1   \n",
       "5544                                capacity     1.0          1   \n",
       "5545                                   works     1.0          1   \n",
       "5546                                    film     1.0          1   \n",
       "5547                              television     1.0          1   \n",
       "5548                                 theater     1.0          1   \n",
       "5549                                   radio     1.0          1   \n",
       "\n",
       "      porter_stemmer_num_steps  diff_len_stem_len  reduction_stem_len  \n",
       "0                          NaN                NaN                 NaN  \n",
       "1                          NaN                NaN                 NaN  \n",
       "2                          NaN                NaN                 NaN  \n",
       "3                          NaN                NaN                 NaN  \n",
       "4                          NaN                NaN                 NaN  \n",
       "5                          NaN                NaN                 NaN  \n",
       "6                          NaN                NaN                 NaN  \n",
       "7                          NaN                NaN                 NaN  \n",
       "8                          NaN                NaN                 NaN  \n",
       "9                          NaN                NaN                 NaN  \n",
       "10                         NaN                NaN                 NaN  \n",
       "11                         NaN                NaN                 NaN  \n",
       "12                         NaN                NaN                 NaN  \n",
       "13                         NaN                NaN                 NaN  \n",
       "14                         NaN                NaN                 NaN  \n",
       "15                         NaN                NaN                 NaN  \n",
       "16                         NaN                NaN                 NaN  \n",
       "17                         NaN                NaN                 NaN  \n",
       "18                         NaN                NaN                 NaN  \n",
       "19                         NaN                NaN                 NaN  \n",
       "20                         NaN                NaN                 NaN  \n",
       "21                         NaN                NaN                 NaN  \n",
       "22                         NaN                NaN                 NaN  \n",
       "23                         NaN                NaN                 NaN  \n",
       "24                         NaN                NaN                 NaN  \n",
       "25                         NaN                NaN                 NaN  \n",
       "26                         NaN                NaN                 NaN  \n",
       "27                         NaN                NaN                 NaN  \n",
       "28                         NaN                NaN                 NaN  \n",
       "29                         NaN                NaN                 NaN  \n",
       "...                        ...                ...                 ...  \n",
       "5520                       NaN                NaN                 NaN  \n",
       "5521                       NaN                NaN                 NaN  \n",
       "5522                       NaN                NaN                 NaN  \n",
       "5523                       NaN                NaN                 NaN  \n",
       "5524                       NaN                NaN                 NaN  \n",
       "5525                       NaN                NaN                 NaN  \n",
       "5526                       NaN                NaN                 NaN  \n",
       "5527                       NaN                NaN                 NaN  \n",
       "5528                       NaN                NaN                 NaN  \n",
       "5529                       NaN                NaN                 NaN  \n",
       "5530                       NaN                NaN                 NaN  \n",
       "5531                       NaN                NaN                 NaN  \n",
       "5532                       NaN                NaN                 NaN  \n",
       "5533                       NaN                NaN                 NaN  \n",
       "5534                       NaN                NaN                 NaN  \n",
       "5535                       NaN                NaN                 NaN  \n",
       "5536                       NaN                NaN                 NaN  \n",
       "5537                       NaN                NaN                 NaN  \n",
       "5538                       NaN                NaN                 NaN  \n",
       "5539                       NaN                NaN                 NaN  \n",
       "5540                       NaN                NaN                 NaN  \n",
       "5541                       NaN                NaN                 NaN  \n",
       "5542                       NaN                NaN                 NaN  \n",
       "5543                       NaN                NaN                 NaN  \n",
       "5544                       NaN                NaN                 NaN  \n",
       "5545                       NaN                NaN                 NaN  \n",
       "5546                       NaN                NaN                 NaN  \n",
       "5547                       NaN                NaN                 NaN  \n",
       "5548                       NaN                NaN                 NaN  \n",
       "5549                       NaN                NaN                 NaN  \n",
       "\n",
       "[5550 rows x 6 columns]"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, ['target', 'length', 'num_words', 'porter_stemmer_num_steps', 'diff_len_stem_len', 'reduction_stem_len']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context-Aware Features\n",
    "Here we compute not only the context extraction/definition in the first place but also the corresponding context features afterwards. Also we need to implement proper strategies to cope with the target occuring multiple times in the sentence. To avoid mistakes, we should use the actual start and end tags from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Context-Token Aggregation\n",
    "First we define how feature values of multiple context-tokens should be aggreagated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def agg_feat_num_average(tokens, func_feature, *args):\n",
    "    return np.mean([func_feature(token, *args) for token in tokens])\n",
    "\n",
    "def agg_feat_num_median(tokens, func_feature, *args):\n",
    "    return np.median([func_feature(token, *args) for token in tokens])\n",
    "\n",
    "def agg_feat_num_max(tokens, func_feature, *args):\n",
    "    return np.max([func_feature(token, *args) for token in tokens])\n",
    "\n",
    "def agg_feat_num_min(tokens, func_feature, *args):\n",
    "    return np.min([func_feature(token, *args) for token in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Context Definition\n",
    "Here we compute different kinds of context definitions. For example, as a baseline we extract all tokens from the sentence except the target. A second approach is to use a n preceeding or n succeding tokens, or a combined window apporach were we extract n tokens preceeding and succeding of the target. A more sophisticated apporach involves dependency parsing of the sentence and applying different extraction heuristics. Finally we also implement a context extraction approach exploting FrameNet semantic parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>target</th>\n",
       "      <th>nat</th>\n",
       "      <th>non_nat</th>\n",
       "      <th>nat_marked</th>\n",
       "      <th>non_nat_marked</th>\n",
       "      <th>binary</th>\n",
       "      <th>prob</th>\n",
       "      <th>length</th>\n",
       "      <th>relative_position</th>\n",
       "      <th>relative_position_left</th>\n",
       "      <th>relative_position_centered</th>\n",
       "      <th>relative_position_right</th>\n",
       "      <th>mult_target</th>\n",
       "      <th>num_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>32L724R85LLGOQ18KTH5U7G6HTKPID</td>\n",
       "      <td>The Russian military is divided into the follo...</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>Russian</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.055147</td>\n",
       "      <td>0.080882</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>32L724R85LLGOQ18KTH5U7G6HTKPID</td>\n",
       "      <td>The Russian military is divided into the follo...</td>\n",
       "      <td>66</td>\n",
       "      <td>73</td>\n",
       "      <td>Russian</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.485294</td>\n",
       "      <td>0.511029</td>\n",
       "      <td>0.536765</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>32L724R85LLGOQ18KTH5U7G6HTKPID</td>\n",
       "      <td>The Russian military is divided into the follo...</td>\n",
       "      <td>94</td>\n",
       "      <td>101</td>\n",
       "      <td>Russian</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.691176</td>\n",
       "      <td>0.716912</td>\n",
       "      <td>0.742647</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>32L724R85LLGOQ18KTH5U7G6HTKPID</td>\n",
       "      <td>The Russian military is divided into the follo...</td>\n",
       "      <td>117</td>\n",
       "      <td>124</td>\n",
       "      <td>Russian</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.860294</td>\n",
       "      <td>0.886029</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1861</th>\n",
       "      <td>3SCKNODZ0XHJWL8ZLN0GZTL23Y8N7K</td>\n",
       "      <td>It is the tallest building in California , the...</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>tallest</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.081081</td>\n",
       "      <td>0.051813</td>\n",
       "      <td>0.069948</td>\n",
       "      <td>0.088083</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id  \\\n",
       "448   32L724R85LLGOQ18KTH5U7G6HTKPID   \n",
       "452   32L724R85LLGOQ18KTH5U7G6HTKPID   \n",
       "455   32L724R85LLGOQ18KTH5U7G6HTKPID   \n",
       "457   32L724R85LLGOQ18KTH5U7G6HTKPID   \n",
       "1861  3SCKNODZ0XHJWL8ZLN0GZTL23Y8N7K   \n",
       "\n",
       "                                               sentence  start  end   target  \\\n",
       "448   The Russian military is divided into the follo...      4   11  Russian   \n",
       "452   The Russian military is divided into the follo...     66   73  Russian   \n",
       "455   The Russian military is divided into the follo...     94  101  Russian   \n",
       "457   The Russian military is divided into the follo...    117  124  Russian   \n",
       "1861  It is the tallest building in California , the...     10   17  tallest   \n",
       "\n",
       "      nat  non_nat  nat_marked  non_nat_marked  binary  prob  length  \\\n",
       "448    10       10           0               0       0  0.00     1.0   \n",
       "452    10       10           0               0       0  0.00     1.0   \n",
       "455    10       10           0               0       0  0.00     1.0   \n",
       "457    10       10           0               0       0  0.00     1.0   \n",
       "1861   10       10           0               1       1  0.05     1.0   \n",
       "\n",
       "      relative_position  relative_position_left  relative_position_centered  \\\n",
       "448            0.040000                0.029412                    0.055147   \n",
       "452            0.040000                0.485294                    0.511029   \n",
       "455            0.040000                0.691176                    0.716912   \n",
       "457            0.040000                0.860294                    0.886029   \n",
       "1861           0.081081                0.051813                    0.069948   \n",
       "\n",
       "      relative_position_right  mult_target  num_words  \n",
       "448                  0.080882          4.0          1  \n",
       "452                  0.536765          4.0          1  \n",
       "455                  0.742647          4.0          1  \n",
       "457                  0.911765          4.0          1  \n",
       "1861                 0.088083          4.0          1  "
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def mult_target(sentence, target):\n",
    "    counter = Counter(word_tokenize(sentence))\n",
    "    targets = word_tokenize(target)\n",
    "    return np.sum([counter[target] for target in targets]) / len(targets)\n",
    "\n",
    "df['mult_target'] = df[['sentence', 'target']].apply(lambda vals : mult_target(*vals), axis = 1)\n",
    "df[df.mult_target == 4].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "from nltk.parse.stanford import StanfordNeuralDependencyParser\n",
    "import os\n",
    "\n",
    "java_path = \"C:/Program Files (x86)/Java/jdk1.8.0_144/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "path_to_jar = 'resources/stanford-dependency-parser/stanford-parser.jar'\n",
    "path_to_models_jar = 'resources/stanford-dependency-parser/stanford-parser-3.9.1-models.jar'\n",
    "\n",
    "dependencyParser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "\n",
    "def post_process_ctx(context):\n",
    "    return [token for token in context if token.isalnum()]\n",
    "\n",
    "def preprocess_target(target):\n",
    "    return target.strip()\n",
    "\n",
    "def target_index_char_based(start, end, ctx_tokens):\n",
    "    size = np.sum([len(token) for token in ctx_tokens]) + len(ctx_tokens)\n",
    "    target_pos = (start + end) / 2\n",
    "    target_pos_rel = target_pos / size\n",
    "    return int(target_pos_rel * len(post_process_ctx(ctx_tokens)))\n",
    "    \n",
    "def ctx_extraction_all(context, target):\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    if target in ctx_tokens:\n",
    "        ctx_tokens.remove(target)\n",
    "    return ctx_tokens\n",
    "\n",
    "def ctx_extraction_all_filtered(context, target):\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    post_ctx_tokens = post_process_ctx(ctx_tokens)\n",
    "    if target in ctx_tokens:\n",
    "        ctx_tokens.remove(target)\n",
    "    return post_process_ctx\n",
    "\n",
    "def ctx_extraction_window_pre_n(context, target, start, end, n = 3):\n",
    "    target = preprocess_target(target)\n",
    "    ctx_tokens = word_tokenize(context[:start])\n",
    "    post_ctx_tokens = post_process_ctx(ctx_tokens)\n",
    "    return post_ctx_tokens[-n:]\n",
    "\n",
    "def ctx_extraction_window_suc_n(context, target, start, end, n = 3):\n",
    "    target = preprocess_target(target)\n",
    "    ctx_tokens = word_tokenize(context[end:])\n",
    "    post_ctx_tokens = post_process_ctx(ctx_tokens)\n",
    "    return post_ctx_tokens[:n]\n",
    "\n",
    "def ctx_extraction_window_pre_suc_n(context, target, start, end, n = 3):\n",
    "    ctx_tokens_pre = ctx_extraction_window_pre_n(context, target, start, end, n)\n",
    "    ctx_tokens_suc = ctx_extraction_window_suc_n(context, target, start, end, n)\n",
    "    ctx_tokens_pre.extend(ctx_tokens_suc)\n",
    "    return ctx_tokens_pre\n",
    "\n",
    "def ctx_extraction_dep_in(context, target):\n",
    "    return [triple[0][0] for parse in dependencyParser.raw_parse(context)\n",
    "            for triple in list(parse.triples()) if triple[2][0] == target]\n",
    "\n",
    "def ctx_extraction_dep_out(context, target):\n",
    "    return [triple[2][0] for parse in dependencyParser.raw_parse(context)\n",
    "            for triple in list(parse.triples()) if triple[0][0] == target]\n",
    "\n",
    "def ctx_extraction_dep_in_out(context, target):\n",
    "    ctx_tokens_in = ctx_extraction_dep_in(context, target)\n",
    "    ctx_tokens_out = ctx_extraction_dep_out(context, target)\n",
    "    ctx_tokens_in.extend(ctx_tokens_out)\n",
    "    return ctx_tokens_in\n",
    "\n",
    "def ctx_extraction_dep_recu_in_n_steps(context, target, n = 2):\n",
    "    deps = [triple for parse in dependencyParser.raw_parse(context)\n",
    "            for triple in parse.triples()]\n",
    "    result_tokens = []\n",
    "    curr_target = [target]\n",
    "    for step in range(0, n):\n",
    "        step_result = [triple[0][0] for triple in deps \n",
    "                       if triple[2][0] in curr_target]\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "    return list(set(result_tokens))\n",
    "\n",
    "def ctx_extraction_dep_recu_out_n_steps(context, target, n = 2):\n",
    "    deps = [triple for parse in dependencyParser.raw_parse(context)\n",
    "            for triple in parse.triples()]\n",
    "    result_tokens = []\n",
    "    curr_target = [target]\n",
    "    for step in range(0, n):\n",
    "        step_result = [triple[2][0] for triple in deps \n",
    "                       if triple[0][0] in curr_target]\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "    return list(set(result_tokens))\n",
    "\n",
    "def ctx_extraction_dep_recu_in_out_n_steps(context, target, n = 2):\n",
    "    deps = [triple for parse in dependencyParser.raw_parse(context)\n",
    "            for triple in parse.triples()]\n",
    "    result_tokens = []\n",
    "    curr_target = [target]\n",
    "    for step in range(0, n):\n",
    "        step_result = [triple[2][0] for triple in deps \n",
    "                       if triple[0][0] in curr_target]\n",
    "        step_result_out = [triple[0][0] for triple in deps \n",
    "                       if triple[2][0] in curr_target]\n",
    "        step_result.extend(step_result_out)\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "    return list(set(result_tokens))\n",
    "\n",
    "def ctx_extraction_dep_recu_in_cover(context, target, cover = 0.1):\n",
    "    deps = [triple for parse in dependencyParser.raw_parse(context)\n",
    "            for triple in parse.triples()]\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    ctx_tokens_post = post_process_ctx(ctx_tokens)\n",
    "    result_tokens = []\n",
    "    curr_target = [target]\n",
    "    curr_cover = 0\n",
    "    while curr_cover < cover:\n",
    "        step_result = [triple[0][0] for triple in deps \n",
    "                       if triple[2][0] in curr_target]\n",
    "        if set(step_result) == set(curr_target):\n",
    "                break\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "        curr_cover = len(result_tokens) / len(ctx_tokens_post)\n",
    "    return list(set(result_tokens))\n",
    "\n",
    "def ctx_extraction_dep_recu_out_cover(context, target, cover = 0.1):\n",
    "    deps = [triple for parse in dependencyParser.raw_parse(context)\n",
    "            for triple in parse.triples()]\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    ctx_tokens_post = post_process_ctx(ctx_tokens)\n",
    "    result_tokens = []\n",
    "    curr_target = [target]\n",
    "    curr_cover = 0\n",
    "    while curr_cover < cover:\n",
    "        step_result = [triple[2][0] for triple in deps \n",
    "                       if triple[0][0] in curr_target]\n",
    "        if set(step_result) == set(curr_target):\n",
    "                break\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "        curr_cover = len(result_tokens) / len(ctx_tokens_post)\n",
    "    return list(set(result_tokens))\n",
    "\n",
    "def ctx_extraction_dep_recu_in_out_cover(context, target, cover = 0.1):\n",
    "    deps = [triple for parse in dependencyParser.raw_parse(context)\n",
    "            for triple in parse.triples()]\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    ctx_tokens_post = post_process_ctx(ctx_tokens)\n",
    "    result_tokens = []\n",
    "    curr_target = [target]\n",
    "    curr_cover = 0\n",
    "    while curr_cover < cover:\n",
    "        step_result = [triple[2][0] for triple in deps \n",
    "                       if triple[0][0] in curr_target]\n",
    "        step_result_out = [triple[0][0] for triple in deps \n",
    "                       if triple[2][0] in curr_target]\n",
    "        step_result.extend(step_result_out)\n",
    "        if set(step_result) == set(curr_target):\n",
    "                break\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "        curr_cover = len(result_tokens) / len(ctx_tokens_post)\n",
    "    print(curr_cover)\n",
    "    return list(set(result_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Normally, the land will be passed down by future generations in a way \" + \\\n",
    "             \"that recognizes the community's traditional connection to that country \"\n",
    "target = 'passed'\n",
    "\n",
    "print('ctx_etraction_all:')\n",
    "print(ctx_extraction_all_filtered(sentence, target))\n",
    "\n",
    "print('ctx_extraction_window_pre_n:')\n",
    "print(ctx_extraction_window_pre_n(sentence, \"Normally\"))\n",
    "print(ctx_extraction_window_pre_n(sentence, \"the\"))\n",
    "print(ctx_extraction_window_pre_n(sentence, \"land\"))\n",
    "print(ctx_extraction_window_pre_n(sentence, target, n = 5))\n",
    "\n",
    "print('ctx_extraction_window_suc_n:')\n",
    "print(ctx_extraction_window_suc_n(sentence, \"country\"))\n",
    "print(ctx_extraction_window_suc_n(sentence, \"to\"))\n",
    "print(ctx_extraction_window_suc_n(sentence, \"connection\"))\n",
    "print(ctx_extraction_window_suc_n(sentence, \"community\", n = 5))\n",
    "\n",
    "print('ctx_extraction_window_pre_suc_n:')\n",
    "print(ctx_extraction_window_pre_suc_n(sentence, \"passed\"))\n",
    "print(ctx_extraction_window_pre_suc_n(sentence, \"the\"))\n",
    "print(ctx_extraction_window_pre_suc_n(sentence, \"to\"))\n",
    "\n",
    "print('ctx_extraction_dep_in:')\n",
    "print(ctx_extraction_dep_in(sentence, \"land\"))\n",
    "\n",
    "print('ctx_extraction_dep_out:')\n",
    "print(ctx_extraction_dep_out(sentence, target))\n",
    "print(ctx_extraction_dep_out(sentence, \"land\"))\n",
    "\n",
    "print('ctx_extraction_dep_in_out:')\n",
    "print(ctx_extraction_dep_in_out(sentence, \"land\"))\n",
    "\n",
    "print('ctx_extraction_dep_recu_in_n_steps:')\n",
    "print(ctx_extraction_dep_recu_in_n_steps(sentence, \"the\", n = 3))\n",
    "\n",
    "print('ctx_extraction_dep_recu_out_n_steps:')\n",
    "print(ctx_extraction_dep_recu_out_n_steps(sentence, \"the\"))\n",
    "\n",
    "print('ctx_extraction_dep_recu_in_out_n_steps:')\n",
    "print(ctx_extraction_dep_recu_in_out_n_steps(sentence, \"the\"))\n",
    "\n",
    "print('ctx_extraction_dep_recu_in_cover:')\n",
    "print(ctx_extraction_dep_recu_in_cover(sentence, \"the\", cover=0.1))\n",
    "\n",
    "print('ctx_extraction_dep_recu_out_cover:')\n",
    "print(ctx_extraction_dep_recu_out_cover(sentence, \"the\", cover=0.1))\n",
    "\n",
    "print('ctx_extraction_dep_recu_in_out_cover:')\n",
    "print(ctx_extraction_dep_recu_in_out_cover(sentence, \"the\", cover=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Context Extraction\n",
    "\n",
    "After we defined all the context extraction approaches, we can apply them on the actual dataset. To do so, we first extract all the distinct sentences from the actual training set and create a new dataframe containing only the sentence ids, the sentence, the target and all the computed contexts. This also makes it easier to integrate context extraction functions implemented in other languages. Afterwards we can compute the context features and join them back with the target features dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>target</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>ctx_extraction_window_pre_n</th>\n",
       "      <th>ctx_extraction_window_suc_n</th>\n",
       "      <th>ctx_extraction_window_pre_suc_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to fut...</td>\n",
       "      <td>passed</td>\n",
       "      <td>28</td>\n",
       "      <td>34</td>\n",
       "      <td>[land, will, be]</td>\n",
       "      <td>[down, to, future]</td>\n",
       "      <td>[land, will, be, down, to, future]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to fut...</td>\n",
       "      <td>land</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>[Normally, the]</td>\n",
       "      <td>[will, be, passed]</td>\n",
       "      <td>[Normally, the, will, be, passed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to fut...</td>\n",
       "      <td>future</td>\n",
       "      <td>43</td>\n",
       "      <td>49</td>\n",
       "      <td>[passed, down, to]</td>\n",
       "      <td>[generations, in, a]</td>\n",
       "      <td>[passed, down, to, generations, in, a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to fut...</td>\n",
       "      <td>future generations</td>\n",
       "      <td>43</td>\n",
       "      <td>61</td>\n",
       "      <td>[passed, down, to]</td>\n",
       "      <td>[in, a, way]</td>\n",
       "      <td>[passed, down, to, in, a, way]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to fut...</td>\n",
       "      <td>generations</td>\n",
       "      <td>50</td>\n",
       "      <td>61</td>\n",
       "      <td>[down, to, future]</td>\n",
       "      <td>[in, a, way]</td>\n",
       "      <td>[down, to, future, in, a, way]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to fut...</td>\n",
       "      <td>recognizes</td>\n",
       "      <td>76</td>\n",
       "      <td>86</td>\n",
       "      <td>[a, way, that]</td>\n",
       "      <td>[the, community, traditional]</td>\n",
       "      <td>[a, way, that, the, community, traditional]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to fut...</td>\n",
       "      <td>community</td>\n",
       "      <td>91</td>\n",
       "      <td>100</td>\n",
       "      <td>[that, recognizes, the]</td>\n",
       "      <td>[traditional, connection, to]</td>\n",
       "      <td>[that, recognizes, the, traditional, connectio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to fut...</td>\n",
       "      <td>traditional</td>\n",
       "      <td>104</td>\n",
       "      <td>115</td>\n",
       "      <td>[recognizes, the, community]</td>\n",
       "      <td>[connection, to, that]</td>\n",
       "      <td>[recognizes, the, community, connection, to, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to fut...</td>\n",
       "      <td>traditional connection to that country</td>\n",
       "      <td>104</td>\n",
       "      <td>142</td>\n",
       "      <td>[recognizes, the, community]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[recognizes, the, community]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to fut...</td>\n",
       "      <td>country</td>\n",
       "      <td>135</td>\n",
       "      <td>142</td>\n",
       "      <td>[connection, to, that]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[connection, to, that]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to fut...</td>\n",
       "      <td>connection</td>\n",
       "      <td>116</td>\n",
       "      <td>126</td>\n",
       "      <td>[the, community, traditional]</td>\n",
       "      <td>[to, that, country]</td>\n",
       "      <td>[the, community, traditional, to, that, country]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>Aboriginal</td>\n",
       "      <td>15</td>\n",
       "      <td>25</td>\n",
       "      <td>[The, passing, of]</td>\n",
       "      <td>[land, rights, legislaton]</td>\n",
       "      <td>[The, passing, of, land, rights, legislaton]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>passing</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>[The]</td>\n",
       "      <td>[of, Aboriginal, land]</td>\n",
       "      <td>[The, of, Aboriginal, land]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>Aboriginal land rights legislaton</td>\n",
       "      <td>15</td>\n",
       "      <td>48</td>\n",
       "      <td>[The, passing, of]</td>\n",
       "      <td>[in, Australia, was]</td>\n",
       "      <td>[The, passing, of, in, Australia, was]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>land rights legislaton</td>\n",
       "      <td>26</td>\n",
       "      <td>48</td>\n",
       "      <td>[passing, of, Aboriginal]</td>\n",
       "      <td>[in, Australia, was]</td>\n",
       "      <td>[passing, of, Aboriginal, in, Australia, was]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>land</td>\n",
       "      <td>26</td>\n",
       "      <td>30</td>\n",
       "      <td>[passing, of, Aboriginal]</td>\n",
       "      <td>[rights, legislaton, in]</td>\n",
       "      <td>[passing, of, Aboriginal, rights, legislaton, in]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>legislaton</td>\n",
       "      <td>38</td>\n",
       "      <td>48</td>\n",
       "      <td>[Aboriginal, land, rights]</td>\n",
       "      <td>[in, Australia, was]</td>\n",
       "      <td>[Aboriginal, land, rights, in, Australia, was]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>rights</td>\n",
       "      <td>31</td>\n",
       "      <td>37</td>\n",
       "      <td>[of, Aboriginal, land]</td>\n",
       "      <td>[legislaton, in, Australia]</td>\n",
       "      <td>[of, Aboriginal, land, legislaton, in, Australia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>preceded</td>\n",
       "      <td>66</td>\n",
       "      <td>74</td>\n",
       "      <td>[in, Australia, was]</td>\n",
       "      <td>[by, a, number]</td>\n",
       "      <td>[in, Australia, was, by, a, number]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>Australia</td>\n",
       "      <td>52</td>\n",
       "      <td>61</td>\n",
       "      <td>[rights, legislaton, in]</td>\n",
       "      <td>[was, preceded, by]</td>\n",
       "      <td>[rights, legislaton, in, was, preceded, by]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>Aboriginal</td>\n",
       "      <td>100</td>\n",
       "      <td>110</td>\n",
       "      <td>[number, of, important]</td>\n",
       "      <td>[protests, including, the]</td>\n",
       "      <td>[number, of, important, protests, including, the]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>number</td>\n",
       "      <td>80</td>\n",
       "      <td>86</td>\n",
       "      <td>[preceded, by, a]</td>\n",
       "      <td>[of, important, Aboriginal]</td>\n",
       "      <td>[preceded, by, a, of, important, Aboriginal]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>important</td>\n",
       "      <td>90</td>\n",
       "      <td>99</td>\n",
       "      <td>[a, number, of]</td>\n",
       "      <td>[Aboriginal, protests, including]</td>\n",
       "      <td>[a, number, of, Aboriginal, protests, including]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>Aboriginal protests</td>\n",
       "      <td>100</td>\n",
       "      <td>119</td>\n",
       "      <td>[number, of, important]</td>\n",
       "      <td>[including, the, 1946]</td>\n",
       "      <td>[number, of, important, including, the, 1946]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>Stockmen</td>\n",
       "      <td>152</td>\n",
       "      <td>160</td>\n",
       "      <td>[the, 1946, Aboriginal]</td>\n",
       "      <td>[Strike, the, 1963]</td>\n",
       "      <td>[the, 1946, Aboriginal, Strike, the, 1963]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>protests</td>\n",
       "      <td>111</td>\n",
       "      <td>119</td>\n",
       "      <td>[of, important, Aboriginal]</td>\n",
       "      <td>[including, the, 1946]</td>\n",
       "      <td>[of, important, Aboriginal, including, the, 1946]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>including</td>\n",
       "      <td>122</td>\n",
       "      <td>131</td>\n",
       "      <td>[important, Aboriginal, protests]</td>\n",
       "      <td>[the, 1946, Aboriginal]</td>\n",
       "      <td>[important, Aboriginal, protests, the, 1946, A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>Aboriginal</td>\n",
       "      <td>141</td>\n",
       "      <td>151</td>\n",
       "      <td>[including, the, 1946]</td>\n",
       "      <td>[Stockmen, Strike, the]</td>\n",
       "      <td>[including, the, 1946, Stockmen, Strike, the]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>Strike</td>\n",
       "      <td>164</td>\n",
       "      <td>170</td>\n",
       "      <td>[1946, Aboriginal, Stockmen]</td>\n",
       "      <td>[the, 1963, Yolngu]</td>\n",
       "      <td>[1946, Aboriginal, Stockmen, the, 1963, Yolngu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>Yolngu</td>\n",
       "      <td>182</td>\n",
       "      <td>188</td>\n",
       "      <td>[Strike, the, 1963]</td>\n",
       "      <td>[Bark, Petition, and]</td>\n",
       "      <td>[Strike, the, 1963, Bark, Petition, and]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5520</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>In the third instar , purple or blue edging de...</td>\n",
       "      <td>develops</td>\n",
       "      <td>44</td>\n",
       "      <td>52</td>\n",
       "      <td>[or, blue, edging]</td>\n",
       "      <td>[on, the, yellow]</td>\n",
       "      <td>[or, blue, edging, on, the, yellow]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5521</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>In the third instar , purple or blue edging de...</td>\n",
       "      <td>yellow</td>\n",
       "      <td>60</td>\n",
       "      <td>66</td>\n",
       "      <td>[develops, on, the]</td>\n",
       "      <td>[stripes, and, the]</td>\n",
       "      <td>[develops, on, the, stripes, and, the]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5522</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>In the third instar , purple or blue edging de...</td>\n",
       "      <td>tail horn</td>\n",
       "      <td>83</td>\n",
       "      <td>92</td>\n",
       "      <td>[stripes, and, the]</td>\n",
       "      <td>[turns, from, black]</td>\n",
       "      <td>[stripes, and, the, turns, from, black]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5523</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>In the third instar , purple or blue edging de...</td>\n",
       "      <td>tail</td>\n",
       "      <td>83</td>\n",
       "      <td>87</td>\n",
       "      <td>[stripes, and, the]</td>\n",
       "      <td>[horn, turns, from]</td>\n",
       "      <td>[stripes, and, the, horn, turns, from]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5524</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>In the third instar , purple or blue edging de...</td>\n",
       "      <td>horn</td>\n",
       "      <td>88</td>\n",
       "      <td>92</td>\n",
       "      <td>[and, the, tail]</td>\n",
       "      <td>[turns, from, black]</td>\n",
       "      <td>[and, the, tail, turns, from, black]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5525</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>In the third instar , purple or blue edging de...</td>\n",
       "      <td>turns</td>\n",
       "      <td>93</td>\n",
       "      <td>98</td>\n",
       "      <td>[the, tail, horn]</td>\n",
       "      <td>[from, black, to]</td>\n",
       "      <td>[the, tail, horn, from, black, to]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5526</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>In the third instar , purple or blue edging de...</td>\n",
       "      <td>black</td>\n",
       "      <td>104</td>\n",
       "      <td>109</td>\n",
       "      <td>[horn, turns, from]</td>\n",
       "      <td>[to, yellow]</td>\n",
       "      <td>[horn, turns, from, to, yellow]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5527</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>In the third instar , purple or blue edging de...</td>\n",
       "      <td>yellow</td>\n",
       "      <td>113</td>\n",
       "      <td>119</td>\n",
       "      <td>[from, black, to]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[from, black, to]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5528</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>The larva grows to about 120-130 mm , and pupa...</td>\n",
       "      <td>larva</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>[The]</td>\n",
       "      <td>[grows, to, about]</td>\n",
       "      <td>[The, grows, to, about]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5529</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>The larva grows to about 120-130 mm , and pupa...</td>\n",
       "      <td>pupates</td>\n",
       "      <td>42</td>\n",
       "      <td>49</td>\n",
       "      <td>[about, mm, and]</td>\n",
       "      <td>[in, an, underground]</td>\n",
       "      <td>[about, mm, and, in, an, underground]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5530</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>The larva grows to about 120-130 mm , and pupa...</td>\n",
       "      <td>grows</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>[The, larva]</td>\n",
       "      <td>[to, about, mm]</td>\n",
       "      <td>[The, larva, to, about, mm]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5531</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>The larva grows to about 120-130 mm , and pupa...</td>\n",
       "      <td>mm</td>\n",
       "      <td>33</td>\n",
       "      <td>35</td>\n",
       "      <td>[grows, to, about]</td>\n",
       "      <td>[and, pupates, in]</td>\n",
       "      <td>[grows, to, about, and, pupates, in]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5532</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>The larva grows to about 120-130 mm , and pupa...</td>\n",
       "      <td>underground</td>\n",
       "      <td>56</td>\n",
       "      <td>67</td>\n",
       "      <td>[pupates, in, an]</td>\n",
       "      <td>[chamber]</td>\n",
       "      <td>[pupates, in, an, chamber]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5533</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>The larva grows to about 120-130 mm , and pupa...</td>\n",
       "      <td>underground chamber</td>\n",
       "      <td>56</td>\n",
       "      <td>75</td>\n",
       "      <td>[pupates, in, an]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[pupates, in, an]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5534</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>The larva grows to about 120-130 mm , and pupa...</td>\n",
       "      <td>chamber</td>\n",
       "      <td>68</td>\n",
       "      <td>75</td>\n",
       "      <td>[in, an, underground]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[in, an, underground]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5535</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>terminology</td>\n",
       "      <td>46</td>\n",
       "      <td>57</td>\n",
       "      <td>[for, female, see]</td>\n",
       "      <td>[is, a, person]</td>\n",
       "      <td>[for, female, see, is, a, person]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5536</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>actor</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>[An]</td>\n",
       "      <td>[sometimes, actress, for]</td>\n",
       "      <td>[An, sometimes, actress, for]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5537</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>actress</td>\n",
       "      <td>21</td>\n",
       "      <td>28</td>\n",
       "      <td>[An, actor, sometimes]</td>\n",
       "      <td>[for, female, see]</td>\n",
       "      <td>[An, actor, sometimes, for, female, see]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5538</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>female</td>\n",
       "      <td>33</td>\n",
       "      <td>39</td>\n",
       "      <td>[sometimes, actress, for]</td>\n",
       "      <td>[see, terminology, is]</td>\n",
       "      <td>[sometimes, actress, for, see, terminology, is]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5539</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>dramatic</td>\n",
       "      <td>86</td>\n",
       "      <td>94</td>\n",
       "      <td>[acts, in, a]</td>\n",
       "      <td>[or, comic, production]</td>\n",
       "      <td>[acts, in, a, or, comic, production]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5540</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>person</td>\n",
       "      <td>65</td>\n",
       "      <td>71</td>\n",
       "      <td>[terminology, is, a]</td>\n",
       "      <td>[who, acts, in]</td>\n",
       "      <td>[terminology, is, a, who, acts, in]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5541</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>acts</td>\n",
       "      <td>76</td>\n",
       "      <td>80</td>\n",
       "      <td>[a, person, who]</td>\n",
       "      <td>[in, a, dramatic]</td>\n",
       "      <td>[a, person, who, in, a, dramatic]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5542</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>comic</td>\n",
       "      <td>98</td>\n",
       "      <td>103</td>\n",
       "      <td>[a, dramatic, or]</td>\n",
       "      <td>[production, and, who]</td>\n",
       "      <td>[a, dramatic, or, production, and, who]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5543</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>production</td>\n",
       "      <td>104</td>\n",
       "      <td>114</td>\n",
       "      <td>[dramatic, or, comic]</td>\n",
       "      <td>[and, who, works]</td>\n",
       "      <td>[dramatic, or, comic, and, who, works]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5544</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>capacity</td>\n",
       "      <td>179</td>\n",
       "      <td>187</td>\n",
       "      <td>[radio, in, that]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[radio, in, that]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5545</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>works</td>\n",
       "      <td>123</td>\n",
       "      <td>128</td>\n",
       "      <td>[production, and, who]</td>\n",
       "      <td>[in, film, television]</td>\n",
       "      <td>[production, and, who, in, film, television]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5546</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>film</td>\n",
       "      <td>132</td>\n",
       "      <td>136</td>\n",
       "      <td>[who, works, in]</td>\n",
       "      <td>[television, theater, or]</td>\n",
       "      <td>[who, works, in, television, theater, or]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5547</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>television</td>\n",
       "      <td>139</td>\n",
       "      <td>149</td>\n",
       "      <td>[works, in, film]</td>\n",
       "      <td>[theater, or, radio]</td>\n",
       "      <td>[works, in, film, theater, or, radio]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5548</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>theater</td>\n",
       "      <td>152</td>\n",
       "      <td>159</td>\n",
       "      <td>[in, film, television]</td>\n",
       "      <td>[or, radio, in]</td>\n",
       "      <td>[in, film, television, or, radio, in]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5549</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>radio</td>\n",
       "      <td>165</td>\n",
       "      <td>170</td>\n",
       "      <td>[television, theater, or]</td>\n",
       "      <td>[in, that, capacity]</td>\n",
       "      <td>[television, theater, or, in, that, capacity]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5550 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id  \\\n",
       "0     3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "1     3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "2     3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "3     3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "4     3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "5     3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "6     3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "7     3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "8     3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "9     3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "10    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "11    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "12    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "13    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "14    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "15    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "16    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "17    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "18    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "19    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "20    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "21    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "22    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "23    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "24    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "25    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "26    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "27    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "28    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "29    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "...                              ...   \n",
       "5520  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5521  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5522  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5523  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5524  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5525  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5526  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5527  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5528  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5529  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5530  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5531  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5532  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5533  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5534  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5535  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5536  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5537  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5538  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5539  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5540  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5541  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5542  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5543  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5544  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5545  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5546  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5547  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5548  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5549  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "\n",
       "                                               sentence  \\\n",
       "0     Normally , the land will be passed down to fut...   \n",
       "1     Normally , the land will be passed down to fut...   \n",
       "2     Normally , the land will be passed down to fut...   \n",
       "3     Normally , the land will be passed down to fut...   \n",
       "4     Normally , the land will be passed down to fut...   \n",
       "5     Normally , the land will be passed down to fut...   \n",
       "6     Normally , the land will be passed down to fut...   \n",
       "7     Normally , the land will be passed down to fut...   \n",
       "8     Normally , the land will be passed down to fut...   \n",
       "9     Normally , the land will be passed down to fut...   \n",
       "10    Normally , the land will be passed down to fut...   \n",
       "11    The passing of Aboriginal land rights legislat...   \n",
       "12    The passing of Aboriginal land rights legislat...   \n",
       "13    The passing of Aboriginal land rights legislat...   \n",
       "14    The passing of Aboriginal land rights legislat...   \n",
       "15    The passing of Aboriginal land rights legislat...   \n",
       "16    The passing of Aboriginal land rights legislat...   \n",
       "17    The passing of Aboriginal land rights legislat...   \n",
       "18    The passing of Aboriginal land rights legislat...   \n",
       "19    The passing of Aboriginal land rights legislat...   \n",
       "20    The passing of Aboriginal land rights legislat...   \n",
       "21    The passing of Aboriginal land rights legislat...   \n",
       "22    The passing of Aboriginal land rights legislat...   \n",
       "23    The passing of Aboriginal land rights legislat...   \n",
       "24    The passing of Aboriginal land rights legislat...   \n",
       "25    The passing of Aboriginal land rights legislat...   \n",
       "26    The passing of Aboriginal land rights legislat...   \n",
       "27    The passing of Aboriginal land rights legislat...   \n",
       "28    The passing of Aboriginal land rights legislat...   \n",
       "29    The passing of Aboriginal land rights legislat...   \n",
       "...                                                 ...   \n",
       "5520  In the third instar , purple or blue edging de...   \n",
       "5521  In the third instar , purple or blue edging de...   \n",
       "5522  In the third instar , purple or blue edging de...   \n",
       "5523  In the third instar , purple or blue edging de...   \n",
       "5524  In the third instar , purple or blue edging de...   \n",
       "5525  In the third instar , purple or blue edging de...   \n",
       "5526  In the third instar , purple or blue edging de...   \n",
       "5527  In the third instar , purple or blue edging de...   \n",
       "5528  The larva grows to about 120-130 mm , and pupa...   \n",
       "5529  The larva grows to about 120-130 mm , and pupa...   \n",
       "5530  The larva grows to about 120-130 mm , and pupa...   \n",
       "5531  The larva grows to about 120-130 mm , and pupa...   \n",
       "5532  The larva grows to about 120-130 mm , and pupa...   \n",
       "5533  The larva grows to about 120-130 mm , and pupa...   \n",
       "5534  The larva grows to about 120-130 mm , and pupa...   \n",
       "5535  An actor ( sometimes actress for female ; see ...   \n",
       "5536  An actor ( sometimes actress for female ; see ...   \n",
       "5537  An actor ( sometimes actress for female ; see ...   \n",
       "5538  An actor ( sometimes actress for female ; see ...   \n",
       "5539  An actor ( sometimes actress for female ; see ...   \n",
       "5540  An actor ( sometimes actress for female ; see ...   \n",
       "5541  An actor ( sometimes actress for female ; see ...   \n",
       "5542  An actor ( sometimes actress for female ; see ...   \n",
       "5543  An actor ( sometimes actress for female ; see ...   \n",
       "5544  An actor ( sometimes actress for female ; see ...   \n",
       "5545  An actor ( sometimes actress for female ; see ...   \n",
       "5546  An actor ( sometimes actress for female ; see ...   \n",
       "5547  An actor ( sometimes actress for female ; see ...   \n",
       "5548  An actor ( sometimes actress for female ; see ...   \n",
       "5549  An actor ( sometimes actress for female ; see ...   \n",
       "\n",
       "                                      target  start  end  \\\n",
       "0                                     passed     28   34   \n",
       "1                                       land     15   19   \n",
       "2                                     future     43   49   \n",
       "3                         future generations     43   61   \n",
       "4                                generations     50   61   \n",
       "5                                 recognizes     76   86   \n",
       "6                                  community     91  100   \n",
       "7                                traditional    104  115   \n",
       "8     traditional connection to that country    104  142   \n",
       "9                                    country    135  142   \n",
       "10                                connection    116  126   \n",
       "11                                Aboriginal     15   25   \n",
       "12                                   passing      4   11   \n",
       "13         Aboriginal land rights legislaton     15   48   \n",
       "14                    land rights legislaton     26   48   \n",
       "15                                      land     26   30   \n",
       "16                                legislaton     38   48   \n",
       "17                                    rights     31   37   \n",
       "18                                  preceded     66   74   \n",
       "19                                 Australia     52   61   \n",
       "20                                Aboriginal    100  110   \n",
       "21                                    number     80   86   \n",
       "22                                 important     90   99   \n",
       "23                       Aboriginal protests    100  119   \n",
       "24                                  Stockmen    152  160   \n",
       "25                                  protests    111  119   \n",
       "26                                 including    122  131   \n",
       "27                                Aboriginal    141  151   \n",
       "28                                    Strike    164  170   \n",
       "29                                    Yolngu    182  188   \n",
       "...                                      ...    ...  ...   \n",
       "5520                                develops     44   52   \n",
       "5521                                  yellow     60   66   \n",
       "5522                               tail horn     83   92   \n",
       "5523                                    tail     83   87   \n",
       "5524                                    horn     88   92   \n",
       "5525                                   turns     93   98   \n",
       "5526                                   black    104  109   \n",
       "5527                                  yellow    113  119   \n",
       "5528                                   larva      4    9   \n",
       "5529                                 pupates     42   49   \n",
       "5530                                   grows     10   15   \n",
       "5531                                      mm     33   35   \n",
       "5532                             underground     56   67   \n",
       "5533                     underground chamber     56   75   \n",
       "5534                                 chamber     68   75   \n",
       "5535                             terminology     46   57   \n",
       "5536                                   actor      3    8   \n",
       "5537                                 actress     21   28   \n",
       "5538                                  female     33   39   \n",
       "5539                                dramatic     86   94   \n",
       "5540                                  person     65   71   \n",
       "5541                                    acts     76   80   \n",
       "5542                                   comic     98  103   \n",
       "5543                              production    104  114   \n",
       "5544                                capacity    179  187   \n",
       "5545                                   works    123  128   \n",
       "5546                                    film    132  136   \n",
       "5547                              television    139  149   \n",
       "5548                                 theater    152  159   \n",
       "5549                                   radio    165  170   \n",
       "\n",
       "            ctx_extraction_window_pre_n        ctx_extraction_window_suc_n  \\\n",
       "0                      [land, will, be]                 [down, to, future]   \n",
       "1                       [Normally, the]                 [will, be, passed]   \n",
       "2                    [passed, down, to]               [generations, in, a]   \n",
       "3                    [passed, down, to]                       [in, a, way]   \n",
       "4                    [down, to, future]                       [in, a, way]   \n",
       "5                        [a, way, that]      [the, community, traditional]   \n",
       "6               [that, recognizes, the]      [traditional, connection, to]   \n",
       "7          [recognizes, the, community]             [connection, to, that]   \n",
       "8          [recognizes, the, community]                                 []   \n",
       "9                [connection, to, that]                                 []   \n",
       "10        [the, community, traditional]                [to, that, country]   \n",
       "11                   [The, passing, of]         [land, rights, legislaton]   \n",
       "12                                [The]             [of, Aboriginal, land]   \n",
       "13                   [The, passing, of]               [in, Australia, was]   \n",
       "14            [passing, of, Aboriginal]               [in, Australia, was]   \n",
       "15            [passing, of, Aboriginal]           [rights, legislaton, in]   \n",
       "16           [Aboriginal, land, rights]               [in, Australia, was]   \n",
       "17               [of, Aboriginal, land]        [legislaton, in, Australia]   \n",
       "18                 [in, Australia, was]                    [by, a, number]   \n",
       "19             [rights, legislaton, in]                [was, preceded, by]   \n",
       "20              [number, of, important]         [protests, including, the]   \n",
       "21                    [preceded, by, a]        [of, important, Aboriginal]   \n",
       "22                      [a, number, of]  [Aboriginal, protests, including]   \n",
       "23              [number, of, important]             [including, the, 1946]   \n",
       "24              [the, 1946, Aboriginal]                [Strike, the, 1963]   \n",
       "25          [of, important, Aboriginal]             [including, the, 1946]   \n",
       "26    [important, Aboriginal, protests]            [the, 1946, Aboriginal]   \n",
       "27               [including, the, 1946]            [Stockmen, Strike, the]   \n",
       "28         [1946, Aboriginal, Stockmen]                [the, 1963, Yolngu]   \n",
       "29                  [Strike, the, 1963]              [Bark, Petition, and]   \n",
       "...                                 ...                                ...   \n",
       "5520                 [or, blue, edging]                  [on, the, yellow]   \n",
       "5521                [develops, on, the]                [stripes, and, the]   \n",
       "5522                [stripes, and, the]               [turns, from, black]   \n",
       "5523                [stripes, and, the]                [horn, turns, from]   \n",
       "5524                   [and, the, tail]               [turns, from, black]   \n",
       "5525                  [the, tail, horn]                  [from, black, to]   \n",
       "5526                [horn, turns, from]                       [to, yellow]   \n",
       "5527                  [from, black, to]                                 []   \n",
       "5528                              [The]                 [grows, to, about]   \n",
       "5529                   [about, mm, and]              [in, an, underground]   \n",
       "5530                       [The, larva]                    [to, about, mm]   \n",
       "5531                 [grows, to, about]                 [and, pupates, in]   \n",
       "5532                  [pupates, in, an]                          [chamber]   \n",
       "5533                  [pupates, in, an]                                 []   \n",
       "5534              [in, an, underground]                                 []   \n",
       "5535                 [for, female, see]                    [is, a, person]   \n",
       "5536                               [An]          [sometimes, actress, for]   \n",
       "5537             [An, actor, sometimes]                 [for, female, see]   \n",
       "5538          [sometimes, actress, for]             [see, terminology, is]   \n",
       "5539                      [acts, in, a]            [or, comic, production]   \n",
       "5540               [terminology, is, a]                    [who, acts, in]   \n",
       "5541                   [a, person, who]                  [in, a, dramatic]   \n",
       "5542                  [a, dramatic, or]             [production, and, who]   \n",
       "5543              [dramatic, or, comic]                  [and, who, works]   \n",
       "5544                  [radio, in, that]                                 []   \n",
       "5545             [production, and, who]             [in, film, television]   \n",
       "5546                   [who, works, in]          [television, theater, or]   \n",
       "5547                  [works, in, film]               [theater, or, radio]   \n",
       "5548             [in, film, television]                    [or, radio, in]   \n",
       "5549          [television, theater, or]               [in, that, capacity]   \n",
       "\n",
       "                        ctx_extraction_window_pre_suc_n  \n",
       "0                    [land, will, be, down, to, future]  \n",
       "1                     [Normally, the, will, be, passed]  \n",
       "2                [passed, down, to, generations, in, a]  \n",
       "3                        [passed, down, to, in, a, way]  \n",
       "4                        [down, to, future, in, a, way]  \n",
       "5           [a, way, that, the, community, traditional]  \n",
       "6     [that, recognizes, the, traditional, connectio...  \n",
       "7     [recognizes, the, community, connection, to, t...  \n",
       "8                          [recognizes, the, community]  \n",
       "9                                [connection, to, that]  \n",
       "10     [the, community, traditional, to, that, country]  \n",
       "11         [The, passing, of, land, rights, legislaton]  \n",
       "12                          [The, of, Aboriginal, land]  \n",
       "13               [The, passing, of, in, Australia, was]  \n",
       "14        [passing, of, Aboriginal, in, Australia, was]  \n",
       "15    [passing, of, Aboriginal, rights, legislaton, in]  \n",
       "16       [Aboriginal, land, rights, in, Australia, was]  \n",
       "17    [of, Aboriginal, land, legislaton, in, Australia]  \n",
       "18                  [in, Australia, was, by, a, number]  \n",
       "19          [rights, legislaton, in, was, preceded, by]  \n",
       "20    [number, of, important, protests, including, the]  \n",
       "21         [preceded, by, a, of, important, Aboriginal]  \n",
       "22     [a, number, of, Aboriginal, protests, including]  \n",
       "23        [number, of, important, including, the, 1946]  \n",
       "24           [the, 1946, Aboriginal, Strike, the, 1963]  \n",
       "25    [of, important, Aboriginal, including, the, 1946]  \n",
       "26    [important, Aboriginal, protests, the, 1946, A...  \n",
       "27        [including, the, 1946, Stockmen, Strike, the]  \n",
       "28      [1946, Aboriginal, Stockmen, the, 1963, Yolngu]  \n",
       "29             [Strike, the, 1963, Bark, Petition, and]  \n",
       "...                                                 ...  \n",
       "5520                [or, blue, edging, on, the, yellow]  \n",
       "5521             [develops, on, the, stripes, and, the]  \n",
       "5522            [stripes, and, the, turns, from, black]  \n",
       "5523             [stripes, and, the, horn, turns, from]  \n",
       "5524               [and, the, tail, turns, from, black]  \n",
       "5525                 [the, tail, horn, from, black, to]  \n",
       "5526                    [horn, turns, from, to, yellow]  \n",
       "5527                                  [from, black, to]  \n",
       "5528                            [The, grows, to, about]  \n",
       "5529              [about, mm, and, in, an, underground]  \n",
       "5530                        [The, larva, to, about, mm]  \n",
       "5531               [grows, to, about, and, pupates, in]  \n",
       "5532                         [pupates, in, an, chamber]  \n",
       "5533                                  [pupates, in, an]  \n",
       "5534                              [in, an, underground]  \n",
       "5535                  [for, female, see, is, a, person]  \n",
       "5536                      [An, sometimes, actress, for]  \n",
       "5537           [An, actor, sometimes, for, female, see]  \n",
       "5538    [sometimes, actress, for, see, terminology, is]  \n",
       "5539               [acts, in, a, or, comic, production]  \n",
       "5540                [terminology, is, a, who, acts, in]  \n",
       "5541                  [a, person, who, in, a, dramatic]  \n",
       "5542            [a, dramatic, or, production, and, who]  \n",
       "5543             [dramatic, or, comic, and, who, works]  \n",
       "5544                                  [radio, in, that]  \n",
       "5545       [production, and, who, in, film, television]  \n",
       "5546          [who, works, in, television, theater, or]  \n",
       "5547              [works, in, film, theater, or, radio]  \n",
       "5548              [in, film, television, or, radio, in]  \n",
       "5549      [television, theater, or, in, that, capacity]  \n",
       "\n",
       "[5550 rows x 8 columns]"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_context = df.loc[:, ['id', 'sentence', 'target', 'start', 'end']]\n",
    "#df_context['ctx_avg_word_length'] = \\\n",
    "#    df_context[['sentence', 'target']].apply(lambda vals : \n",
    "#                               ctx_extraction_window_pre_suc_n(vals[0], vals[1]), axis = 1)\n",
    "df_context['ctx_extraction_window_pre_n'] = df.apply(lambda columns : \n",
    "                                        ctx_extraction_window_pre_n(columns['sentence'], columns['target'], \\\n",
    "                                                                   columns['start'], columns['end']), axis = 1)\n",
    "\n",
    "df_context['ctx_extraction_window_suc_n'] = df.apply(lambda columns : \n",
    "                                        ctx_extraction_window_suc_n(columns['sentence'], columns['target'], \\\n",
    "                                                                   columns['start'], columns['end']), axis = 1)\n",
    "\n",
    "df_context['ctx_extraction_window_pre_suc_n'] = df.apply(lambda columns : \n",
    "                                        ctx_extraction_window_pre_suc_n(columns['sentence'], columns['target'], \\\n",
    "                                                                   columns['start'], columns['end']), axis = 1)\n",
    "\n",
    "df_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "Here we compute individual feature importance based on different metrics. For example, we implement and compute the F-Score, providing an idea of the discrimination power the feature has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_importance_f_score(dataframe, feat_name, label_name):\n",
    "    df = dataframe.copy()\n",
    "    mean_feat = np.mean(df.loc[:, [feat_name]])[0]\n",
    "    means = df.loc[: , [feat_name, label_name]].groupby(label_name).mean().reset_index()\n",
    "    mean_negativ = means.loc[means[label_name] == 0, [feat_name]][feat_name][0]\n",
    "    mean_positiv = means.loc[means[label_name] == 1, [feat_name]][feat_name][1]\n",
    "    # Compute the sum of deviations of the class mean from the overall mean\n",
    "    class_mean_devs = (mean_positiv - mean_feat)**2 + (mean_negativ - mean_feat)**2\n",
    "    # Compute neagtive instance based values\n",
    "    neg_inst = df.loc[df[label_name] == 0, [feat_name]]\n",
    "    std_dev_neg = (np.sum((neg_inst - mean_negativ)**2) / (len(neg_inst) - 1))[feat_name]\n",
    "    #Compute positive instance based values\n",
    "    pos_inst = df.loc[df[label_name] == 1, [feat_name]]\n",
    "    std_dev_pos = (np.sum((pos_inst - mean_positiv)**2) / (len(pos_inst) - 1))[feat_name]\n",
    "    return class_mean_devs / (std_dev_neg + std_dev_pos)\n",
    "\n",
    "def compute_all_feat_importance_metrics(dataframe, label_name):\n",
    "    pass\n",
    "    \n",
    "\n",
    "df_feat = df.drop(['id', 'sentence', 'target', 'nat', 'non_nat', \n",
    "                   'nat_marked', 'non_nat_marked', 'prob'], axis = 1)\n",
    "print(df_feat.mean())\n",
    "print(df_feat.groupby('binary').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'id', 'sentence', \"start\", \"end\", \"target\", \n",
    "              \"nat\", \"non_nat\", \"nat_marked\", \"non_nat_marked\", \"binary\", \"prob\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
