{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "First, we load all the data we need into pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.wsd import lesk\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_ENGLISH_WIKIPEDIA = \"../cwishareddataset/traindevset/\" + \\\n",
    "                           \"english/Wikipedia_Train.tsv\"\n",
    "df = pd.read_csv(TRAIN_ENGLISH_WIKIPEDIA, sep = \"\\t\")\n",
    "df.columns = ['id', 'sentence', \"start\", \"end\", \"target\", \n",
    "              \"nat\", \"non_nat\", \"nat_marked\", \"non_nat_marked\", \"binary\", \"prob\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation\n",
    "Since many labels are multi-word expression, we first of all define some aggregation functions that aggregate feature values over multiple tokens. Implementing this seperately allows to easily exchange the used aggregation function and keeps the feature computation functions clean. These feature computation functions should only compute features for a single target word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def agg_feat_num_average(target, func_feature, *args):\n",
    "    return np.mean([func_feature(token, *args) for token in word_tokenize(target)])\n",
    "\n",
    "def agg_feat_num_median(target, func_feature, *args):\n",
    "    return np.median([func_feature(token, *args) for token in word_tokenize(target)])\n",
    "\n",
    "def agg_feat_num_max(target, func_feature, *args):\n",
    "    return np.max([func_feature(token, *args) for token in word_tokenize(target)])\n",
    "\n",
    "def agg_feat_num_min(target, func_feature, *args):\n",
    "    return np.min([func_feature(token, *args) for token in word_tokenize(target)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orthographic features\n",
    "Here we start computing simple features like the length of the target word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "df['length'] = df.target.apply(lambda target : agg_feat_num_average(target, len))\n",
    "df['num_words'] = df.target.apply(lambda target : len(word_tokenize(target)))\n",
    "#Relative position of the target word based on tokens\n",
    "df['relative_position'] = df[['sentence', 'target']].apply(lambda vals : \n",
    "            (nltk.word_tokenize(vals[0]).index(vals[1].split()[0])) / len((nltk.word_tokenize(vals[0]))), axis = 1)\n",
    "# Relative positions of the target word based on character counting\n",
    "df['relative_position_left'] = df[['sentence', 'start']].apply(lambda vals : vals[1] / len(vals[0]), axis = 1)\n",
    "df['relative_position_centered'] = df[['sentence', 'start', 'end']].apply(lambda vals : \n",
    "            ((vals[1] + vals[2]) / 2) / len(vals[0]), axis = 1)\n",
    "df['relative_position_right'] = df[['sentence', 'end']].apply(lambda vals : vals[1] / len(vals[0]), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet Features\n",
    "Here we implement all the relevant features based on WordNet and SentiWordNet. For example, the number of synsets the target word is contained in or the average length of the lemmas of all the synsets the target word is contained in. Note that all features that are computed in the following exploit neither the POS-Tag of the target word nor Word Sense Disambiguation by e.g. UKB-Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "def wn_synset_freq(target):\n",
    "    return len(wn.synsets(target))\n",
    "\n",
    "def wn_synset_avg_lemma_freq(target):\n",
    "    return np.nan_to_num(np.mean([len(synset.lemmas()) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_lemma_len(target):\n",
    "    return np.nan_to_num(np.nanmean([len(lemma.name()) \n",
    "            for synset in wn.synsets(target) \n",
    "            for lemma in synset.lemmas()]))\n",
    "\n",
    "def wn_synset_avg_hypernyms(target):\n",
    "    return np.nan_to_num(np.nanmean([len(synset.hypernyms()) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_hyponyms(target):\n",
    "    return np.nan_to_num(np.mean([len(synset.hyponyms()) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_sum_hypernyms(target):\n",
    "    return np.sum(([len(synset.hypernyms()) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_definition_len(target):\n",
    "    return np.nan_to_num(np.mean([len(str(synset.definition())) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_hyptree_depth(target):\n",
    "    return np.nan_to_num(np.mean([synset.max_depth() \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_num_distinct_pos(target):\n",
    "    return len(set([synset.pos() for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_num_relations(target):\n",
    "    return np.nan_to_num(np.mean([np.sum([len(synset.hypernyms()), len(synset.hyponyms()), \n",
    "             len(synset.instance_hypernyms()), len(synset.instance_hyponyms()),\n",
    "             len(synset.member_holonyms()), len(synset.substance_holonyms()),\n",
    "             len(synset.part_holonyms()), len(synset.member_meronyms()),\n",
    "             len(synset.substance_meronyms()), len(synset.part_meronyms())]) \n",
    "             for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_freq_pos(target, pos):\n",
    "    return len(wn.synsets(target, pos = pos))\n",
    "\n",
    "def wn_synset_sense_entropy_uniform(target):\n",
    "    num_senses = len(wn.synsets(target))\n",
    "    return -np.sum([((1 / num_senses) * np.log2(1 / num_senses)) \n",
    "                     for index in range(0, num_senses)])\n",
    "\n",
    "def wn_synset_sense_entropy_pos_uniform(target):\n",
    "    num_senses = len(wn.synsets(target))\n",
    "    pos_distribution = [len(wn.synsets(target, pos = wn.NOUN)),\n",
    "                        len(wn.synsets(target, pos = wn.VERB)),\n",
    "                        len(wn.synsets(target, pos = wn.ADJ)),\n",
    "                        len(wn.synsets(target, pos = wn.ADV))]\n",
    "    return -np.sum([(np.nan_to_num((count / num_senses) * np.log2(count / num_senses))) \n",
    "            for count in pos_distribution]) if num_senses != 0 else 0\n",
    "\n",
    "def wn_synsets_sense_entropy_pos_central(target, pos):\n",
    "    num_senses_pos = len(wn.synsets(target, pos = pos))\n",
    "    return -np.sum([((1 / num_senses_pos) * np.log2(1 / num_senses_pos))\n",
    "                     for index in range(0, num_senses_pos)])\n",
    "     \n",
    "\n",
    "def swn_avg_objective_score(target):\n",
    "    return np.nan_to_num(np.mean([senti_synset.obj_score() \n",
    "                for senti_synset in swn.senti_synsets(target)]))\n",
    "\n",
    "def pos_tag(sentence, target):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    wordPOSPairs = [token for token in nltk.pos_tag(tokens) if token[0] == target]\n",
    "    return wordPOSPairs[0][1] if len(wordPOSPairs) > 0 else None\n",
    "\n",
    "# TODO consider using stanford lemmatizer and compute word similarity metric\n",
    "# to orignal target\n",
    "def wordnet_lemma_len(target):\n",
    "    return len(wordNetLemmatizer.lemmatize(target))\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    if not tag:\n",
    "        return None\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    "    if tag.startswith('V'):\n",
    "        return 'v'\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    "    if tag.startswith('R'):\n",
    "        return 'r'\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\k_kol\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2889: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\k_kol\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: RuntimeWarning: Mean of empty slice\n",
      "  \n",
      "C:\\Users\\k_kol\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: RuntimeWarning: Mean of empty slice\n"
     ]
    }
   ],
   "source": [
    "df['wn_synset_freq'] = df.target.apply(lambda target : agg_feat_num_average(target, wn_synset_freq))\n",
    "df['wn_synset_avg_lemma_freq'] = df.target.apply(lambda target : agg_feat_num_average(target, wn_synset_avg_lemma_freq))\n",
    "df['wn_synset_avg_lemma_len'] = df.target.apply(lambda target : agg_feat_num_average(target, wn_synset_avg_lemma_len))\n",
    "\n",
    "df['wn_synset_diff_len_avg_lemma_len'] = df.wn_synset_avg_lemma_len - df.length\n",
    "df['wn_synset_avg_hypernyms'] = df.target.apply(lambda target : agg_feat_num_average(target, wn_synset_avg_hypernyms))\n",
    "df['wn_synset_sum_hypernyms'] = df.target.apply(lambda target : agg_feat_num_average(target, wn_synset_sum_hypernyms))\n",
    "df['wn_synset_avg_hyponyms'] = df.target.apply(lambda target : agg_feat_num_average(target, wn_synset_avg_hyponyms))\n",
    "\n",
    "df['wn_synset_avg_definition_len'] = df.target.apply(lambda target : \n",
    "                                                     agg_feat_num_average(target, wn_synset_avg_definition_len))\n",
    "df['wn_synset_avg_hyptree_depth'] = df.target.apply(lambda target :\n",
    "                                                     agg_feat_num_average(target, wn_synset_avg_hyptree_depth))\n",
    "df['wn_synset_num_distinct_pos'] = df.target.apply(lambda target : \n",
    "                                                     agg_feat_num_average(target, wn_synset_num_distinct_pos))\n",
    "df['wn_synset_avg_num_relations'] = df.target.apply(lambda target : \n",
    "                                                     agg_feat_num_average(target, wn_synset_avg_num_relations))\n",
    "\n",
    "df['wn_synset_avg_freq_pos_noun'] = df.target.apply(lambda target : \n",
    "                                                    agg_feat_num_average(target, wn_synset_avg_freq_pos, wn.NOUN))\n",
    "df['wn_synset_avg_freq_pos_verb'] = df.target.apply(lambda target : \n",
    "                                                    agg_feat_num_average(target, wn_synset_avg_freq_pos, wn.VERB))\n",
    "df['wn_synset_avg_freq_pos_adj'] = df.target.apply(lambda target : \n",
    "                                                   agg_feat_num_average(target, wn_synset_avg_freq_pos, wn.ADJ))\n",
    "df['wn_synset_avg_freq_pos_adv'] = df.target.apply(lambda target : \n",
    "                                                   agg_feat_num_average(target, wn_synset_avg_freq_pos, wn.ADV))\n",
    "\n",
    "df['wn_synset_avg_freq_pos_noun_norm'] = np.nan_to_num(df.wn_synset_avg_freq_pos_noun / df.wn_synset_freq)\n",
    "df['wn_synset_avg_freq_pos_verb_norm'] = np.nan_to_num(df.wn_synset_avg_freq_pos_verb / df.wn_synset_freq)\n",
    "df['wn_synset_avg_freq_pos_adj_norm'] = np.nan_to_num(df.wn_synset_avg_freq_pos_adj / df.wn_synset_freq)\n",
    "df['wn_synset_avg_freq_pos_adv_norm'] = np.nan_to_num(df.wn_synset_avg_freq_pos_adv / df.wn_synset_freq)\n",
    "\n",
    "df['pos_tag'] = df[['sentence', 'target']].apply(lambda vals : pos_tag(*vals), axis = 1)\n",
    "df['wn_synset_sense_entropy_uniform'] = df.target.apply(lambda target : \n",
    "                                                        agg_feat_num_average(target, wn_synset_sense_entropy_uniform))\n",
    "df['wn_synset_sense_entropy_pos_uniform'] = df.target.apply(lambda target :\n",
    "                                                        agg_feat_num_average(target, wn_synset_sense_entropy_pos_uniform))\n",
    "df['wn_synsets_sense_entropy_pos_central'] = df[['target', 'pos_tag']].apply(\n",
    "    lambda vals : wn_synsets_sense_entropy_pos_central(vals[0], penn_to_wn(vals[1])), axis = 1)\n",
    "\n",
    "df['swn_avg_objective_score'] = df.target.apply(lambda target : agg_feat_num_average(target, swn_avg_objective_score))\n",
    "\n",
    "df['wordnet_lemma_len'] = df.target.apply(lambda target : agg_feat_num_average(target, wordnet_lemma_len))\n",
    "df['diff_len_wordnet_lemma_len'] = df.length - df.wordnet_lemma_len\n",
    "df['reduction_lemma_len'] = 1 - df.wordnet_lemma_len / df.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[:,['target', 'length', 'wordnet_lemma_len', 'diff_len_wordnet_lemma_len', 'reduction_lemma_len']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PorterStemmer, StanfordNLP and Dependency Tree Features\n",
    "Here we implement features based on the PorterStemmer library from nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "from nltk.stem.wordnet import *\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "\n",
    "java_path = \"C:/Program Files (x86)/Java/jdk1.8.0_144/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "path_to_jar = 'resources/stanford-dependency-parser/stanford-parser.jar'\n",
    "path_to_models_jar = 'resources/stanford-dependency-parser/stanford-parser-3.9.1-models.jar'\n",
    "\n",
    "porterStemmer = PorterStemmer()\n",
    "wordNetLemmatizer = WordNetLemmatizer()\n",
    "nerTagger = StanfordNERTagger('resources/stanford-ner-tagger/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "               'resources/stanford-ner-tagger/stanford-ner.jar',\n",
    "               encoding='utf-8')\n",
    "dependencyParser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "\n",
    "def porter_stem_len(target):\n",
    "    return len(str(porterStemmer.stem(target)))\n",
    "\n",
    "def porter_stemmer_num_steps(target):\n",
    "    stem = target.lower()\n",
    "    applied_steps = 0\n",
    "    if porterStemmer.mode == porterStemmer.NLTK_EXTENSIONS and target in porterStemmer.pool:\n",
    "            return applied_steps\n",
    "    if porterStemmer.mode != porterStemmer.ORIGINAL_ALGORITHM and len(target) <= 2:\n",
    "            return applied_steps\n",
    "    step_funcs = [porterStemmer._step1a, porterStemmer._step1b, porterStemmer._step1c,\n",
    "                  porterStemmer._step2, porterStemmer._step3, porterStemmer._step3,\n",
    "                  porterStemmer._step4, porterStemmer._step5a, porterStemmer._step5b]\n",
    "    for step_func in step_funcs:\n",
    "        stem_step = step_func(stem)\n",
    "        if stem_step != stem:\n",
    "            stem = stem_step\n",
    "            applied_steps += 1\n",
    "    return applied_steps\n",
    "\n",
    "def is_named_entity(sentence, target):\n",
    "    tokenized_sent = word_tokenize(sentence)\n",
    "    tagged_sent = nerTagger.tag(tokenized_sent)\n",
    "    for token, tag in tagged_sent:\n",
    "        if token == target and tag != 'O':\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def named_entity_type(sentence, target):\n",
    "    tokenized_sent = word_tokenize(sentence)\n",
    "    tagged_sent = nerTagger.tag(tokenized_sent)\n",
    "    return [tag for token, tag in tagged_sent if token == target][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porter stemmer stem length, number of applied steps,\n",
    "# difference of stem length to target and reduction ratio\n",
    "df['porter_stem_len'] = df.target.apply(lambda target : agg_feat_num_average(target, porter_stem_len))\n",
    "df['porter_stemmer_num_steps'] = df.target.apply(lambda target : agg_feat_num_average(target, porter_stemmer_num_steps))\n",
    "df['diff_len_stem_len'] = df.length - df.porter_stem_len\n",
    "df['reduction_stem_len'] = 1 - df.porter_stem_len / df.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\k_kol\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\k_kol\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1367: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>length</th>\n",
       "      <th>num_words</th>\n",
       "      <th>porter_stemmer_num_steps</th>\n",
       "      <th>diff_len_stem_len</th>\n",
       "      <th>reduction_stem_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>passed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>land</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>future</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>future generations</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>generations</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>recognizes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>community</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>traditional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>traditional connection to that country</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>country</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>connection</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Aboriginal</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>passing</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Aboriginal land rights legislaton</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>land rights legislaton</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>land</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>legislaton</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rights</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>preceded</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Australia</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Aboriginal</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>number</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>important</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Aboriginal protests</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Stockmen</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>protests</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>including</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Aboriginal</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Strike</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Yolngu</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5520</th>\n",
       "      <td>develops</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5521</th>\n",
       "      <td>yellow</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5522</th>\n",
       "      <td>tail horn</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5523</th>\n",
       "      <td>tail</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5524</th>\n",
       "      <td>horn</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5525</th>\n",
       "      <td>turns</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5526</th>\n",
       "      <td>black</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5527</th>\n",
       "      <td>yellow</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5528</th>\n",
       "      <td>larva</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5529</th>\n",
       "      <td>pupates</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5530</th>\n",
       "      <td>grows</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5531</th>\n",
       "      <td>mm</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5532</th>\n",
       "      <td>underground</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5533</th>\n",
       "      <td>underground chamber</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5534</th>\n",
       "      <td>chamber</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5535</th>\n",
       "      <td>terminology</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5536</th>\n",
       "      <td>actor</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5537</th>\n",
       "      <td>actress</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5538</th>\n",
       "      <td>female</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5539</th>\n",
       "      <td>dramatic</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5540</th>\n",
       "      <td>person</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5541</th>\n",
       "      <td>acts</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5542</th>\n",
       "      <td>comic</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5543</th>\n",
       "      <td>production</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5544</th>\n",
       "      <td>capacity</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5545</th>\n",
       "      <td>works</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5546</th>\n",
       "      <td>film</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5547</th>\n",
       "      <td>television</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5548</th>\n",
       "      <td>theater</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5549</th>\n",
       "      <td>radio</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5550 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      target  length  num_words  \\\n",
       "0                                     passed     1.0          1   \n",
       "1                                       land     1.0          1   \n",
       "2                                     future     1.0          1   \n",
       "3                         future generations     1.0          2   \n",
       "4                                generations     1.0          1   \n",
       "5                                 recognizes     1.0          1   \n",
       "6                                  community     1.0          1   \n",
       "7                                traditional     1.0          1   \n",
       "8     traditional connection to that country     1.0          5   \n",
       "9                                    country     1.0          1   \n",
       "10                                connection     1.0          1   \n",
       "11                                Aboriginal     1.0          1   \n",
       "12                                   passing     1.0          1   \n",
       "13         Aboriginal land rights legislaton     1.0          4   \n",
       "14                    land rights legislaton     1.0          3   \n",
       "15                                      land     1.0          1   \n",
       "16                                legislaton     1.0          1   \n",
       "17                                    rights     1.0          1   \n",
       "18                                  preceded     1.0          1   \n",
       "19                                 Australia     1.0          1   \n",
       "20                                Aboriginal     1.0          1   \n",
       "21                                    number     1.0          1   \n",
       "22                                 important     1.0          1   \n",
       "23                       Aboriginal protests     1.0          2   \n",
       "24                                  Stockmen     1.0          1   \n",
       "25                                  protests     1.0          1   \n",
       "26                                 including     1.0          1   \n",
       "27                                Aboriginal     1.0          1   \n",
       "28                                    Strike     1.0          1   \n",
       "29                                    Yolngu     1.0          1   \n",
       "...                                      ...     ...        ...   \n",
       "5520                                develops     1.0          1   \n",
       "5521                                  yellow     1.0          1   \n",
       "5522                               tail horn     1.0          2   \n",
       "5523                                    tail     1.0          1   \n",
       "5524                                    horn     1.0          1   \n",
       "5525                                   turns     1.0          1   \n",
       "5526                                   black     1.0          1   \n",
       "5527                                  yellow     1.0          1   \n",
       "5528                                   larva     1.0          1   \n",
       "5529                                 pupates     1.0          1   \n",
       "5530                                   grows     1.0          1   \n",
       "5531                                      mm     1.0          1   \n",
       "5532                             underground     1.0          1   \n",
       "5533                     underground chamber     1.0          2   \n",
       "5534                                 chamber     1.0          1   \n",
       "5535                             terminology     1.0          1   \n",
       "5536                                   actor     1.0          1   \n",
       "5537                                 actress     1.0          1   \n",
       "5538                                  female     1.0          1   \n",
       "5539                                dramatic     1.0          1   \n",
       "5540                                  person     1.0          1   \n",
       "5541                                    acts     1.0          1   \n",
       "5542                                   comic     1.0          1   \n",
       "5543                              production     1.0          1   \n",
       "5544                                capacity     1.0          1   \n",
       "5545                                   works     1.0          1   \n",
       "5546                                    film     1.0          1   \n",
       "5547                              television     1.0          1   \n",
       "5548                                 theater     1.0          1   \n",
       "5549                                   radio     1.0          1   \n",
       "\n",
       "      porter_stemmer_num_steps  diff_len_stem_len  reduction_stem_len  \n",
       "0                          NaN                NaN                 NaN  \n",
       "1                          NaN                NaN                 NaN  \n",
       "2                          NaN                NaN                 NaN  \n",
       "3                          NaN                NaN                 NaN  \n",
       "4                          NaN                NaN                 NaN  \n",
       "5                          NaN                NaN                 NaN  \n",
       "6                          NaN                NaN                 NaN  \n",
       "7                          NaN                NaN                 NaN  \n",
       "8                          NaN                NaN                 NaN  \n",
       "9                          NaN                NaN                 NaN  \n",
       "10                         NaN                NaN                 NaN  \n",
       "11                         NaN                NaN                 NaN  \n",
       "12                         NaN                NaN                 NaN  \n",
       "13                         NaN                NaN                 NaN  \n",
       "14                         NaN                NaN                 NaN  \n",
       "15                         NaN                NaN                 NaN  \n",
       "16                         NaN                NaN                 NaN  \n",
       "17                         NaN                NaN                 NaN  \n",
       "18                         NaN                NaN                 NaN  \n",
       "19                         NaN                NaN                 NaN  \n",
       "20                         NaN                NaN                 NaN  \n",
       "21                         NaN                NaN                 NaN  \n",
       "22                         NaN                NaN                 NaN  \n",
       "23                         NaN                NaN                 NaN  \n",
       "24                         NaN                NaN                 NaN  \n",
       "25                         NaN                NaN                 NaN  \n",
       "26                         NaN                NaN                 NaN  \n",
       "27                         NaN                NaN                 NaN  \n",
       "28                         NaN                NaN                 NaN  \n",
       "29                         NaN                NaN                 NaN  \n",
       "...                        ...                ...                 ...  \n",
       "5520                       NaN                NaN                 NaN  \n",
       "5521                       NaN                NaN                 NaN  \n",
       "5522                       NaN                NaN                 NaN  \n",
       "5523                       NaN                NaN                 NaN  \n",
       "5524                       NaN                NaN                 NaN  \n",
       "5525                       NaN                NaN                 NaN  \n",
       "5526                       NaN                NaN                 NaN  \n",
       "5527                       NaN                NaN                 NaN  \n",
       "5528                       NaN                NaN                 NaN  \n",
       "5529                       NaN                NaN                 NaN  \n",
       "5530                       NaN                NaN                 NaN  \n",
       "5531                       NaN                NaN                 NaN  \n",
       "5532                       NaN                NaN                 NaN  \n",
       "5533                       NaN                NaN                 NaN  \n",
       "5534                       NaN                NaN                 NaN  \n",
       "5535                       NaN                NaN                 NaN  \n",
       "5536                       NaN                NaN                 NaN  \n",
       "5537                       NaN                NaN                 NaN  \n",
       "5538                       NaN                NaN                 NaN  \n",
       "5539                       NaN                NaN                 NaN  \n",
       "5540                       NaN                NaN                 NaN  \n",
       "5541                       NaN                NaN                 NaN  \n",
       "5542                       NaN                NaN                 NaN  \n",
       "5543                       NaN                NaN                 NaN  \n",
       "5544                       NaN                NaN                 NaN  \n",
       "5545                       NaN                NaN                 NaN  \n",
       "5546                       NaN                NaN                 NaN  \n",
       "5547                       NaN                NaN                 NaN  \n",
       "5548                       NaN                NaN                 NaN  \n",
       "5549                       NaN                NaN                 NaN  \n",
       "\n",
       "[5550 rows x 6 columns]"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, ['target', 'length', 'num_words', 'porter_stemmer_num_steps', 'diff_len_stem_len', 'reduction_stem_len']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context-Aware Features\n",
    "Here we compute not only the context extraction/definition in the first place but also the corresponding context features afterwards. Also we need to implement proper strategies to cope with the target occuring multiple times in the sentence. To avoid mistakes, we should use the actual start and end tags from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Context-Token Aggregation\n",
    "First we define how feature values of multiple context-tokens should be aggreagated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def agg_ctx_feat_num_average(tokens, func_feature, *args):\n",
    "    return np.mean([func_feature(token, *args) for token in tokens])\n",
    "\n",
    "def agg_ctx_feat_num_median(tokens, func_feature, *args):\n",
    "    return np.median([func_feature(token, *args) for token in tokens])\n",
    "\n",
    "def agg_ctx_feat_num_max(tokens, func_feature, *args):\n",
    "    return np.max([func_feature(token, *args) for token in tokens])\n",
    "\n",
    "def agg_ctx_feat_num_min(tokens, func_feature, *args):\n",
    "    return np.min([func_feature(token, *args) for token in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Context Definition\n",
    "Here we compute different kinds of context definitions. For example, as a baseline we extract all tokens from the sentence except the target. A second approach is to use a n preceeding or n succeding tokens, or a combined window apporach were we extract n tokens preceeding and succeding of the target. A more sophisticated apporach involves dependency parsing of the sentence and applying different extraction heuristics. Finally we also implement a context extraction approach exploting FrameNet semantic parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def mult_target(sentence, target):\n",
    "    counter = Counter(word_tokenize(sentence))\n",
    "    targets = word_tokenize(target)\n",
    "    return np.sum([counter[target] for target in targets]) / len(targets)\n",
    "\n",
    "df['mult_target'] = df[['sentence', 'target']].apply(lambda vals : mult_target(*vals), axis = 1)\n",
    "df[df.mult_target == 4].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "from nltk.parse.stanford import StanfordNeuralDependencyParser\n",
    "import os\n",
    "\n",
    "java_path = \"C:/Program Files (x86)/Java/jdk1.8.0_144/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "path_to_jar = 'resources/stanford-dependency-parser/stanford-parser.jar'\n",
    "path_to_models_jar = 'resources/stanford-dependency-parser/stanford-parser-3.9.1-models.jar'\n",
    "\n",
    "dependencyParser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "\n",
    "def post_process_ctx(context):\n",
    "    return [token for token in context if token.isalnum()]\n",
    "\n",
    "def preprocess_target(target):\n",
    "    return target.strip()\n",
    "\n",
    "def target_index_char_based(start, end, ctx_tokens):\n",
    "    size = np.sum([len(token) for token in ctx_tokens]) + len(ctx_tokens)\n",
    "    target_pos = (start + end) / 2\n",
    "    target_pos_rel = target_pos / size\n",
    "    return int(target_pos_rel * len(post_process_ctx(ctx_tokens)))\n",
    "\n",
    "def targets_with_index(start, end, context):\n",
    "    curr_pos = 0\n",
    "    targets = []\n",
    "    for index, token in enumerate(word_tokenize(context), 1):\n",
    "        targets.append((token, index, curr_pos, (curr_pos + len(token))))\n",
    "        curr_pos += len(token) + 1\n",
    "    print(targets)\n",
    "    return [(target[0], target[1]) for target in targets \\\n",
    "            if target[2] >= start and target[3] <= end]\n",
    "\n",
    "def dependency_parse(sentence):\n",
    "    dependency_parser = dependencyParser.raw_parse(sentence)\n",
    "    dependencies = []\n",
    "    parsetree = list(dependency_parser)[0]\n",
    "    for index, node in parsetree.nodes.items():\n",
    "        for relation, dependant in parsetree.nodes[index]['deps'].items():\n",
    "            triple = ((node['word'], index), relation, \\\n",
    "                      (parsetree.nodes[dependant[0]]['word'], dependant[0]))\n",
    "            if relation != 'root': dependencies.append(triple)\n",
    "    return dependencies\n",
    "\n",
    "def ctx_extraction_all(context, target):\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    if target in ctx_tokens:\n",
    "        ctx_tokens.remove(target)\n",
    "    return ctx_tokens\n",
    "\n",
    "def ctx_extraction_all_filtered(context, target):\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    post_ctx_tokens = post_process_ctx(ctx_tokens)\n",
    "    if target in ctx_tokens:\n",
    "        ctx_tokens.remove(target)\n",
    "    return post_process_ctx\n",
    "\n",
    "def ctx_extraction_window_pre_n(context, target, start, end, n = 3):\n",
    "    target = preprocess_target(target)\n",
    "    ctx_tokens = word_tokenize(context[:start])\n",
    "    post_ctx_tokens = post_process_ctx(ctx_tokens)\n",
    "    return post_ctx_tokens[-n:]\n",
    "\n",
    "def ctx_extraction_window_suc_n(context, target, start, end, n = 3):\n",
    "    target = preprocess_target(target)\n",
    "    ctx_tokens = word_tokenize(context[end:])\n",
    "    post_ctx_tokens = post_process_ctx(ctx_tokens)\n",
    "    return post_ctx_tokens[:n]\n",
    "\n",
    "def ctx_extraction_window_pre_suc_n(context, target, start, end, n = 3):\n",
    "    ctx_tokens_pre = ctx_extraction_window_pre_n(context, target, start, end, n)\n",
    "    ctx_tokens_suc = ctx_extraction_window_suc_n(context, target, start, end, n)\n",
    "    ctx_tokens_pre.extend(ctx_tokens_suc)\n",
    "    return ctx_tokens_pre\n",
    "\n",
    "def ctx_extraction_dep_in(context, target, start, end):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    return list(set([triple[0][0] for triple in triples if triple[2] in targets]))\n",
    "\n",
    "def ctx_extraction_dep_out(context, target, start, end):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    return list(set([triple[2][0] for triple in triples if triple[0] in targets]))\n",
    "\n",
    "def ctx_extraction_dep_in_out(context, target, start, end):\n",
    "    ctx_tokens_in = ctx_extraction_dep_in(context, target, start, end)\n",
    "    ctx_tokens_out = ctx_extraction_dep_out(context, target, start, end)\n",
    "    ctx_tokens_in.extend(ctx_tokens_out)\n",
    "    return list(set(ctx_tokens_in))\n",
    "\n",
    "def ctx_extraction_dep_recu_in_n_steps(context, target, start, end, n = 2):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    result_tokens = []\n",
    "    curr_target = targets\n",
    "    for step in range(0, n):\n",
    "        step_result = [triple[0] for triple in triples \n",
    "                       if triple[2] in curr_target]\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "    return list(set([token[0] for token in result_tokens]))\n",
    "\n",
    "def ctx_extraction_dep_recu_out_n_steps(context, target, start, end, n = 2):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    result_tokens = []\n",
    "    curr_target = targets\n",
    "    for step in range(0, n):\n",
    "        step_result = [triple[2] for triple in triples \n",
    "                       if triple[0] in curr_target]\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "    return list(set([token[0] for token in result_tokens]))\n",
    "\n",
    "def ctx_extraction_dep_recu_in_out_n_steps(context, target, start, end, n = 2):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    result_tokens = []\n",
    "    curr_target = targets\n",
    "    for step in range(0, n):\n",
    "        step_result = [triple[2] for triple in triples \n",
    "                       if triple[0] in curr_target]\n",
    "        step_result_out = [triple[0] for triple in triples \n",
    "                       if triple[2] in curr_target]\n",
    "        step_result.extend(step_result_out)\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "    return list(set([token[0] for token in result_tokens]))\n",
    "\n",
    "def ctx_extraction_dep_recu_in_cover(context, target, start, end, cover = 0.1):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    ctx_tokens_post = post_process_ctx(ctx_tokens)\n",
    "    result_tokens = []\n",
    "    curr_target = targets\n",
    "    curr_cover = 0\n",
    "    while curr_cover < cover:\n",
    "        step_result = [triple[0] for triple in triples \n",
    "                       if triple[2] in curr_target]\n",
    "        if set(step_result) == set(curr_target):\n",
    "                break\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "        curr_cover = len(result_tokens) / len(ctx_tokens_post)\n",
    "    return list(set([token[0] for token in result_tokens]))\n",
    "\n",
    "def ctx_extraction_dep_recu_out_cover(context, target, start, end, cover = 0.1):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    ctx_tokens_post = post_process_ctx(ctx_tokens)\n",
    "    result_tokens = []\n",
    "    curr_target = targets\n",
    "    curr_cover = 0\n",
    "    while curr_cover < cover:\n",
    "        step_result = [triple[2] for triple in triples \n",
    "                       if triple[0] in curr_target]\n",
    "        if set(step_result) == set(curr_target):\n",
    "                break\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "        curr_cover = len(result_tokens) / len(ctx_tokens_post)\n",
    "    return list(set([token[0] for token in result_tokens]))\n",
    "\n",
    "def ctx_extraction_dep_recu_in_out_cover(context, target, start, end, cover = 0.1):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    ctx_tokens_post = post_process_ctx(ctx_tokens)\n",
    "    result_tokens = []\n",
    "    curr_target = targets\n",
    "    curr_cover = 0\n",
    "    while curr_cover < cover:\n",
    "        step_result = [triple[2] for triple in triples \n",
    "                       if triple[0] in curr_target]\n",
    "        step_result_out = [triple[0] for triple in triples \n",
    "                       if triple[2] in curr_target]\n",
    "        step_result.extend(step_result_out)\n",
    "        if set(step_result) == set(curr_target):\n",
    "                break\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "        curr_cover = len(result_tokens) / len(ctx_tokens_post)\n",
    "    return list(set([token[0] for token in result_tokens]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Normally, the land will be passed down by future generations in a way \" + \\\n",
    "             \"that recognizes the community's traditional connection to that country \"\n",
    "target = 'passed'\n",
    "\n",
    "print('ctx_etraction_all:')\n",
    "print(ctx_extraction_all_filtered(sentence, target))\n",
    "\n",
    "print('ctx_extraction_window_pre_n:')\n",
    "print(ctx_extraction_window_pre_n(sentence, \"Normally\"))\n",
    "print(ctx_extraction_window_pre_n(sentence, \"the\"))\n",
    "print(ctx_extraction_window_pre_n(sentence, \"land\"))\n",
    "print(ctx_extraction_window_pre_n(sentence, target, n = 5))\n",
    "\n",
    "print('ctx_extraction_window_suc_n:')\n",
    "print(ctx_extraction_window_suc_n(sentence, \"country\"))\n",
    "print(ctx_extraction_window_suc_n(sentence, \"to\"))\n",
    "print(ctx_extraction_window_suc_n(sentence, \"connection\"))\n",
    "print(ctx_extraction_window_suc_n(sentence, \"community\", n = 5))\n",
    "\n",
    "print('ctx_extraction_window_pre_suc_n:')\n",
    "print(ctx_extraction_window_pre_suc_n(sentence, \"passed\"))\n",
    "print(ctx_extraction_window_pre_suc_n(sentence, \"the\"))\n",
    "print(ctx_extraction_window_pre_suc_n(sentence, \"to\"))\n",
    "\n",
    "print('ctx_extraction_dep_in:')\n",
    "print(ctx_extraction_dep_in(sentence, \"land\"))\n",
    "\n",
    "print('ctx_extraction_dep_out:')\n",
    "print(ctx_extraction_dep_out(sentence, target))\n",
    "print(ctx_extraction_dep_out(sentence, \"land\"))\n",
    "\n",
    "print('ctx_extraction_dep_in_out:')\n",
    "print(ctx_extraction_dep_in_out(sentence, \"land\"))\n",
    "\n",
    "print('ctx_extraction_dep_recu_in_n_steps:')\n",
    "print(ctx_extraction_dep_recu_in_n_steps(sentence, \"the\", n = 3))\n",
    "\n",
    "print('ctx_extraction_dep_recu_out_n_steps:')\n",
    "print(ctx_extraction_dep_recu_out_n_steps(sentence, \"the\"))\n",
    "\n",
    "print('ctx_extraction_dep_recu_in_out_n_steps:')\n",
    "print(ctx_extraction_dep_recu_in_out_n_steps(sentence, \"the\"))\n",
    "\n",
    "print('ctx_extraction_dep_recu_in_cover:')\n",
    "print(ctx_extraction_dep_recu_in_cover(sentence, \"the\", cover=0.1))\n",
    "\n",
    "print('ctx_extraction_dep_recu_out_cover:')\n",
    "print(ctx_extraction_dep_recu_out_cover(sentence, \"the\", cover=0.1))\n",
    "\n",
    "print('ctx_extraction_dep_recu_in_out_cover:')\n",
    "print(ctx_extraction_dep_recu_in_out_cover(sentence, \"the\", cover=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Context Extraction\n",
    "\n",
    "After we defined all the context extraction approaches, we can apply them on the actual dataset. To do so, we first extract all the distinct sentences from the actual training set and create a new dataframe containing only the sentence ids, the sentence, the target and all the computed contexts. This also makes it easier to integrate context extraction functions implemented in other languages. Afterwards we can compute the context features and join them back with the target features dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df_context = df.loc[0:1, ['id', 'sentence', 'target', 'start', 'end']]\n",
    "\n",
    "df_context['ctx_extraction_window_pre_n'] = df_context.apply(lambda columns : \n",
    "                                        ctx_extraction_window_pre_n(columns['sentence'], columns['target'], \\\n",
    "                                                                   columns['start'], columns['end']), axis = 1)\n",
    "\n",
    "df_context['ctx_extraction_window_suc_n'] = df_context.apply(lambda columns : \n",
    "                                        ctx_extraction_window_suc_n(columns['sentence'], columns['target'], \\\n",
    "                                                                   columns['start'], columns['end']), axis = 1)\n",
    "\n",
    "df_context['ctx_extraction_window_pre_suc_n'] = df_context.apply(lambda columns : \n",
    "                                        ctx_extraction_window_pre_suc_n(columns['sentence'], columns['target'], \\\n",
    "                                                                   columns['start'], columns['end']), axis = 1)\n",
    "\n",
    "df_context['ctx_extraction_dep_in'] = df_context.apply(lambda columns : \n",
    "                                        ctx_extraction_dep_in(columns['sentence'], columns['target'], \\\n",
    "                                                                   columns['start'], columns['end']), axis = 1)\n",
    "\n",
    "df_context['ctx_extraction_dep_out'] = df_context.apply(lambda columns : \n",
    "                                        ctx_extraction_dep_out(columns['sentence'], columns['target'], \\\n",
    "                                                                   columns['start'], columns['end']), axis = 1)\n",
    "# 1. Compute dep_in_out using defined function\n",
    "df_context['ctx_extraction_dep_in_out'] = df_context.apply(lambda columns : \n",
    "                                        ctx_extraction_dep_in_out(columns['sentence'], columns['target'], \\\n",
    "                                                                   columns['start'], columns['end']), axis = 1)\n",
    "\n",
    "# 2. Compute dep_in_out by combining precomputed results\n",
    "df_context['ctx_extraction_dep_in_out_dir'] = df_context[['ctx_extraction_dep_in', \\\n",
    "                                                      'ctx_extraction_dep_out']].apply(lambda vals : vals[0]+vals[1], axis=1)\n",
    "\n",
    "\n",
    "df_context['ctx_extraction_dep_recu_in_n_steps']  = df_context.apply(lambda columns : \n",
    "                                        ctx_extraction_dep_recu_in_n_steps(columns['sentence'], columns['target'], \\\n",
    "                                        columns['start'], columns['end'], n=2), axis = 1)\n",
    "\n",
    "df_context['ctx_extraction_dep_recu_out_n_steps']  = df_context.apply(lambda columns : \n",
    "                                        ctx_extraction_dep_recu_out_n_steps(columns['sentence'], columns['target'], \\\n",
    "                                        columns['start'], columns['end'], n=2), axis = 1)\n",
    "\n",
    "\n",
    "df_context['ctx_extraction_dep_recu_in_out_n_steps']  = df_context.apply(lambda columns : \n",
    "                                        ctx_extraction_dep_recu_in_out_n_steps(columns['sentence'], columns['target'], \\\n",
    "                                        columns['start'], columns['end'], n=2), axis = 1)\n",
    "\n",
    "\n",
    "df_context['ctx_extraction_dep_recu_in_cover']  = df_context.apply(lambda columns : \n",
    "                                        ctx_extraction_dep_recu_in_cover(columns['sentence'], columns['target'], \\\n",
    "                                        columns['start'], columns['end'], cover=0.2), axis = 1)\n",
    "\n",
    "\n",
    "df_context['ctx_extraction_dep_recu_out_cover']  = df_context.apply(lambda columns : \n",
    "                                        ctx_extraction_dep_recu_out_cover(columns['sentence'], columns['target'], \\\n",
    "                                        columns['start'], columns['end'], cover=0.2), axis = 1)\n",
    "\n",
    "\n",
    "df_context['ctx_extraction_dep_recu_in_out_cover']  = df_context.apply(lambda columns : \n",
    "                                        ctx_extraction_dep_recu_in_out_cover(columns['sentence'], columns['target'], \\\n",
    "                                        columns['start'], columns['end'], cover=0.2), axis = 1)\n",
    "\n",
    "df_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Context Features\n",
    "After defining all the context definitions and extracting the different kinds of contexts from the sentence, we compute features on the context words. Therefore we first define which of the precomputed contexts to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_context['context'] = df_context['ctx_extraction_window_pre_suc_n']\n",
    "\n",
    "df_context['avg_ctx_length'] = df_context.context.apply(lambda context : agg_ctx_feat_num_average(context, len))\n",
    "df_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "Here we compute individual feature importance based on different metrics. For example, we implement and compute the F-Score, providing an idea of the discrimination power the feature has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_importance_f_score(dataframe, feat_name, label_name):\n",
    "    df = dataframe.copy()\n",
    "    mean_feat = np.mean(df.loc[:, [feat_name]])[0]\n",
    "    means = df.loc[: , [feat_name, label_name]].groupby(label_name).mean().reset_index()\n",
    "    mean_negativ = means.loc[means[label_name] == 0, [feat_name]][feat_name][0]\n",
    "    mean_positiv = means.loc[means[label_name] == 1, [feat_name]][feat_name][1]\n",
    "    # Compute the sum of deviations of the class mean from the overall mean\n",
    "    class_mean_devs = (mean_positiv - mean_feat)**2 + (mean_negativ - mean_feat)**2\n",
    "    # Compute neagtive instance based values\n",
    "    neg_inst = df.loc[df[label_name] == 0, [feat_name]]\n",
    "    std_dev_neg = (np.sum((neg_inst - mean_negativ)**2) / (len(neg_inst) - 1))[feat_name]\n",
    "    #Compute positive instance based values\n",
    "    pos_inst = df.loc[df[label_name] == 1, [feat_name]]\n",
    "    std_dev_pos = (np.sum((pos_inst - mean_positiv)**2) / (len(pos_inst) - 1))[feat_name]\n",
    "    return class_mean_devs / (std_dev_neg + std_dev_pos)\n",
    "\n",
    "def compute_all_feat_importance_metrics(dataframe, label_name):\n",
    "    pass\n",
    "    \n",
    "\n",
    "df_feat = df.drop(['id', 'sentence', 'target', 'nat', 'non_nat', \n",
    "                   'nat_marked', 'non_nat_marked', 'prob'], axis = 1)\n",
    "print(df_feat.mean())\n",
    "print(df_feat.groupby('binary').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'id', 'sentence', \"start\", \"end\", \"target\", \n",
    "              \"nat\", \"non_nat\", \"nat_marked\", \"non_nat_marked\", \"binary\", \"prob\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
