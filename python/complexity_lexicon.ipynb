{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complexity Lexicon Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1.0) Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "\n",
    "Dataset = namedtuple('Dataset', 'name, train, test')\n",
    "Model = namedtuple('Model', 'type, name, dimension, corpus, model')\n",
    "\n",
    "MAIN_PATH_DATASET = \"../cwishareddataset/traindevset/english/\"\n",
    "genres = ['Wikipedia', 'WikiNews', 'News']\n",
    "datasets = ['Train', 'Dev']\n",
    "columns = ['id', 'sentence', \"start\", \"end\", \"target\", \n",
    "           \"nat\", \"non_nat\", \"nat_marked\", \"non_nat_marked\", \"binary\", \"prob\"]\n",
    "\n",
    "\n",
    "datasets = [Dataset('Wikipedia', 'Train', 'Dev'),\n",
    "            Dataset('WikiNews', 'Train', 'Dev'),\n",
    "            Dataset('News', 'Train', 'Dev')]\n",
    "\n",
    "feature_categories = []\n",
    "\n",
    "def load_df(path):\n",
    "    df = pd.read_csv(path, header=None, sep = \"\\t\")\n",
    "    df.columns = columns\n",
    "    return df\n",
    "\n",
    "datasets = [Dataset(d.name, load_df(MAIN_PATH_DATASET + d.name + '_' + d.train + '.tsv'),\n",
    "                            load_df(MAIN_PATH_DATASET + d.name + '_' + d.test + '.tsv'))\n",
    "                            for d in datasets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1.1) Load Embedding Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.1.1) Load GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model : glove.6B.50d.txt\n",
      "load model : glove.twitter.27B.25d.txt\n",
      "[Model(type='glove', name='glove.6B.50d.txt', dimension=50, corpus='wikipedia+gigaword5', model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x00000039A4E52978>), Model(type='glove', name='glove.twitter.27B.25d.txt', dimension=25, corpus='twitter', model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x00000039B24ABDD8>)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "MAIN_PATH = 'D:/workspace_python/CoWoReId/python/resources/word-embeddings/'\n",
    "\n",
    "glove_defs = [#Model('glove', 'glove.42B.300d.txt', 300, 'cc42B', None),  \n",
    "              #Model('glove', 'glove.840B.300d.txt', 300, 'cc840B', None), \n",
    "              Model('glove', 'glove.6B.50d.txt', 50, 'wikipedia+gigaword5', None), \n",
    "              #Model('glove', 'glove.6B.100d.txt',100, 'wikipedia+gigaword5', None),\n",
    "              #Model('glove', 'glove.6B.200d.txt', 200, 'wikipedia+gigaword5', None), \n",
    "              #Model('glove', 'glove.6B.300d.txt', 300, 'wikipedia+gigaword5', None),\n",
    "              Model('glove', 'glove.twitter.27B.25d.txt', 25, 'twitter', None)]\n",
    "              #Model('glove', 'glove.twitter.27B.50d.txt', 50, 'twitter', None), \n",
    "              #Model('glove', 'glove.twitter.27B.100d.txt', 100, 'twitter', None), \n",
    "              #Model('glove', 'glove.twitter.27B.200d.txt', 200, 'twitter', None)]\n",
    "\n",
    "glove_models = []\n",
    "for model in glove_defs:\n",
    "    glove_file = datapath(MAIN_PATH + model.name)\n",
    "    tmp_file = get_tmpfile(model.name + '-temp')\n",
    "    glove2word2vec(glove_file, tmp_file)\n",
    "    vecs = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "    glove_models.append(Model(model.type, model.name, model.dimension, model.corpus, vecs))\n",
    "    print('load model : {}'.format(model.name))\n",
    "    \n",
    "print(glove_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.1.2) Load word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "MAIN_PATH = 'D:/workspace_python/CoWoReId/python/resources/word-embeddings/'\n",
    "model_word2vec = Model('word2vec', 'GoogleNews-vectors-negative300.bin' , 300, 'GoogleNews', \\\n",
    "            KeyedVectors.load_word2vec_format(datapath(MAIN_PATH + 'GoogleNews-vectors-negative300.bin'), binary=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.1.3) Load FastText embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.wrappers import FastText\n",
    "\n",
    "MAIN_PATH = 'D:/workspace_python/CoWoReId/python/resources/word-embeddings/'\n",
    "model_fastText = Model('FastText', 'wiki.en.vec', 300, 'Wikipedia', \\\n",
    "                 KeyedVectors.load_word2vec_format(MAIN_PATH + 'wiki.en.vec'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "models.extend(glove_models)\n",
    "models.append(model_word2vec)\n",
    "models.append(model_fastText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Boostrapped Lexicon Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngram_embedding_similarity(model, word_l, word_r, missing_strat, ngram_repr):\n",
    "    vecs_l = [model[word] if word in model.vocab \n",
    "                  else missing_strat(word, model.vector_size) \n",
    "                  for word in word_l.split()]   \n",
    "    vecs_r = [model[word] if word in model.vocab \\\n",
    "                else missing_strat(word, model.vector_size)\n",
    "                for word in word_r.split()]\n",
    "    vec_l = ngram_repr(np.array(vecs_l))\n",
    "    vec_r = ngram_repr(np.array(vecs_r))\n",
    "    return np.dot(vec_l,vec_r) / (np.linalg.norm(vec_l) \\\n",
    "            * np.linalg.norm(vec_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bootstrap_lexicon(model, vocab, seeds_l, seeds_r, embedding_sim, \\\n",
    "                      missing_strat, ref_term, ngram_repr, epochs=10, \\\n",
    "                      bound_l=-1, bound_r=1, thresh_l=-0.5,thresh_r=0.5):\n",
    "    if not all(seed in vocab for seed in seeds_l):\n",
    "        raise ValueError('Not all left seeds contained in vocabulary')\n",
    "    if not all(seed in vocab for seed in seeds_r):\n",
    "        raise ValueError('Not all right seeds contained in vocabulary')\n",
    "    num_missing = np.sum([1 for word in vocab if word not in model.vocab])\n",
    "    print('Missing vocab in model : {} / {}%'.format(num_missing, \\\n",
    "          (num_missing/len(vocab)*100)))\n",
    "    # 1. Initialize the left and right seeds\n",
    "    se_l = {seed : bound_l for seed in seeds_l}\n",
    "    se_r = {seed : bound_r for seed in seeds_r}\n",
    "    lexicon = se_l.copy()\n",
    "    lexicon.update(se_r)\n",
    "    for curr_epoch in range(1,epochs+1):\n",
    "        # 2. Compute left and right weights\n",
    "        #print(se_l)\n",
    "        sum_l = np.abs(np.sum([score for word, score in se_l.items()]))\n",
    "        sum_r = np.abs(np.sum([score for word, score in se_r.items()]))\n",
    "        weight_l = sum_r / (sum_r + sum_l)\n",
    "        weight_r = sum_l / (sum_r + sum_l)\n",
    "        print(sum_l)\n",
    "        print(sum_r)\n",
    "        print('Epoch {} : Se_l_size = {}, Se_r_size = {}, weight_l = {}, weight_r = {},'.format(\\\n",
    "                      curr_epoch, len(se_l), len(se_r), weight_l, weight_r))\n",
    "        for curr_word in vocab:\n",
    "            if curr_word in se_l or curr_word in se_r:\n",
    "                continue\n",
    "            # Compute the weighted left and right scores and sum them\n",
    "            score_l = [(weight_l * score * \\\n",
    "                        embedding_sim(model, curr_word, seed, missing_strat, ngram_repr)) \\\n",
    "                        for seed, score in se_l.items()]\n",
    "            score_r = [(weight_r * score * \\\n",
    "                        embedding_sim(model, curr_word, seed, missing_strat, ngram_repr)) \\\n",
    "                        for seed, score in se_r.items()]\n",
    "            print('Word : {} = {}'.format(curr_word, score_l))\n",
    "            print(score_r)\n",
    "            score = np.sum(score_l) + np.sum(score_r)\n",
    "            print('final score : {}'.format(score))\n",
    "            lexicon[curr_word] = score\n",
    "            #print('{} : {}'.format(curr_word, score))\n",
    "            # Add word to the seed set if the score is low or high enough\n",
    "            if score <= thresh_l: se_l[curr_word] = score\n",
    "            if score >= thresh_r: se_r[curr_word] = score\n",
    "        #print(lexicon)\n",
    "    # 3. Compute final scores and normalize them\n",
    "    sim_ref = lexicon.get(ref_term)\n",
    "    print('SIM_REF:{}'.format(sim_ref))\n",
    "    if not sim_ref:\n",
    "        return ValueError('Reference term {} not found in lexicon'.format(ref_term))\n",
    "    coll_l = {seed : (score - sim_ref) for seed, score in lexicon.items() \\\n",
    "                if (score - sim_ref) < 0}\n",
    "    coll_r = {seed : (score - sim_ref) for seed, score in lexicon.items() \\\n",
    "                if (score - sim_ref) > 0}\n",
    "    max_l = np.max(np.abs([score for _, score in coll_l.items()]))\n",
    "    max_r = np.max(np.abs([score for _, score in coll_r.items()]))\n",
    "    #print(coll_l)\n",
    "    #print(max_l)\n",
    "    lexicon[ref_term] = lexicon[ref_term] - sim_ref\n",
    "    for word, score in coll_l.items():\n",
    "        lexicon[word] = score / max_l\n",
    "    for word, score in coll_r.items():\n",
    "        lexicon[word] = score / max_r\n",
    "    return lexicon\n",
    "\n",
    "\n",
    "def bootstrap_lexicon_simple_norm(model, vocab, seeds_l, seeds_r, embedding_sim, \\\n",
    "                      missing_strat, ngram_repr, epochs=10, \\\n",
    "                      bound_l=-1, bound_r=1, thresh_l=-0.5,thresh_r=0.5):\n",
    "    if not all(seed in vocab for seed in seeds_l):\n",
    "        raise ValueError('Not all left seeds contained in vocabulary')\n",
    "    if not all(seed in vocab for seed in seeds_r):\n",
    "        raise ValueError('Not all right seeds contained in vocabulary')\n",
    "    num_missing = np.sum([1 for word in vocab if word not in model.vocab])\n",
    "    print('Missing vocab in model : {} / {}%'.format(num_missing, \\\n",
    "          (num_missing/len(vocab)*100)))\n",
    "    # 1. Initialize the left and right seeds\n",
    "    se_l = {seed : bound_l for seed in seeds_l}\n",
    "    se_r = {seed : bound_r for seed in seeds_r}\n",
    "    lexicon = se_l.copy()\n",
    "    lexicon.update(se_r)\n",
    "    for curr_epoch in range(1,epochs+1):\n",
    "        # 2. Compute left and right weights\n",
    "        #print(se_l)\n",
    "        sum_l = np.abs(np.sum([score for word, score in se_l.items()]))\n",
    "        sum_r = np.abs(np.sum([score for word, score in se_r.items()]))\n",
    "        weight_l = sum_r / (sum_r + sum_l)\n",
    "        weight_r = sum_l / (sum_r + sum_l)\n",
    "        print('Epoch {} : Se_l_size = {}, Se_r_size = {}, weight_l = {}, weight_r = {},'.format(\\\n",
    "                      curr_epoch, len(se_l), len(se_r), weight_l, weight_r))\n",
    "        for curr_word in vocab:\n",
    "            if curr_word in se_l or curr_word in se_r:\n",
    "                continue\n",
    "            # Compute the weighted left and right scores and sum them\n",
    "            score_l = [(weight_l * score * \\\n",
    "                        embedding_sim(model, curr_word, seed, missing_strat, ngram_repr)) \\\n",
    "                        for seed, score in se_l.items()]\n",
    "            score_r = [(weight_r * score * \\\n",
    "                        embedding_sim(model, curr_word, seed, missing_strat, ngram_repr)) \\\n",
    "                        for seed, score in se_r.items()]\n",
    "            print('Word : {} = {}'.format(curr_word, score_l))\n",
    "            print(score_r)\n",
    "            score = np.sum(score_l) + np.sum(score_r)\n",
    "            print('final score : {}'.format(score))\n",
    "            lexicon[curr_word] = score\n",
    "            #print('{} : {}'.format(curr_word, score))\n",
    "            # Add word to the seed set if the score is low or high enough\n",
    "            if score <= thresh_l: se_l[curr_word] = score\n",
    "            if score >= thresh_r: se_r[curr_word] = score\n",
    "        #print(lexicon)\n",
    "    # 3. Compute final scores and normalize them\n",
    "    coll_l = {seed : score for seed, score in lexicon.items() \\\n",
    "                if score < 0}\n",
    "    coll_r = {seed : score for seed, score in lexicon.items() \\\n",
    "                if score > 0}\n",
    "    max_l = np.max(np.abs([score for _, score in coll_l.items()]))\n",
    "    max_r = np.max(np.abs([score for _, score in coll_r.items()]))\n",
    "    for word, score in coll_l.items():\n",
    "        lexicon[word] = score / max_l\n",
    "    for word, score in coll_r.items():\n",
    "        lexicon[word] = score / max_r\n",
    "    return lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Toy-Example Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bootstrapped_lexicon import bootstrap_lexicon_simple_norm\n",
    "from bootstrapped_lexicon import missing_strat_random\n",
    "from bootstrapped_lexicon import ngram_embedding_similarity\n",
    "from bootstrapped_lexicon import ngram_repr_bag_of_words\n",
    "import gensim\n",
    "\n",
    "seeds_complex = ['aboriginal']\n",
    "seeds_non_complex = ['bad']\n",
    "\n",
    "# Build the vocabulary\n",
    "vocabulary = []\n",
    "vocabulary.extend(seeds_complex)\n",
    "vocabulary.extend(seeds_non_complex)\n",
    "vocabulary.extend(['good', 'Inuit', 'and'])\n",
    "\n",
    "print('---------Complex Seeds----------------')\n",
    "print(seeds_complex)\n",
    "print('---------Non-Complex Seeds----------------')\n",
    "print(seeds_non_complex)\n",
    "print('---------Vocabulary----------------')\n",
    "print(vocabulary)\n",
    "\n",
    "lexicon = bootstrap_lexicon_simple_norm(model, vocabulary, seeds_non_complex, seeds_complex, \\\n",
    "                  ngram_embedding_similarity, missing_strat_random, \\\n",
    "                  ngram_repr_bag_of_words, epochs=3, thresh_l=-0.3, thresh_r=0.4)\n",
    "print(lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Complexity Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from bootstrapped_lexicon import bootstrap_lexicon\n",
    "from bootstrapped_lexicon import bootstrap_lexicon_simple_norm\n",
    "from bootstrapped_lexicon import missing_strat_random\n",
    "from bootstrapped_lexicon import ngram_embedding_similarity\n",
    "from bootstrapped_lexicon import ngram_repr_bag_of_words\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('resources/' + \\\n",
    "            'word-embeddings/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "TRAIN_ENGLISH_WIKIPEDIA = \"../cwishareddataset/traindevset/\" + \\\n",
    "                           \"english/Wikipedia_Train.tsv\"\n",
    "TEST_ENGLISH_WIKIPEDIA = \"../cwishareddataset/testset/\" + \\\n",
    "                           \"english/Wikipedia_Test.tsv\"\n",
    "df = pd.read_csv(TRAIN_ENGLISH_WIKIPEDIA, sep = \"\\t\")\n",
    "df_test = pd.read_csv(TEST_ENGLISH_WIKIPEDIA, sep = '\\t')\n",
    "df.columns = ['id', 'sentence', \"start\", \"end\", \"target\", \n",
    "              \"nat\", \"non_nat\", \"nat_marked\", \"non_nat_marked\", \"binary\", \"prob\"]\n",
    "df_test.columns = ['id', 'sentence', \"start\", \"end\", \"target\", \n",
    "              \"nat\", \"non_nat\", \"nat_marked\", \"non_nat_marked\", \"binary\", \"prob\"]\n",
    "\n",
    "df['num_tokens'] = df.target.apply(lambda target : len(target.split()))\n",
    "\n",
    "# Get the basic seeds for complex and non_complex words\n",
    "\n",
    "#seeds_complex = [word.lower().strip() for word in df.loc[df['binary'] == 1,'target'].tolist()]\n",
    "#seen = set()\n",
    "#seen_add = seen.add\n",
    "#seeds_complex = [x for x in seeds_complex if not (x in seen or seen_add(x))]\n",
    "\n",
    "\n",
    "#seeds_non_complex = [word.lower().strip() for word in df.loc[df['binary'] == 0,'target'].tolist()]\n",
    "#seen = set()\n",
    "#seen_add = seen.add\n",
    "#seeds_non_complex = [x for x in seeds_non_complex if not (x in seen or seen_add(x))]\n",
    "\n",
    "seeds_complex = ['aboriginal']\n",
    "seeds_non_complex = ['bad']\n",
    "\n",
    "# Build the vocabulary\n",
    "vocabulary = []\n",
    "vocabulary.extend(seeds_complex)\n",
    "vocabulary.extend(seeds_non_complex)\n",
    "vocabulary.extend(['good', 'Inuit'])\n",
    "#vocabulary.extend([ngram.strip().lower() for target in df_test['target'].tolist() \\\n",
    " #                   for ngram in target.split()])\n",
    "#ref = 'and'\n",
    "#vocabulary.append(ref)\n",
    "print('---------Complex Seeds----------------')\n",
    "print(len(seeds_complex))\n",
    "print('---------Non-Complex Seeds----------------')\n",
    "print(len(seeds_non_complex))\n",
    "print('---------Vocabulary----------------')\n",
    "print(len(vocabulary))\n",
    "\n",
    "lexicon = bootstrap_lexicon_simple_norm(model, vocabulary, seeds_non_complex, seeds_complex, \\\n",
    "                  ngram_embedding_similarity, missing_strat_random, \\\n",
    "                  ngram_repr_bag_of_words, epochs=3, thresh_l=-0.3, thresh_r=0.4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
