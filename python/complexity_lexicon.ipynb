{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complexity Lexicon Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1.0) Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "\n",
    "Dataset = namedtuple('Dataset', 'name, train, test')\n",
    "Model = namedtuple('Model', 'type, name, dimension, corpus, model')\n",
    "\n",
    "MAIN_PATH_DATASET = \"../cwishareddataset/traindevset/english/\"\n",
    "genres = ['Wikipedia', 'WikiNews', 'News']\n",
    "datasets = ['Train', 'Dev']\n",
    "columns = ['id', 'sentence', \"start\", \"end\", \"target\", \n",
    "           \"nat\", \"non_nat\", \"nat_marked\", \"non_nat_marked\", \"binary\", \"prob\"]\n",
    "\n",
    "\n",
    "datasets = [Dataset('Wikipedia', 'Train', 'Dev'),\n",
    "            Dataset('WikiNews', 'Train', 'Dev'),\n",
    "            Dataset('News', 'Train', 'Dev')]\n",
    "\n",
    "feature_categories = []\n",
    "\n",
    "def load_df(path):\n",
    "    df = pd.read_csv(path, header=None, sep = \"\\t\")\n",
    "    df.columns = columns\n",
    "    return df\n",
    "\n",
    "datasets = [Dataset(d.name, load_df(MAIN_PATH_DATASET + d.name + '_' + d.train + '.tsv'),\n",
    "                            load_df(MAIN_PATH_DATASET + d.name + '_' + d.test + '.tsv'))\n",
    "                            for d in datasets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1.1) Load Embedding Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.1.1) Load GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model : glove.6B.50d.txt\n",
      "load model : glove.twitter.27B.25d.txt\n",
      "[Model(type='glove', name='glove.6B.50d.txt', dimension=50, corpus='wikipedia+gigaword5', model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x0000001D60287A20>), Model(type='glove', name='glove.twitter.27B.25d.txt', dimension=25, corpus='twitter', model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x0000001D6DCFBE80>)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "MAIN_PATH = 'D:/workspace_python/CoWoReId/python/resources/word-embeddings/'\n",
    "\n",
    "glove_defs = [#Model('glove', 'glove.42B.300d.txt', 300, 'cc42B', None),  \n",
    "              #Model('glove', 'glove.840B.300d.txt', 300, 'cc840B', None), \n",
    "              Model('glove', 'glove.6B.50d.txt', 50, 'wikipedia+gigaword5', None), \n",
    "              #Model('glove', 'glove.6B.100d.txt',100, 'wikipedia+gigaword5', None),\n",
    "              #Model('glove', 'glove.6B.200d.txt', 200, 'wikipedia+gigaword5', None), \n",
    "              #Model('glove', 'glove.6B.300d.txt', 300, 'wikipedia+gigaword5', None),\n",
    "              Model('glove', 'glove.twitter.27B.25d.txt', 25, 'twitter', None)]\n",
    "              #Model('glove', 'glove.twitter.27B.50d.txt', 50, 'twitter', None), \n",
    "              #Model('glove', 'glove.twitter.27B.100d.txt', 100, 'twitter', None), \n",
    "              #Model('glove', 'glove.twitter.27B.200d.txt', 200, 'twitter', None)]\n",
    "\n",
    "glove_models = []\n",
    "for model in glove_defs:\n",
    "    glove_file = datapath(MAIN_PATH + model.name)\n",
    "    tmp_file = get_tmpfile(model.name + '-temp')\n",
    "    glove2word2vec(glove_file, tmp_file)\n",
    "    vecs = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "    glove_models.append(Model(model.type, model.name, model.dimension, model.corpus, vecs))\n",
    "    print('load model : {}'.format(model.name))\n",
    "    \n",
    "print(glove_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.1.2) Load word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "MAIN_PATH = 'D:/workspace_python/CoWoReId/python/resources/word-embeddings/'\n",
    "model_word2vec = Model('word2vec', 'GoogleNews-vectors-negative300.bin' , 300, 'GoogleNews', \\\n",
    "            KeyedVectors.load_word2vec_format(datapath(MAIN_PATH + 'GoogleNews-vectors-negative300.bin'), binary=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.1.3) Load FastText embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.wrappers import FastText\n",
    "\n",
    "MAIN_PATH = 'D:/workspace_python/CoWoReId/python/resources/word-embeddings/'\n",
    "model_fastText = Model('FastText', 'wiki.en.vec', 300, 'Wikipedia', \\\n",
    "                 KeyedVectors.load_word2vec_format(MAIN_PATH + 'wiki.en.vec'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "models.extend(glove_models)\n",
    "models.append(model_word2vec)\n",
    "models.append(model_fastText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Boostrapped Lexicon Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngram_embedding_similarity(model, word_l, word_r, missing_strat, ngram_repr):\n",
    "    vecs_l = [model[word.strip().lower()] if word.strip().lower() in model.vocab \n",
    "                  else missing_strat(word, model.vector_size) \n",
    "                  for word in word_l.split()]   \n",
    "    vecs_r = [model[word.strip().lower()] if word.strip().lower() in model.vocab \\\n",
    "                else missing_strat(word, model.vector_size)\n",
    "                for word in word_r.split()]\n",
    "    vec_l = ngram_repr(np.array(vecs_l))\n",
    "    vec_r = ngram_repr(np.array(vecs_r))\n",
    "    return np.dot(vec_l,vec_r) / (np.linalg.norm(vec_l) \\\n",
    "            * np.linalg.norm(vec_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import Tracer\n",
    "\n",
    "def bootstrap_lexicon(model, vocab, seeds_l, seeds_r, embedding_sim, \\\n",
    "                      missing_strat, ref_term, ngram_repr, agg_embed=True, epochs=10, \\\n",
    "                      bound_l=-1, bound_r=1, thresh_l=-0.5,thresh_r=0.5):\n",
    "    if not all(seed in vocab for seed in seeds_l):\n",
    "        raise ValueError('Not all left seeds contained in vocabulary')\n",
    "    if not all(seed in vocab for seed in seeds_r):\n",
    "        raise ValueError('Not all right seeds contained in vocabulary')\n",
    "    num_missing = np.sum([np.sum([1 for word in mwe.split()\n",
    "                        if word.strip().lower() not in model.vocab]) for mwe in vocab])\n",
    "    print('Missing vocab in model : {} / {}%'.format(num_missing, \\\n",
    "          (num_missing/len(vocab)*100)))\n",
    "    if agg_embed:\n",
    "        internal_vocab = vocab\n",
    "    else:\n",
    "        internal_vocab = [word for mwe in vocab for word in mwe.split()]\n",
    "    # 1. Initialize the left and right seeds\n",
    "    se_l = {seed : bound_l for seed in seeds_l}\n",
    "    se_r = {seed : bound_r for seed in seeds_r}\n",
    "    lexicon = se_l.copy()\n",
    "    lexicon.update(se_r)\n",
    "    for curr_epoch in range(1,epochs+1):\n",
    "        # 2. Compute left and right weights\n",
    "        #print(se_l)\n",
    "        sum_l = np.abs(np.sum([score for word, score in se_l.items()]))\n",
    "        sum_r = np.abs(np.sum([score for word, score in se_r.items()]))\n",
    "        weight_l = sum_r / (sum_r + sum_l)\n",
    "        weight_r = sum_l / (sum_r + sum_l)\n",
    "        #print(sum_l)\n",
    "        #print(sum_r)\n",
    "        print('Epoch {} : Se_l_size = {}, Se_r_size = {}, weight_l = {}, weight_r = {},'.format(\\\n",
    "                      curr_epoch, len(se_l), len(se_r), weight_l, weight_r))\n",
    "        for index, curr_word in enumerate(internal_vocab):\n",
    "            if((index % 500) == 0):\n",
    "                print('{}|{}:{}%   '.format(index, len(internal_vocab), \n",
    "                                    (index/len(internal_vocab))), end='')\n",
    "            if curr_word in se_l or curr_word in se_r:\n",
    "                continue\n",
    "            # Compute the weighted left and right scores and sum them\n",
    "            score_l = [(weight_l * score * \\\n",
    "                        embedding_sim(model, curr_word, seed, missing_strat, ngram_repr)) \\\n",
    "                        for seed, score in se_l.items()]\n",
    "            score_r = [(weight_r * score * \\\n",
    "                        embedding_sim(model, curr_word, seed, missing_strat, ngram_repr)) \\\n",
    "                        for seed, score in se_r.items()]\n",
    "            #print('Word : {}'.format(curr_word))\n",
    "            #print(score_r)\n",
    "            #print(score_l)\n",
    "            score = np.sum(score_l) + np.sum(score_r)\n",
    "            #print('final score : {}'.format(score))\n",
    "            lexicon[curr_word] = score\n",
    "            Tracer()()\n",
    "            #print('{} : {}'.format(curr_word, score))\n",
    "            # Add word to the seed set if the score is low or high enough\n",
    "            if score <= thresh_l: se_l[curr_word] = score\n",
    "            if score >= thresh_r: se_r[curr_word] = score\n",
    "        #print(lexicon)\n",
    "    # 3. Compute final scores and normalize them\n",
    "    sim_ref = lexicon.get(ref_term)\n",
    "    #print('SIM_REF:{}'.format(sim_ref))\n",
    "    if not sim_ref:\n",
    "        return ValueError('Reference term {} not found in lexicon'.format(ref_term))\n",
    "    coll_l = {seed : (score - sim_ref) for seed, score in lexicon.items() \\\n",
    "                if (score - sim_ref) < 0}\n",
    "    coll_r = {seed : (score - sim_ref) for seed, score in lexicon.items() \\\n",
    "                if (score - sim_ref) > 0}\n",
    "    max_l = np.max(np.abs([score for _, score in coll_l.items()]))\n",
    "    max_r = np.max(np.abs([score for _, score in coll_r.items()]))\n",
    "    #print(coll_l)\n",
    "    #print(max_l)\n",
    "    lexicon[ref_term] = lexicon[ref_term] - sim_ref\n",
    "    for word, score in coll_l.items():\n",
    "        lexicon[word] = score / max_l\n",
    "    for word, score in coll_r.items():\n",
    "        lexicon[word] = score / max_r\n",
    "    # 4. Aggregate MWE complexity scores\n",
    "    if not agg_embed:\n",
    "        for mwe in vocab:\n",
    "            lexicon[mwe] = np.mean([lexicon[word] for word in mwe.split()])\n",
    "    return lexicon\n",
    "\n",
    "\n",
    "def bootstrap_lexicon_simple_norm(models, vocab, seeds_l, seeds_r, embedding_sim, \\\n",
    "                      missing_strat, ngram_repr, agg_embed=True, epochs=10, \\\n",
    "                      bound_l=-1, bound_r=1, thresh_l=-0.5,thresh_r=0.5):\n",
    "    if not all(seed in vocab for seed in seeds_l):\n",
    "        raise ValueError('Not all left seeds contained in vocabulary')\n",
    "    if not all(seed in vocab for seed in seeds_r):\n",
    "        raise ValueError('Not all right seeds contained in vocabulary')\n",
    "    num_missing = np.sum([1 for word in vocab if word not in model.vocab])\n",
    "    print('Missing vocab in model : {} / {}%'.format(num_missing, \\\n",
    "          (num_missing/len(vocab)*100)))\n",
    "    if agg_embed:\n",
    "        internal_vocab = vocab\n",
    "    else:\n",
    "        internal_vocab = [word for mwe in vocab for word in mwe.split()]\n",
    "    # 1. Initialize the left and right seeds\n",
    "    se_l = {seed : bound_l for seed in seeds_l}\n",
    "    se_r = {seed : bound_r for seed in seeds_r}\n",
    "    lexicon = se_l.copy()\n",
    "    lexicon.update(se_r)\n",
    "    for curr_epoch in range(1,epochs+1):\n",
    "        # 2. Compute left and right weights\n",
    "        #print(se_l)\n",
    "        sum_l = np.abs(np.sum([score for word, score in se_l.items()]))\n",
    "        sum_r = np.abs(np.sum([score for word, score in se_r.items()]))\n",
    "        weight_l = sum_r / (sum_r + sum_l)\n",
    "        weight_r = sum_l / (sum_r + sum_l)\n",
    "        print('Epoch {} : Se_l_size = {}, Se_r_size = {}, weight_l = {}, weight_r = {},'.format(\\\n",
    "                      curr_epoch, len(se_l), len(se_r), weight_l, weight_r))\n",
    "        for curr_word in internal_vocab:\n",
    "            if curr_word in se_l or curr_word in se_r:\n",
    "                continue\n",
    "            # Compute the weighted left and right scores and sum them\n",
    "            score_l = [(weight_l * score * \\\n",
    "                        embedding_sim(model, curr_word, seed, missing_strat, ngram_repr)) \\\n",
    "                        for seed, score in se_l.items()]\n",
    "            score_r = [(weight_r * score * \\\n",
    "                        embedding_sim(model, curr_word, seed, missing_strat, ngram_repr)) \\\n",
    "                        for seed, score in se_r.items()]\n",
    "            print('Word : {} = {}'.format(curr_word, score_l))\n",
    "            print(score_r)\n",
    "            score = np.sum(score_l) + np.sum(score_r)\n",
    "            print('final score : {}'.format(score))\n",
    "            lexicon[curr_word] = score\n",
    "            #print('{} : {}'.format(curr_word, score))\n",
    "            # Add word to the seed set if the score is low or high enough\n",
    "            if score <= thresh_l: se_l[curr_word] = score\n",
    "            if score >= thresh_r: se_r[curr_word] = score\n",
    "        #print(lexicon)\n",
    "    # 3. Compute final scores and normalize them\n",
    "    coll_l = {seed : score for seed, score in lexicon.items() \\\n",
    "                if score < 0}\n",
    "    coll_r = {seed : score for seed, score in lexicon.items() \\\n",
    "                if score > 0}\n",
    "    max_l = np.max(np.abs([score for _, score in coll_l.items()]))\n",
    "    max_r = np.max(np.abs([score for _, score in coll_r.items()]))\n",
    "    for word, score in coll_l.items():\n",
    "        lexicon[word] = score / max_l\n",
    "    for word, score in coll_r.items():\n",
    "        lexicon[word] = score / max_r\n",
    "    # 4. Aggregate MWE complexity scores\n",
    "    if not agg_embed:\n",
    "        for mwe in vocab:\n",
    "            lexicon[mwe] = np.mean([lexicon[word] for word in mwe.split()])\n",
    "    return lexicon\n",
    "\n",
    "def bootstrap_lexicon_multiple_embeddings(models, vocab, seeds_l, seeds_r, embedding_sim, \\\n",
    "                      missing_strat, ref_term, ngram_repr, agg_embed=True, epochs=10, \\\n",
    "                      bound_l=-1, bound_r=1, thresh_l=-0.5,thresh_r=0.5):\n",
    "    if not all(seed in vocab for seed in seeds_l):\n",
    "        raise ValueError('Not all left seeds contained in vocabulary')\n",
    "    if not all(seed in vocab for seed in seeds_r):\n",
    "        raise ValueError('Not all right seeds contained in vocabulary')\n",
    "    num_missing = np.sum([1 for word in vocab if not \\\n",
    "                    any([word in model.vocab for model in models])])\n",
    "    print('Missing vocab in models : {} / {}%'.format(num_missing, \\\n",
    "          (num_missing/len(vocab)*100)))\n",
    "    if agg_embed:\n",
    "        internal_vocab = vocab\n",
    "    else:\n",
    "        internal_vocab = [word for mwe in vocab for word in mwe.split()]\n",
    "    # 1. Initialize the left and right seeds\n",
    "    se_l = {seed : bound_l for seed in seeds_l}\n",
    "    se_r = {seed : bound_r for seed in seeds_r}\n",
    "    lexicon = se_l.copy()\n",
    "    lexicon.update(se_r)\n",
    "    for curr_epoch in range(1,epochs+1):\n",
    "        # 2. Compute left and right weights\n",
    "        #print(se_l)\n",
    "        sum_l = np.abs(np.sum([score for word, score in se_l.items()]))\n",
    "        sum_r = np.abs(np.sum([score for word, score in se_r.items()]))\n",
    "        weight_l = sum_r / (sum_r + sum_l)\n",
    "        weight_r = sum_l / (sum_r + sum_l)\n",
    "        print(sum_l)\n",
    "        print(sum_r)\n",
    "        print('Epoch {} : Se_l_size = {}, Se_r_size = {}, weight_l = {}, weight_r = {},'.format(\\\n",
    "                      curr_epoch, len(se_l), len(se_r), weight_l, weight_r))\n",
    "        for curr_word in internal_vocab:\n",
    "            if curr_word in se_l or curr_word in se_r:\n",
    "                continue\n",
    "            # Compute the weighted left and right scores and sum them\n",
    "            score_l = [(weight_l * score * \\\n",
    "                        np.mean([embedding_sim(model, curr_word, seed, missing_strat, ngram_repr) \n",
    "                                for model in models])) \\\n",
    "                        for seed, score in se_l.items()]\n",
    "            score_r = [(weight_r * score * \\\n",
    "                        np.mean([embedding_sim(model, curr_word, seed, missing_strat, ngram_repr)\n",
    "                                for model in models])) \\\n",
    "                        for seed, score in se_r.items()]\n",
    "            print('Word : {}'.format(curr_word))\n",
    "            print(score_r)\n",
    "            print(score_l)\n",
    "            score = np.sum(score_l) + np.sum(score_r)\n",
    "            print('final score : {}'.format(score))\n",
    "            lexicon[curr_word] = score\n",
    "            #print('{} : {}'.format(curr_word, score))\n",
    "            # Add word to the seed set if the score is low or high enough\n",
    "            if score <= thresh_l: se_l[curr_word] = score\n",
    "            if score >= thresh_r: se_r[curr_word] = score\n",
    "        #print(lexicon)\n",
    "    # 3. Compute final scores and normalize them\n",
    "    sim_ref = lexicon.get(ref_term)\n",
    "    print('SIM_REF:{}'.format(sim_ref))\n",
    "    if not sim_ref:\n",
    "        return ValueError('Reference term {} not found in lexicon'.format(ref_term))\n",
    "    coll_l = {seed : (score - sim_ref) for seed, score in lexicon.items() \\\n",
    "                if (score - sim_ref) < 0}\n",
    "    coll_r = {seed : (score - sim_ref) for seed, score in lexicon.items() \\\n",
    "                if (score - sim_ref) > 0}\n",
    "    max_l = np.max(np.abs([score for _, score in coll_l.items()]))\n",
    "    max_r = np.max(np.abs([score for _, score in coll_r.items()]))\n",
    "    #print(coll_l)\n",
    "    #print(max_l)\n",
    "    lexicon[ref_term] = lexicon[ref_term] - sim_ref\n",
    "    for word, score in coll_l.items():\n",
    "        lexicon[word] = score / max_l\n",
    "    for word, score in coll_r.items():\n",
    "        lexicon[word] = score / max_r\n",
    "    # 4. Aggregate MWE complexity scores\n",
    "    if not agg_embed:\n",
    "        for mwe in vocab:\n",
    "            lexicon[mwe] = np.mean([lexicon[word] for word in mwe.split()])\n",
    "    return lexicon\n",
    "\n",
    "def bootstrap_lexicon_no_weight(model, vocab, seeds_l, seeds_r, embedding_sim, \\\n",
    "                      missing_strat, ref_term, ngram_repr, agg_embed=True, epochs=10, \\\n",
    "                      bound_l=-1, bound_r=1, thresh_l=-0.5,thresh_r=0.5):\n",
    "    if not all(seed in vocab for seed in seeds_l):\n",
    "        raise ValueError('Not all left seeds contained in vocabulary')\n",
    "    if not all(seed in vocab for seed in seeds_r):\n",
    "        raise ValueError('Not all right seeds contained in vocabulary')\n",
    "    num_missing = np.sum([1 for word in vocab if not \\\n",
    "                    any([word in model.vocab for model in models])])\n",
    "    print('Missing vocab in models : {} / {}%'.format(num_missing, \\\n",
    "          (num_missing/len(vocab)*100)))\n",
    "    if agg_embed:\n",
    "        internal_vocab = vocab\n",
    "    else:\n",
    "        internal_vocab = [word for mwe in vocab for word in mwe.split()]\n",
    "    # 1. Initialize the left and right seeds\n",
    "    se_l = {seed : bound_l for seed in seeds_l}\n",
    "    se_r = {seed : bound_r for seed in seeds_r}\n",
    "    lexicon = se_l.copy()\n",
    "    lexicon.update(se_r)\n",
    "    for curr_epoch in range(1,epochs+1):\n",
    "        # 2. Compute left and right weights\n",
    "        #print(se_l)\n",
    "        sum_l = np.abs(np.sum([score for word, score in se_l.items()]))\n",
    "        sum_r = np.abs(np.sum([score for word, score in se_r.items()]))\n",
    "        weight_l = sum_r / (sum_r + sum_l)\n",
    "        weight_r = sum_l / (sum_r + sum_l)\n",
    "        print(sum_l)\n",
    "        print(sum_r)\n",
    "        print('Epoch {} : Se_l_size = {}, Se_r_size = {}, weight_l = {}, weight_r = {},'.format(\\\n",
    "                      curr_epoch, len(se_l), len(se_r), weight_l, weight_r))\n",
    "        for curr_word in internal_vocab:\n",
    "            if curr_word in se_l or curr_word in se_r:\n",
    "                continue\n",
    "            # Compute the weighted left and right scores and sum them\n",
    "            score_l = [(score * \\\n",
    "                        np.mean([embedding_sim(model, curr_word, seed, missing_strat, ngram_repr) \n",
    "                                for model in models])) \\\n",
    "                        for seed, score in se_l.items()]\n",
    "            score_r = [(score * \\\n",
    "                        np.mean([embedding_sim(model, curr_word, seed, missing_strat, ngram_repr)\n",
    "                                for model in models])) \\\n",
    "                        for seed, score in se_r.items()]\n",
    "            print('Word : {}'.format(curr_word))\n",
    "            print(score_r)\n",
    "            print(score_l)\n",
    "            score = np.mean(score_l) + np.mean(score_r)\n",
    "            print('final score : {}'.format(score))\n",
    "            lexicon[curr_word] = score\n",
    "            #print('{} : {}'.format(curr_word, score))\n",
    "            # Add word to the seed set if the score is low or high enough\n",
    "            if score <= thresh_l: se_l[curr_word] = score\n",
    "            if score >= thresh_r: se_r[curr_word] = score\n",
    "        #print(lexicon)\n",
    "    # 3. Compute final scores and normalize them\n",
    "    sim_ref = lexicon.get(ref_term)\n",
    "    print('SIM_REF:{}'.format(sim_ref))\n",
    "    if not sim_ref:\n",
    "        return ValueError('Reference term {} not found in lexicon'.format(ref_term))\n",
    "    coll_l = {seed : (score - sim_ref) for seed, score in lexicon.items() \\\n",
    "                if (score - sim_ref) < 0}\n",
    "    coll_r = {seed : (score - sim_ref) for seed, score in lexicon.items() \\\n",
    "                if (score - sim_ref) > 0}\n",
    "    max_l = np.max(np.abs([score for _, score in coll_l.items()]))\n",
    "    max_r = np.max(np.abs([score for _, score in coll_r.items()]))\n",
    "    #print(coll_l)\n",
    "    #print(max_l)\n",
    "    lexicon[ref_term] = lexicon[ref_term] - sim_ref\n",
    "    for word, score in coll_l.items():\n",
    "        lexicon[word] = score / max_l\n",
    "    for word, score in coll_r.items():\n",
    "        lexicon[word] = score / max_r\n",
    "    # 4. Aggregate MWE complexity scores\n",
    "    if not agg_embed:\n",
    "        for mwe in vocab:\n",
    "            lexicon[mwe] = np.mean([lexicon[word] for word in mwe.split()])\n",
    "    return lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Toy-Example Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Complex Seeds----------------\n",
      "['aboriginal']\n",
      "---------Non-Complex Seeds----------------\n",
      "['bad']\n",
      "---------Vocabulary----------------\n",
      "['aboriginal', 'bad', 'good', 'inuit', 'and', 'to this end']\n",
      "Missing vocab in model : 1 / 16.666666666666664%\n",
      "1\n",
      "1\n",
      "Epoch 1 : Se_l_size = 1, Se_r_size = 1, weight_l = 0.5, weight_r = 0.5,\n",
      "Word : good\n",
      "[0.09518544375896454]\n",
      "[-0.398244708776474]\n",
      "final score : -0.30305926501750946\n",
      "Word : inuit\n",
      "[0.34829169511795044]\n",
      "[0.06974420696496964]\n",
      "final score : 0.4180359020829201\n",
      "Word : and\n",
      "[0.17070919275283813]\n",
      "[-0.30853986740112305]\n",
      "final score : -0.1378306746482849\n",
      "Word : to\n",
      "[0.13541758060455322]\n",
      "[-0.3115454614162445]\n",
      "final score : -0.17612788081169128\n",
      "Word : this\n",
      "[0.131220281124115]\n",
      "[-0.3739129602909088]\n",
      "final score : -0.24269267916679382\n",
      "Word : end\n",
      "[0.08204780519008636]\n",
      "[-0.3172760307788849]\n",
      "final score : -0.23522822558879852\n",
      "1\n",
      "1\n",
      "Epoch 2 : Se_l_size = 1, Se_r_size = 1, weight_l = 0.5, weight_r = 0.5,\n",
      "Word : good\n",
      "[0.09518544375896454]\n",
      "[-0.398244708776474]\n",
      "final score : -0.30305926501750946\n",
      "Word : inuit\n",
      "[0.34829169511795044]\n",
      "[0.06974420696496964]\n",
      "final score : 0.4180359020829201\n",
      "Word : and\n",
      "[0.17070919275283813]\n",
      "[-0.30853986740112305]\n",
      "final score : -0.1378306746482849\n",
      "Word : to\n",
      "[0.13541758060455322]\n",
      "[-0.3115454614162445]\n",
      "final score : -0.17612788081169128\n",
      "Word : this\n",
      "[0.131220281124115]\n",
      "[-0.3739129602909088]\n",
      "final score : -0.24269267916679382\n",
      "Word : end\n",
      "[0.08204780519008636]\n",
      "[-0.3172760307788849]\n",
      "final score : -0.23522822558879852\n",
      "1\n",
      "1\n",
      "Epoch 3 : Se_l_size = 1, Se_r_size = 1, weight_l = 0.5, weight_r = 0.5,\n",
      "Word : good\n",
      "[0.09518544375896454]\n",
      "[-0.398244708776474]\n",
      "final score : -0.30305926501750946\n",
      "Word : inuit\n",
      "[0.34829169511795044]\n",
      "[0.06974420696496964]\n",
      "final score : 0.4180359020829201\n",
      "Word : and\n",
      "[0.17070919275283813]\n",
      "[-0.30853986740112305]\n",
      "final score : -0.1378306746482849\n",
      "Word : to\n",
      "[0.13541758060455322]\n",
      "[-0.3115454614162445]\n",
      "final score : -0.17612788081169128\n",
      "Word : this\n",
      "[0.131220281124115]\n",
      "[-0.3739129602909088]\n",
      "final score : -0.24269267916679382\n",
      "Word : end\n",
      "[0.08204780519008636]\n",
      "[-0.3172760307788849]\n",
      "final score : -0.23522822558879852\n",
      "SIM_REF:-0.1378306746482849\n",
      "{'bad': -1.0, 'aboriginal': 1.0, 'good': -0.19164285426394737, 'inuit': 0.4885318959282136, 'and': 0.0, 'to': -0.04441958793625989, 'this': -0.12162576588506126, 'end': -0.11296800764835963, 'to this end': -0.09300445382322693}\n"
     ]
    }
   ],
   "source": [
    "from ngram_representation import missing_strat_random\n",
    "from ngram_representation import ngram_repr_bow_mean\n",
    "import gensim\n",
    "\n",
    "seeds_complex = ['aboriginal']\n",
    "seeds_non_complex = ['bad']\n",
    "\n",
    "# Build the vocabulary\n",
    "vocabulary = []\n",
    "vocabulary.extend(seeds_complex)\n",
    "vocabulary.extend(seeds_non_complex)\n",
    "vocabulary.extend(['good', 'inuit', 'and', 'to this end'])\n",
    "\n",
    "print('---------Complex Seeds----------------')\n",
    "print(seeds_complex)\n",
    "print('---------Non-Complex Seeds----------------')\n",
    "print(seeds_non_complex)\n",
    "print('---------Vocabulary----------------')\n",
    "print(vocabulary)\n",
    "\n",
    "model = models[0].model\n",
    "ref_term = 'and'\n",
    "lexicon = bootstrap_lexicon(model, vocabulary, seeds_non_complex, seeds_complex, \\\n",
    "                  ngram_embedding_similarity, missing_strat_random, ref_term, \\\n",
    "                  ngram_repr_bow_mean, agg_embed=False, epochs=3, thresh_l=-0.5, thresh_r=0.5)\n",
    "print(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Complex Seeds----------------\n",
      "['aboriginal']\n",
      "---------Non-Complex Seeds----------------\n",
      "['bad']\n",
      "---------Vocabulary----------------\n",
      "['aboriginal', 'bad', 'good', 'inuit', 'and', 'to this end', 'extraordinary']\n",
      "Missing vocab in models : 1 / 14.285714285714285%\n",
      "1\n",
      "1\n",
      "Epoch 1 : Se_l_size = 1, Se_r_size = 1, weight_l = 0.5, weight_r = 0.5,\n",
      "Word : good\n",
      "[0.11704779416322708]\n",
      "[-0.4278191030025482]\n",
      "final score : -0.31077130883932114\n",
      "Word : inuit\n",
      "[0.3482291102409363]\n",
      "[0.0792631059885025]\n",
      "final score : 0.4274922162294388\n",
      "Word : and\n",
      "[0.19104844331741333]\n",
      "[-0.3692472577095032]\n",
      "final score : -0.17819881439208984\n",
      "Word : to\n",
      "[0.15895500779151917]\n",
      "[-0.36740386486053467]\n",
      "final score : -0.2084488570690155\n",
      "Word : this\n",
      "[0.15309616923332214]\n",
      "[-0.4176422953605652]\n",
      "final score : -0.26454612612724304\n",
      "Word : end\n",
      "[0.1130194142460823]\n",
      "[-0.3775354027748108]\n",
      "final score : -0.2645159885287285\n",
      "Word : extraordinary\n",
      "[0.20164288580417633]\n",
      "[-0.2545890212059021]\n",
      "final score : -0.05294613540172577\n",
      "1\n",
      "1\n",
      "Epoch 2 : Se_l_size = 1, Se_r_size = 1, weight_l = 0.5, weight_r = 0.5,\n",
      "Word : good\n",
      "[0.11704779416322708]\n",
      "[-0.4278191030025482]\n",
      "final score : -0.31077130883932114\n",
      "Word : inuit\n",
      "[0.3482291102409363]\n",
      "[0.0792631059885025]\n",
      "final score : 0.4274922162294388\n",
      "Word : and\n",
      "[0.19104844331741333]\n",
      "[-0.3692472577095032]\n",
      "final score : -0.17819881439208984\n",
      "Word : to\n",
      "[0.15895500779151917]\n",
      "[-0.36740386486053467]\n",
      "final score : -0.2084488570690155\n",
      "Word : this\n",
      "[0.15309616923332214]\n",
      "[-0.4176422953605652]\n",
      "final score : -0.26454612612724304\n",
      "Word : end\n",
      "[0.1130194142460823]\n",
      "[-0.3775354027748108]\n",
      "final score : -0.2645159885287285\n",
      "Word : extraordinary\n",
      "[0.20164288580417633]\n",
      "[-0.2545890212059021]\n",
      "final score : -0.05294613540172577\n",
      "1\n",
      "1\n",
      "Epoch 3 : Se_l_size = 1, Se_r_size = 1, weight_l = 0.5, weight_r = 0.5,\n",
      "Word : good\n",
      "[0.11704779416322708]\n",
      "[-0.4278191030025482]\n",
      "final score : -0.31077130883932114\n",
      "Word : inuit\n",
      "[0.3482291102409363]\n",
      "[0.0792631059885025]\n",
      "final score : 0.4274922162294388\n",
      "Word : and\n",
      "[0.19104844331741333]\n",
      "[-0.3692472577095032]\n",
      "final score : -0.17819881439208984\n",
      "Word : to\n",
      "[0.15895500779151917]\n",
      "[-0.36740386486053467]\n",
      "final score : -0.2084488570690155\n",
      "Word : this\n",
      "[0.15309616923332214]\n",
      "[-0.4176422953605652]\n",
      "final score : -0.26454612612724304\n",
      "Word : end\n",
      "[0.1130194142460823]\n",
      "[-0.3775354027748108]\n",
      "final score : -0.2645159885287285\n",
      "Word : extraordinary\n",
      "[0.20164288580417633]\n",
      "[-0.2545890212059021]\n",
      "final score : -0.05294613540172577\n",
      "SIM_REF:-0.17819881439208984\n",
      "{'bad': -1.0, 'aboriginal': 1.0, 'good': -0.1613194246600698, 'inuit': 0.5140821932791066, 'and': 0.0, 'to': -0.036809441512972295, 'this': -0.1050708045295403, 'end': -0.10503413191450597, 'extraordinary': 0.10630861061848051, 'to this end': -0.08230479265233952}\n"
     ]
    }
   ],
   "source": [
    "from ngram_representation import missing_strat_random\n",
    "from ngram_representation import ngram_repr_bow_mean\n",
    "import gensim\n",
    "\n",
    "seeds_complex = ['aboriginal']\n",
    "seeds_non_complex = ['bad']\n",
    "\n",
    "# Build the vocabulary\n",
    "vocabulary = []\n",
    "vocabulary.extend(seeds_complex)\n",
    "vocabulary.extend(seeds_non_complex)\n",
    "vocabulary.extend(['good', 'inuit', 'and', 'to this end', 'extraordinary'])\n",
    "\n",
    "print('---------Complex Seeds----------------')\n",
    "print(seeds_complex)\n",
    "print('---------Non-Complex Seeds----------------')\n",
    "print(seeds_non_complex)\n",
    "print('---------Vocabulary----------------')\n",
    "print(vocabulary)\n",
    "\n",
    "models_selected = [model.model for model in models if model.type == 'glove']\n",
    "ref_term = 'and'\n",
    "lexicon = bootstrap_lexicon_multiple_embeddings(models_selected, vocabulary, seeds_non_complex, seeds_complex, \\\n",
    "                  ngram_embedding_similarity, missing_strat_random, ref_term, \\\n",
    "                  ngram_repr_bow_mean, agg_embed=False, epochs=3, thresh_l=-0.5, thresh_r=0.5)\n",
    "print(lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Complexity Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4.1) MWE with Embedding Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4.2) MWE with Complexity Score Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Training Targets----------------\n",
      "Targets complex : 7688\n",
      "Target non-complex : 6316\n",
      "Targets clean-complex : 6559\n",
      "Target clean-non-complex : 5187\n",
      "---------Testing Seeds----------------\n",
      "Target test : 3328\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "Dataset :Wikipedia\n",
      "---------Training Targets----------------\n",
      "Targets complex : 1901\n",
      "Target non-complex : 1603\n",
      "---------Testing Seeds----------------\n",
      "Target test : 694\n",
      "Missing vocab in model : 192.0 / 4.5736064792758455%\n",
      "Epoch 1 : Se_l_size = 1603, Se_r_size = 1901, weight_l = 0.5425228310502284, weight_r = 0.4574771689497717,\n",
      "0|5306:0.0%   > \u001b[0;32m<ipython-input-55-d8c434c28a3a>\u001b[0m(48)\u001b[0;36mbootstrap_lexicon\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     46 \u001b[0;31m            \u001b[1;31m#print('{} : {}'.format(curr_word, score))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     47 \u001b[0;31m            \u001b[1;31m# Add word to the seed set if the score is low or high enough\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 48 \u001b[0;31m            \u001b[1;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mthresh_l\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mse_l\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurr_word\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     49 \u001b[0;31m            \u001b[1;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mthresh_r\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mse_r\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurr_word\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     50 \u001b[0;31m        \u001b[1;31m#print(lexicon)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> score\n",
      "-70.84000555889162\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ngram_representation import missing_strat_random\n",
    "from ngram_representation import ngram_repr_bow_mean\n",
    "from ngram_representation import ngram_repr_bow_max\n",
    "from ngram_representation import ngram_repr_bow_min\n",
    "from ngram_representation import ngram_repr_wiki_weighted_bow\n",
    "\n",
    "TRAIN_ENGLISH_WIKIPEDIA = \"../cwishareddataset/traindevset/\" + \\\n",
    "                           \"english/Wikipedia_Train.tsv\"\n",
    "TEST_ENGLISH_WIKIPEDIA = \"../cwishareddataset/testset/\" + \\\n",
    "                           \"english/Wikipedia_Test.tsv\"\n",
    "df = pd.read_csv(TRAIN_ENGLISH_WIKIPEDIA, sep = \"\\t\")\n",
    "df_test = pd.read_csv(TEST_ENGLISH_WIKIPEDIA, sep = '\\t')\n",
    "df.columns = ['id', 'sentence', \"start\", \"end\", \"target\", \n",
    "              \"nat\", \"non_nat\", \"nat_marked\", \"non_nat_marked\", \"binary\", \"prob\"]\n",
    "df_test.columns = ['id', 'sentence', \"start\", \"end\", \"target\", \n",
    "              \"nat\", \"non_nat\", \"nat_marked\", \"non_nat_marked\", \"binary\", \"prob\"]\n",
    "\n",
    "Target = namedtuple('Target', 'target, dataset')\n",
    "Lexicon = namedtuple('Lexicon', 'name, epochs, thresh_l, thresh_r, agg_embed, lexicon')\n",
    "\n",
    "# Get the basic seeds for complex and non_complex words\n",
    "# Group targets into complex and non-complex and flatten MWE\n",
    "targets_complex = set([Target(mwe, ds.name) for ds in datasets \n",
    "                    for mwe in ds.train.loc[ds.train['binary'] == 1,]['target'].tolist()])\n",
    "targets_non_complex = set([Target(mwe, ds.name) for ds in datasets \n",
    "                    for mwe in ds.train.loc[ds.train['binary'] == 0,]['target'].tolist()])\n",
    "\n",
    "# Get the rest of the vocabulary as the test data to compute\n",
    "# the complexity score on\n",
    "targets_test = [Target(mwe, ds.name) for ds in datasets \n",
    "                for mwe in ds.test['target'].tolist()]\n",
    "\n",
    "# Clean groups from overlapping words\n",
    "targets_complex_cleaned = list(targets_complex.difference(targets_non_complex))\n",
    "targets_non_complex_cleaned = list(targets_non_complex.difference(targets_complex))\n",
    "targets_complex = list(targets_complex)\n",
    "targets_non_complex = list(targets_non_complex)\n",
    "\n",
    "print('---------Training Targets----------------')\n",
    "print('Targets complex : {}'.format(len(targets_complex)))\n",
    "print('Target non-complex : {}'.format(len(targets_non_complex)))\n",
    "print('Targets clean-complex : {}'.format(len(targets_complex_cleaned)))\n",
    "print('Target clean-non-complex : {}'.format(len(targets_non_complex_cleaned)))\n",
    "print('---------Testing Seeds----------------')\n",
    "print('Target test : {}'.format(len(targets_test)))\n",
    "\n",
    "lexicons = []\n",
    "# Parameters for lexicon construction\n",
    "ref_term = 'and'\n",
    "epochs=3\n",
    "thresh_l=-0.5\n",
    "thresh_r=0.5\n",
    "agg_embed=False\n",
    "embedding_sim = ngram_embedding_similarity\n",
    "ngram_repr = ngram_repr_bow_mean\n",
    "missing_strat = missing_strat_random\n",
    "model = models[0].model\n",
    "\n",
    "for ds in datasets:\n",
    "    t_complex = [target.target for target in targets_complex_cleaned \n",
    "                 if target.dataset == ds.name]\n",
    "    t_non_complex = [target.target for target in targets_non_complex_cleaned\n",
    "                    if target.dataset == ds.name]\n",
    "    t_test = [target.target for target in targets_test \n",
    "              if target.dataset == ds.name]\n",
    "    vocabulary = []\n",
    "    vocabulary.extend(t_complex)\n",
    "    vocabulary.extend(t_non_complex)\n",
    "    vocabulary.extend(t_test)\n",
    "    print('-----------------------------------------')\n",
    "    print('-----------------------------------------')\n",
    "    print('Dataset :{}'.format(ds.name))\n",
    "    print('---------Training Targets----------------')\n",
    "    print('Targets complex : {}'.format(len(t_complex)))\n",
    "    print('Target non-complex : {}'.format(len(t_non_complex)))\n",
    "    print('---------Testing Seeds----------------')\n",
    "    print('Target test : {}'.format(len(t_test)))\n",
    "    lexicon = bootstrap_lexicon(model, vocabulary, t_non_complex, t_complex, \\\n",
    "                  embedding_sim=embedding_sim, \\\n",
    "                  missing_strat=missing_strat, ref_term=ref_term, \\\n",
    "                  ngram_repr=ngram_repr, agg_embed=agg_embed, epochs=epochs, \\\n",
    "                  thresh_l=thresh_l, thresh_r=thresh_r)\n",
    "    lexicons.append(Lexicon(ds.name, epochs, thresh_l, thresh_r, agg_embed, lexicon))\n",
    "\n",
    "print(len(lexicons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('knowledge', -0.6338390641618713), ('mind', -1.0)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(word, score) for word, score in lexicon.items() if score < -0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
