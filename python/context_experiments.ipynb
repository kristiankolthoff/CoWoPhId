{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context-Aware Complex Word Identification\n",
    "Here we devise and implement all the relevant methods for evaluating the influence of context words for the complexity of a given target word. Thus, we implement various context definition methods that extract context words for a target based on different ideas (e.g. local context, grammatical context and semantic context). Afterwards we compute features for the context and use these features to represent the context in the classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.wsd import lesk\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from collections import defaultdict\n",
    "\n",
    "Dataset = namedtuple('Dataset', 'name, train, test')\n",
    "FeatureDataset = namedtuple('FeatureDataset', 'name, fc, agg, train, test')\n",
    "FeatureCategory = namedtuple('FeatureCategory', 'name, func')\n",
    "Aggregation = namedtuple('Aggregation', 'name, agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "MAIN_PATH_DATASET = \"../cwishareddataset/traindevset/english/\"\n",
    "genres = ['Wikipedia', 'WikiNews', 'News']\n",
    "datasets = ['Train', 'Dev']\n",
    "columns = ['id', 'sentence', \"start\", \"end\", \"target\", \n",
    "           \"nat\", \"non_nat\", \"nat_marked\", \"non_nat_marked\", \"binary\", \"prob\"]\n",
    "\n",
    "\n",
    "datasets = [Dataset('Wikipedia', 'Train', 'Dev'),\n",
    "            Dataset('WikiNews', 'Train', 'Dev'),\n",
    "            Dataset('News', 'Train', 'Dev')]\n",
    "\n",
    "feature_categories = []\n",
    "\n",
    "def load_df(path):\n",
    "    df = pd.read_csv(path, header=None, sep = \"\\t\")\n",
    "    df.columns = columns\n",
    "    return df\n",
    "\n",
    "datasets = [Dataset(d.name, load_df(MAIN_PATH_DATASET + d.name + '_' + d.train + '.tsv'),\n",
    "                            load_df(MAIN_PATH_DATASET + d.name + '_' + d.test + '.tsv'))\n",
    "                            for d in datasets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import *\n",
    "from nltk import word_tokenize\n",
    "from functools import lru_cache\n",
    "from utils import penn_to_wn\n",
    "import re\n",
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "wordNetLemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def overlaps(start1, end1, start2, end2):\n",
    "    return bool(range(max(start1, start2), min(end1, end2)+1))\n",
    "\n",
    "tbl = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "                      if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(tbl)\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def targets_with_index(start, end, context):\n",
    "    curr_pos = 0\n",
    "    targets = []\n",
    "    j = 0\n",
    "    w = 0\n",
    "    curr_split = ''\n",
    "    ctx_split = context.split()\n",
    "    whitespaces = re.findall('\\s+', context)\n",
    "    num_whitespaces = [len(token) for token in whitespaces]\n",
    "    num_whitespaces.append(1)\n",
    "    tokens = word_tokenize(context)\n",
    "    tokens = ['\"' if token not in context else token for token in tokens]\n",
    "    for index, token in enumerate(tokens, 1):\n",
    "        targets.append((token, index, curr_pos, (curr_pos + len(token))))\n",
    "        curr_pos += len(token)\n",
    "        curr_split += token\n",
    "        if ctx_split[j] == curr_split:\n",
    "            curr_pos += num_whitespaces[w]\n",
    "            j += 1\n",
    "            w += 1\n",
    "            curr_split = ''\n",
    "    vals = [(target[0], target[1]) for target in targets \\\n",
    "            if overlaps(start, end, target[2], target[3])]\n",
    "    return [val for val in vals if val[0] != '\"']\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def wordnet_pos_tagging(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    return nltk.pos_tag(tokens)\n",
    "\n",
    "def pos_tags(start, end, sentence):\n",
    "    wordPOSPairs = wordnet_pos_tagging(sentence)\n",
    "    targets_index = targets_with_index(start, end, sentence)\n",
    "    results = [wordPOSPairs[tpl[1]-1][1] for tpl in targets_index]\n",
    "    filtered_results = [result for result in results \n",
    "                        if remove_punctuation(result).strip() and result != 'POS']\n",
    "    return filtered_results if len(filtered_results) > 0 else None\n",
    "\n",
    "def wordnet_lemma(target, pos):\n",
    "    tokens = nltk.word_tokenize(target)\n",
    "    if pos:\n",
    "        pos = [penn_to_wn(poss) if penn_to_wn(poss) else 'n' for poss in pos]\n",
    "        lemmas = [wordNetLemmatizer.lemmatize(token, poss)\n",
    "                     for token, poss in zip(tokens, pos)]\n",
    "        return ' '.join(lemmas)\n",
    "    return target\n",
    "\n",
    "def preprocessing(dataframe):\n",
    "    df = dataframe.copy()\n",
    "    df['p_sentence'] = df.sentence.apply(lambda sent : sent.strip().lower())\n",
    "    df['sentence'] = df.sentence.apply(lambda sent : sent.replace(\"''\", \"``\"))\n",
    "    df['p_target'] = df.target.apply(lambda target : target.strip().lower())\n",
    "    df['pos_tags'] = df[['start', 'end', 'sentence']].apply(lambda vals : pos_tags(*vals), axis = 1)\n",
    "    df['pos_tags_pt'] = df.pos_tags.apply(lambda pos : [penn_to_wn(poss) if penn_to_wn(poss) else 'n' for poss in pos])\n",
    "    df['lemma'] = df[['target', 'pos_tags']].apply(lambda vals : wordnet_lemma(*vals), axis = 1)\n",
    "    df['p_lemma'] = df.lemma.apply(lambda lemma : lemma.strip().lower())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessed_datasets = [Dataset(ds.name, preprocessing(ds.train), \n",
    "                               preprocessing(ds.test)) for ds in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasets = preprocessed_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Context-Token Aggregation\n",
    "First we define how feature values of multiple context-tokens should be aggreagated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_freq_wiki = {}\n",
    "sum_counts = 0\n",
    "with open(\"resources/word-freq-dumps/enwiki-20150602-words-frequency.txt\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        word, freq = line.partition(\" \")[::2]\n",
    "        sum_counts+=int(freq)\n",
    "        word_freq_wiki[word.strip()] = int(freq)\n",
    "        \n",
    "def get_unigram_probability(word):\n",
    "    return word_freq_wiki.get(word,1) / (sum_counts + len(word_freq_wiki))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def agg_ctx_feat_num_average(tokens, func_feature, *args, **kwargs):\n",
    "    if all(isinstance(tpl, tuple) for tpl in tokens):\n",
    "        return np.mean([func_feature(token, *args) for token, dist in tokens])\n",
    "    return np.mean([func_feature(token, *args) for token in tokens])\n",
    "\n",
    "def agg_ctx_feat_num_weighted_average(tokens, func_feature, alpha, *args):\n",
    "    if all(isinstance(tpl, tuple) for tpl in tokens):\n",
    "        if len(tokens)==1:\n",
    "            return np.mean([func_feature(token, *args) for token, dist in tokens])\n",
    "        prob_sum = np.sum([(alpha/(alpha+get_unigram_probability(token))) for token, dist in tokens])\n",
    "        return np.mean([((alpha/(alpha+get_unigram_probability(token)))/prob_sum) * \n",
    "                func_feature(token, *args) for token, dist in tokens])\n",
    "    prob_sum = np.sum([(alpha/(alpha+get_unigram_probability(token))) for token in tokens])\n",
    "    return np.mean([((alpha/(alpha+get_unigram_probability(token)))/prob_sum) * \n",
    "                func_feature(token, *args) for token in tokens])\n",
    "\n",
    "agg_feat_num_weighted_average_medium = lambda target, func_feature, *args: \\\n",
    "                        agg_ctx_feat_num_weighted_average(target, func_feature, 0.0001, *args)\n",
    "\n",
    "def agg_ctx_feat_num_distance(tokens, func_feature, *args):\n",
    "    if all(isinstance(tpl, tuple) for tpl in tokens):\n",
    "        if len(tokens)==1:\n",
    "            return np.mean([func_feature(token, *args) for token, dist in tokens])\n",
    "        dist_sum = np.sum(dist for token, dist in tokens)\n",
    "        return np.sum([(func_feature(token, *args) * ((dist_sum-dist)/dist_sum)) \n",
    "                    for token, dist in tokens])\n",
    "    return np.mean([func_feature(token, *args) for token in tokens])\n",
    "\n",
    "def agg_ctx_feat_num_median(tokens, func_feature, *args):\n",
    "    if all(isinstance(tpl, tuple) for tpl in tokens):\n",
    "        return np.median([func_feature(token, *args) for token, dist in tokens])\n",
    "    return np.median([func_feature(token, *args) for token in tokens])\n",
    "\n",
    "def agg_ctx_feat_num_max(tokens, func_feature, *args):\n",
    "    if all(isinstance(tpl, tuple) for tpl in tokens):\n",
    "        return np.max([func_feature(token, *args) for token, dist in tokens])\n",
    "    return np.max([func_feature(token, *args) for token in tokens])\n",
    "\n",
    "def agg_ctx_feat_num_min(tokens, func_feature, *args):\n",
    "    if all(isinstance(tpl, tuple) for tpl in tokens):\n",
    "        return np.min([func_feature(token, *args) for token, dist in tokens])\n",
    "    return np.min([func_feature(token, *args) for token in tokens])\n",
    "\n",
    "def agg_ctx_feat_num_sum(tokens, func_feature, *args):\n",
    "    if all(isinstance(tpl, tuple) for tpl in tokens):\n",
    "        return np.sum([func_feature(token, *args) for token, dist in tokens])\n",
    "    return np.sum([func_feature(token, *args) for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_default = [Aggregation('mean', agg_ctx_feat_num_average)]\n",
    "agg_distance = [Aggregation('dist', agg_ctx_feat_num_distance)]\n",
    "agg_weighted = [Aggregation('weighted', agg_feat_num_weighted_average_medium)]\n",
    "aggs_small = [Aggregation('mean', agg_ctx_feat_num_average), Aggregation('max', agg_ctx_feat_num_max)]\n",
    "aggs_all = [Aggregation('mean', agg_ctx_feat_num_average), Aggregation('median', agg_ctx_feat_num_median),\n",
    "            Aggregation('max', agg_ctx_feat_num_max), Aggregation('min', agg_ctx_feat_num_min)]\n",
    "           #Aggregation('weighted_mean', agg_ctx_feat_num_weighted_average_medium)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aggs = agg_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def concat_feature_datasets(*args):\n",
    "    zipped = zip(*args)\n",
    "    concat_features = []\n",
    "    for dataset in zipped:\n",
    "        df_train = None\n",
    "        df_test = None\n",
    "        fcs = []\n",
    "        aggs = []\n",
    "        for tpl in dataset:\n",
    "            if not fcs:\n",
    "                df_train = tpl.train.copy()\n",
    "                df_test = tpl.test.copy()\n",
    "            else:\n",
    "                df_train = pd.concat([df_train, tpl.train.copy()], axis = 1)\n",
    "                df_test = pd.concat([df_test, tpl.test.copy()], axis = 1)\n",
    "            fcs.append(tpl.fc)\n",
    "            aggs.append(tpl.agg)\n",
    "        concat_features.append(FeatureDataset(tpl.name, fcs, aggs,\n",
    "                    df_train.loc[:,~df_train.columns.duplicated()], \n",
    "                    df_test.loc[:,~df_test.columns.duplicated()]))\n",
    "    return concat_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Context Definition and Extraction\n",
    "Here we compute different kinds of context definitions. For example, as a baseline we extract all tokens from the sentence except the target. A second approach is to use a n preceeding or n succeding tokens, or a combined window apporach were we extract n tokens preceeding and succeding of the target. A more sophisticated apporach involves dependency parsing of the sentence and applying different extraction heuristics. Finally we also implement a context extraction approach exploting FrameNet semantic parsing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Context Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.parse.corenlp import *\n",
    "import os\n",
    "from functools import lru_cache\n",
    "\n",
    "# First make sure that the StanfordCoreNLP Server is running under port 9010\n",
    "# cd to stanfordCoreNLP directory\n",
    "# java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9010 -timeout 15000\n",
    "parser = CoreNLPDependencyParser(url='http://localhost:9010/')\n",
    "\n",
    "with open(\"resources/dictionaries/stopwords_en.txt\", encoding=\"utf8\") as file:\n",
    "    content = [line.strip().lower() for line in file.readlines()]\n",
    "    stop_words = set(content)\n",
    "    \n",
    "def overlaps(start1, end1, start2, end2):\n",
    "    return bool(range(max(start1, start2), min(end1, end2)+1))\n",
    "\n",
    "def post_process_ctx(context, filtering=True):\n",
    "    return [token for token in context if \n",
    "            (token.isalnum() and (not filtering\n",
    "        or preprocess_target(token).lower() not in stop_words))]\n",
    "\n",
    "def preprocess_target(target):\n",
    "    return target.strip()\n",
    "\n",
    "def target_index_char_based(start, end, ctx_tokens):\n",
    "    size = np.sum([len(token) for token in ctx_tokens]) + len(ctx_tokens)\n",
    "    target_pos = (start + end) / 2\n",
    "    target_pos_rel = target_pos / size\n",
    "    return int(target_pos_rel * len(post_process_ctx(ctx_tokens)))\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def targets_with_index(start, end, context):\n",
    "    curr_pos = 0\n",
    "    targets = []\n",
    "    j = 0\n",
    "    w = 0\n",
    "    curr_split = ''\n",
    "    ctx_split = context.split()\n",
    "    whitespaces = re.findall('\\s+', context)\n",
    "    num_whitespaces = [len(token) for token in whitespaces]\n",
    "    num_whitespaces.append(1)\n",
    "    tokens = word_tokenize(context)\n",
    "    tokens = ['\"' if token not in context else token for token in tokens]\n",
    "    for index, token in enumerate(tokens, 1):\n",
    "        targets.append((token, index, curr_pos, (curr_pos + len(token))))\n",
    "        curr_pos += len(token)\n",
    "        curr_split += token\n",
    "        if ctx_split[j] == curr_split:\n",
    "            curr_pos += num_whitespaces[w]\n",
    "            j += 1\n",
    "            w += 1\n",
    "            curr_split = ''\n",
    "    vals = [(target[0], target[1]) for target in targets \\\n",
    "            if overlaps(start, end, target[2], target[3])]\n",
    "    return [val for val in vals if val[0] != '\"']\n",
    "\n",
    "from joblib import Memory\n",
    "memory = Memory(location='resources/dependency-cache', verbose=0)\n",
    "@memory.cache\n",
    "def dependency_parse_with_root(sentence):\n",
    "    try:\n",
    "        dependency_parser = parser.raw_parse(sentence)\n",
    "        dependencies = []\n",
    "        parsetree = list(dependency_parser)[0]\n",
    "        for index, node in parsetree.nodes.items():\n",
    "            for relation, dependant in parsetree.nodes[index]['deps'].items():\n",
    "                for dep in dependant:\n",
    "                    triple = ((node['word'], index), relation, \\\n",
    "                              (parsetree.nodes[dep]['word'], dep))\n",
    "                    dependencies.append(triple)\n",
    "        return dependencies\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def dependency_parse(sentence):\n",
    "    dependencies = dependency_parse_with_root(sentence)\n",
    "    filtered_dependencies = [triple for triple in dependencies if triple[1] != 'ROOT']\n",
    "    return filtered_dependencies\n",
    "\n",
    "def ctx_extraction_sentence(context, target):\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    if target in ctx_tokens:\n",
    "        ctx_tokens.remove(target)\n",
    "    return ctx_tokens\n",
    "\n",
    "def ctx_extraction_sentence_filtered(context, target, start, end, filtering = True):\n",
    "    context = context[:start] + context[end:]\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    post_ctx_tokens = post_process_ctx(ctx_tokens, filtering)\n",
    "    return ctx_tokens\n",
    "\n",
    "def ctx_extraction_hit(context, filtering = True):\n",
    "    hit_tokens = [token for sentence in context for token in word_tokenize(sentence)]\n",
    "    post_ctx_tokens = post_process_ctx(hit_tokens, filtering)\n",
    "    return post_ctx_tokens\n",
    "\n",
    "def ctx_extraction_window_pre_n(context, target, start, end, \n",
    "                            filtering = True, n = 3, dist = True):\n",
    "    target = preprocess_target(target)\n",
    "    ctx_tokens = word_tokenize(context[:start])\n",
    "    post_ctx_tokens = post_process_ctx(ctx_tokens, filtering)\n",
    "    return [(elem, index) for index, elem in zip(range(n, 0, -1), post_ctx_tokens[-n:])] \\\n",
    "                if dist else post_ctx_tokens[-n:]\n",
    "\n",
    "def ctx_extraction_window_suc_n(context, target, start, end, \n",
    "                            filtering = True, n = 3, dist = True):\n",
    "    target = preprocess_target(target)\n",
    "    ctx_tokens = word_tokenize(context[end:])\n",
    "    post_ctx_tokens = post_process_ctx(ctx_tokens, filtering)\n",
    "    return [(elem, index) for index, elem in zip(range(1, (n+1)), post_ctx_tokens[:n])] \\\n",
    "                if dist else post_ctx_tokens[:n]\n",
    "\n",
    "def ctx_extraction_window_pre_suc_n(context, target, start, end, \n",
    "                                filtering = True, n = 3, dist = True):\n",
    "    ctx_tokens_pre = ctx_extraction_window_pre_n(context, target, start, end, filtering, n, dist)\n",
    "    ctx_tokens_suc = ctx_extraction_window_suc_n(context, target, start, end, filtering, n, dist)\n",
    "    ctx_tokens_pre.extend(ctx_tokens_suc)\n",
    "    return ctx_tokens_pre\n",
    "\n",
    "def ctx_extraction_dep_in(context, target, start, end, dist = True):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    selec_tuples = list(set([triple for triple in triples \\\n",
    "                if triple[2] in targets and triple[0] not in targets]))\n",
    "    return [(triple[0][0], np.abs(triple[0][1]-triple[2][1])) for triple in selec_tuples] if dist \\\n",
    "                else [triple[0][0] for triple in selec_tuples]\n",
    "\n",
    "def ctx_extraction_dep_out(context, target, start, end, dist = True):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    selec_tuples = list(set([triple for triple in triples \\\n",
    "                if triple[0] in targets and triple[2] not in targets]))\n",
    "    return [(triple[2][0], np.abs(triple[0][1]-triple[2][1])) for triple in selec_tuples] if dist \\\n",
    "                else [triple[2][0] for triple in selec_tuples]\n",
    "\n",
    "def ctx_extraction_dep_in_out(context, target, start, end, dist = True):\n",
    "    ctx_tokens_in = ctx_extraction_dep_in(context, target, start, end, dist)\n",
    "    ctx_tokens_out = ctx_extraction_dep_out(context, target, start, end, dist)\n",
    "    ctx_tokens_in.extend(ctx_tokens_out)\n",
    "    return list(set(ctx_tokens_in))\n",
    "\n",
    "def ctx_extraction_dep_recu_in_n_steps(context, target, start, \n",
    "                            end, n = 2, dist = True):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    result_tokens = []\n",
    "    curr_target = targets\n",
    "    for step in range(0, n):\n",
    "        step_result = [triple[0] for triple in triples \n",
    "                       if triple[2] in curr_target]\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "    mean_target_index = np.mean([tgt[1] for tgt in targets])\n",
    "    unique = list(set([result for result in result_tokens if result not in targets]))\n",
    "    return [(token[0], np.abs(token[1]-mean_target_index))\n",
    "                    for token in unique] if dist \\\n",
    "                else [token[0] for token in unique]\n",
    "\n",
    "def ctx_extraction_dep_recu_out_n_steps(context, target, start, \n",
    "                        end, n = 2, dist = True):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    result_tokens = []\n",
    "    curr_target = targets\n",
    "    for step in range(0, n):\n",
    "        step_result = [triple[2] for triple in triples \n",
    "                       if triple[0] in curr_target]\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "    mean_target_index = np.mean([tgt[1] for tgt in targets])\n",
    "    unique = list(set([result for result in result_tokens if result not in targets]))\n",
    "    return [(token[0], np.abs(token[1]-mean_target_index))\n",
    "                    for token in unique] if dist \\\n",
    "                else [token[0] for token in unique]\n",
    "\n",
    "def ctx_extraction_dep_recu_in_out_n_steps(context, target, start, \n",
    "                            end, n = 2, dist = True):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    result_tokens = []\n",
    "    curr_target = targets\n",
    "    for step in range(0, n):\n",
    "        step_result = [triple[2] for triple in triples \n",
    "                       if triple[0] in curr_target]\n",
    "        step_result_out = [triple[0] for triple in triples \n",
    "                       if triple[2] in curr_target]\n",
    "        step_result.extend(step_result_out)\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "    mean_target_index = np.mean([tgt[1] for tgt in targets])\n",
    "    unique = list(set([result for result in result_tokens if result not in targets]))\n",
    "    return [(token[0], np.abs(token[1]-mean_target_index))\n",
    "                    for token in unique] if dist \\\n",
    "                else [token[0] for token in unique]\n",
    "\n",
    "def ctx_extraction_dep_recu_in_cover(context, target, start, \n",
    "                        end, cover = 0.1, dist = True):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    ctx_tokens_post = post_process_ctx(ctx_tokens)\n",
    "    result_tokens = []\n",
    "    curr_target = targets\n",
    "    curr_cover = 0\n",
    "    while curr_cover < cover:\n",
    "        step_result = [triple[0] for triple in triples \n",
    "                       if triple[2] in curr_target]\n",
    "        if set(step_result) == set(curr_target):\n",
    "                break\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "        curr_cover = len(result_tokens) / len(ctx_tokens_post)\n",
    "    mean_target_index = np.mean([tgt[1] for tgt in targets])\n",
    "    unique = list(set([result for result in result_tokens if result not in targets]))\n",
    "    return [(token[0], np.abs(token[1]-mean_target_index))\n",
    "                    for token in unique] if dist \\\n",
    "                else [token[0] for token in unique]\n",
    "\n",
    "def ctx_extraction_dep_recu_out_cover(context, target, start, \n",
    "                        end, cover = 0.1, dist = True):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    ctx_tokens_post = post_process_ctx(ctx_tokens)\n",
    "    result_tokens = []\n",
    "    curr_target = targets\n",
    "    curr_cover = 0\n",
    "    while curr_cover < cover:\n",
    "        step_result = [triple[2] for triple in triples \n",
    "                       if triple[0] in curr_target]\n",
    "        if set(step_result) == set(curr_target):\n",
    "                break\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "        curr_cover = len(result_tokens) / len(ctx_tokens_post)\n",
    "    mean_target_index = np.mean([tgt[1] for tgt in targets])\n",
    "    unique = list(set([result for result in result_tokens if result not in targets]))\n",
    "    return [(token[0], np.abs(token[1]-mean_target_index))\n",
    "                    for token in unique] if dist \\\n",
    "                else [token[0] for token in unique]\n",
    "\n",
    "def ctx_extraction_dep_recu_in_out_cover(context, target, start,\n",
    "                        end, cover = 0.1, dist = True):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    ctx_tokens_post = post_process_ctx(ctx_tokens)\n",
    "    result_tokens = []\n",
    "    curr_target = targets\n",
    "    curr_cover = 0\n",
    "    while curr_cover < cover:\n",
    "        step_result = [triple[2] for triple in triples \n",
    "                       if triple[0] in curr_target]\n",
    "        step_result_out = [triple[0] for triple in triples \n",
    "                       if triple[2] in curr_target]\n",
    "        step_result.extend(step_result_out)\n",
    "        if set(step_result) == set(curr_target):\n",
    "                break\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "        curr_cover = len(result_tokens) / len(ctx_tokens_post)\n",
    "    mean_target_index = np.mean([tgt[1] for tgt in targets])\n",
    "    unique = list(set([result for result in result_tokens if result not in targets]))\n",
    "    return [(token[0], np.abs(token[1]-mean_target_index))\n",
    "                    for token in unique] if dist \\\n",
    "                else [token[0] for token in unique]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Normally, the land will be passed down by future generations in a way \" + \\\n",
    "             \"that recognizes the community's traditional connection to that country .\"\n",
    "target = 'passed'\n",
    "\n",
    "print('ctx_etraction_all:')\n",
    "print(ctx_extraction_sentence_filtered(sentence, target, 28, 34,))\n",
    "\n",
    "print('ctx_extraction_window_pre_n:')\n",
    "print(ctx_extraction_window_pre_n(sentence, \"Normally\", 0, 8, filtering=False))\n",
    "print(ctx_extraction_window_pre_n(sentence, \"the\", 11, 14, filtering=False))\n",
    "print(ctx_extraction_window_pre_n(sentence, \"land\", 15, 19, filtering=False))\n",
    "print(ctx_extraction_window_pre_n(sentence, \"to\", 126, 128, filtering=False))\n",
    "print(ctx_extraction_window_pre_n(sentence, target, 28, 34, n = 5, filtering=False))\n",
    "\n",
    "print('ctx_extraction_window_suc_n:')\n",
    "print(ctx_extraction_window_suc_n(sentence, \"country\", 135, 142, filtering=False))\n",
    "print(ctx_extraction_window_suc_n(sentence, \"to\", 126, 128, filtering=False))\n",
    "print(ctx_extraction_window_suc_n(sentence, \"connection\", 115, 125, filtering=False))\n",
    "print(ctx_extraction_window_suc_n(sentence, \"community\", 91, 100, n = 5, filtering=False))\n",
    "\n",
    "print('ctx_extraction_window_pre_suc_n:')\n",
    "print(ctx_extraction_window_pre_suc_n(sentence, \"passed\", 28, 34, filtering=False))\n",
    "print(ctx_extraction_window_pre_suc_n(sentence, \"the\", 11, 14, filtering=False))\n",
    "print(ctx_extraction_window_pre_suc_n(sentence, \"to\", 127, 129, filtering=False))\n",
    "\n",
    "print('ctx_extraction_dep_in:')\n",
    "print(ctx_extraction_dep_in(sentence, \"land\", 15, 19))\n",
    "\n",
    "print('ctx_extraction_dep_out:')\n",
    "print(ctx_extraction_dep_out(sentence, target, 28, 34))\n",
    "print(ctx_extraction_dep_out(sentence, \"land\", 15, 19))\n",
    "\n",
    "print('ctx_extraction_dep_in_out:')\n",
    "print(ctx_extraction_dep_in_out(sentence, \"land\", 15, 19))\n",
    "\n",
    "print('ctx_extraction_dep_recu_in_n_steps:')\n",
    "print(ctx_extraction_dep_recu_in_n_steps(sentence, \"the\", 11, 14, n = 3))\n",
    "\n",
    "print('ctx_extraction_dep_recu_out_n_steps:')\n",
    "print(ctx_extraction_dep_recu_out_n_steps(sentence, \"the\", 11, 14))\n",
    "\n",
    "print('ctx_extraction_dep_recu_in_out_n_steps:')\n",
    "print(ctx_extraction_dep_recu_in_out_n_steps(sentence, \"the\", 11, 14))\n",
    "\n",
    "print('ctx_extraction_dep_recu_in_cover:')\n",
    "print(ctx_extraction_dep_recu_in_cover(sentence, \"the\", 11, 14, cover=0.1))\n",
    "\n",
    "print('ctx_extraction_dep_recu_out_cover:')\n",
    "print(ctx_extraction_dep_recu_out_cover(sentence, \"the\", 11, 14, cover=0.1))\n",
    "\n",
    "print('ctx_extraction_dep_recu_in_out_cover:')\n",
    "print(ctx_extraction_dep_recu_in_out_cover(sentence, \"the\", 11, 14, cover=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Context Extraction\n",
    "\n",
    "After we defined all the context extraction approaches, we can apply them on the actual dataset. To do so, we first extract all the distinct sentences from the actual training set and create a new dataframe containing only the sentence ids, the sentence, the target and all the computed contexts. This also makes it easier to integrate context extraction functions implemented in other languages. Afterwards we can compute the context features and join them back with the target features dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Context = namedtuple('Context', 'name, params, func')\n",
    "ContextFeatureCategory = namedtuple('ContextFeatureCategory', 'name, func')\n",
    "ContextDataset = namedtuple('ContextDataset', 'name, context, train, test')\n",
    "ContextFeatureDataset = namedtuple('ContextFeatureDataset', 'name, context, fc, agg, train, test')\n",
    "ctx_fcs = []\n",
    "ctx_feature_datasets = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.2.1) Extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctx_window(dataframe, n, filtering, window_func):\n",
    "    df = dataframe.copy()\n",
    "    df['context'] = df.apply(lambda columns : \n",
    "                window_func(columns['sentence'], columns['target'], \\\n",
    "                columns['start'], columns['end'],  n = n, filtering = filtering), axis = 1)\n",
    "    return df\n",
    "\n",
    "ctx_window_pre_2_nf = Context('ctx_window_pre_n', {'n':2, 'filtering':False}, \\\n",
    "                              lambda dataframe : ctx_window(dataframe, 2, False, ctx_extraction_window_pre_n))\n",
    "ctx_window_pre_2_f = Context('ctx_window_pre_n', {'n':2, 'filtering':True}, \\\n",
    "                             lambda dataframe : ctx_window(dataframe, 2, True, ctx_extraction_window_pre_n))\n",
    "ctx_window_suc_n_2_nf = Context('ctx_window_suc_n',  {'n':2, 'filtering':False}, \\\n",
    "                            lambda dataframe : ctx_window(dataframe, 2, False, ctx_extraction_window_suc_n))\n",
    "ctx_window_pre_suc_n_2_nf = Context('ctx_window_pre_suc_n',  {'n':2, 'filtering':False}, \\\n",
    "                            lambda dataframe : ctx_window(dataframe, 2, False, ctx_extraction_window_pre_suc_n))\n",
    "\n",
    "\n",
    "\n",
    "def ctx_dependency(dataframe, filtering, dep_func):\n",
    "    df = dataframe.copy()\n",
    "    df['context'] = df.apply(lambda columns : \n",
    "            dep_func(columns['sentence'], columns['target'], \\\n",
    "            columns['start'], columns['end']), axis = 1)\n",
    "    return df\n",
    "\n",
    "ctx_dep_in_2_nf = Context('ctx_dep_in', {'n':2, 'filtering':False}, \\\n",
    "                              lambda dataframe : ctx_dependency(dataframe, False, ctx_extraction_dep_in))\n",
    "ctx_dep_out_2_nf = Context('ctx_dep_out', {'n':2, 'filtering':False}, \\\n",
    "                              lambda dataframe : ctx_dependency(dataframe, False, ctx_extraction_dep_out))\n",
    "ctx_dep_in_out_2_nf = Context('ctx_dep_in_out', {'n':2, 'filtering':False}, \\\n",
    "                              lambda dataframe : ctx_dependency(dataframe, False, ctx_extraction_dep_in_out))\n",
    "\n",
    "\n",
    "\n",
    "def ctx_dependency_recu_steps(dataframe, n, filtering, dep_func):\n",
    "    df = dataframe.copy()\n",
    "    df['context']  = df.apply(lambda columns : \n",
    "                dep_func(columns['sentence'], columns['target'], \\\n",
    "                columns['start'], columns['end'], n=n), axis = 1)\n",
    "    return df\n",
    "\n",
    "ctx_dep_rec_in_2_nf = Context('ctx_dep_rec_in_n', {'n':2, 'filtering':False}, \\\n",
    "                lambda dataframe : ctx_dependency_recu_steps(dataframe, 2, False, ctx_extraction_dep_recu_in_n_steps))\n",
    "ctx_dep_rec_out_2_nf = Context('ctx_dep_rec_out_n', {'n':2, 'filtering':False}, \\\n",
    "                lambda dataframe : ctx_dependency_recu_steps(dataframe, 2, False, ctx_extraction_dep_recu_out_n_steps))\n",
    "ctx_dep_rec_in_out_2_nf = Context('ctx_dep_rec_in_out_n', {'n':2, 'filtering':False}, \\\n",
    "                lambda dataframe : ctx_dependency_recu_steps(dataframe, 2, False, ctx_extraction_dep_recu_in_out_n_steps))\n",
    "\n",
    "\n",
    "\n",
    "def ctx_dependency_recu_cover(dataframe, cover, filtering, dep_func):\n",
    "    df = dataframe.copy()\n",
    "    df['context']  = df.apply(lambda columns : \n",
    "                dep_func(columns['sentence'], columns['target'], \\\n",
    "                columns['start'], columns['end'], cover=cover), axis = 1)\n",
    "    return df\n",
    "\n",
    "ctx_dep_rec_in_02_nf = Context('ctx_dep_rec_in_02', {'cover': 0.2, 'filtering':False}, \\\n",
    "                lambda dataframe : ctx_dependency_recu_cover(dataframe, 0.2, False, ctx_extraction_dep_recu_in_cover))\n",
    "ctx_dep_rec_out_02_nf = Context('ctx_dep_rec_out_02', {'cover': 0.2, 'filtering':False}, \\\n",
    "                lambda dataframe : ctx_dependency_recu_cover(dataframe, 0.2, False, ctx_extraction_dep_recu_out_cover))\n",
    "ctx_dep_rec_in_out_02_nf = Context('ctx_dep_rec_in_out_02', {'cover': 0.2, 'filtering':False}, \\\n",
    "                lambda dataframe : ctx_dependency_recu_cover(dataframe, 0.2, False, ctx_extraction_dep_recu_in_out_cover))\n",
    "\n",
    "\n",
    "\n",
    "def ctx_sentence(dataframe, filtering):\n",
    "    df = dataframe.copy()\n",
    "    df['context']  = df.apply(lambda columns : \n",
    "                ctx_extraction_sentence_filtered(columns['sentence'], columns['target'], \\\n",
    "                columns['start'], columns['end'], filtering=filtering), axis = 1)\n",
    "    return df\n",
    "\n",
    "ctx_sentence_nf = Context('ctx_sentence', {'filtering':False}, lambda dataframe : ctx_sentence(dataframe, False))\n",
    "\n",
    "\n",
    "\n",
    "def ctx_hit(dataframe, filtering):\n",
    "    df = dataframe.copy()\n",
    "    df = df.join(df.groupby('id')['sentence'].apply(lambda sentences : \\\n",
    "                    tuple(ctx_extraction_hit(list(set(sentences))))), on='id', rsuffix='_hits')\n",
    "    df['sentence_hits'] = df.sentence_hits.apply(lambda hits : list(hits))\n",
    "    df.rename(columns={'sentence_hits':'context'}, inplace=True)\n",
    "    return df\n",
    "\n",
    "ctx_hit_nf = Context('ctx_sentence', {'filtering':False}, lambda dataframe : ctx_hit(dataframe, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.2.2) Context Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "tbl = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "                      if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(tbl)\n",
    "\n",
    "def preprocess_ctx(context):\n",
    "    if all(isinstance(tpl, tuple) for tpl in context):\n",
    "        stripped = [(token.strip().lower(), dist) for token, dist in context]\n",
    "        return [(token, dist) for token, dist in stripped if remove_punctuation(token)]\n",
    "    stripped = [token.strip().lower() for token in context]\n",
    "    return [token for token in stripped if remove_punctuation(token)]\n",
    "\n",
    "def preprocess_ctx_df(dataframe):\n",
    "    df = dataframe.copy()\n",
    "    df['p_context'] = df.context.apply(lambda context : preprocess_ctx(context))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = [ctx_window_pre_2_nf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_datasets = [ContextDataset(ds.name, ctx, preprocess_ctx_df(ctx.func(ds.train.loc[:30,])), \n",
    "                preprocess_ctx_df(ctx.func(ds.test.loc[:30,])))\n",
    "                for ctx in contexts\n",
    "                for ds in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>target</th>\n",
       "      <th>nat</th>\n",
       "      <th>non_nat</th>\n",
       "      <th>nat_marked</th>\n",
       "      <th>non_nat_marked</th>\n",
       "      <th>binary</th>\n",
       "      <th>prob</th>\n",
       "      <th>p_sentence</th>\n",
       "      <th>p_target</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>pos_tags_pt</th>\n",
       "      <th>lemma</th>\n",
       "      <th>p_lemma</th>\n",
       "      <th>context</th>\n",
       "      <th>p_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>Normally</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .</td>\n",
       "      <td>normally</td>\n",
       "      <td>[RB]</td>\n",
       "      <td>[r]</td>\n",
       "      <td>Normally</td>\n",
       "      <td>normally</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .</td>\n",
       "      <td>28</td>\n",
       "      <td>34</td>\n",
       "      <td>passed</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .</td>\n",
       "      <td>passed</td>\n",
       "      <td>[VBN]</td>\n",
       "      <td>[v]</td>\n",
       "      <td>pass</td>\n",
       "      <td>pass</td>\n",
       "      <td>[(will, 2), (be, 1)]</td>\n",
       "      <td>[(will, 2), (be, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>land</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .</td>\n",
       "      <td>land</td>\n",
       "      <td>[NN]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>land</td>\n",
       "      <td>land</td>\n",
       "      <td>[(Normally, 2), (the, 1)]</td>\n",
       "      <td>[(normally, 2), (the, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .</td>\n",
       "      <td>43</td>\n",
       "      <td>49</td>\n",
       "      <td>future</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .</td>\n",
       "      <td>future</td>\n",
       "      <td>[JJ]</td>\n",
       "      <td>[a]</td>\n",
       "      <td>future</td>\n",
       "      <td>future</td>\n",
       "      <td>[(down, 2), (to, 1)]</td>\n",
       "      <td>[(down, 2), (to, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .</td>\n",
       "      <td>43</td>\n",
       "      <td>61</td>\n",
       "      <td>future generations</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.15</td>\n",
       "      <td>normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .</td>\n",
       "      <td>future generations</td>\n",
       "      <td>[JJ, NNS]</td>\n",
       "      <td>[a, n]</td>\n",
       "      <td>future generation</td>\n",
       "      <td>future generation</td>\n",
       "      <td>[(down, 2), (to, 1)]</td>\n",
       "      <td>[(down, 2), (to, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .</td>\n",
       "      <td>50</td>\n",
       "      <td>61</td>\n",
       "      <td>generations</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .</td>\n",
       "      <td>generations</td>\n",
       "      <td>[NNS]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>generation</td>\n",
       "      <td>generation</td>\n",
       "      <td>[(to, 2), (future, 1)]</td>\n",
       "      <td>[(to, 2), (future, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .</td>\n",
       "      <td>76</td>\n",
       "      <td>86</td>\n",
       "      <td>recognizes</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.30</td>\n",
       "      <td>normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .</td>\n",
       "      <td>recognizes</td>\n",
       "      <td>[VBZ]</td>\n",
       "      <td>[v]</td>\n",
       "      <td>recognize</td>\n",
       "      <td>recognize</td>\n",
       "      <td>[(way, 2), (that, 1)]</td>\n",
       "      <td>[(way, 2), (that, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .</td>\n",
       "      <td>91</td>\n",
       "      <td>100</td>\n",
       "      <td>community</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .</td>\n",
       "      <td>community</td>\n",
       "      <td>[NN]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>community</td>\n",
       "      <td>community</td>\n",
       "      <td>[(recognizes, 2), (the, 1)]</td>\n",
       "      <td>[(recognizes, 2), (the, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .</td>\n",
       "      <td>104</td>\n",
       "      <td>115</td>\n",
       "      <td>traditional</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.20</td>\n",
       "      <td>normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .</td>\n",
       "      <td>traditional</td>\n",
       "      <td>[JJ]</td>\n",
       "      <td>[a]</td>\n",
       "      <td>traditional</td>\n",
       "      <td>traditional</td>\n",
       "      <td>[(the, 2), (community, 1)]</td>\n",
       "      <td>[(the, 2), (community, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .</td>\n",
       "      <td>104</td>\n",
       "      <td>142</td>\n",
       "      <td>traditional connection to that country</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .</td>\n",
       "      <td>traditional connection to that country</td>\n",
       "      <td>[JJ, NN, TO, DT, NN]</td>\n",
       "      <td>[a, n, n, n, n]</td>\n",
       "      <td>traditional connection to that country</td>\n",
       "      <td>traditional connection to that country</td>\n",
       "      <td>[(the, 2), (community, 1)]</td>\n",
       "      <td>[(the, 2), (community, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .</td>\n",
       "      <td>135</td>\n",
       "      <td>142</td>\n",
       "      <td>country</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .</td>\n",
       "      <td>country</td>\n",
       "      <td>[NN]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>country</td>\n",
       "      <td>country</td>\n",
       "      <td>[(to, 2), (that, 1)]</td>\n",
       "      <td>[(to, 2), (that, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .</td>\n",
       "      <td>116</td>\n",
       "      <td>126</td>\n",
       "      <td>connection</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .</td>\n",
       "      <td>connection</td>\n",
       "      <td>[NN]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>connection</td>\n",
       "      <td>connection</td>\n",
       "      <td>[(community, 2), (traditional, 1)]</td>\n",
       "      <td>[(community, 2), (traditional, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...</td>\n",
       "      <td>15</td>\n",
       "      <td>25</td>\n",
       "      <td>Aboriginal</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.55</td>\n",
       "      <td>the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...</td>\n",
       "      <td>aboriginal</td>\n",
       "      <td>[NNP]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>Aboriginal</td>\n",
       "      <td>aboriginal</td>\n",
       "      <td>[(passing, 2), (of, 1)]</td>\n",
       "      <td>[(passing, 2), (of, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>passing</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...</td>\n",
       "      <td>passing</td>\n",
       "      <td>[NN]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>passing</td>\n",
       "      <td>passing</td>\n",
       "      <td>[(The, 2)]</td>\n",
       "      <td>[(the, 2)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...</td>\n",
       "      <td>15</td>\n",
       "      <td>48</td>\n",
       "      <td>Aboriginal land rights legislaton</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...</td>\n",
       "      <td>aboriginal land rights legislaton</td>\n",
       "      <td>[NNP, NN, NNS, NN]</td>\n",
       "      <td>[n, n, n, n]</td>\n",
       "      <td>Aboriginal land right legislaton</td>\n",
       "      <td>aboriginal land right legislaton</td>\n",
       "      <td>[(passing, 2), (of, 1)]</td>\n",
       "      <td>[(passing, 2), (of, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...</td>\n",
       "      <td>26</td>\n",
       "      <td>48</td>\n",
       "      <td>land rights legislaton</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...</td>\n",
       "      <td>land rights legislaton</td>\n",
       "      <td>[NN, NNS, NN]</td>\n",
       "      <td>[n, n, n]</td>\n",
       "      <td>land right legislaton</td>\n",
       "      <td>land right legislaton</td>\n",
       "      <td>[(of, 2), (Aboriginal, 1)]</td>\n",
       "      <td>[(of, 2), (aboriginal, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...</td>\n",
       "      <td>26</td>\n",
       "      <td>30</td>\n",
       "      <td>land</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...</td>\n",
       "      <td>land</td>\n",
       "      <td>[NN]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>land</td>\n",
       "      <td>land</td>\n",
       "      <td>[(of, 2), (Aboriginal, 1)]</td>\n",
       "      <td>[(of, 2), (aboriginal, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...</td>\n",
       "      <td>38</td>\n",
       "      <td>48</td>\n",
       "      <td>legislaton</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...</td>\n",
       "      <td>legislaton</td>\n",
       "      <td>[NN]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>legislaton</td>\n",
       "      <td>legislaton</td>\n",
       "      <td>[(land, 2), (rights, 1)]</td>\n",
       "      <td>[(land, 2), (rights, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...</td>\n",
       "      <td>31</td>\n",
       "      <td>37</td>\n",
       "      <td>rights</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...</td>\n",
       "      <td>rights</td>\n",
       "      <td>[NNS]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>right</td>\n",
       "      <td>right</td>\n",
       "      <td>[(Aboriginal, 2), (land, 1)]</td>\n",
       "      <td>[(aboriginal, 2), (land, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...</td>\n",
       "      <td>66</td>\n",
       "      <td>74</td>\n",
       "      <td>preceded</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.70</td>\n",
       "      <td>the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...</td>\n",
       "      <td>preceded</td>\n",
       "      <td>[VBN]</td>\n",
       "      <td>[v]</td>\n",
       "      <td>precede</td>\n",
       "      <td>precede</td>\n",
       "      <td>[(Australia, 2), (was, 1)]</td>\n",
       "      <td>[(australia, 2), (was, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...</td>\n",
       "      <td>52</td>\n",
       "      <td>61</td>\n",
       "      <td>Australia</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...</td>\n",
       "      <td>australia</td>\n",
       "      <td>[NNP]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>Australia</td>\n",
       "      <td>australia</td>\n",
       "      <td>[(legislaton, 2), (in, 1)]</td>\n",
       "      <td>[(legislaton, 2), (in, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...</td>\n",
       "      <td>100</td>\n",
       "      <td>110</td>\n",
       "      <td>Aboriginal</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...</td>\n",
       "      <td>aboriginal</td>\n",
       "      <td>[NNP]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>Aboriginal</td>\n",
       "      <td>aboriginal</td>\n",
       "      <td>[(of, 2), (important, 1)]</td>\n",
       "      <td>[(of, 2), (important, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...</td>\n",
       "      <td>80</td>\n",
       "      <td>86</td>\n",
       "      <td>number</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...</td>\n",
       "      <td>number</td>\n",
       "      <td>[NN]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>number</td>\n",
       "      <td>number</td>\n",
       "      <td>[(by, 2), (a, 1)]</td>\n",
       "      <td>[(by, 2), (a, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...</td>\n",
       "      <td>90</td>\n",
       "      <td>99</td>\n",
       "      <td>important</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...</td>\n",
       "      <td>important</td>\n",
       "      <td>[JJ]</td>\n",
       "      <td>[a]</td>\n",
       "      <td>important</td>\n",
       "      <td>important</td>\n",
       "      <td>[(number, 2), (of, 1)]</td>\n",
       "      <td>[(number, 2), (of, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...</td>\n",
       "      <td>100</td>\n",
       "      <td>119</td>\n",
       "      <td>Aboriginal protests</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...</td>\n",
       "      <td>aboriginal protests</td>\n",
       "      <td>[NNP, NNS]</td>\n",
       "      <td>[n, n]</td>\n",
       "      <td>Aboriginal protest</td>\n",
       "      <td>aboriginal protest</td>\n",
       "      <td>[(of, 2), (important, 1)]</td>\n",
       "      <td>[(of, 2), (important, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...</td>\n",
       "      <td>152</td>\n",
       "      <td>160</td>\n",
       "      <td>Stockmen</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...</td>\n",
       "      <td>stockmen</td>\n",
       "      <td>[NNP]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>Stockmen</td>\n",
       "      <td>stockmen</td>\n",
       "      <td>[(1946, 2), (Aboriginal, 1)]</td>\n",
       "      <td>[(1946, 2), (aboriginal, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...</td>\n",
       "      <td>111</td>\n",
       "      <td>119</td>\n",
       "      <td>protests</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...</td>\n",
       "      <td>protests</td>\n",
       "      <td>[NNS]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>protest</td>\n",
       "      <td>protest</td>\n",
       "      <td>[(important, 2), (Aboriginal, 1)]</td>\n",
       "      <td>[(important, 2), (aboriginal, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...</td>\n",
       "      <td>122</td>\n",
       "      <td>131</td>\n",
       "      <td>including</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...</td>\n",
       "      <td>including</td>\n",
       "      <td>[VBG]</td>\n",
       "      <td>[v]</td>\n",
       "      <td>include</td>\n",
       "      <td>include</td>\n",
       "      <td>[(Aboriginal, 2), (protests, 1)]</td>\n",
       "      <td>[(aboriginal, 2), (protests, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...</td>\n",
       "      <td>141</td>\n",
       "      <td>151</td>\n",
       "      <td>Aboriginal</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...</td>\n",
       "      <td>aboriginal</td>\n",
       "      <td>[NNP]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>Aboriginal</td>\n",
       "      <td>aboriginal</td>\n",
       "      <td>[(the, 2), (1946, 1)]</td>\n",
       "      <td>[(the, 2), (1946, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...</td>\n",
       "      <td>164</td>\n",
       "      <td>170</td>\n",
       "      <td>Strike</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...</td>\n",
       "      <td>strike</td>\n",
       "      <td>[NN]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>Strike</td>\n",
       "      <td>strike</td>\n",
       "      <td>[(Aboriginal, 2), (Stockmen, 1)]</td>\n",
       "      <td>[(aboriginal, 2), (stockmen, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...</td>\n",
       "      <td>182</td>\n",
       "      <td>188</td>\n",
       "      <td>Yolngu</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...</td>\n",
       "      <td>yolngu</td>\n",
       "      <td>[NNP]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>Yolngu</td>\n",
       "      <td>yolngu</td>\n",
       "      <td>[(the, 2), (1963, 1)]</td>\n",
       "      <td>[(the, 2), (1963, 1)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                id  \\\n",
       "0   3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "1   3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "2   3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "3   3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "4   3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "5   3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "6   3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "7   3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "8   3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "9   3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "10  3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "11  3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "12  3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "13  3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "14  3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "15  3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "16  3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "17  3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "18  3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "19  3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "20  3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "21  3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "22  3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "23  3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "24  3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "25  3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "26  3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "27  3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "28  3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "29  3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "30  3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "\n",
       "                                                                                                                                                                                                   sentence  \\\n",
       "0                                                          Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .   \n",
       "1                                                          Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .   \n",
       "2                                                          Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .   \n",
       "3                                                          Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .   \n",
       "4                                                          Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .   \n",
       "5                                                          Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .   \n",
       "6                                                          Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .   \n",
       "7                                                          Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .   \n",
       "8                                                          Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .   \n",
       "9                                                          Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .   \n",
       "10                                                         Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .   \n",
       "11                                                         Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .   \n",
       "12  The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...   \n",
       "13  The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...   \n",
       "14  The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...   \n",
       "15  The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...   \n",
       "16  The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...   \n",
       "17  The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...   \n",
       "18  The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...   \n",
       "19  The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...   \n",
       "20  The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...   \n",
       "21  The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...   \n",
       "22  The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...   \n",
       "23  The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...   \n",
       "24  The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...   \n",
       "25  The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...   \n",
       "26  The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...   \n",
       "27  The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...   \n",
       "28  The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...   \n",
       "29  The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...   \n",
       "30  The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...   \n",
       "\n",
       "    start  end                                  target  nat  non_nat  \\\n",
       "0       0    8                                Normally   10       10   \n",
       "1      28   34                                  passed   10       10   \n",
       "2      15   19                                    land   10       10   \n",
       "3      43   49                                  future   10       10   \n",
       "4      43   61                      future generations   10       10   \n",
       "5      50   61                             generations   10       10   \n",
       "6      76   86                              recognizes   10       10   \n",
       "7      91  100                               community   10       10   \n",
       "8     104  115                             traditional   10       10   \n",
       "9     104  142  traditional connection to that country   10       10   \n",
       "10    135  142                                 country   10       10   \n",
       "11    116  126                              connection   10       10   \n",
       "12     15   25                              Aboriginal   10       10   \n",
       "13      4   11                                 passing   10       10   \n",
       "14     15   48       Aboriginal land rights legislaton   10       10   \n",
       "15     26   48                  land rights legislaton   10       10   \n",
       "16     26   30                                    land   10       10   \n",
       "17     38   48                              legislaton   10       10   \n",
       "18     31   37                                  rights   10       10   \n",
       "19     66   74                                preceded   10       10   \n",
       "20     52   61                               Australia   10       10   \n",
       "21    100  110                              Aboriginal   10       10   \n",
       "22     80   86                                  number   10       10   \n",
       "23     90   99                               important   10       10   \n",
       "24    100  119                     Aboriginal protests   10       10   \n",
       "25    152  160                                Stockmen   10       10   \n",
       "26    111  119                                protests   10       10   \n",
       "27    122  131                               including   10       10   \n",
       "28    141  151                              Aboriginal   10       10   \n",
       "29    164  170                                  Strike   10       10   \n",
       "30    182  188                                  Yolngu   10       10   \n",
       "\n",
       "    nat_marked  non_nat_marked  binary  prob  \\\n",
       "0            0               1       1  0.05   \n",
       "1            0               1       1  0.05   \n",
       "2            0               0       0  0.00   \n",
       "3            1               0       1  0.05   \n",
       "4            1               2       1  0.15   \n",
       "5            3               2       1  0.25   \n",
       "6            2               4       1  0.30   \n",
       "7            0               0       0  0.00   \n",
       "8            1               3       1  0.20   \n",
       "9            0               0       0  0.00   \n",
       "10           0               1       1  0.05   \n",
       "11           0               0       0  0.00   \n",
       "12           6               5       1  0.55   \n",
       "13           0               0       0  0.00   \n",
       "14           0               1       1  0.05   \n",
       "15           1               0       1  0.05   \n",
       "16           0               0       0  0.00   \n",
       "17          10               5       1  0.75   \n",
       "18           0               0       0  0.00   \n",
       "19           8               6       1  0.70   \n",
       "20           0               0       0  0.00   \n",
       "21           1               1       1  0.10   \n",
       "22           0               0       0  0.00   \n",
       "23           0               0       0  0.00   \n",
       "24           0               1       1  0.05   \n",
       "25           0               0       0  0.00   \n",
       "26           0               0       0  0.00   \n",
       "27           0               0       0  0.00   \n",
       "28           0               0       0  0.00   \n",
       "29           0               1       1  0.05   \n",
       "30           0               0       0  0.00   \n",
       "\n",
       "                                                                                                                                                                                                 p_sentence  \\\n",
       "0                                                          normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .   \n",
       "1                                                          normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .   \n",
       "2                                                          normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .   \n",
       "3                                                          normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .   \n",
       "4                                                          normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .   \n",
       "5                                                          normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .   \n",
       "6                                                          normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .   \n",
       "7                                                          normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .   \n",
       "8                                                          normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .   \n",
       "9                                                          normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .   \n",
       "10                                                         normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .   \n",
       "11                                                         normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .   \n",
       "12  the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...   \n",
       "13  the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...   \n",
       "14  the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...   \n",
       "15  the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...   \n",
       "16  the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...   \n",
       "17  the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...   \n",
       "18  the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...   \n",
       "19  the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...   \n",
       "20  the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...   \n",
       "21  the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...   \n",
       "22  the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...   \n",
       "23  the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...   \n",
       "24  the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...   \n",
       "25  the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...   \n",
       "26  the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...   \n",
       "27  the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...   \n",
       "28  the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...   \n",
       "29  the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...   \n",
       "30  the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...   \n",
       "\n",
       "                                  p_target              pos_tags  \\\n",
       "0                                 normally                  [RB]   \n",
       "1                                   passed                 [VBN]   \n",
       "2                                     land                  [NN]   \n",
       "3                                   future                  [JJ]   \n",
       "4                       future generations             [JJ, NNS]   \n",
       "5                              generations                 [NNS]   \n",
       "6                               recognizes                 [VBZ]   \n",
       "7                                community                  [NN]   \n",
       "8                              traditional                  [JJ]   \n",
       "9   traditional connection to that country  [JJ, NN, TO, DT, NN]   \n",
       "10                                 country                  [NN]   \n",
       "11                              connection                  [NN]   \n",
       "12                              aboriginal                 [NNP]   \n",
       "13                                 passing                  [NN]   \n",
       "14       aboriginal land rights legislaton    [NNP, NN, NNS, NN]   \n",
       "15                  land rights legislaton         [NN, NNS, NN]   \n",
       "16                                    land                  [NN]   \n",
       "17                              legislaton                  [NN]   \n",
       "18                                  rights                 [NNS]   \n",
       "19                                preceded                 [VBN]   \n",
       "20                               australia                 [NNP]   \n",
       "21                              aboriginal                 [NNP]   \n",
       "22                                  number                  [NN]   \n",
       "23                               important                  [JJ]   \n",
       "24                     aboriginal protests            [NNP, NNS]   \n",
       "25                                stockmen                 [NNP]   \n",
       "26                                protests                 [NNS]   \n",
       "27                               including                 [VBG]   \n",
       "28                              aboriginal                 [NNP]   \n",
       "29                                  strike                  [NN]   \n",
       "30                                  yolngu                 [NNP]   \n",
       "\n",
       "        pos_tags_pt                                   lemma  \\\n",
       "0               [r]                                Normally   \n",
       "1               [v]                                    pass   \n",
       "2               [n]                                    land   \n",
       "3               [a]                                  future   \n",
       "4            [a, n]                       future generation   \n",
       "5               [n]                              generation   \n",
       "6               [v]                               recognize   \n",
       "7               [n]                               community   \n",
       "8               [a]                             traditional   \n",
       "9   [a, n, n, n, n]  traditional connection to that country   \n",
       "10              [n]                                 country   \n",
       "11              [n]                              connection   \n",
       "12              [n]                              Aboriginal   \n",
       "13              [n]                                 passing   \n",
       "14     [n, n, n, n]        Aboriginal land right legislaton   \n",
       "15        [n, n, n]                   land right legislaton   \n",
       "16              [n]                                    land   \n",
       "17              [n]                              legislaton   \n",
       "18              [n]                                   right   \n",
       "19              [v]                                 precede   \n",
       "20              [n]                               Australia   \n",
       "21              [n]                              Aboriginal   \n",
       "22              [n]                                  number   \n",
       "23              [a]                               important   \n",
       "24           [n, n]                      Aboriginal protest   \n",
       "25              [n]                                Stockmen   \n",
       "26              [n]                                 protest   \n",
       "27              [v]                                 include   \n",
       "28              [n]                              Aboriginal   \n",
       "29              [n]                                  Strike   \n",
       "30              [n]                                  Yolngu   \n",
       "\n",
       "                                   p_lemma  \\\n",
       "0                                 normally   \n",
       "1                                     pass   \n",
       "2                                     land   \n",
       "3                                   future   \n",
       "4                        future generation   \n",
       "5                               generation   \n",
       "6                                recognize   \n",
       "7                                community   \n",
       "8                              traditional   \n",
       "9   traditional connection to that country   \n",
       "10                                 country   \n",
       "11                              connection   \n",
       "12                              aboriginal   \n",
       "13                                 passing   \n",
       "14        aboriginal land right legislaton   \n",
       "15                   land right legislaton   \n",
       "16                                    land   \n",
       "17                              legislaton   \n",
       "18                                   right   \n",
       "19                                 precede   \n",
       "20                               australia   \n",
       "21                              aboriginal   \n",
       "22                                  number   \n",
       "23                               important   \n",
       "24                      aboriginal protest   \n",
       "25                                stockmen   \n",
       "26                                 protest   \n",
       "27                                 include   \n",
       "28                              aboriginal   \n",
       "29                                  strike   \n",
       "30                                  yolngu   \n",
       "\n",
       "                               context                           p_context  \n",
       "0                                   []                                  []  \n",
       "1                 [(will, 2), (be, 1)]                [(will, 2), (be, 1)]  \n",
       "2            [(Normally, 2), (the, 1)]           [(normally, 2), (the, 1)]  \n",
       "3                 [(down, 2), (to, 1)]                [(down, 2), (to, 1)]  \n",
       "4                 [(down, 2), (to, 1)]                [(down, 2), (to, 1)]  \n",
       "5               [(to, 2), (future, 1)]              [(to, 2), (future, 1)]  \n",
       "6                [(way, 2), (that, 1)]               [(way, 2), (that, 1)]  \n",
       "7          [(recognizes, 2), (the, 1)]         [(recognizes, 2), (the, 1)]  \n",
       "8           [(the, 2), (community, 1)]          [(the, 2), (community, 1)]  \n",
       "9           [(the, 2), (community, 1)]          [(the, 2), (community, 1)]  \n",
       "10                [(to, 2), (that, 1)]                [(to, 2), (that, 1)]  \n",
       "11  [(community, 2), (traditional, 1)]  [(community, 2), (traditional, 1)]  \n",
       "12             [(passing, 2), (of, 1)]             [(passing, 2), (of, 1)]  \n",
       "13                          [(The, 2)]                          [(the, 2)]  \n",
       "14             [(passing, 2), (of, 1)]             [(passing, 2), (of, 1)]  \n",
       "15          [(of, 2), (Aboriginal, 1)]          [(of, 2), (aboriginal, 1)]  \n",
       "16          [(of, 2), (Aboriginal, 1)]          [(of, 2), (aboriginal, 1)]  \n",
       "17            [(land, 2), (rights, 1)]            [(land, 2), (rights, 1)]  \n",
       "18        [(Aboriginal, 2), (land, 1)]        [(aboriginal, 2), (land, 1)]  \n",
       "19          [(Australia, 2), (was, 1)]          [(australia, 2), (was, 1)]  \n",
       "20          [(legislaton, 2), (in, 1)]          [(legislaton, 2), (in, 1)]  \n",
       "21           [(of, 2), (important, 1)]           [(of, 2), (important, 1)]  \n",
       "22                   [(by, 2), (a, 1)]                   [(by, 2), (a, 1)]  \n",
       "23              [(number, 2), (of, 1)]              [(number, 2), (of, 1)]  \n",
       "24           [(of, 2), (important, 1)]           [(of, 2), (important, 1)]  \n",
       "25        [(1946, 2), (Aboriginal, 1)]        [(1946, 2), (aboriginal, 1)]  \n",
       "26   [(important, 2), (Aboriginal, 1)]   [(important, 2), (aboriginal, 1)]  \n",
       "27    [(Aboriginal, 2), (protests, 1)]    [(aboriginal, 2), (protests, 1)]  \n",
       "28               [(the, 2), (1946, 1)]               [(the, 2), (1946, 1)]  \n",
       "29    [(Aboriginal, 2), (Stockmen, 1)]    [(aboriginal, 2), (stockmen, 1)]  \n",
       "30               [(the, 2), (1963, 1)]               [(the, 2), (1963, 1)]  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx_datasets[0].train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Context Features\n",
    "After defining all the context definitions and extracting the different kinds of contexts from the sentence, we compute features on the context words. Therefore we first define which of the precomputed contexts to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4.1) Context Complexity Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_freq_wiki = {}\n",
    "freq_sum_wiki = 0\n",
    "with open(\"resources/word-freq-dumps/enwiki-20150602-words-frequency.txt\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        word, freq = line.partition(\" \")[::2]\n",
    "        word_freq_wiki[word.strip()] = int(freq)\n",
    "        freq_sum_wiki+=int(freq)\n",
    "        \n",
    "def get_dict_count(target, freqs):\n",
    "    return freqs.get(target.strip().lower(), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ctx_features_context_complexity(dataframe, agg):\n",
    "    df = dataframe.copy()\n",
    "    df['ctx_length'] = df.p_context.apply(lambda context : agg(context, len))\n",
    "    df['ctx_freq_wiki'] = df.p_context.apply(lambda context : agg(context, get_dict_count, word_freq_wiki))\n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "ctx_fc_context_complexity = ContextFeatureCategory('baseline_1', ctx_features_context_complexity)\n",
    "feature_categories.append(ctx_fc_context_complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "ctx_datasets_fc_context_complexity = [ContextFeatureDataset(ctx_ds.name, ctx_ds.context, ctx_fc_context_complexity, agg, \n",
    "        ctx_fc_context_complexity.func(ctx_ds.train, agg.agg), ctx_fc_context_complexity.func(ctx_ds.test, agg.agg)) \n",
    "        for ctx_ds in ctx_datasets for agg in aggs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>target</th>\n",
       "      <th>nat</th>\n",
       "      <th>non_nat</th>\n",
       "      <th>nat_marked</th>\n",
       "      <th>non_nat_marked</th>\n",
       "      <th>binary</th>\n",
       "      <th>prob</th>\n",
       "      <th>p_sentence</th>\n",
       "      <th>p_target</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>pos_tags_pt</th>\n",
       "      <th>lemma</th>\n",
       "      <th>p_lemma</th>\n",
       "      <th>context</th>\n",
       "      <th>p_context</th>\n",
       "      <th>ctx_length</th>\n",
       "      <th>ctx_freq_wiki</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>passing</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...</td>\n",
       "      <td>passing</td>\n",
       "      <td>[NN]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>passing</td>\n",
       "      <td>passing</td>\n",
       "      <td>[(The, 2)]</td>\n",
       "      <td>[(the, 2)]</td>\n",
       "      <td>3.0</td>\n",
       "      <td>121935704.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                id  \\\n",
       "13  3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "\n",
       "                                                                                                                                                                                                   sentence  \\\n",
       "13  The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...   \n",
       "\n",
       "    start  end   target  nat  non_nat  nat_marked  non_nat_marked  binary  \\\n",
       "13      4   11  passing   10       10           0               0       0   \n",
       "\n",
       "    prob  \\\n",
       "13   0.0   \n",
       "\n",
       "                                                                                                                                                                                                 p_sentence  \\\n",
       "13  the passing of aboriginal land rights legislaton in australia was preceded by a number of important aboriginal protests , including the 1946 aboriginal stockmen 's strike , the 1963 yolngu bark pe...   \n",
       "\n",
       "   p_target pos_tags pos_tags_pt    lemma  p_lemma     context   p_context  \\\n",
       "13  passing     [NN]         [n]  passing  passing  [(The, 2)]  [(the, 2)]   \n",
       "\n",
       "    ctx_length  ctx_freq_wiki  \n",
       "13         3.0    121935704.0  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = ctx_datasets_fc_context_complexity[0].train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from joblib import Memory\n",
    "memory = Memory(location='resources/dependency-cache', verbose=0)\n",
    "@memory.cache\n",
    "def dependency_parse_with_root(sentence):\n",
    "    try:\n",
    "        dependency_parser = dependencyParser.raw_parse(sentence)\n",
    "        dependencies = []\n",
    "        parsetree = list(dependency_parser)[0]\n",
    "        for index, node in parsetree.nodes.items():\n",
    "            for relation, dependant in parsetree.nodes[index]['deps'].items():\n",
    "                for dep in dependant:\n",
    "                    triple = ((node['word'], index), relation, \\\n",
    "                              (parsetree.nodes[dep]['word'], dep))\n",
    "                    dependencies.append(triple)\n",
    "        return dependencies\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def dependency_parse(sentence):\n",
    "    dependencies = dependency_parse_with_root(sentence)\n",
    "    filtered_dependencies = [triple for triple in dependencies if triple[1] != 'root']\n",
    "    return filtered_dependencies\n",
    "\n",
    "\n",
    "def dep_dist_to_head(target, start, end, context):\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    return np.nan_to_num(np.mean([np.abs(triple[0][1] - triple[2][1])-1 \n",
    "                                for triple in triples if triple[2] in targets]))\n",
    "\n",
    "def dep_dist_to_root(target, start, end, context):\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse_with_root(context)\n",
    "    root_nodes = list(filter(lambda triple : triple[1] == 'root' , triples))\n",
    "    if root_nodes: \n",
    "        root_node = root_nodes[0]\n",
    "    else:\n",
    "        return 0\n",
    "    dist = np.nan_to_num(np.mean([np.abs(root_node[2][1] - triple[2][1])-1 \n",
    "                                for triple in triples if triple[2] in targets]))\n",
    "    return dist if dist != -1 else 0\n",
    "\n",
    "def dep_relation_to_head(target, start, end, context):\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse_with_root(context)\n",
    "    relations = [triple[1] for triple in triples if triple[2] in targets]\n",
    "    return relations[0] if len(relations) == 1 else 'misc'\n",
    "    \n",
    "\n",
    "def dep_head_word_len(target, start, end, context):\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    return np.nan_to_num(np.mean([len(triple[0][0]) \n",
    "        for triple in triples if triple[2] in targets]))\n",
    "\n",
    "def dep_num_dependents(target, start, end, context):\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse_with_root(context)\n",
    "    return len([triple[1] for triple in triples if triple[0] in targets])\n",
    "\n",
    "def dep_max_num_dependents(context):\n",
    "    triples = dependency_parse_with_root(context)\n",
    "    most = Counter([triple[0][0] for triple in triples]).most_common(1)\n",
    "    return most[0][1] if most else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " df['dep_dist_to_head'] = df[['target', 'start', 'end', 'sentence']].apply(lambda vals : \n",
    "                                                                            dep_dist_to_head(*vals), axis=1)\n",
    "    df['dep_dist_to_root'] = df[['target', 'start', 'end', 'sentence']].apply(lambda vals : \n",
    "                                                                            dep_dist_to_root(*vals), axis=1)\n",
    "    df['dep_dist_to_root_norm'] = df[['dep_dist_to_root', 'sentence']].apply(lambda vals : \\\n",
    "                                                        float(vals[0]) / (len(word_tokenize(vals[1]))-1), axis=1)\n",
    "    df['dep_relation_to_head'] = df[['target', 'start', 'end', 'sentence']].apply(lambda vals : \\\n",
    "                                                                dep_relation_to_head(*vals), axis = 1)\n",
    "    df['dep_num_dependents'] = df[['target', 'start', 'end', 'sentence']].apply(lambda vals : \\\n",
    "                                                                        dep_num_dependents(*vals), axis = 1)\n",
    "    df['dep_max_num_dependents'] = df.sentence.apply(lambda sentence : dep_max_num_dependents(sentence))\n",
    "    df['dep_num_dependents_norm'] = df.dep_num_dependents / df.dep_max_num_dependents\n",
    "    df['dep_head_word_len'] = df[['target', 'start', 'end', 'sentence']].apply(lambda vals : \\\n",
    "                                                                        dep_head_word_len(*vals), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Readability Measures\n",
    "Here we implement some of the most popular and well-known historical readability measures. Most of them need multiple sentences to compute them properly, however, we will apply them on the extracted context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>rb_dalechall_score</th>\n",
       "      <th>rb_flesch_score</th>\n",
       "      <th>rb_fleschkincaid_score</th>\n",
       "      <th>rb_gunningfog_score</th>\n",
       "      <th>rb_polysyblword_count</th>\n",
       "      <th>rb_smog_score</th>\n",
       "      <th>rb_sybl_count</th>\n",
       "      <th>rb_sybl_count_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>passed</td>\n",
       "      <td>[land, future, generations, recognizes]</td>\n",
       "      <td>15.677400</td>\n",
       "      <td>-8.725</td>\n",
       "      <td>15.470000</td>\n",
       "      <td>21.600000</td>\n",
       "      <td>2</td>\n",
       "      <td>11.208143</td>\n",
       "      <td>10</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>land</td>\n",
       "      <td>[passed, future, generations]</td>\n",
       "      <td>14.311967</td>\n",
       "      <td>6.390</td>\n",
       "      <td>13.113333</td>\n",
       "      <td>14.533333</td>\n",
       "      <td>1</td>\n",
       "      <td>8.841846</td>\n",
       "      <td>7</td>\n",
       "      <td>2.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>future</td>\n",
       "      <td>[land, passed, generations, recognizes, commun...</td>\n",
       "      <td>13.358500</td>\n",
       "      <td>-1.280</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>13.023867</td>\n",
       "      <td>12</td>\n",
       "      <td>2.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>future generations</td>\n",
       "      <td>[land, passed, recognizes, community, traditio...</td>\n",
       "      <td>13.358500</td>\n",
       "      <td>15.640</td>\n",
       "      <td>12.320000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>13.023867</td>\n",
       "      <td>11</td>\n",
       "      <td>2.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>generations</td>\n",
       "      <td>[land, passed, future, recognizes, community, ...</td>\n",
       "      <td>14.460767</td>\n",
       "      <td>17.445</td>\n",
       "      <td>12.316667</td>\n",
       "      <td>22.400000</td>\n",
       "      <td>3</td>\n",
       "      <td>13.023867</td>\n",
       "      <td>13</td>\n",
       "      <td>2.166667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               target                                            context  \\\n",
       "0              passed            [land, future, generations, recognizes]   \n",
       "1                land                      [passed, future, generations]   \n",
       "2              future  [land, passed, generations, recognizes, commun...   \n",
       "3  future generations  [land, passed, recognizes, community, traditio...   \n",
       "4         generations  [land, passed, future, recognizes, community, ...   \n",
       "\n",
       "   rb_dalechall_score  rb_flesch_score  rb_fleschkincaid_score  \\\n",
       "0           15.677400           -8.725               15.470000   \n",
       "1           14.311967            6.390               13.113333   \n",
       "2           13.358500           -1.280               14.680000   \n",
       "3           13.358500           15.640               12.320000   \n",
       "4           14.460767           17.445               12.316667   \n",
       "\n",
       "   rb_gunningfog_score  rb_polysyblword_count  rb_smog_score  rb_sybl_count  \\\n",
       "0            21.600000                      2      11.208143             10   \n",
       "1            14.533333                      1       8.841846              7   \n",
       "2            26.000000                      3      13.023867             12   \n",
       "3            26.000000                      3      13.023867             11   \n",
       "4            22.400000                      3      13.023867             13   \n",
       "\n",
       "   rb_sybl_count_ratio  \n",
       "0             2.500000  \n",
       "1             2.333333  \n",
       "2             2.400000  \n",
       "3             2.200000  \n",
       "4             2.166667  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textatistic import Textatistic\n",
    "\n",
    "df_context['rb_dalechall_score'] = df_context.context.apply(lambda context : \\\n",
    "                                                Textatistic(' '.join(context) + '.').dalechall_score)\n",
    "df_context['rb_flesch_score'] = df_context.context.apply(lambda context : \\\n",
    "                                                Textatistic(' '.join(context) + '.').flesch_score)\n",
    "df_context['rb_fleschkincaid_score'] = df_context.context.apply(lambda context : \\\n",
    "                                                Textatistic(' '.join(context) + '.').fleschkincaid_score)\n",
    "df_context['rb_gunningfog_score'] = df_context.context.apply(lambda context : \\\n",
    "                                                Textatistic(' '.join(context) + '.').gunningfog_score)\n",
    "df_context['rb_polysyblword_count'] = df_context.context.apply(lambda context : \\\n",
    "                                                Textatistic(' '.join(context) + '.').polysyblword_count)\n",
    "df_context['rb_smog_score'] = df_context.context.apply(lambda context : \\\n",
    "                                                Textatistic(' '.join(context) + '.').smog_score)\n",
    "df_context['rb_sybl_count'] = df_context.context.apply(lambda context : \\\n",
    "                                                Textatistic(' '.join(context) + '.').sybl_count)\n",
    "df_context['rb_sybl_count_ratio'] = df_context.context.apply(lambda context : \\\n",
    "                                                Textatistic(' '.join(context) + '.').sybl_count / len(context))\n",
    "\n",
    "df_context[['target', 'context', 'rb_dalechall_score', 'rb_flesch_score', 'rb_fleschkincaid_score', \\\n",
    "            'rb_gunningfog_score', 'rb_polysyblword_count', 'rb_smog_score', 'rb_sybl_count', 'rb_sybl_count_ratio']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Linguistic Features\n",
    "Here we implement some of the most popular and well-known historical readability measures. Most of them need multiple sentences to compute them properly, however, we will apply them on the extracted context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_context['context'] = df_context['ctx_extraction_window_pre_suc_n']\n",
    "\n",
    "df_context['ctx_num_tokens'] = df_context.context.apply(lambda context : len(context))\n",
    "df_context['ctx_avg_length'] = df_context.context.apply(lambda context : agg_ctx_feat_num_average(context, len))\n",
    "df_context['ctx_avg_word_freq_wiki'] = df_context.context.apply(lambda context : \\\n",
    "                                                    agg_ctx_feat_num_average(context, get_dict_count, word_freq_wiki))\n",
    "df_context.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Result = namedtuple('Result', 'dataset, fc, agg, measure')\n",
    "Dataset = namedtuple('Dataset', 'name, train, test')\n",
    "FeatureDataset = namedtuple('FeatureDataset', 'name, fc, agg, train, test')\n",
    "FeatureCategory = namedtuple('FeatureCategory', 'name, func')\n",
    "Feature = namedtuple('Feature', 'name, fc_name, train, test')\n",
    "Metric = namedtuple('Metric', 'name, func')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_labels_for_binary_df(dataframe):\n",
    "    df = dataframe.copy()\n",
    "    df = df.drop(['id', 'sentence', 'target', 'nat', 'non_nat', \n",
    "                  'nat_marked', 'non_nat_marked', 'prob', 'start', \n",
    "                  'end', 'p_target', 'lemma', 'p_lemma', 'pos_tags', 'pos_tags_pt'], axis = 1)\n",
    "    return df\n",
    "\n",
    "def remove_labels_for_regr_df(dataframe):\n",
    "    df = dataframe.copy()\n",
    "    df = df.drop(['id', 'sentence', 'target', 'nat', 'non_nat', \n",
    "                  'nat_marked', 'non_nat_marked', 'binary', 'start', \n",
    "                  'end', 'p_target', 'lemma', 'p_lemma', 'pos_tags', 'pos_tags_pt'], axis = 1)\n",
    "    return df\n",
    "    \n",
    "def transform_feat_to_num(train, test):\n",
    "    train_copy = train.copy()\n",
    "    test_copy = test.copy()\n",
    "    train_copy = train_copy.replace(np.inf, 0)\n",
    "    train_copy = train_copy.replace(np.nan, 0)\n",
    "    test_copy = test_copy.replace(np.inf, 0)\n",
    "    test_copy = test_copy.replace(np.nan, 0)\n",
    "    shape_train = train.shape\n",
    "    shape_test = test.shape\n",
    "    df = train_copy.append(test_copy, ignore_index=True)\n",
    "    df = pd.get_dummies(df)\n",
    "    return (df.loc[0:(shape_train[0]-1),], \n",
    "            df.loc[shape_train[0]:df.shape[0],])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn import model_selection\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "def get_majority_class_prediction(train, test):\n",
    "    dummy = DummyClassifier(strategy='most_frequent', \n",
    "                            random_state=None, constant=None)\n",
    "    x_train = train.loc[:, train.columns != 'binary']\n",
    "    y_train = train.binary.values\n",
    "    x_test = test\n",
    "    y_test = test.binary.values\n",
    "    dummy.fit(x_train, y_train)\n",
    "    prediction = dummy.predict(x_test)\n",
    "    f1score = f1_score(y_test, prediction)\n",
    "    return f1score\n",
    "\n",
    "def always_complex_prediction(train, test):\n",
    "    y_test = test.binary.values\n",
    "    prediction = [1 for val in y_test]\n",
    "    f1score = precision_recall_fscore_support(y_test, prediction)\n",
    "    return f1score\n",
    "\n",
    "def svm(train, test):\n",
    "    print('average_classification')\n",
    "    x_train = train.loc[:, train.columns != 'binary']\n",
    "    y_train = train.binary.values\n",
    "    x_test = test.loc[:, test.columns != 'binary']\n",
    "    y_test = test.binary.values\n",
    "    seed = 7\n",
    "    #knn = MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto',\n",
    "     #  beta_1=0.9, beta_2=0.999, early_stopping=False,\n",
    "    #   epsilon=1e-08, hidden_layer_sizes=(5, 100), learning_rate='constant',\n",
    "    #   learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
    "    #   nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
    "    #   solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
    "    #   warm_start=False)\n",
    "    knn = svm.SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "        decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
    "        max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "        tol=0.001, verbose=False)\n",
    "    knn.fit(x_train, y_train) \n",
    "    prediction = knn.predict(x_test)\n",
    "    f1score = f1_score(y_test, prediction)\n",
    "    #kfold = model_selection.KFold(n_splits=2, random_state=seed)\n",
    "    #cv_results = model_selection.cross_val_score(knn, x_train, y_train, cv=kfold, scoring=make_scorer(f1_score))\n",
    "    return f1score\n",
    "\n",
    "def xgboost(train, test):\n",
    "    x_train = train.loc[:, train.columns != 'binary']\n",
    "    y_train = train.binary.values\n",
    "    x_test = test.loc[:, test.columns != 'binary']\n",
    "    y_test = test.binary.values\n",
    "    xgtrain = xgb.DMatrix(x_train.values, label=y_train)\n",
    "    xgtest = xgb.DMatrix(x_test.values, label=y_test)\n",
    "    xg_test_x = xgb.DMatrix(x_test.values)\n",
    "    param = {'max_depth': 30, 'eta': 1, 'silent': 1, 'objective': 'binary:logistic',  'n_estimators':5000}\n",
    "    evallist = [(xgtest, 'eval'), (xgtrain, 'train')]\n",
    "    num_round = 70\n",
    "    bst = xgb.train(param, xgtrain, num_round, evallist)\n",
    "    prediction = bst.predict(xg_test_x)\n",
    "    prediction_binary = list(map(lambda val: 1 if val>0.5 else 0, prediction))\n",
    "    f1score = precision_recall_fscore_support(y_test, prediction_binary)\n",
    "    return f1score\n",
    "\n",
    "def xgboost_with_bst(train, test):\n",
    "    x_train = train.loc[:, train.columns != 'binary']\n",
    "    y_train = train.binary.values\n",
    "    x_test = test.loc[:, test.columns != 'binary']\n",
    "    y_test = test.binary.values\n",
    "    xgtrain = xgb.DMatrix(x_train.values, label=y_train, feature_names=x_train.columns.values)\n",
    "    xgtest = xgb.DMatrix(x_test.values, label=y_test, feature_names=x_test.columns.values)\n",
    "    xg_test_x = xgb.DMatrix(x_test.values, feature_names=x_test.columns.values)\n",
    "    param = {'max_depth': 30, 'eta': 1, 'silent': 1, 'objective': 'binary:logistic',  'n_estimators':5000}\n",
    "    evallist = [(xgtest, 'eval'), (xgtrain, 'train')]\n",
    "    num_round = 70\n",
    "    bst = xgb.train(param, xgtrain, num_round, evallist)\n",
    "    prediction = bst.predict(xg_test_x)\n",
    "    prediction_binary = list(map(lambda val: 1 if val>0.5 else 0, prediction))\n",
    "    f1score = precision_recall_fscore_support(y_test, prediction_binary)\n",
    "    return f1score, bst\n",
    "\n",
    "def adaboost(train, test):\n",
    "    x_train = train.loc[:, train.columns != 'binary']\n",
    "    y_train = train.binary.values\n",
    "    x_test = test.loc[:, test.columns != 'binary']\n",
    "    y_test = test.binary.values\n",
    "    adab = AdaBoostClassifier(base_estimator=None, n_estimators=5000, \n",
    "                          learning_rate=1.0, algorithm='SAMME.R',\n",
    "                          random_state=None)\n",
    "    adab.fit(x_train, y_train) \n",
    "    prediction = adab.predict(x_test)\n",
    "    f1score = precision_recall_fscore_support(y_test, prediction)\n",
    "    return f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = [Result(fs, fs.fc, fs.agg,\n",
    "    xgboost(*transform_feat_to_num(remove_labels_for_binary_df(fs.train), \n",
    "        remove_labels_for_binary_df(fs.test)))) for fs in all_fc_datasets]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
