{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context-Aware Complex Word Identification\n",
    "Here we devise and implement all the relevant methods for evaluating the influence of context words for the complexity of a given target word. Thus, we implement various context definition methods that extract context words for a target based on different ideas (e.g. local context, grammatical context and semantic context). Afterwards we compute features for the context and use these features to represent the context in the classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.wsd import lesk\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from collections import defaultdict\n",
    "\n",
    "Dataset = namedtuple('Dataset', 'name, train, test')\n",
    "FeatureDataset = namedtuple('FeatureDataset', 'name, fc, agg, train, test')\n",
    "FeatureCategory = namedtuple('FeatureCategory', 'name, func')\n",
    "Aggregation = namedtuple('Aggregation', 'name, agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "MAIN_PATH_DATASET = \"../cwishareddataset/traindevset/english/\"\n",
    "genres = ['Wikipedia', 'WikiNews', 'News']\n",
    "datasets = ['Train', 'Dev']\n",
    "columns = ['id', 'sentence', \"start\", \"end\", \"target\", \n",
    "           \"nat\", \"non_nat\", \"nat_marked\", \"non_nat_marked\", \"binary\", \"prob\"]\n",
    "\n",
    "\n",
    "datasets = [Dataset('Wikipedia', 'Train', 'Dev'),\n",
    "            Dataset('WikiNews', 'Train', 'Dev'),\n",
    "            Dataset('News', 'Train', 'Dev')]\n",
    "\n",
    "feature_categories = []\n",
    "\n",
    "def load_df(path):\n",
    "    df = pd.read_csv(path, header=None, sep = \"\\t\")\n",
    "    df.columns = columns\n",
    "    return df\n",
    "\n",
    "datasets = [Dataset(d.name, load_df(MAIN_PATH_DATASET + d.name + '_' + d.train + '.tsv'),\n",
    "                            load_df(MAIN_PATH_DATASET + d.name + '_' + d.test + '.tsv'))\n",
    "                            for d in datasets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import *\n",
    "from nltk import word_tokenize\n",
    "from functools import lru_cache\n",
    "from utils import penn_to_wn\n",
    "import re\n",
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "wordNetLemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def overlaps(start1, end1, start2, end2):\n",
    "    return bool(range(max(start1, start2), min(end1, end2)+1))\n",
    "\n",
    "tbl = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "                      if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(tbl)\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def targets_with_index(start, end, context):\n",
    "    curr_pos = 0\n",
    "    targets = []\n",
    "    j = 0\n",
    "    w = 0\n",
    "    curr_split = ''\n",
    "    ctx_split = context.split()\n",
    "    whitespaces = re.findall('\\s+', context)\n",
    "    num_whitespaces = [len(token) for token in whitespaces]\n",
    "    num_whitespaces.append(1)\n",
    "    tokens = word_tokenize(context)\n",
    "    tokens = ['\"' if token not in context else token for token in tokens]\n",
    "    for index, token in enumerate(tokens, 1):\n",
    "        targets.append((token, index, curr_pos, (curr_pos + len(token))))\n",
    "        curr_pos += len(token)\n",
    "        curr_split += token\n",
    "        if ctx_split[j] == curr_split:\n",
    "            curr_pos += num_whitespaces[w]\n",
    "            j += 1\n",
    "            w += 1\n",
    "            curr_split = ''\n",
    "    vals = [(target[0], target[1]) for target in targets \\\n",
    "            if overlaps(start, end, target[2], target[3])]\n",
    "    return [val for val in vals if val[0] != '\"']\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def wordnet_pos_tagging(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    return nltk.pos_tag(tokens)\n",
    "\n",
    "def pos_tags(start, end, sentence):\n",
    "    wordPOSPairs = wordnet_pos_tagging(sentence)\n",
    "    targets_index = targets_with_index(start, end, sentence)\n",
    "    results = [wordPOSPairs[tpl[1]-1][1] for tpl in targets_index]\n",
    "    filtered_results = [result for result in results \n",
    "                        if remove_punctuation(result).strip() and result != 'POS']\n",
    "    return filtered_results if len(filtered_results) > 0 else None\n",
    "\n",
    "def wordnet_lemma(target, pos):\n",
    "    tokens = nltk.word_tokenize(target)\n",
    "    if pos:\n",
    "        pos = [penn_to_wn(poss) if penn_to_wn(poss) else 'n' for poss in pos]\n",
    "        lemmas = [wordNetLemmatizer.lemmatize(token, poss)\n",
    "                     for token, poss in zip(tokens, pos)]\n",
    "        return ' '.join(lemmas)\n",
    "    return target\n",
    "\n",
    "def preprocessing(dataframe):\n",
    "    df = dataframe.copy()\n",
    "    df['p_sentence'] = df.sentence.apply(lambda sent : sent.strip().lower())\n",
    "    df['sentence'] = df.sentence.apply(lambda sent : sent.replace(\"''\", \"``\"))\n",
    "    df['p_target'] = df.target.apply(lambda target : target.strip().lower())\n",
    "    df['pos_tags'] = df[['start', 'end', 'sentence']].apply(lambda vals : pos_tags(*vals), axis = 1)\n",
    "    df['pos_tags_pt'] = df.pos_tags.apply(lambda pos : [penn_to_wn(poss) if penn_to_wn(poss) else 'n' for poss in pos])\n",
    "    df['lemma'] = df[['target', 'pos_tags']].apply(lambda vals : wordnet_lemma(*vals), axis = 1)\n",
    "    df['p_lemma'] = df.lemma.apply(lambda lemma : lemma.strip().lower())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessed_datasets = [Dataset(ds.name, preprocessing(ds.train), \n",
    "                               preprocessing(ds.test)) for ds in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasets = preprocessed_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Context-Token Aggregation\n",
    "First we define how feature values of multiple context-tokens should be aggreagated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def agg_ctx_feat_num_average(tokens, func_feature, *args, **kwargs):\n",
    "#     if 'pos' in kwargs:\n",
    "#         pos = kwargs.pop('pos')\n",
    "#         return np.mean([func_feature(token, *args, pos=poss) for token, poss in zip(word_tokenize(tokens), pos)])\n",
    "    return np.mean([func_feature(token, *args) for token in tokens])\n",
    "\n",
    "def agg_ctx_feat_num_weighted_average(target, func_feature, alpha, *args):\n",
    "    return np.mean([(alpha/(alpha+get_unigram_probability(token))) * \n",
    "                func_feature(token, *args) for token in word_tokenize(target)])\n",
    "\n",
    "def agg_ctx_feat_num_distance(target, func_feature, *args):\n",
    "    pass\n",
    "\n",
    "def agg_ctx_feat_num_median(tokens, func_feature, *args):\n",
    "    return np.median([func_feature(token, *args) for token in tokens])\n",
    "\n",
    "def agg_ctx_feat_num_max(tokens, func_feature, *args):\n",
    "    return np.max([func_feature(token, *args) for token in tokens])\n",
    "\n",
    "def agg_ctx_feat_num_min(tokens, func_feature, *args):\n",
    "    return np.min([func_feature(token, *args) for token in tokens])\n",
    "\n",
    "def agg_ctx_feat_num_sum(tokens, func_feature, *args):\n",
    "    return np.sum([func_feature(token, *args) for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_default = [Aggregation('mean', agg_ctx_feat_num_average)]\n",
    "aggs_small = [Aggregation('mean', agg_ctx_feat_num_average), Aggregation('max', agg_ctx_feat_num_max)]\n",
    "aggs_all = [Aggregation('mean', agg_ctx_feat_num_average), Aggregation('median', agg_ctx_feat_num_median),\n",
    "            Aggregation('max', agg_ctx_feat_num_max), Aggregation('min', agg_ctx_feat_num_min)]\n",
    "           #Aggregation('weighted_mean', agg_ctx_feat_num_weighted_average_medium)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aggs = agg_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def concat_feature_datasets(*args):\n",
    "    zipped = zip(*args)\n",
    "    concat_features = []\n",
    "    for dataset in zipped:\n",
    "        df_train = None\n",
    "        df_test = None\n",
    "        fcs = []\n",
    "        aggs = []\n",
    "        for tpl in dataset:\n",
    "            if not fcs:\n",
    "                df_train = tpl.train.copy()\n",
    "                df_test = tpl.test.copy()\n",
    "            else:\n",
    "                df_train = pd.concat([df_train, tpl.train.copy()], axis = 1)\n",
    "                df_test = pd.concat([df_test, tpl.test.copy()], axis = 1)\n",
    "            fcs.append(tpl.fc)\n",
    "            aggs.append(tpl.agg)\n",
    "        concat_features.append(FeatureDataset(tpl.name, fcs, aggs,\n",
    "                    df_train.loc[:,~df_train.columns.duplicated()], \n",
    "                    df_test.loc[:,~df_test.columns.duplicated()]))\n",
    "    return concat_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Context Definition and Extraction\n",
    "Here we compute different kinds of context definitions. For example, as a baseline we extract all tokens from the sentence except the target. A second approach is to use a n preceeding or n succeding tokens, or a combined window apporach were we extract n tokens preceeding and succeding of the target. A more sophisticated apporach involves dependency parsing of the sentence and applying different extraction heuristics. Finally we also implement a context extraction approach exploting FrameNet semantic parsing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Context Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.parse.corenlp import *\n",
    "import os\n",
    "from functools import lru_cache\n",
    "\n",
    "# First make sure that the StanfordCoreNLP Server is running under port 9010\n",
    "parser = CoreNLPDependencyParser(url='http://localhost:9010/')\n",
    "\n",
    "with open(\"resources/dictionaries/stopwords_en.txt\", encoding=\"utf8\") as file:\n",
    "    content = [line.strip().lower() for line in file.readlines()]\n",
    "    stop_words = set(content)\n",
    "    \n",
    "def overlaps(start1, end1, start2, end2):\n",
    "    return bool(range(max(start1, start2), min(end1, end2)+1))\n",
    "\n",
    "def post_process_ctx(context, filtering=True):\n",
    "    return [token for token in context if \n",
    "            (token.isalnum() and (not filtering\n",
    "        or preprocess_target(token).lower() not in stop_words))]\n",
    "\n",
    "def preprocess_target(target):\n",
    "    return target.strip()\n",
    "\n",
    "def target_index_char_based(start, end, ctx_tokens):\n",
    "    size = np.sum([len(token) for token in ctx_tokens]) + len(ctx_tokens)\n",
    "    target_pos = (start + end) / 2\n",
    "    target_pos_rel = target_pos / size\n",
    "    return int(target_pos_rel * len(post_process_ctx(ctx_tokens)))\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def targets_with_index(start, end, context):\n",
    "    curr_pos = 0\n",
    "    targets = []\n",
    "    j = 0\n",
    "    w = 0\n",
    "    curr_split = ''\n",
    "    ctx_split = context.split()\n",
    "    whitespaces = re.findall('\\s+', context)\n",
    "    num_whitespaces = [len(token) for token in whitespaces]\n",
    "    num_whitespaces.append(1)\n",
    "    tokens = word_tokenize(context)\n",
    "    tokens = ['\"' if token not in context else token for token in tokens]\n",
    "    for index, token in enumerate(tokens, 1):\n",
    "        targets.append((token, index, curr_pos, (curr_pos + len(token))))\n",
    "        curr_pos += len(token)\n",
    "        curr_split += token\n",
    "        if ctx_split[j] == curr_split:\n",
    "            curr_pos += num_whitespaces[w]\n",
    "            j += 1\n",
    "            w += 1\n",
    "            curr_split = ''\n",
    "    vals = [(target[0], target[1]) for target in targets \\\n",
    "            if overlaps(start, end, target[2], target[3])]\n",
    "    return [val for val in vals if val[0] != '\"']\n",
    "\n",
    "from joblib import Memory\n",
    "memory = Memory(location='resources/dependency-cache', verbose=0)\n",
    "@memory.cache\n",
    "def dependency_parse_with_root(sentence):\n",
    "    try:\n",
    "        dependency_parser = parser.raw_parse(sentence)\n",
    "        dependencies = []\n",
    "        parsetree = list(dependency_parser)[0]\n",
    "        for index, node in parsetree.nodes.items():\n",
    "            for relation, dependant in parsetree.nodes[index]['deps'].items():\n",
    "                for dep in dependant:\n",
    "                    triple = ((node['word'], index), relation, \\\n",
    "                              (parsetree.nodes[dep]['word'], dep))\n",
    "                    dependencies.append(triple)\n",
    "        return dependencies\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def dependency_parse(sentence):\n",
    "    dependencies = dependency_parse_with_root(sentence)\n",
    "    filtered_dependencies = [triple for triple in dependencies if triple[1] != 'ROOT']\n",
    "    return filtered_dependencies\n",
    "\n",
    "def ctx_extraction_all(context, target):\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    if target in ctx_tokens:\n",
    "        ctx_tokens.remove(target)\n",
    "    return ctx_tokens\n",
    "\n",
    "def ctx_extraction_all_filtered(context, target, filtering = True):\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    post_ctx_tokens = post_process_ctx(ctx_tokens, filtering)\n",
    "    if target in ctx_tokens:\n",
    "        ctx_tokens.remove(target)\n",
    "    return post_process_ctx\n",
    "\n",
    "def ctx_extraction_window_pre_n(context, target, start, end, filtering = True , n = 3):\n",
    "    target = preprocess_target(target)\n",
    "    ctx_tokens = word_tokenize(context[:start])\n",
    "    post_ctx_tokens = post_process_ctx(ctx_tokens, filtering)\n",
    "    return post_ctx_tokens[-n:]\n",
    "\n",
    "def ctx_extraction_window_suc_n(context, target, start, end, filtering = True, n = 3):\n",
    "    target = preprocess_target(target)\n",
    "    ctx_tokens = word_tokenize(context[end:])\n",
    "    post_ctx_tokens = post_process_ctx(ctx_tokens, filtering)\n",
    "    return post_ctx_tokens[:n]\n",
    "\n",
    "def ctx_extraction_window_pre_suc_n(context, target, start, end, filtering = True, n = 3):\n",
    "    ctx_tokens_pre = ctx_extraction_window_pre_n(context, target, start, end, filtering, n)\n",
    "    ctx_tokens_suc = ctx_extraction_window_suc_n(context, target, start, end, filtering, n)\n",
    "    ctx_tokens_pre.extend(ctx_tokens_suc)\n",
    "    return ctx_tokens_pre\n",
    "\n",
    "def ctx_extraction_dep_in(context, target, start, end):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    return list(set([triple[0][0] for triple in triples if triple[2] in targets]))\n",
    "\n",
    "def ctx_extraction_dep_out(context, target, start, end):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    return list(set([triple[2][0] for triple in triples if triple[0] in targets]))\n",
    "\n",
    "def ctx_extraction_dep_in_out(context, target, start, end):\n",
    "    ctx_tokens_in = ctx_extraction_dep_in(context, target, start, end)\n",
    "    ctx_tokens_out = ctx_extraction_dep_out(context, target, start, end)\n",
    "    ctx_tokens_in.extend(ctx_tokens_out)\n",
    "    return list(set(ctx_tokens_in))\n",
    "\n",
    "def ctx_extraction_dep_recu_in_n_steps(context, target, start, end, n = 2):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    result_tokens = []\n",
    "    curr_target = targets\n",
    "    for step in range(0, n):\n",
    "        step_result = [triple[0] for triple in triples \n",
    "                       if triple[2] in curr_target]\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "    return list(set([token[0] for token in result_tokens]))\n",
    "\n",
    "def ctx_extraction_dep_recu_out_n_steps(context, target, start, end, n = 2):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    result_tokens = []\n",
    "    curr_target = targets\n",
    "    for step in range(0, n):\n",
    "        step_result = [triple[2] for triple in triples \n",
    "                       if triple[0] in curr_target]\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "    return list(set([token[0] for token in result_tokens]))\n",
    "\n",
    "def ctx_extraction_dep_recu_in_out_n_steps(context, target, start, end, n = 2):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    result_tokens = []\n",
    "    curr_target = targets\n",
    "    for step in range(0, n):\n",
    "        step_result = [triple[2] for triple in triples \n",
    "                       if triple[0] in curr_target]\n",
    "        step_result_out = [triple[0] for triple in triples \n",
    "                       if triple[2] in curr_target]\n",
    "        step_result.extend(step_result_out)\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "    return list(set([token[0] for token in result_tokens]))\n",
    "\n",
    "def ctx_extraction_dep_recu_in_cover(context, target, start, end, cover = 0.1):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    ctx_tokens_post = post_process_ctx(ctx_tokens)\n",
    "    result_tokens = []\n",
    "    curr_target = targets\n",
    "    curr_cover = 0\n",
    "    while curr_cover < cover:\n",
    "        step_result = [triple[0] for triple in triples \n",
    "                       if triple[2] in curr_target]\n",
    "        if set(step_result) == set(curr_target):\n",
    "                break\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "        curr_cover = len(result_tokens) / len(ctx_tokens_post)\n",
    "    return list(set([token[0] for token in result_tokens]))\n",
    "\n",
    "def ctx_extraction_dep_recu_out_cover(context, target, start, end, cover = 0.1):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    ctx_tokens_post = post_process_ctx(ctx_tokens)\n",
    "    result_tokens = []\n",
    "    curr_target = targets\n",
    "    curr_cover = 0\n",
    "    while curr_cover < cover:\n",
    "        step_result = [triple[2] for triple in triples \n",
    "                       if triple[0] in curr_target]\n",
    "        if set(step_result) == set(curr_target):\n",
    "                break\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "        curr_cover = len(result_tokens) / len(ctx_tokens_post)\n",
    "    return list(set([token[0] for token in result_tokens]))\n",
    "\n",
    "def ctx_extraction_dep_recu_in_out_cover(context, target, start, end, cover = 0.1):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    ctx_tokens_post = post_process_ctx(ctx_tokens)\n",
    "    result_tokens = []\n",
    "    curr_target = targets\n",
    "    curr_cover = 0\n",
    "    while curr_cover < cover:\n",
    "        step_result = [triple[2] for triple in triples \n",
    "                       if triple[0] in curr_target]\n",
    "        step_result_out = [triple[0] for triple in triples \n",
    "                       if triple[2] in curr_target]\n",
    "        step_result.extend(step_result_out)\n",
    "        if set(step_result) == set(curr_target):\n",
    "                break\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "        curr_cover = len(result_tokens) / len(ctx_tokens_post)\n",
    "    return list(set([token[0] for token in result_tokens]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Normally, the land will be passed down by future generations in a way \" + \\\n",
    "             \"that recognizes the community's traditional connection to that country .\"\n",
    "target = 'passed'\n",
    "\n",
    "print('ctx_etraction_all:')\n",
    "print(ctx_extraction_all_filtered(sentence, target))\n",
    "\n",
    "print('ctx_extraction_window_pre_n:')\n",
    "print(ctx_extraction_window_pre_n(sentence, \"Normally\", 0, 8, filtering=False))\n",
    "print(ctx_extraction_window_pre_n(sentence, \"the\", 11, 14, filtering=False))\n",
    "print(ctx_extraction_window_pre_n(sentence, \"land\", 15, 19, filtering=False))\n",
    "print(ctx_extraction_window_pre_n(sentence, \"to\", 126, 128, filtering=False))\n",
    "print(ctx_extraction_window_pre_n(sentence, target, 28, 34, n = 5, filtering=False))\n",
    "\n",
    "print('ctx_extraction_window_suc_n:')\n",
    "print(ctx_extraction_window_suc_n(sentence, \"country\", 135, 142, filtering=False))\n",
    "print(ctx_extraction_window_suc_n(sentence, \"to\", 126, 128, filtering=False))\n",
    "print(ctx_extraction_window_suc_n(sentence, \"connection\", 115, 125, filtering=False))\n",
    "print(ctx_extraction_window_suc_n(sentence, \"community\", 91, 100, n = 5, filtering=False))\n",
    "\n",
    "print('ctx_extraction_window_pre_suc_n:')\n",
    "print(ctx_extraction_window_pre_suc_n(sentence, \"passed\", 28, 34, filtering=False))\n",
    "print(ctx_extraction_window_pre_suc_n(sentence, \"the\", 11, 14, filtering=False))\n",
    "print(ctx_extraction_window_pre_suc_n(sentence, \"to\", 127, 129, filtering=False))\n",
    "\n",
    "print('ctx_extraction_dep_in:')\n",
    "print(ctx_extraction_dep_in(sentence, \"land\", 15, 19))\n",
    "\n",
    "print('ctx_extraction_dep_out:')\n",
    "print(ctx_extraction_dep_out(sentence, target, 28, 34))\n",
    "print(ctx_extraction_dep_out(sentence, \"land\", 15, 19))\n",
    "\n",
    "print('ctx_extraction_dep_in_out:')\n",
    "print(ctx_extraction_dep_in_out(sentence, \"land\", 15, 19))\n",
    "\n",
    "print('ctx_extraction_dep_recu_in_n_steps:')\n",
    "print(ctx_extraction_dep_recu_in_n_steps(sentence, \"the\", 11, 14, n = 3))\n",
    "\n",
    "print('ctx_extraction_dep_recu_out_n_steps:')\n",
    "print(ctx_extraction_dep_recu_out_n_steps(sentence, \"the\", 11, 14))\n",
    "\n",
    "print('ctx_extraction_dep_recu_in_out_n_steps:')\n",
    "print(ctx_extraction_dep_recu_in_out_n_steps(sentence, \"the\", 11, 14))\n",
    "\n",
    "print('ctx_extraction_dep_recu_in_cover:')\n",
    "print(ctx_extraction_dep_recu_in_cover(sentence, \"the\", 11, 14, cover=0.1))\n",
    "\n",
    "print('ctx_extraction_dep_recu_out_cover:')\n",
    "print(ctx_extraction_dep_recu_out_cover(sentence, \"the\", 11, 14, cover=0.1))\n",
    "\n",
    "print('ctx_extraction_dep_recu_in_out_cover:')\n",
    "print(ctx_extraction_dep_recu_in_out_cover(sentence, \"the\", 11, 14, cover=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Context Extraction\n",
    "\n",
    "After we defined all the context extraction approaches, we can apply them on the actual dataset. To do so, we first extract all the distinct sentences from the actual training set and create a new dataframe containing only the sentence ids, the sentence, the target and all the computed contexts. This also makes it easier to integrate context extraction functions implemented in other languages. Afterwards we can compute the context features and join them back with the target features dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "Context = namedtuple('Context', 'name, params, func')\n",
    "ContextFeatureCategory = namedtuple('ContextFeatureCategory', 'name, func')\n",
    "ContextDataset = namedtuple('ContextDataset', 'name, context, train, test')\n",
    "ContextFeatureDataset = namedtuple('ContextFeatureDataset', 'name, context, fc, agg, train, test')\n",
    "ctx_fcs = []\n",
    "ctx_feature_datasets = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.2.1) Extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctx_window(dataframe, n, filtering, window_func):\n",
    "    df = dataframe.copy()\n",
    "    df['context'] = df.apply(lambda columns : \n",
    "                window_func(columns['sentence'], columns['target'], \\\n",
    "                columns['start'], columns['end'],  n = n, filtering = filtering), axis = 1)\n",
    "    return df\n",
    "\n",
    "ctx_window_pre_2_nf = Context('ctx_window_pre_n', {'n':2, 'filtering':False}, \\\n",
    "                              lambda dataframe : ctx_window(dataframe, 2, False, ctx_extraction_window_pre_n))\n",
    "ctx_window_pre_2_f = Context('ctx_window_pre_n', {'n':2, 'filtering':True}, \\\n",
    "                             lambda dataframe : ctx_window(dataframe, 2, True, ctx_extraction_window_pre_n))\n",
    "ctx_window_suc_n_2_nf = Context('ctx_window_suc_n',  {'n':2, 'filtering':False}, \\\n",
    "                            lambda dataframe : ctx_window(dataframe, 2, False, ctx_extraction_window_suc_n))\n",
    "ctx_window_pre_suc_n_2_nf = Context('ctx_window_pre_suc_n',  {'n':2, 'filtering':False}, \\\n",
    "                            lambda dataframe : ctx_window(dataframe, 2, False, ctx_extraction_window_pre_suc_n))\n",
    "\n",
    "\n",
    "\n",
    "def ctx_dependency(dataframe, filtering, dep_func):\n",
    "    df = dataframe.copy()\n",
    "    df['context'] = df.apply(lambda columns : \n",
    "            dep_func(columns['sentence'], columns['target'], \\\n",
    "            columns['start'], columns['end']), axis = 1)\n",
    "    return df\n",
    "\n",
    "ctx_dep_in_2_nf = Context('ctx_dep_in', {'n':2, 'filtering':False}, \\\n",
    "                              lambda dataframe : ctx_dependency(dataframe, False, ctx_extraction_dep_in))\n",
    "ctx_dep_out_2_nf = Context('ctx_dep_out', {'n':2, 'filtering':False}, \\\n",
    "                              lambda dataframe : ctx_dependency(dataframe, False, ctx_extraction_dep_out))\n",
    "ctx_dep_in_out_2_nf = Context('ctx_dep_in_out', {'n':2, 'filtering':False}, \\\n",
    "                              lambda dataframe : ctx_dependency(dataframe, False, ctx_extraction_dep_in_out))\n",
    "\n",
    "\n",
    "\n",
    "def ctx_dependency_recu_steps(dataframe, n, filtering, dep_func):\n",
    "    df = dataframe.copy()\n",
    "    df['context']  = df.apply(lambda columns : \n",
    "                dep_func(columns['sentence'], columns['target'], \\\n",
    "                columns['start'], columns['end'], n=n), axis = 1)\n",
    "    return df\n",
    "\n",
    "ctx_dep_rec_in_2_nf = Context('ctx_dep_rec_in_n', {'n':2, 'filtering':False}, \\\n",
    "                lambda dataframe : ctx_dependency_recu_steps(dataframe, 2, False, ctx_extraction_dep_recu_in_n_steps))\n",
    "ctx_dep_rec_out_2_nf = Context('ctx_dep_rec_out_n', {'n':2, 'filtering':False}, \\\n",
    "                lambda dataframe : ctx_dependency_recu_steps(dataframe, 2, False, ctx_extraction_dep_recu_out_n_steps))\n",
    "ctx_dep_rec_in_out_2_nf = Context('ctx_dep_rec_in_out_n', {'n':2, 'filtering':False}, \\\n",
    "                lambda dataframe : ctx_dependency_recu_steps(dataframe, 2, False, ctx_extraction_dep_recu_in_out_n_steps))\n",
    "\n",
    "\n",
    "\n",
    "def ctx_dependency_recu_cover(dataframe, cover, filtering, dep_func):\n",
    "    df = dataframe.copy()\n",
    "    df['context']  = df.apply(lambda columns : \n",
    "                dep_func(columns['sentence'], columns['target'], \\\n",
    "                columns['start'], columns['end'], cover=cover), axis = 1)\n",
    "    return df\n",
    "\n",
    "ctx_dep_rec_in_02_nf = Context('ctx_dep_rec_in_02', {'cover': 0.2, 'filtering':False}, \\\n",
    "                lambda dataframe : ctx_dependency_recu_cover(dataframe, 0.2, False, ctx_extraction_dep_recu_in_cover))\n",
    "ctx_dep_rec_out_02_nf = Context('ctx_dep_rec_out_02', {'cover': 0.2, 'filtering':False}, \\\n",
    "                lambda dataframe : ctx_dependency_recu_cover(dataframe, 0.2, False, ctx_extraction_dep_recu_out_cover))\n",
    "ctx_dep_rec_in_out_02_nf = Context('ctx_dep_rec_in_out_02', {'cover': 0.2, 'filtering':False}, \\\n",
    "                lambda dataframe : ctx_dependency_recu_cover(dataframe, 0.2, False, ctx_extraction_dep_recu_in_out_cover))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.2.2) Context Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "tbl = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "                      if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(tbl)\n",
    "\n",
    "def preprocess_ctx(context):\n",
    "    stripped = [token.strip().lower() for token in context]\n",
    "    return [token for token in stripped if remove_punctuation(token)]\n",
    "\n",
    "def preprocess_ctx_df(dataframe):\n",
    "    df = dataframe.copy()\n",
    "    df['p_context'] = df.context.apply(lambda context : preprocess_ctx(context))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = [ctx_window_pre_suc_n_2_nf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_datasets = [ContextDataset(ds.name, ctx, preprocess_ctx_df(ctx.func(ds.train)), \n",
    "                preprocess_ctx_df(ctx.func(ds.test)))\n",
    "                for ctx in contexts\n",
    "                for ds in datasets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Context Features\n",
    "After defining all the context definitions and extracting the different kinds of contexts from the sentence, we compute features on the context words. Therefore we first define which of the precomputed contexts to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4.1) Context Complexity Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_freq_wiki = {}\n",
    "freq_sum_wiki = 0\n",
    "with open(\"resources/word-freq-dumps/enwiki-20150602-words-frequency.txt\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        word, freq = line.partition(\" \")[::2]\n",
    "        word_freq_wiki[word.strip()] = int(freq)\n",
    "        freq_sum_wiki+=int(freq)\n",
    "        \n",
    "def get_dict_count(target, freqs):\n",
    "    return freqs.get(target.strip().lower(), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ctx_features_context_complexity(dataframe, agg):\n",
    "    df = dataframe.copy()\n",
    "    df['ctx_length'] = df.p_context.apply(lambda context : agg(context, len))\n",
    "    df['ctx_freq_wiki'] = df.p_context.apply(lambda context : agg(context, get_dict_count, word_freq_wiki))\n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "ctx_fc_context_complexity = ContextFeatureCategory('baseline_1', ctx_features_context_complexity)\n",
    "feature_categories.append(ctx_fc_context_complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_datasets_fc_context_complexity = [ContextFeatureDataset(ctx_ds.name, ctx_ds.context, ctx_fc_context_complexity, agg, \n",
    "        ctx_fc_context_complexity.func(ctx_ds.train, agg.agg), ctx_fc_context_complexity.func(ctx_ds.test, agg.agg)) \n",
    "        for ctx_ds in ctx_datasets for agg in aggs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>target</th>\n",
       "      <th>nat</th>\n",
       "      <th>non_nat</th>\n",
       "      <th>nat_marked</th>\n",
       "      <th>non_nat_marked</th>\n",
       "      <th>binary</th>\n",
       "      <th>prob</th>\n",
       "      <th>p_target</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>pos_tags_pt</th>\n",
       "      <th>lemma</th>\n",
       "      <th>p_lemma</th>\n",
       "      <th>p_sentence</th>\n",
       "      <th>context</th>\n",
       "      <th>p_context</th>\n",
       "      <th>ctx_length</th>\n",
       "      <th>ctx_freq_wiki</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to fut...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>Normally</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>normally</td>\n",
       "      <td>[RB]</td>\n",
       "      <td>[r]</td>\n",
       "      <td>Normally</td>\n",
       "      <td>normally</td>\n",
       "      <td>normally , the land will be passed down to fut...</td>\n",
       "      <td>[the, land]</td>\n",
       "      <td>[the, land]</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>6.121852e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to fut...</td>\n",
       "      <td>28</td>\n",
       "      <td>34</td>\n",
       "      <td>passed</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>passed</td>\n",
       "      <td>[VBN]</td>\n",
       "      <td>[v]</td>\n",
       "      <td>pass</td>\n",
       "      <td>pass</td>\n",
       "      <td>normally , the land will be passed down to fut...</td>\n",
       "      <td>[will, be, down, to]</td>\n",
       "      <td>[will, be, down, to]</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.019463e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to fut...</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>land</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>land</td>\n",
       "      <td>[NN]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>land</td>\n",
       "      <td>land</td>\n",
       "      <td>normally , the land will be passed down to fut...</td>\n",
       "      <td>[Normally, the, will, be]</td>\n",
       "      <td>[normally, the, will, be]</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>3.185175e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to fut...</td>\n",
       "      <td>43</td>\n",
       "      <td>49</td>\n",
       "      <td>future</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>future</td>\n",
       "      <td>[JJ]</td>\n",
       "      <td>[a]</td>\n",
       "      <td>future</td>\n",
       "      <td>future</td>\n",
       "      <td>normally , the land will be passed down to fut...</td>\n",
       "      <td>[down, to, generations, in]</td>\n",
       "      <td>[down, to, generations, in]</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>2.112122e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to fut...</td>\n",
       "      <td>43</td>\n",
       "      <td>61</td>\n",
       "      <td>future generations</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.15</td>\n",
       "      <td>future generations</td>\n",
       "      <td>[JJ, NNS]</td>\n",
       "      <td>[a, n]</td>\n",
       "      <td>future generation</td>\n",
       "      <td>future generation</td>\n",
       "      <td>normally , the land will be passed down to fut...</td>\n",
       "      <td>[down, to, in, a]</td>\n",
       "      <td>[down, to, in, a]</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>2.111276e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to fut...</td>\n",
       "      <td>50</td>\n",
       "      <td>61</td>\n",
       "      <td>generations</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>generations</td>\n",
       "      <td>[NNS]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>generation</td>\n",
       "      <td>generation</td>\n",
       "      <td>normally , the land will be passed down to fut...</td>\n",
       "      <td>[to, future, in, a]</td>\n",
       "      <td>[to, future, in, a]</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>2.104169e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to fut...</td>\n",
       "      <td>76</td>\n",
       "      <td>86</td>\n",
       "      <td>recognizes</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.30</td>\n",
       "      <td>recognizes</td>\n",
       "      <td>[VBZ]</td>\n",
       "      <td>[v]</td>\n",
       "      <td>recognize</td>\n",
       "      <td>recognize</td>\n",
       "      <td>normally , the land will be passed down to fut...</td>\n",
       "      <td>[way, that, the, community]</td>\n",
       "      <td>[way, that, the, community]</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>3.318412e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to fut...</td>\n",
       "      <td>91</td>\n",
       "      <td>100</td>\n",
       "      <td>community</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>community</td>\n",
       "      <td>[NN]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>community</td>\n",
       "      <td>community</td>\n",
       "      <td>normally , the land will be passed down to fut...</td>\n",
       "      <td>[recognizes, the, traditional, connection]</td>\n",
       "      <td>[recognizes, the, traditional, connection]</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>3.056799e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to fut...</td>\n",
       "      <td>104</td>\n",
       "      <td>115</td>\n",
       "      <td>traditional</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.20</td>\n",
       "      <td>traditional</td>\n",
       "      <td>[JJ]</td>\n",
       "      <td>[a]</td>\n",
       "      <td>traditional</td>\n",
       "      <td>traditional</td>\n",
       "      <td>normally , the land will be passed down to fut...</td>\n",
       "      <td>[the, community, connection, to]</td>\n",
       "      <td>[the, community, connection, to]</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.935440e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to fut...</td>\n",
       "      <td>104</td>\n",
       "      <td>142</td>\n",
       "      <td>traditional connection to that country</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>traditional connection to that country</td>\n",
       "      <td>[JJ, NN, TO, DT, NN]</td>\n",
       "      <td>[a, n, n, n, n]</td>\n",
       "      <td>traditional connection to that country</td>\n",
       "      <td>traditional connection to that country</td>\n",
       "      <td>normally , the land will be passed down to fut...</td>\n",
       "      <td>[the, community]</td>\n",
       "      <td>[the, community]</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.124369e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to fut...</td>\n",
       "      <td>135</td>\n",
       "      <td>142</td>\n",
       "      <td>country</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>country</td>\n",
       "      <td>[NN]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>country</td>\n",
       "      <td>country</td>\n",
       "      <td>normally , the land will be passed down to fut...</td>\n",
       "      <td>[to, that]</td>\n",
       "      <td>[to, that]</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.229473e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to fut...</td>\n",
       "      <td>116</td>\n",
       "      <td>126</td>\n",
       "      <td>connection</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>connection</td>\n",
       "      <td>[NN]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>connection</td>\n",
       "      <td>connection</td>\n",
       "      <td>normally , the land will be passed down to fut...</td>\n",
       "      <td>[community, traditional, to, that]</td>\n",
       "      <td>[community, traditional, to, that]</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>1.134338e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>15</td>\n",
       "      <td>25</td>\n",
       "      <td>Aboriginal</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.55</td>\n",
       "      <td>aboriginal</td>\n",
       "      <td>[NNP]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>Aboriginal</td>\n",
       "      <td>aboriginal</td>\n",
       "      <td>the passing of aboriginal land rights legislat...</td>\n",
       "      <td>[passing, of, land, rights]</td>\n",
       "      <td>[passing, of, land, rights]</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>1.472406e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>passing</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>passing</td>\n",
       "      <td>[NN]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>passing</td>\n",
       "      <td>passing</td>\n",
       "      <td>the passing of aboriginal land rights legislat...</td>\n",
       "      <td>[The, of, Aboriginal]</td>\n",
       "      <td>[the, of, aboriginal]</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.999576e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>15</td>\n",
       "      <td>48</td>\n",
       "      <td>Aboriginal land rights legislaton</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>aboriginal land rights legislaton</td>\n",
       "      <td>[NNP, NN, NNS, NN]</td>\n",
       "      <td>[n, n, n, n]</td>\n",
       "      <td>Aboriginal land right legislaton</td>\n",
       "      <td>aboriginal land right legislaton</td>\n",
       "      <td>the passing of aboriginal land rights legislat...</td>\n",
       "      <td>[passing, of, in, Australia]</td>\n",
       "      <td>[passing, of, in, australia]</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.689054e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>26</td>\n",
       "      <td>48</td>\n",
       "      <td>land rights legislaton</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>land rights legislaton</td>\n",
       "      <td>[NN, NNS, NN]</td>\n",
       "      <td>[n, n, n]</td>\n",
       "      <td>land right legislaton</td>\n",
       "      <td>land right legislaton</td>\n",
       "      <td>the passing of aboriginal land rights legislat...</td>\n",
       "      <td>[of, Aboriginal, in, Australia]</td>\n",
       "      <td>[of, aboriginal, in, australia]</td>\n",
       "      <td>5.750000</td>\n",
       "      <td>2.687577e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>26</td>\n",
       "      <td>30</td>\n",
       "      <td>land</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>land</td>\n",
       "      <td>[NN]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>land</td>\n",
       "      <td>land</td>\n",
       "      <td>the passing of aboriginal land rights legislat...</td>\n",
       "      <td>[of, Aboriginal, rights, legislaton]</td>\n",
       "      <td>[of, aboriginal, rights, legislaton]</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.458396e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>38</td>\n",
       "      <td>48</td>\n",
       "      <td>legislaton</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>legislaton</td>\n",
       "      <td>[NN]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>legislaton</td>\n",
       "      <td>legislaton</td>\n",
       "      <td>the passing of aboriginal land rights legislat...</td>\n",
       "      <td>[land, rights, in, Australia]</td>\n",
       "      <td>[land, rights, in, australia]</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>1.255927e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>31</td>\n",
       "      <td>37</td>\n",
       "      <td>rights</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>rights</td>\n",
       "      <td>[NNS]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>right</td>\n",
       "      <td>right</td>\n",
       "      <td>the passing of aboriginal land rights legislat...</td>\n",
       "      <td>[Aboriginal, land, legislaton, in]</td>\n",
       "      <td>[aboriginal, land, legislaton, in]</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>1.240277e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>66</td>\n",
       "      <td>74</td>\n",
       "      <td>preceded</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.70</td>\n",
       "      <td>preceded</td>\n",
       "      <td>[VBN]</td>\n",
       "      <td>[v]</td>\n",
       "      <td>precede</td>\n",
       "      <td>precede</td>\n",
       "      <td>the passing of aboriginal land rights legislat...</td>\n",
       "      <td>[Australia, was, by, a]</td>\n",
       "      <td>[australia, was, by, a]</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>8.299596e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>52</td>\n",
       "      <td>61</td>\n",
       "      <td>Australia</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>australia</td>\n",
       "      <td>[NNP]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>Australia</td>\n",
       "      <td>australia</td>\n",
       "      <td>the passing of aboriginal land rights legislat...</td>\n",
       "      <td>[legislaton, in, was, preceded]</td>\n",
       "      <td>[legislaton, in, was, preceded]</td>\n",
       "      <td>5.750000</td>\n",
       "      <td>1.747121e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>100</td>\n",
       "      <td>110</td>\n",
       "      <td>Aboriginal</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>aboriginal</td>\n",
       "      <td>[NNP]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>Aboriginal</td>\n",
       "      <td>aboriginal</td>\n",
       "      <td>the passing of aboriginal land rights legislat...</td>\n",
       "      <td>[of, important, protests, including]</td>\n",
       "      <td>[of, important, protests, including]</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.492840e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>80</td>\n",
       "      <td>86</td>\n",
       "      <td>number</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>number</td>\n",
       "      <td>[NN]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>number</td>\n",
       "      <td>number</td>\n",
       "      <td>the passing of aboriginal land rights legislat...</td>\n",
       "      <td>[by, a, of, important]</td>\n",
       "      <td>[by, a, of, important]</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>1.761584e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>90</td>\n",
       "      <td>99</td>\n",
       "      <td>important</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>important</td>\n",
       "      <td>[JJ]</td>\n",
       "      <td>[a]</td>\n",
       "      <td>important</td>\n",
       "      <td>important</td>\n",
       "      <td>the passing of aboriginal land rights legislat...</td>\n",
       "      <td>[number, of, Aboriginal, protests]</td>\n",
       "      <td>[number, of, aboriginal, protests]</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>1.477941e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>100</td>\n",
       "      <td>119</td>\n",
       "      <td>Aboriginal protests</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>aboriginal protests</td>\n",
       "      <td>[NNP, NNS]</td>\n",
       "      <td>[n, n]</td>\n",
       "      <td>Aboriginal protest</td>\n",
       "      <td>aboriginal protest</td>\n",
       "      <td>the passing of aboriginal land rights legislat...</td>\n",
       "      <td>[of, important, including, the]</td>\n",
       "      <td>[of, important, including, the]</td>\n",
       "      <td>5.750000</td>\n",
       "      <td>4.540267e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>152</td>\n",
       "      <td>160</td>\n",
       "      <td>Stockmen</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>stockmen</td>\n",
       "      <td>[NNP]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>Stockmen</td>\n",
       "      <td>stockmen</td>\n",
       "      <td>the passing of aboriginal land rights legislat...</td>\n",
       "      <td>[1946, Aboriginal, Strike, the]</td>\n",
       "      <td>[1946, aboriginal, strike, the]</td>\n",
       "      <td>5.750000</td>\n",
       "      <td>3.050896e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>111</td>\n",
       "      <td>119</td>\n",
       "      <td>protests</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>protests</td>\n",
       "      <td>[NNS]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>protest</td>\n",
       "      <td>protest</td>\n",
       "      <td>the passing of aboriginal land rights legislat...</td>\n",
       "      <td>[important, Aboriginal, including, the]</td>\n",
       "      <td>[important, aboriginal, including, the]</td>\n",
       "      <td>7.750000</td>\n",
       "      <td>3.090293e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>122</td>\n",
       "      <td>131</td>\n",
       "      <td>including</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>including</td>\n",
       "      <td>[VBG]</td>\n",
       "      <td>[v]</td>\n",
       "      <td>include</td>\n",
       "      <td>include</td>\n",
       "      <td>the passing of aboriginal land rights legislat...</td>\n",
       "      <td>[Aboriginal, protests, the, 1946]</td>\n",
       "      <td>[aboriginal, protests, the, 1946]</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>3.050016e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>141</td>\n",
       "      <td>151</td>\n",
       "      <td>Aboriginal</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>aboriginal</td>\n",
       "      <td>[NNP]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>Aboriginal</td>\n",
       "      <td>aboriginal</td>\n",
       "      <td>the passing of aboriginal land rights legislat...</td>\n",
       "      <td>[the, 1946, Stockmen, Strike]</td>\n",
       "      <td>[the, 1946, stockmen, strike]</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>3.050245e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislat...</td>\n",
       "      <td>164</td>\n",
       "      <td>170</td>\n",
       "      <td>Strike</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>strike</td>\n",
       "      <td>[NN]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>Strike</td>\n",
       "      <td>strike</td>\n",
       "      <td>the passing of aboriginal land rights legislat...</td>\n",
       "      <td>[Aboriginal, Stockmen, the, 1963]</td>\n",
       "      <td>[aboriginal, stockmen, the, 1963]</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>3.049058e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5521</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>In the third instar , purple or blue edging de...</td>\n",
       "      <td>44</td>\n",
       "      <td>52</td>\n",
       "      <td>develops</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>develops</td>\n",
       "      <td>[NNS]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>develops</td>\n",
       "      <td>develops</td>\n",
       "      <td>in the third instar , purple or blue edging de...</td>\n",
       "      <td>[blue, edging, on, the]</td>\n",
       "      <td>[blue, edging, on, the]</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>3.395373e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5522</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>In the third instar , purple or blue edging de...</td>\n",
       "      <td>60</td>\n",
       "      <td>66</td>\n",
       "      <td>yellow</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>yellow</td>\n",
       "      <td>[JJ]</td>\n",
       "      <td>[a]</td>\n",
       "      <td>yellow</td>\n",
       "      <td>yellow</td>\n",
       "      <td>in the third instar , purple or blue edging de...</td>\n",
       "      <td>[on, the, stripes, and]</td>\n",
       "      <td>[on, the, stripes, and]</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>4.636381e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5523</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>In the third instar , purple or blue edging de...</td>\n",
       "      <td>83</td>\n",
       "      <td>92</td>\n",
       "      <td>tail horn</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>tail horn</td>\n",
       "      <td>[NN, NN]</td>\n",
       "      <td>[n, n]</td>\n",
       "      <td>tail horn</td>\n",
       "      <td>tail horn</td>\n",
       "      <td>in the third instar , purple or blue edging de...</td>\n",
       "      <td>[and, the, turns, from]</td>\n",
       "      <td>[and, the, turns, from]</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>4.520576e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5524</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>In the third instar , purple or blue edging de...</td>\n",
       "      <td>83</td>\n",
       "      <td>87</td>\n",
       "      <td>tail</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>tail</td>\n",
       "      <td>[NN]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>tail</td>\n",
       "      <td>tail</td>\n",
       "      <td>in the third instar , purple or blue edging de...</td>\n",
       "      <td>[and, the, horn, turns]</td>\n",
       "      <td>[and, the, horn, turns]</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>4.297020e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5525</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>In the third instar , purple or blue edging de...</td>\n",
       "      <td>88</td>\n",
       "      <td>92</td>\n",
       "      <td>horn</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>horn</td>\n",
       "      <td>[NN]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>horn</td>\n",
       "      <td>horn</td>\n",
       "      <td>in the third instar , purple or blue edging de...</td>\n",
       "      <td>[the, tail, turns, from]</td>\n",
       "      <td>[the, tail, turns, from]</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.275636e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5526</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>In the third instar , purple or blue edging de...</td>\n",
       "      <td>93</td>\n",
       "      <td>98</td>\n",
       "      <td>turns</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>turns</td>\n",
       "      <td>[VBZ]</td>\n",
       "      <td>[v]</td>\n",
       "      <td>turn</td>\n",
       "      <td>turn</td>\n",
       "      <td>in the third instar , purple or blue edging de...</td>\n",
       "      <td>[tail, horn, from, black]</td>\n",
       "      <td>[tail, horn, from, black]</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>2.370318e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5527</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>In the third instar , purple or blue edging de...</td>\n",
       "      <td>104</td>\n",
       "      <td>109</td>\n",
       "      <td>black</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>black</td>\n",
       "      <td>[JJ]</td>\n",
       "      <td>[a]</td>\n",
       "      <td>black</td>\n",
       "      <td>black</td>\n",
       "      <td>in the third instar , purple or blue edging de...</td>\n",
       "      <td>[turns, from, to, yellow]</td>\n",
       "      <td>[turns, from, to, yellow]</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>1.099492e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5528</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>In the third instar , purple or blue edging de...</td>\n",
       "      <td>113</td>\n",
       "      <td>119</td>\n",
       "      <td>yellow</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>yellow</td>\n",
       "      <td>[VB]</td>\n",
       "      <td>[v]</td>\n",
       "      <td>yellow</td>\n",
       "      <td>yellow</td>\n",
       "      <td>in the third instar , purple or blue edging de...</td>\n",
       "      <td>[black, to]</td>\n",
       "      <td>[black, to]</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>1.763678e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5529</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>The larva grows to about 120-130 mm , and pupa...</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>larva</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.15</td>\n",
       "      <td>larva</td>\n",
       "      <td>[NN]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>larva</td>\n",
       "      <td>larva</td>\n",
       "      <td>the larva grows to about 120-130 mm , and pupa...</td>\n",
       "      <td>[The, grows, to]</td>\n",
       "      <td>[the, grows, to]</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>5.227121e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5530</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>The larva grows to about 120-130 mm , and pupa...</td>\n",
       "      <td>42</td>\n",
       "      <td>49</td>\n",
       "      <td>pupates</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>pupates</td>\n",
       "      <td>[NNS]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>pupates</td>\n",
       "      <td>pupates</td>\n",
       "      <td>the larva grows to about 120-130 mm , and pupa...</td>\n",
       "      <td>[mm, and, in, an]</td>\n",
       "      <td>[mm, and, in, an]</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>2.642678e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5531</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>The larva grows to about 120-130 mm , and pupa...</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>grows</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>grows</td>\n",
       "      <td>[VBZ]</td>\n",
       "      <td>[v]</td>\n",
       "      <td>grow</td>\n",
       "      <td>grow</td>\n",
       "      <td>the larva grows to about 120-130 mm , and pupa...</td>\n",
       "      <td>[The, larva, to, about]</td>\n",
       "      <td>[the, larva, to, about]</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>3.957208e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5532</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>The larva grows to about 120-130 mm , and pupa...</td>\n",
       "      <td>33</td>\n",
       "      <td>35</td>\n",
       "      <td>mm</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>mm</td>\n",
       "      <td>[NN]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>mm</td>\n",
       "      <td>mm</td>\n",
       "      <td>the larva grows to about 120-130 mm , and pupa...</td>\n",
       "      <td>[to, about, and, pupates]</td>\n",
       "      <td>[to, about, and, pupates]</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>2.154837e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5533</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>The larva grows to about 120-130 mm , and pupa...</td>\n",
       "      <td>56</td>\n",
       "      <td>67</td>\n",
       "      <td>underground</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>underground</td>\n",
       "      <td>[JJ]</td>\n",
       "      <td>[a]</td>\n",
       "      <td>underground</td>\n",
       "      <td>underground</td>\n",
       "      <td>the larva grows to about 120-130 mm , and pupa...</td>\n",
       "      <td>[in, an, chamber]</td>\n",
       "      <td>[in, an, chamber]</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>1.861177e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5534</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>The larva grows to about 120-130 mm , and pupa...</td>\n",
       "      <td>56</td>\n",
       "      <td>75</td>\n",
       "      <td>underground chamber</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>underground chamber</td>\n",
       "      <td>[JJ, NN]</td>\n",
       "      <td>[a, n]</td>\n",
       "      <td>underground chamber</td>\n",
       "      <td>underground chamber</td>\n",
       "      <td>the larva grows to about 120-130 mm , and pupa...</td>\n",
       "      <td>[in, an]</td>\n",
       "      <td>[in, an]</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.787725e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5535</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>The larva grows to about 120-130 mm , and pupa...</td>\n",
       "      <td>68</td>\n",
       "      <td>75</td>\n",
       "      <td>chamber</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>chamber</td>\n",
       "      <td>[NN]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>chamber</td>\n",
       "      <td>chamber</td>\n",
       "      <td>the larva grows to about 120-130 mm , and pupa...</td>\n",
       "      <td>[an, underground]</td>\n",
       "      <td>[an, underground]</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>3.371978e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5536</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>46</td>\n",
       "      <td>57</td>\n",
       "      <td>terminology</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>terminology</td>\n",
       "      <td>[NN]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>terminology</td>\n",
       "      <td>terminology</td>\n",
       "      <td>an actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>[female, see, is, a]</td>\n",
       "      <td>[female, see, is, a]</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.572670e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5537</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>actor</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>actor</td>\n",
       "      <td>[NN]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>actor</td>\n",
       "      <td>actor</td>\n",
       "      <td>an actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>[An, sometimes, actress]</td>\n",
       "      <td>[an, sometimes, actress]</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.349400e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5538</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>21</td>\n",
       "      <td>28</td>\n",
       "      <td>actress</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>actress</td>\n",
       "      <td>[RB]</td>\n",
       "      <td>[r]</td>\n",
       "      <td>actress</td>\n",
       "      <td>actress</td>\n",
       "      <td>an actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>[actor, sometimes, for, female]</td>\n",
       "      <td>[actor, sometimes, for, female]</td>\n",
       "      <td>5.750000</td>\n",
       "      <td>3.760851e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5539</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>33</td>\n",
       "      <td>39</td>\n",
       "      <td>female</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>female</td>\n",
       "      <td>[NN]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>female</td>\n",
       "      <td>female</td>\n",
       "      <td>an actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>[actress, for, see, terminology]</td>\n",
       "      <td>[actress, for, see, terminology]</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.728399e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5540</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>86</td>\n",
       "      <td>94</td>\n",
       "      <td>dramatic</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.30</td>\n",
       "      <td>dramatic</td>\n",
       "      <td>[JJ]</td>\n",
       "      <td>[a]</td>\n",
       "      <td>dramatic</td>\n",
       "      <td>dramatic</td>\n",
       "      <td>an actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>[in, a, or, comic]</td>\n",
       "      <td>[in, a, or, comic]</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.336000e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5541</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>65</td>\n",
       "      <td>71</td>\n",
       "      <td>person</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>person</td>\n",
       "      <td>[NN]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>person</td>\n",
       "      <td>person</td>\n",
       "      <td>an actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>[is, a, who, acts]</td>\n",
       "      <td>[is, a, who, acts]</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>5.225406e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5542</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>76</td>\n",
       "      <td>80</td>\n",
       "      <td>acts</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>acts</td>\n",
       "      <td>[VBZ]</td>\n",
       "      <td>[v]</td>\n",
       "      <td>act</td>\n",
       "      <td>act</td>\n",
       "      <td>an actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>[person, who, in, a]</td>\n",
       "      <td>[person, who, in, a]</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.311120e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5543</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>98</td>\n",
       "      <td>103</td>\n",
       "      <td>comic</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>comic</td>\n",
       "      <td>[JJ]</td>\n",
       "      <td>[a]</td>\n",
       "      <td>comic</td>\n",
       "      <td>comic</td>\n",
       "      <td>an actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>[dramatic, or, production, and]</td>\n",
       "      <td>[dramatic, or, production, and]</td>\n",
       "      <td>5.750000</td>\n",
       "      <td>1.365776e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5544</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>104</td>\n",
       "      <td>114</td>\n",
       "      <td>production</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>production</td>\n",
       "      <td>[NN]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>production</td>\n",
       "      <td>production</td>\n",
       "      <td>an actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>[or, comic, and, who]</td>\n",
       "      <td>[or, comic, and, who]</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>1.434014e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5545</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>179</td>\n",
       "      <td>187</td>\n",
       "      <td>capacity</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.20</td>\n",
       "      <td>capacity</td>\n",
       "      <td>[NN]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>capacity</td>\n",
       "      <td>capacity</td>\n",
       "      <td>an actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>[in, that]</td>\n",
       "      <td>[in, that]</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.941626e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5546</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>123</td>\n",
       "      <td>128</td>\n",
       "      <td>works</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>works</td>\n",
       "      <td>[VBZ]</td>\n",
       "      <td>[v]</td>\n",
       "      <td>work</td>\n",
       "      <td>work</td>\n",
       "      <td>an actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>[and, who, in, film]</td>\n",
       "      <td>[and, who, in, film]</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.583376e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5547</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>132</td>\n",
       "      <td>136</td>\n",
       "      <td>film</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>film</td>\n",
       "      <td>[NN]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>film</td>\n",
       "      <td>film</td>\n",
       "      <td>an actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>[works, in, television, theater]</td>\n",
       "      <td>[works, in, television, theater]</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.253514e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5548</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>139</td>\n",
       "      <td>149</td>\n",
       "      <td>television</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>television</td>\n",
       "      <td>[NN]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>television</td>\n",
       "      <td>television</td>\n",
       "      <td>an actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>[in, film, theater, or]</td>\n",
       "      <td>[in, film, theater, or]</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>1.367774e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5549</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>152</td>\n",
       "      <td>159</td>\n",
       "      <td>theater</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>theater</td>\n",
       "      <td>[NN]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>theater</td>\n",
       "      <td>theater</td>\n",
       "      <td>an actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>[film, television, or, radio]</td>\n",
       "      <td>[film, television, or, radio]</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>1.608875e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5550</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>An actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>165</td>\n",
       "      <td>170</td>\n",
       "      <td>radio</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>radio</td>\n",
       "      <td>[NN]</td>\n",
       "      <td>[n]</td>\n",
       "      <td>radio</td>\n",
       "      <td>radio</td>\n",
       "      <td>an actor ( sometimes actress for female ; see ...</td>\n",
       "      <td>[theater, or, in, that]</td>\n",
       "      <td>[theater, or, in, that]</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>1.580310e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5551 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id  \\\n",
       "0     3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "1     3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "2     3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "3     3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "4     3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "5     3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "6     3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "7     3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "8     3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "9     3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "10    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "11    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "12    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "13    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "14    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "15    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "16    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "17    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "18    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "19    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "20    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "21    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "22    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "23    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "24    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "25    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "26    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "27    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "28    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "29    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "...                              ...   \n",
       "5521  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5522  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5523  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5524  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5525  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5526  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5527  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5528  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5529  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5530  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5531  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5532  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5533  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5534  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5535  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5536  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5537  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5538  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5539  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5540  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5541  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5542  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5543  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5544  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5545  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5546  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5547  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5548  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5549  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5550  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "\n",
       "                                               sentence  start  end  \\\n",
       "0     Normally , the land will be passed down to fut...      0    8   \n",
       "1     Normally , the land will be passed down to fut...     28   34   \n",
       "2     Normally , the land will be passed down to fut...     15   19   \n",
       "3     Normally , the land will be passed down to fut...     43   49   \n",
       "4     Normally , the land will be passed down to fut...     43   61   \n",
       "5     Normally , the land will be passed down to fut...     50   61   \n",
       "6     Normally , the land will be passed down to fut...     76   86   \n",
       "7     Normally , the land will be passed down to fut...     91  100   \n",
       "8     Normally , the land will be passed down to fut...    104  115   \n",
       "9     Normally , the land will be passed down to fut...    104  142   \n",
       "10    Normally , the land will be passed down to fut...    135  142   \n",
       "11    Normally , the land will be passed down to fut...    116  126   \n",
       "12    The passing of Aboriginal land rights legislat...     15   25   \n",
       "13    The passing of Aboriginal land rights legislat...      4   11   \n",
       "14    The passing of Aboriginal land rights legislat...     15   48   \n",
       "15    The passing of Aboriginal land rights legislat...     26   48   \n",
       "16    The passing of Aboriginal land rights legislat...     26   30   \n",
       "17    The passing of Aboriginal land rights legislat...     38   48   \n",
       "18    The passing of Aboriginal land rights legislat...     31   37   \n",
       "19    The passing of Aboriginal land rights legislat...     66   74   \n",
       "20    The passing of Aboriginal land rights legislat...     52   61   \n",
       "21    The passing of Aboriginal land rights legislat...    100  110   \n",
       "22    The passing of Aboriginal land rights legislat...     80   86   \n",
       "23    The passing of Aboriginal land rights legislat...     90   99   \n",
       "24    The passing of Aboriginal land rights legislat...    100  119   \n",
       "25    The passing of Aboriginal land rights legislat...    152  160   \n",
       "26    The passing of Aboriginal land rights legislat...    111  119   \n",
       "27    The passing of Aboriginal land rights legislat...    122  131   \n",
       "28    The passing of Aboriginal land rights legislat...    141  151   \n",
       "29    The passing of Aboriginal land rights legislat...    164  170   \n",
       "...                                                 ...    ...  ...   \n",
       "5521  In the third instar , purple or blue edging de...     44   52   \n",
       "5522  In the third instar , purple or blue edging de...     60   66   \n",
       "5523  In the third instar , purple or blue edging de...     83   92   \n",
       "5524  In the third instar , purple or blue edging de...     83   87   \n",
       "5525  In the third instar , purple or blue edging de...     88   92   \n",
       "5526  In the third instar , purple or blue edging de...     93   98   \n",
       "5527  In the third instar , purple or blue edging de...    104  109   \n",
       "5528  In the third instar , purple or blue edging de...    113  119   \n",
       "5529  The larva grows to about 120-130 mm , and pupa...      4    9   \n",
       "5530  The larva grows to about 120-130 mm , and pupa...     42   49   \n",
       "5531  The larva grows to about 120-130 mm , and pupa...     10   15   \n",
       "5532  The larva grows to about 120-130 mm , and pupa...     33   35   \n",
       "5533  The larva grows to about 120-130 mm , and pupa...     56   67   \n",
       "5534  The larva grows to about 120-130 mm , and pupa...     56   75   \n",
       "5535  The larva grows to about 120-130 mm , and pupa...     68   75   \n",
       "5536  An actor ( sometimes actress for female ; see ...     46   57   \n",
       "5537  An actor ( sometimes actress for female ; see ...      3    8   \n",
       "5538  An actor ( sometimes actress for female ; see ...     21   28   \n",
       "5539  An actor ( sometimes actress for female ; see ...     33   39   \n",
       "5540  An actor ( sometimes actress for female ; see ...     86   94   \n",
       "5541  An actor ( sometimes actress for female ; see ...     65   71   \n",
       "5542  An actor ( sometimes actress for female ; see ...     76   80   \n",
       "5543  An actor ( sometimes actress for female ; see ...     98  103   \n",
       "5544  An actor ( sometimes actress for female ; see ...    104  114   \n",
       "5545  An actor ( sometimes actress for female ; see ...    179  187   \n",
       "5546  An actor ( sometimes actress for female ; see ...    123  128   \n",
       "5547  An actor ( sometimes actress for female ; see ...    132  136   \n",
       "5548  An actor ( sometimes actress for female ; see ...    139  149   \n",
       "5549  An actor ( sometimes actress for female ; see ...    152  159   \n",
       "5550  An actor ( sometimes actress for female ; see ...    165  170   \n",
       "\n",
       "                                      target  nat  non_nat  nat_marked  \\\n",
       "0                                   Normally   10       10           0   \n",
       "1                                     passed   10       10           0   \n",
       "2                                       land   10       10           0   \n",
       "3                                     future   10       10           1   \n",
       "4                         future generations   10       10           1   \n",
       "5                                generations   10       10           3   \n",
       "6                                 recognizes   10       10           2   \n",
       "7                                  community   10       10           0   \n",
       "8                                traditional   10       10           1   \n",
       "9     traditional connection to that country   10       10           0   \n",
       "10                                   country   10       10           0   \n",
       "11                                connection   10       10           0   \n",
       "12                                Aboriginal   10       10           6   \n",
       "13                                   passing   10       10           0   \n",
       "14         Aboriginal land rights legislaton   10       10           0   \n",
       "15                    land rights legislaton   10       10           1   \n",
       "16                                      land   10       10           0   \n",
       "17                                legislaton   10       10          10   \n",
       "18                                    rights   10       10           0   \n",
       "19                                  preceded   10       10           8   \n",
       "20                                 Australia   10       10           0   \n",
       "21                                Aboriginal   10       10           1   \n",
       "22                                    number   10       10           0   \n",
       "23                                 important   10       10           0   \n",
       "24                       Aboriginal protests   10       10           0   \n",
       "25                                  Stockmen   10       10           0   \n",
       "26                                  protests   10       10           0   \n",
       "27                                 including   10       10           0   \n",
       "28                                Aboriginal   10       10           0   \n",
       "29                                    Strike   10       10           0   \n",
       "...                                      ...  ...      ...         ...   \n",
       "5521                                develops   10       10           0   \n",
       "5522                                  yellow   10       10           0   \n",
       "5523                               tail horn   10       10           0   \n",
       "5524                                    tail   10       10           0   \n",
       "5525                                    horn   10       10           0   \n",
       "5526                                   turns   10       10           0   \n",
       "5527                                   black   10       10           0   \n",
       "5528                                  yellow   10       10           0   \n",
       "5529                                   larva   10       10           3   \n",
       "5530                                 pupates   10       10           7   \n",
       "5531                                   grows   10       10           0   \n",
       "5532                                      mm   10       10           0   \n",
       "5533                             underground   10       10           0   \n",
       "5534                     underground chamber   10       10           0   \n",
       "5535                                 chamber   10       10           1   \n",
       "5536                             terminology   10       10           7   \n",
       "5537                                   actor   10       10           0   \n",
       "5538                                 actress   10       10           0   \n",
       "5539                                  female   10       10           0   \n",
       "5540                                dramatic   10       10           4   \n",
       "5541                                  person   10       10           0   \n",
       "5542                                    acts   10       10           0   \n",
       "5543                                   comic   10       10           2   \n",
       "5544                              production   10       10           0   \n",
       "5545                                capacity   10       10           4   \n",
       "5546                                   works   10       10           0   \n",
       "5547                                    film   10       10           0   \n",
       "5548                              television   10       10           0   \n",
       "5549                                 theater   10       10           0   \n",
       "5550                                   radio   10       10           0   \n",
       "\n",
       "      non_nat_marked  binary  prob                                p_target  \\\n",
       "0                  1       1  0.05                                normally   \n",
       "1                  1       1  0.05                                  passed   \n",
       "2                  0       0  0.00                                    land   \n",
       "3                  0       1  0.05                                  future   \n",
       "4                  2       1  0.15                      future generations   \n",
       "5                  2       1  0.25                             generations   \n",
       "6                  4       1  0.30                              recognizes   \n",
       "7                  0       0  0.00                               community   \n",
       "8                  3       1  0.20                             traditional   \n",
       "9                  0       0  0.00  traditional connection to that country   \n",
       "10                 1       1  0.05                                 country   \n",
       "11                 0       0  0.00                              connection   \n",
       "12                 5       1  0.55                              aboriginal   \n",
       "13                 0       0  0.00                                 passing   \n",
       "14                 1       1  0.05       aboriginal land rights legislaton   \n",
       "15                 0       1  0.05                  land rights legislaton   \n",
       "16                 0       0  0.00                                    land   \n",
       "17                 5       1  0.75                              legislaton   \n",
       "18                 0       0  0.00                                  rights   \n",
       "19                 6       1  0.70                                preceded   \n",
       "20                 0       0  0.00                               australia   \n",
       "21                 1       1  0.10                              aboriginal   \n",
       "22                 0       0  0.00                                  number   \n",
       "23                 0       0  0.00                               important   \n",
       "24                 1       1  0.05                     aboriginal protests   \n",
       "25                 0       0  0.00                                stockmen   \n",
       "26                 0       0  0.00                                protests   \n",
       "27                 0       0  0.00                               including   \n",
       "28                 0       0  0.00                              aboriginal   \n",
       "29                 1       1  0.05                                  strike   \n",
       "...              ...     ...   ...                                     ...   \n",
       "5521               0       0  0.00                                develops   \n",
       "5522               0       0  0.00                                  yellow   \n",
       "5523               2       1  0.10                               tail horn   \n",
       "5524               0       0  0.00                                    tail   \n",
       "5525               1       1  0.05                                    horn   \n",
       "5526               0       0  0.00                                   turns   \n",
       "5527               0       0  0.00                                   black   \n",
       "5528               0       0  0.00                                  yellow   \n",
       "5529               0       1  0.15                                   larva   \n",
       "5530               9       1  0.80                                 pupates   \n",
       "5531               0       0  0.00                                   grows   \n",
       "5532               0       0  0.00                                      mm   \n",
       "5533               1       1  0.05                             underground   \n",
       "5534               1       1  0.05                     underground chamber   \n",
       "5535               1       1  0.10                                 chamber   \n",
       "5536               3       1  0.50                             terminology   \n",
       "5537               0       0  0.00                                   actor   \n",
       "5538               0       0  0.00                                 actress   \n",
       "5539               0       0  0.00                                  female   \n",
       "5540               2       1  0.30                                dramatic   \n",
       "5541               0       0  0.00                                  person   \n",
       "5542               0       0  0.00                                    acts   \n",
       "5543               0       1  0.10                                   comic   \n",
       "5544               1       1  0.05                              production   \n",
       "5545               0       1  0.20                                capacity   \n",
       "5546               0       0  0.00                                   works   \n",
       "5547               0       0  0.00                                    film   \n",
       "5548               0       0  0.00                              television   \n",
       "5549               0       0  0.00                                 theater   \n",
       "5550               0       0  0.00                                   radio   \n",
       "\n",
       "                  pos_tags      pos_tags_pt  \\\n",
       "0                     [RB]              [r]   \n",
       "1                    [VBN]              [v]   \n",
       "2                     [NN]              [n]   \n",
       "3                     [JJ]              [a]   \n",
       "4                [JJ, NNS]           [a, n]   \n",
       "5                    [NNS]              [n]   \n",
       "6                    [VBZ]              [v]   \n",
       "7                     [NN]              [n]   \n",
       "8                     [JJ]              [a]   \n",
       "9     [JJ, NN, TO, DT, NN]  [a, n, n, n, n]   \n",
       "10                    [NN]              [n]   \n",
       "11                    [NN]              [n]   \n",
       "12                   [NNP]              [n]   \n",
       "13                    [NN]              [n]   \n",
       "14      [NNP, NN, NNS, NN]     [n, n, n, n]   \n",
       "15           [NN, NNS, NN]        [n, n, n]   \n",
       "16                    [NN]              [n]   \n",
       "17                    [NN]              [n]   \n",
       "18                   [NNS]              [n]   \n",
       "19                   [VBN]              [v]   \n",
       "20                   [NNP]              [n]   \n",
       "21                   [NNP]              [n]   \n",
       "22                    [NN]              [n]   \n",
       "23                    [JJ]              [a]   \n",
       "24              [NNP, NNS]           [n, n]   \n",
       "25                   [NNP]              [n]   \n",
       "26                   [NNS]              [n]   \n",
       "27                   [VBG]              [v]   \n",
       "28                   [NNP]              [n]   \n",
       "29                    [NN]              [n]   \n",
       "...                    ...              ...   \n",
       "5521                 [NNS]              [n]   \n",
       "5522                  [JJ]              [a]   \n",
       "5523              [NN, NN]           [n, n]   \n",
       "5524                  [NN]              [n]   \n",
       "5525                  [NN]              [n]   \n",
       "5526                 [VBZ]              [v]   \n",
       "5527                  [JJ]              [a]   \n",
       "5528                  [VB]              [v]   \n",
       "5529                  [NN]              [n]   \n",
       "5530                 [NNS]              [n]   \n",
       "5531                 [VBZ]              [v]   \n",
       "5532                  [NN]              [n]   \n",
       "5533                  [JJ]              [a]   \n",
       "5534              [JJ, NN]           [a, n]   \n",
       "5535                  [NN]              [n]   \n",
       "5536                  [NN]              [n]   \n",
       "5537                  [NN]              [n]   \n",
       "5538                  [RB]              [r]   \n",
       "5539                  [NN]              [n]   \n",
       "5540                  [JJ]              [a]   \n",
       "5541                  [NN]              [n]   \n",
       "5542                 [VBZ]              [v]   \n",
       "5543                  [JJ]              [a]   \n",
       "5544                  [NN]              [n]   \n",
       "5545                  [NN]              [n]   \n",
       "5546                 [VBZ]              [v]   \n",
       "5547                  [NN]              [n]   \n",
       "5548                  [NN]              [n]   \n",
       "5549                  [NN]              [n]   \n",
       "5550                  [NN]              [n]   \n",
       "\n",
       "                                       lemma  \\\n",
       "0                                   Normally   \n",
       "1                                       pass   \n",
       "2                                       land   \n",
       "3                                     future   \n",
       "4                          future generation   \n",
       "5                                 generation   \n",
       "6                                  recognize   \n",
       "7                                  community   \n",
       "8                                traditional   \n",
       "9     traditional connection to that country   \n",
       "10                                   country   \n",
       "11                                connection   \n",
       "12                                Aboriginal   \n",
       "13                                   passing   \n",
       "14          Aboriginal land right legislaton   \n",
       "15                     land right legislaton   \n",
       "16                                      land   \n",
       "17                                legislaton   \n",
       "18                                     right   \n",
       "19                                   precede   \n",
       "20                                 Australia   \n",
       "21                                Aboriginal   \n",
       "22                                    number   \n",
       "23                                 important   \n",
       "24                        Aboriginal protest   \n",
       "25                                  Stockmen   \n",
       "26                                   protest   \n",
       "27                                   include   \n",
       "28                                Aboriginal   \n",
       "29                                    Strike   \n",
       "...                                      ...   \n",
       "5521                                develops   \n",
       "5522                                  yellow   \n",
       "5523                               tail horn   \n",
       "5524                                    tail   \n",
       "5525                                    horn   \n",
       "5526                                    turn   \n",
       "5527                                   black   \n",
       "5528                                  yellow   \n",
       "5529                                   larva   \n",
       "5530                                 pupates   \n",
       "5531                                    grow   \n",
       "5532                                      mm   \n",
       "5533                             underground   \n",
       "5534                     underground chamber   \n",
       "5535                                 chamber   \n",
       "5536                             terminology   \n",
       "5537                                   actor   \n",
       "5538                                 actress   \n",
       "5539                                  female   \n",
       "5540                                dramatic   \n",
       "5541                                  person   \n",
       "5542                                     act   \n",
       "5543                                   comic   \n",
       "5544                              production   \n",
       "5545                                capacity   \n",
       "5546                                    work   \n",
       "5547                                    film   \n",
       "5548                              television   \n",
       "5549                                 theater   \n",
       "5550                                   radio   \n",
       "\n",
       "                                     p_lemma  \\\n",
       "0                                   normally   \n",
       "1                                       pass   \n",
       "2                                       land   \n",
       "3                                     future   \n",
       "4                          future generation   \n",
       "5                                 generation   \n",
       "6                                  recognize   \n",
       "7                                  community   \n",
       "8                                traditional   \n",
       "9     traditional connection to that country   \n",
       "10                                   country   \n",
       "11                                connection   \n",
       "12                                aboriginal   \n",
       "13                                   passing   \n",
       "14          aboriginal land right legislaton   \n",
       "15                     land right legislaton   \n",
       "16                                      land   \n",
       "17                                legislaton   \n",
       "18                                     right   \n",
       "19                                   precede   \n",
       "20                                 australia   \n",
       "21                                aboriginal   \n",
       "22                                    number   \n",
       "23                                 important   \n",
       "24                        aboriginal protest   \n",
       "25                                  stockmen   \n",
       "26                                   protest   \n",
       "27                                   include   \n",
       "28                                aboriginal   \n",
       "29                                    strike   \n",
       "...                                      ...   \n",
       "5521                                develops   \n",
       "5522                                  yellow   \n",
       "5523                               tail horn   \n",
       "5524                                    tail   \n",
       "5525                                    horn   \n",
       "5526                                    turn   \n",
       "5527                                   black   \n",
       "5528                                  yellow   \n",
       "5529                                   larva   \n",
       "5530                                 pupates   \n",
       "5531                                    grow   \n",
       "5532                                      mm   \n",
       "5533                             underground   \n",
       "5534                     underground chamber   \n",
       "5535                                 chamber   \n",
       "5536                             terminology   \n",
       "5537                                   actor   \n",
       "5538                                 actress   \n",
       "5539                                  female   \n",
       "5540                                dramatic   \n",
       "5541                                  person   \n",
       "5542                                     act   \n",
       "5543                                   comic   \n",
       "5544                              production   \n",
       "5545                                capacity   \n",
       "5546                                    work   \n",
       "5547                                    film   \n",
       "5548                              television   \n",
       "5549                                 theater   \n",
       "5550                                   radio   \n",
       "\n",
       "                                             p_sentence  \\\n",
       "0     normally , the land will be passed down to fut...   \n",
       "1     normally , the land will be passed down to fut...   \n",
       "2     normally , the land will be passed down to fut...   \n",
       "3     normally , the land will be passed down to fut...   \n",
       "4     normally , the land will be passed down to fut...   \n",
       "5     normally , the land will be passed down to fut...   \n",
       "6     normally , the land will be passed down to fut...   \n",
       "7     normally , the land will be passed down to fut...   \n",
       "8     normally , the land will be passed down to fut...   \n",
       "9     normally , the land will be passed down to fut...   \n",
       "10    normally , the land will be passed down to fut...   \n",
       "11    normally , the land will be passed down to fut...   \n",
       "12    the passing of aboriginal land rights legislat...   \n",
       "13    the passing of aboriginal land rights legislat...   \n",
       "14    the passing of aboriginal land rights legislat...   \n",
       "15    the passing of aboriginal land rights legislat...   \n",
       "16    the passing of aboriginal land rights legislat...   \n",
       "17    the passing of aboriginal land rights legislat...   \n",
       "18    the passing of aboriginal land rights legislat...   \n",
       "19    the passing of aboriginal land rights legislat...   \n",
       "20    the passing of aboriginal land rights legislat...   \n",
       "21    the passing of aboriginal land rights legislat...   \n",
       "22    the passing of aboriginal land rights legislat...   \n",
       "23    the passing of aboriginal land rights legislat...   \n",
       "24    the passing of aboriginal land rights legislat...   \n",
       "25    the passing of aboriginal land rights legislat...   \n",
       "26    the passing of aboriginal land rights legislat...   \n",
       "27    the passing of aboriginal land rights legislat...   \n",
       "28    the passing of aboriginal land rights legislat...   \n",
       "29    the passing of aboriginal land rights legislat...   \n",
       "...                                                 ...   \n",
       "5521  in the third instar , purple or blue edging de...   \n",
       "5522  in the third instar , purple or blue edging de...   \n",
       "5523  in the third instar , purple or blue edging de...   \n",
       "5524  in the third instar , purple or blue edging de...   \n",
       "5525  in the third instar , purple or blue edging de...   \n",
       "5526  in the third instar , purple or blue edging de...   \n",
       "5527  in the third instar , purple or blue edging de...   \n",
       "5528  in the third instar , purple or blue edging de...   \n",
       "5529  the larva grows to about 120-130 mm , and pupa...   \n",
       "5530  the larva grows to about 120-130 mm , and pupa...   \n",
       "5531  the larva grows to about 120-130 mm , and pupa...   \n",
       "5532  the larva grows to about 120-130 mm , and pupa...   \n",
       "5533  the larva grows to about 120-130 mm , and pupa...   \n",
       "5534  the larva grows to about 120-130 mm , and pupa...   \n",
       "5535  the larva grows to about 120-130 mm , and pupa...   \n",
       "5536  an actor ( sometimes actress for female ; see ...   \n",
       "5537  an actor ( sometimes actress for female ; see ...   \n",
       "5538  an actor ( sometimes actress for female ; see ...   \n",
       "5539  an actor ( sometimes actress for female ; see ...   \n",
       "5540  an actor ( sometimes actress for female ; see ...   \n",
       "5541  an actor ( sometimes actress for female ; see ...   \n",
       "5542  an actor ( sometimes actress for female ; see ...   \n",
       "5543  an actor ( sometimes actress for female ; see ...   \n",
       "5544  an actor ( sometimes actress for female ; see ...   \n",
       "5545  an actor ( sometimes actress for female ; see ...   \n",
       "5546  an actor ( sometimes actress for female ; see ...   \n",
       "5547  an actor ( sometimes actress for female ; see ...   \n",
       "5548  an actor ( sometimes actress for female ; see ...   \n",
       "5549  an actor ( sometimes actress for female ; see ...   \n",
       "5550  an actor ( sometimes actress for female ; see ...   \n",
       "\n",
       "                                         context  \\\n",
       "0                                    [the, land]   \n",
       "1                           [will, be, down, to]   \n",
       "2                      [Normally, the, will, be]   \n",
       "3                    [down, to, generations, in]   \n",
       "4                              [down, to, in, a]   \n",
       "5                            [to, future, in, a]   \n",
       "6                    [way, that, the, community]   \n",
       "7     [recognizes, the, traditional, connection]   \n",
       "8               [the, community, connection, to]   \n",
       "9                               [the, community]   \n",
       "10                                    [to, that]   \n",
       "11            [community, traditional, to, that]   \n",
       "12                   [passing, of, land, rights]   \n",
       "13                         [The, of, Aboriginal]   \n",
       "14                  [passing, of, in, Australia]   \n",
       "15               [of, Aboriginal, in, Australia]   \n",
       "16          [of, Aboriginal, rights, legislaton]   \n",
       "17                 [land, rights, in, Australia]   \n",
       "18            [Aboriginal, land, legislaton, in]   \n",
       "19                       [Australia, was, by, a]   \n",
       "20               [legislaton, in, was, preceded]   \n",
       "21          [of, important, protests, including]   \n",
       "22                        [by, a, of, important]   \n",
       "23            [number, of, Aboriginal, protests]   \n",
       "24               [of, important, including, the]   \n",
       "25               [1946, Aboriginal, Strike, the]   \n",
       "26       [important, Aboriginal, including, the]   \n",
       "27             [Aboriginal, protests, the, 1946]   \n",
       "28                 [the, 1946, Stockmen, Strike]   \n",
       "29             [Aboriginal, Stockmen, the, 1963]   \n",
       "...                                          ...   \n",
       "5521                     [blue, edging, on, the]   \n",
       "5522                     [on, the, stripes, and]   \n",
       "5523                     [and, the, turns, from]   \n",
       "5524                     [and, the, horn, turns]   \n",
       "5525                    [the, tail, turns, from]   \n",
       "5526                   [tail, horn, from, black]   \n",
       "5527                   [turns, from, to, yellow]   \n",
       "5528                                 [black, to]   \n",
       "5529                            [The, grows, to]   \n",
       "5530                           [mm, and, in, an]   \n",
       "5531                     [The, larva, to, about]   \n",
       "5532                   [to, about, and, pupates]   \n",
       "5533                           [in, an, chamber]   \n",
       "5534                                    [in, an]   \n",
       "5535                           [an, underground]   \n",
       "5536                        [female, see, is, a]   \n",
       "5537                    [An, sometimes, actress]   \n",
       "5538             [actor, sometimes, for, female]   \n",
       "5539            [actress, for, see, terminology]   \n",
       "5540                          [in, a, or, comic]   \n",
       "5541                          [is, a, who, acts]   \n",
       "5542                        [person, who, in, a]   \n",
       "5543             [dramatic, or, production, and]   \n",
       "5544                       [or, comic, and, who]   \n",
       "5545                                  [in, that]   \n",
       "5546                        [and, who, in, film]   \n",
       "5547            [works, in, television, theater]   \n",
       "5548                     [in, film, theater, or]   \n",
       "5549               [film, television, or, radio]   \n",
       "5550                     [theater, or, in, that]   \n",
       "\n",
       "                                       p_context  ctx_length  ctx_freq_wiki  \n",
       "0                                    [the, land]    3.500000   6.121852e+07  \n",
       "1                           [will, be, down, to]    3.000000   1.019463e+07  \n",
       "2                      [normally, the, will, be]    4.250000   3.185175e+07  \n",
       "3                    [down, to, generations, in]    4.750000   2.112122e+07  \n",
       "4                              [down, to, in, a]    2.250000   2.111276e+07  \n",
       "5                            [to, future, in, a]    2.750000   2.104169e+07  \n",
       "6                    [way, that, the, community]    4.750000   3.318412e+07  \n",
       "7     [recognizes, the, traditional, connection]    8.500000   3.056799e+07  \n",
       "8               [the, community, connection, to]    6.000000   3.935440e+07  \n",
       "9                               [the, community]    6.000000   6.124369e+07  \n",
       "10                                    [to, that]    3.000000   2.229473e+07  \n",
       "11            [community, traditional, to, that]    6.500000   1.134338e+07  \n",
       "12                   [passing, of, land, rights]    4.750000   1.472406e+07  \n",
       "13                         [the, of, aboriginal]    5.000000   5.999576e+07  \n",
       "14                  [passing, of, in, australia]    5.000000   2.689054e+07  \n",
       "15               [of, aboriginal, in, australia]    5.750000   2.687577e+07  \n",
       "16          [of, aboriginal, rights, legislaton]    7.000000   1.458396e+07  \n",
       "17                 [land, rights, in, australia]    5.250000   1.255927e+07  \n",
       "18            [aboriginal, land, legislaton, in]    6.500000   1.240277e+07  \n",
       "19                       [australia, was, by, a]    3.750000   8.299596e+06  \n",
       "20               [legislaton, in, was, preceded]    5.750000   1.747121e+07  \n",
       "21          [of, important, protests, including]    7.000000   1.492840e+07  \n",
       "22                        [by, a, of, important]    3.500000   1.761584e+07  \n",
       "23            [number, of, aboriginal, protests]    6.500000   1.477941e+07  \n",
       "24               [of, important, including, the]    5.750000   4.540267e+07  \n",
       "25               [1946, aboriginal, strike, the]    5.750000   3.050896e+07  \n",
       "26       [important, aboriginal, including, the]    7.750000   3.090293e+07  \n",
       "27             [aboriginal, protests, the, 1946]    6.250000   3.050016e+07  \n",
       "28                 [the, 1946, stockmen, strike]    5.250000   3.050245e+07  \n",
       "29             [aboriginal, stockmen, the, 1963]    6.250000   3.049058e+07  \n",
       "...                                          ...         ...            ...  \n",
       "5521                     [blue, edging, on, the]    3.750000   3.395373e+07  \n",
       "5522                     [on, the, stripes, and]    3.750000   4.636381e+07  \n",
       "5523                     [and, the, turns, from]    3.750000   4.520576e+07  \n",
       "5524                     [and, the, horn, turns]    3.750000   4.297020e+07  \n",
       "5525                    [the, tail, turns, from]    4.000000   3.275636e+07  \n",
       "5526                   [tail, horn, from, black]    4.250000   2.370318e+06  \n",
       "5527                   [turns, from, to, yellow]    4.250000   1.099492e+07  \n",
       "5528                                 [black, to]    3.500000   1.763678e+07  \n",
       "5529                            [the, grows, to]    3.333333   5.227121e+07  \n",
       "5530                           [mm, and, in, an]    2.250000   2.642678e+07  \n",
       "5531                     [the, larva, to, about]    3.750000   3.957208e+07  \n",
       "5532                   [to, about, and, pupates]    4.250000   2.154837e+07  \n",
       "5533                           [in, an, chamber]    3.666667   1.861177e+07  \n",
       "5534                                    [in, an]    2.000000   2.787725e+07  \n",
       "5535                           [an, underground]    6.500000   3.371978e+06  \n",
       "5536                        [female, see, is, a]    3.000000   4.572670e+06  \n",
       "5537                    [an, sometimes, actress]    6.000000   2.349400e+06  \n",
       "5538             [actor, sometimes, for, female]    5.750000   3.760851e+06  \n",
       "5539            [actress, for, see, terminology]    6.000000   3.728399e+06  \n",
       "5540                          [in, a, or, comic]    2.500000   1.336000e+07  \n",
       "5541                          [is, a, who, acts]    2.500000   5.225406e+06  \n",
       "5542                        [person, who, in, a]    3.000000   1.311120e+07  \n",
       "5543             [dramatic, or, production, and]    5.750000   1.365776e+07  \n",
       "5544                       [or, comic, and, who]    3.250000   1.434014e+07  \n",
       "5545                                  [in, that]    3.000000   2.941626e+07  \n",
       "5546                        [and, who, in, film]    3.000000   2.583376e+07  \n",
       "5547            [works, in, television, theater]    6.000000   1.253514e+07  \n",
       "5548                     [in, film, theater, or]    3.750000   1.367774e+07  \n",
       "5549               [film, television, or, radio]    5.250000   1.608875e+06  \n",
       "5550                     [theater, or, in, that]    3.750000   1.580310e+07  \n",
       "\n",
       "[5551 rows x 21 columns]"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx_datasets_fc_context_complexity[0].train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from joblib import Memory\n",
    "memory = Memory(location='resources/dependency-cache', verbose=0)\n",
    "@memory.cache\n",
    "def dependency_parse_with_root(sentence):\n",
    "    try:\n",
    "        dependency_parser = dependencyParser.raw_parse(sentence)\n",
    "        dependencies = []\n",
    "        parsetree = list(dependency_parser)[0]\n",
    "        for index, node in parsetree.nodes.items():\n",
    "            for relation, dependant in parsetree.nodes[index]['deps'].items():\n",
    "                for dep in dependant:\n",
    "                    triple = ((node['word'], index), relation, \\\n",
    "                              (parsetree.nodes[dep]['word'], dep))\n",
    "                    dependencies.append(triple)\n",
    "        return dependencies\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def dependency_parse(sentence):\n",
    "    dependencies = dependency_parse_with_root(sentence)\n",
    "    filtered_dependencies = [triple for triple in dependencies if triple[1] != 'root']\n",
    "    return filtered_dependencies\n",
    "\n",
    "\n",
    "def dep_dist_to_head(target, start, end, context):\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    return np.nan_to_num(np.mean([np.abs(triple[0][1] - triple[2][1])-1 \n",
    "                                for triple in triples if triple[2] in targets]))\n",
    "\n",
    "def dep_dist_to_root(target, start, end, context):\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse_with_root(context)\n",
    "    root_nodes = list(filter(lambda triple : triple[1] == 'root' , triples))\n",
    "    if root_nodes: \n",
    "        root_node = root_nodes[0]\n",
    "    else:\n",
    "        return 0\n",
    "    dist = np.nan_to_num(np.mean([np.abs(root_node[2][1] - triple[2][1])-1 \n",
    "                                for triple in triples if triple[2] in targets]))\n",
    "    return dist if dist != -1 else 0\n",
    "\n",
    "def dep_relation_to_head(target, start, end, context):\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse_with_root(context)\n",
    "    relations = [triple[1] for triple in triples if triple[2] in targets]\n",
    "    return relations[0] if len(relations) == 1 else 'misc'\n",
    "    \n",
    "\n",
    "def dep_head_word_len(target, start, end, context):\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    return np.nan_to_num(np.mean([len(triple[0][0]) \n",
    "        for triple in triples if triple[2] in targets]))\n",
    "\n",
    "def dep_num_dependents(target, start, end, context):\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse_with_root(context)\n",
    "    return len([triple[1] for triple in triples if triple[0] in targets])\n",
    "\n",
    "def dep_max_num_dependents(context):\n",
    "    triples = dependency_parse_with_root(context)\n",
    "    most = Counter([triple[0][0] for triple in triples]).most_common(1)\n",
    "    return most[0][1] if most else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " df['dep_dist_to_head'] = df[['target', 'start', 'end', 'sentence']].apply(lambda vals : \n",
    "                                                                            dep_dist_to_head(*vals), axis=1)\n",
    "    df['dep_dist_to_root'] = df[['target', 'start', 'end', 'sentence']].apply(lambda vals : \n",
    "                                                                            dep_dist_to_root(*vals), axis=1)\n",
    "    df['dep_dist_to_root_norm'] = df[['dep_dist_to_root', 'sentence']].apply(lambda vals : \\\n",
    "                                                        float(vals[0]) / (len(word_tokenize(vals[1]))-1), axis=1)\n",
    "    df['dep_relation_to_head'] = df[['target', 'start', 'end', 'sentence']].apply(lambda vals : \\\n",
    "                                                                dep_relation_to_head(*vals), axis = 1)\n",
    "    df['dep_num_dependents'] = df[['target', 'start', 'end', 'sentence']].apply(lambda vals : \\\n",
    "                                                                        dep_num_dependents(*vals), axis = 1)\n",
    "    df['dep_max_num_dependents'] = df.sentence.apply(lambda sentence : dep_max_num_dependents(sentence))\n",
    "    df['dep_num_dependents_norm'] = df.dep_num_dependents / df.dep_max_num_dependents\n",
    "    df['dep_head_word_len'] = df[['target', 'start', 'end', 'sentence']].apply(lambda vals : \\\n",
    "                                                                        dep_head_word_len(*vals), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Readability Measures\n",
    "Here we implement some of the most popular and well-known historical readability measures. Most of them need multiple sentences to compute them properly, however, we will apply them on the extracted context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>rb_dalechall_score</th>\n",
       "      <th>rb_flesch_score</th>\n",
       "      <th>rb_fleschkincaid_score</th>\n",
       "      <th>rb_gunningfog_score</th>\n",
       "      <th>rb_polysyblword_count</th>\n",
       "      <th>rb_smog_score</th>\n",
       "      <th>rb_sybl_count</th>\n",
       "      <th>rb_sybl_count_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>passed</td>\n",
       "      <td>[land, future, generations, recognizes]</td>\n",
       "      <td>15.677400</td>\n",
       "      <td>-8.725</td>\n",
       "      <td>15.470000</td>\n",
       "      <td>21.600000</td>\n",
       "      <td>2</td>\n",
       "      <td>11.208143</td>\n",
       "      <td>10</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>land</td>\n",
       "      <td>[passed, future, generations]</td>\n",
       "      <td>14.311967</td>\n",
       "      <td>6.390</td>\n",
       "      <td>13.113333</td>\n",
       "      <td>14.533333</td>\n",
       "      <td>1</td>\n",
       "      <td>8.841846</td>\n",
       "      <td>7</td>\n",
       "      <td>2.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>future</td>\n",
       "      <td>[land, passed, generations, recognizes, commun...</td>\n",
       "      <td>13.358500</td>\n",
       "      <td>-1.280</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>13.023867</td>\n",
       "      <td>12</td>\n",
       "      <td>2.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>future generations</td>\n",
       "      <td>[land, passed, recognizes, community, traditio...</td>\n",
       "      <td>13.358500</td>\n",
       "      <td>15.640</td>\n",
       "      <td>12.320000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>13.023867</td>\n",
       "      <td>11</td>\n",
       "      <td>2.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>generations</td>\n",
       "      <td>[land, passed, future, recognizes, community, ...</td>\n",
       "      <td>14.460767</td>\n",
       "      <td>17.445</td>\n",
       "      <td>12.316667</td>\n",
       "      <td>22.400000</td>\n",
       "      <td>3</td>\n",
       "      <td>13.023867</td>\n",
       "      <td>13</td>\n",
       "      <td>2.166667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               target                                            context  \\\n",
       "0              passed            [land, future, generations, recognizes]   \n",
       "1                land                      [passed, future, generations]   \n",
       "2              future  [land, passed, generations, recognizes, commun...   \n",
       "3  future generations  [land, passed, recognizes, community, traditio...   \n",
       "4         generations  [land, passed, future, recognizes, community, ...   \n",
       "\n",
       "   rb_dalechall_score  rb_flesch_score  rb_fleschkincaid_score  \\\n",
       "0           15.677400           -8.725               15.470000   \n",
       "1           14.311967            6.390               13.113333   \n",
       "2           13.358500           -1.280               14.680000   \n",
       "3           13.358500           15.640               12.320000   \n",
       "4           14.460767           17.445               12.316667   \n",
       "\n",
       "   rb_gunningfog_score  rb_polysyblword_count  rb_smog_score  rb_sybl_count  \\\n",
       "0            21.600000                      2      11.208143             10   \n",
       "1            14.533333                      1       8.841846              7   \n",
       "2            26.000000                      3      13.023867             12   \n",
       "3            26.000000                      3      13.023867             11   \n",
       "4            22.400000                      3      13.023867             13   \n",
       "\n",
       "   rb_sybl_count_ratio  \n",
       "0             2.500000  \n",
       "1             2.333333  \n",
       "2             2.400000  \n",
       "3             2.200000  \n",
       "4             2.166667  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textatistic import Textatistic\n",
    "\n",
    "df_context['rb_dalechall_score'] = df_context.context.apply(lambda context : \\\n",
    "                                                Textatistic(' '.join(context) + '.').dalechall_score)\n",
    "df_context['rb_flesch_score'] = df_context.context.apply(lambda context : \\\n",
    "                                                Textatistic(' '.join(context) + '.').flesch_score)\n",
    "df_context['rb_fleschkincaid_score'] = df_context.context.apply(lambda context : \\\n",
    "                                                Textatistic(' '.join(context) + '.').fleschkincaid_score)\n",
    "df_context['rb_gunningfog_score'] = df_context.context.apply(lambda context : \\\n",
    "                                                Textatistic(' '.join(context) + '.').gunningfog_score)\n",
    "df_context['rb_polysyblword_count'] = df_context.context.apply(lambda context : \\\n",
    "                                                Textatistic(' '.join(context) + '.').polysyblword_count)\n",
    "df_context['rb_smog_score'] = df_context.context.apply(lambda context : \\\n",
    "                                                Textatistic(' '.join(context) + '.').smog_score)\n",
    "df_context['rb_sybl_count'] = df_context.context.apply(lambda context : \\\n",
    "                                                Textatistic(' '.join(context) + '.').sybl_count)\n",
    "df_context['rb_sybl_count_ratio'] = df_context.context.apply(lambda context : \\\n",
    "                                                Textatistic(' '.join(context) + '.').sybl_count / len(context))\n",
    "\n",
    "df_context[['target', 'context', 'rb_dalechall_score', 'rb_flesch_score', 'rb_fleschkincaid_score', \\\n",
    "            'rb_gunningfog_score', 'rb_polysyblword_count', 'rb_smog_score', 'rb_sybl_count', 'rb_sybl_count_ratio']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Linguistic Features\n",
    "Here we implement some of the most popular and well-known historical readability measures. Most of them need multiple sentences to compute them properly, however, we will apply them on the extracted context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_context['context'] = df_context['ctx_extraction_window_pre_suc_n']\n",
    "\n",
    "df_context['ctx_num_tokens'] = df_context.context.apply(lambda context : len(context))\n",
    "df_context['ctx_avg_length'] = df_context.context.apply(lambda context : agg_ctx_feat_num_average(context, len))\n",
    "df_context['ctx_avg_word_freq_wiki'] = df_context.context.apply(lambda context : \\\n",
    "                                                    agg_ctx_feat_num_average(context, get_dict_count, word_freq_wiki))\n",
    "df_context.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Result = namedtuple('Result', 'dataset, fc, agg, measure')\n",
    "Dataset = namedtuple('Dataset', 'name, train, test')\n",
    "FeatureDataset = namedtuple('FeatureDataset', 'name, fc, agg, train, test')\n",
    "FeatureCategory = namedtuple('FeatureCategory', 'name, func')\n",
    "Feature = namedtuple('Feature', 'name, fc_name, train, test')\n",
    "Metric = namedtuple('Metric', 'name, func')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_labels_for_binary_df(dataframe):\n",
    "    df = dataframe.copy()\n",
    "    df = df.drop(['id', 'sentence', 'target', 'nat', 'non_nat', \n",
    "                  'nat_marked', 'non_nat_marked', 'prob', 'start', \n",
    "                  'end', 'p_target', 'lemma', 'p_lemma', 'pos_tags', 'pos_tags_pt'], axis = 1)\n",
    "    return df\n",
    "\n",
    "def remove_labels_for_regr_df(dataframe):\n",
    "    df = dataframe.copy()\n",
    "    df = df.drop(['id', 'sentence', 'target', 'nat', 'non_nat', \n",
    "                  'nat_marked', 'non_nat_marked', 'binary', 'start', \n",
    "                  'end', 'p_target', 'lemma', 'p_lemma', 'pos_tags', 'pos_tags_pt'], axis = 1)\n",
    "    return df\n",
    "    \n",
    "def transform_feat_to_num(train, test):\n",
    "    train_copy = train.copy()\n",
    "    test_copy = test.copy()\n",
    "    train_copy = train_copy.replace(np.inf, 0)\n",
    "    train_copy = train_copy.replace(np.nan, 0)\n",
    "    test_copy = test_copy.replace(np.inf, 0)\n",
    "    test_copy = test_copy.replace(np.nan, 0)\n",
    "    shape_train = train.shape\n",
    "    shape_test = test.shape\n",
    "    df = train_copy.append(test_copy, ignore_index=True)\n",
    "    df = pd.get_dummies(df)\n",
    "    return (df.loc[0:(shape_train[0]-1),], \n",
    "            df.loc[shape_train[0]:df.shape[0],])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn import model_selection\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "def get_majority_class_prediction(train, test):\n",
    "    dummy = DummyClassifier(strategy='most_frequent', \n",
    "                            random_state=None, constant=None)\n",
    "    x_train = train.loc[:, train.columns != 'binary']\n",
    "    y_train = train.binary.values\n",
    "    x_test = test\n",
    "    y_test = test.binary.values\n",
    "    dummy.fit(x_train, y_train)\n",
    "    prediction = dummy.predict(x_test)\n",
    "    f1score = f1_score(y_test, prediction)\n",
    "    return f1score\n",
    "\n",
    "def always_complex_prediction(train, test):\n",
    "    y_test = test.binary.values\n",
    "    prediction = [1 for val in y_test]\n",
    "    f1score = precision_recall_fscore_support(y_test, prediction)\n",
    "    return f1score\n",
    "\n",
    "def svm(train, test):\n",
    "    print('average_classification')\n",
    "    x_train = train.loc[:, train.columns != 'binary']\n",
    "    y_train = train.binary.values\n",
    "    x_test = test.loc[:, test.columns != 'binary']\n",
    "    y_test = test.binary.values\n",
    "    seed = 7\n",
    "    #knn = MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto',\n",
    "     #  beta_1=0.9, beta_2=0.999, early_stopping=False,\n",
    "    #   epsilon=1e-08, hidden_layer_sizes=(5, 100), learning_rate='constant',\n",
    "    #   learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
    "    #   nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
    "    #   solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
    "    #   warm_start=False)\n",
    "    knn = svm.SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "        decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
    "        max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "        tol=0.001, verbose=False)\n",
    "    knn.fit(x_train, y_train) \n",
    "    prediction = knn.predict(x_test)\n",
    "    f1score = f1_score(y_test, prediction)\n",
    "    #kfold = model_selection.KFold(n_splits=2, random_state=seed)\n",
    "    #cv_results = model_selection.cross_val_score(knn, x_train, y_train, cv=kfold, scoring=make_scorer(f1_score))\n",
    "    return f1score\n",
    "\n",
    "def xgboost(train, test):\n",
    "    x_train = train.loc[:, train.columns != 'binary']\n",
    "    y_train = train.binary.values\n",
    "    x_test = test.loc[:, test.columns != 'binary']\n",
    "    y_test = test.binary.values\n",
    "    xgtrain = xgb.DMatrix(x_train.values, label=y_train)\n",
    "    xgtest = xgb.DMatrix(x_test.values, label=y_test)\n",
    "    xg_test_x = xgb.DMatrix(x_test.values)\n",
    "    param = {'max_depth': 30, 'eta': 1, 'silent': 1, 'objective': 'binary:logistic',  'n_estimators':5000}\n",
    "    evallist = [(xgtest, 'eval'), (xgtrain, 'train')]\n",
    "    num_round = 70\n",
    "    bst = xgb.train(param, xgtrain, num_round, evallist)\n",
    "    prediction = bst.predict(xg_test_x)\n",
    "    prediction_binary = list(map(lambda val: 1 if val>0.5 else 0, prediction))\n",
    "    f1score = precision_recall_fscore_support(y_test, prediction_binary)\n",
    "    return f1score\n",
    "\n",
    "def xgboost_with_bst(train, test):\n",
    "    x_train = train.loc[:, train.columns != 'binary']\n",
    "    y_train = train.binary.values\n",
    "    x_test = test.loc[:, test.columns != 'binary']\n",
    "    y_test = test.binary.values\n",
    "    xgtrain = xgb.DMatrix(x_train.values, label=y_train, feature_names=x_train.columns.values)\n",
    "    xgtest = xgb.DMatrix(x_test.values, label=y_test, feature_names=x_test.columns.values)\n",
    "    xg_test_x = xgb.DMatrix(x_test.values, feature_names=x_test.columns.values)\n",
    "    param = {'max_depth': 30, 'eta': 1, 'silent': 1, 'objective': 'binary:logistic',  'n_estimators':5000}\n",
    "    evallist = [(xgtest, 'eval'), (xgtrain, 'train')]\n",
    "    num_round = 70\n",
    "    bst = xgb.train(param, xgtrain, num_round, evallist)\n",
    "    prediction = bst.predict(xg_test_x)\n",
    "    prediction_binary = list(map(lambda val: 1 if val>0.5 else 0, prediction))\n",
    "    f1score = precision_recall_fscore_support(y_test, prediction_binary)\n",
    "    return f1score, bst\n",
    "\n",
    "def adaboost(train, test):\n",
    "    x_train = train.loc[:, train.columns != 'binary']\n",
    "    y_train = train.binary.values\n",
    "    x_test = test.loc[:, test.columns != 'binary']\n",
    "    y_test = test.binary.values\n",
    "    adab = AdaBoostClassifier(base_estimator=None, n_estimators=5000, \n",
    "                          learning_rate=1.0, algorithm='SAMME.R',\n",
    "                          random_state=None)\n",
    "    adab.fit(x_train, y_train) \n",
    "    prediction = adab.predict(x_test)\n",
    "    f1score = precision_recall_fscore_support(y_test, prediction)\n",
    "    return f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = [Result(fs, fs.fc, fs.agg,\n",
    "    xgboost(*transform_feat_to_num(remove_labels_for_binary_df(fs.train), \n",
    "        remove_labels_for_binary_df(fs.test)))) for fs in all_fc_datasets]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
