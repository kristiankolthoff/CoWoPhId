{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context-Aware Complex Word Identification\n",
    "Here we devise and implement all the relevant methods for evaluating the influence of context words for the complexity of a given target word. Thus, we implement various context definition methods that extract context words for a target based on different ideas (e.g. local context, grammatical context and semantic context). Afterwards we compute features for the context and use these features to represent the context in the classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.wsd import lesk\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from collections import defaultdict\n",
    "\n",
    "Dataset = namedtuple('Dataset', 'name, train, test')\n",
    "FeatureDataset = namedtuple('FeatureDataset', 'name, fc, agg, train, test')\n",
    "FeatureCategory = namedtuple('FeatureCategory', 'name, func')\n",
    "Aggregation = namedtuple('Aggregation', 'name, agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "MAIN_PATH_DATASET = \"../cwishareddataset/traindevset/english/\"\n",
    "genres = ['Wikipedia', 'WikiNews', 'News']\n",
    "datasets = ['Train', 'Dev']\n",
    "columns = ['id', 'sentence', \"start\", \"end\", \"target\", \n",
    "           \"nat\", \"non_nat\", \"nat_marked\", \"non_nat_marked\", \"binary\", \"prob\"]\n",
    "\n",
    "\n",
    "datasets = [Dataset('Wikipedia', 'Train', 'Dev'),\n",
    "            Dataset('WikiNews', 'Train', 'Dev'),\n",
    "            Dataset('News', 'Train', 'Dev')]\n",
    "\n",
    "feature_categories = []\n",
    "\n",
    "def load_df(path):\n",
    "    df = pd.read_csv(path, header=None, sep = \"\\t\")\n",
    "    df.columns = columns\n",
    "    return df\n",
    "\n",
    "datasets = [Dataset(d.name, load_df(MAIN_PATH_DATASET + d.name + '_' + d.train + '.tsv'),\n",
    "                            load_df(MAIN_PATH_DATASET + d.name + '_' + d.test + '.tsv'))\n",
    "                            for d in datasets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import *\n",
    "from nltk import word_tokenize\n",
    "from functools import lru_cache\n",
    "from utils import penn_to_wn\n",
    "import re\n",
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "wordNetLemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def overlaps(start1, end1, start2, end2):\n",
    "    return bool(range(max(start1, start2), min(end1, end2)+1))\n",
    "\n",
    "tbl = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "                      if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(tbl)\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def targets_with_index(start, end, context):\n",
    "    curr_pos = 0\n",
    "    targets = []\n",
    "    j = 0\n",
    "    w = 0\n",
    "    curr_split = ''\n",
    "    ctx_split = context.split()\n",
    "    whitespaces = re.findall('\\s+', context)\n",
    "    num_whitespaces = [len(token) for token in whitespaces]\n",
    "    num_whitespaces.append(1)\n",
    "    tokens = word_tokenize(context)\n",
    "    tokens = ['\"' if token not in context else token for token in tokens]\n",
    "    for index, token in enumerate(tokens, 1):\n",
    "        targets.append((token, index, curr_pos, (curr_pos + len(token))))\n",
    "        curr_pos += len(token)\n",
    "        curr_split += token\n",
    "        if ctx_split[j] == curr_split:\n",
    "            curr_pos += num_whitespaces[w]\n",
    "            j += 1\n",
    "            w += 1\n",
    "            curr_split = ''\n",
    "    vals = [(target[0], target[1]) for target in targets \\\n",
    "            if overlaps(start, end, target[2], target[3])]\n",
    "    return [val for val in vals if val[0] != '\"']\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def wordnet_pos_tagging(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    return nltk.pos_tag(tokens)\n",
    "\n",
    "def pos_tags(start, end, sentence):\n",
    "    wordPOSPairs = wordnet_pos_tagging(sentence)\n",
    "    targets_index = targets_with_index(start, end, sentence)\n",
    "    results = [wordPOSPairs[tpl[1]-1][1] for tpl in targets_index]\n",
    "    filtered_results = [result for result in results \n",
    "                        if remove_punctuation(result).strip() and result != 'POS']\n",
    "    return filtered_results if len(filtered_results) > 0 else None\n",
    "\n",
    "def wordnet_lemma(target, pos):\n",
    "    tokens = nltk.word_tokenize(target)\n",
    "    if pos:\n",
    "        pos = [penn_to_wn(poss) if penn_to_wn(poss) else 'n' for poss in pos]\n",
    "        lemmas = [wordNetLemmatizer.lemmatize(token, poss)\n",
    "                     for token, poss in zip(tokens, pos)]\n",
    "        return ' '.join(lemmas)\n",
    "    return target\n",
    "\n",
    "def preprocessing(dataframe):\n",
    "    df = dataframe.copy()\n",
    "    df['p_sentence'] = df.sentence.apply(lambda sent : sent.strip().lower())\n",
    "    df['sentence'] = df.sentence.apply(lambda sent : sent.replace(\"''\", \"``\"))\n",
    "    df['p_target'] = df.target.apply(lambda target : target.strip().lower())\n",
    "    df['pos_tags'] = df[['start', 'end', 'sentence']].apply(lambda vals : pos_tags(*vals), axis = 1)\n",
    "    df['pos_tags_pt'] = df.pos_tags.apply(lambda pos : [penn_to_wn(poss) if penn_to_wn(poss) else 'n' for poss in pos])\n",
    "    df['lemma'] = df[['target', 'pos_tags']].apply(lambda vals : wordnet_lemma(*vals), axis = 1)\n",
    "    df['p_lemma'] = df.lemma.apply(lambda lemma : lemma.strip().lower())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessed_datasets = [Dataset(ds.name, preprocessing(ds.train), \n",
    "                               preprocessing(ds.test)) for ds in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasets = preprocessed_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Context-Token Aggregation\n",
    "First we define how feature values of multiple context-tokens should be aggreagated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_freq_wiki = {}\n",
    "sum_counts = 0\n",
    "with open(\"resources/word-freq-dumps/enwiki-20150602-words-frequency.txt\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        word, freq = line.partition(\" \")[::2]\n",
    "        sum_counts+=int(freq)\n",
    "        word_freq_wiki[word.strip()] = int(freq)\n",
    "        \n",
    "def get_unigram_probability(word):\n",
    "    return word_freq_wiki.get(word,1) / (sum_counts + len(word_freq_wiki))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def agg_ctx_feat_num_average(tokens, func_feature, *args, **kwargs):\n",
    "    if all(isinstance(tpl, tuple) for tpl in tokens):\n",
    "        return np.mean([func_feature(token, *args) for token, dist in tokens])\n",
    "    return np.mean([func_feature(token, *args) for token in tokens])\n",
    "\n",
    "def agg_ctx_feat_num_weighted_average(tokens, func_feature, alpha, *args):\n",
    "    if all(isinstance(tpl, tuple) for tpl in tokens):\n",
    "        if len(tokens)==1:\n",
    "            return np.mean([func_feature(token, *args) for token, dist in tokens])\n",
    "        prob_sum = np.sum([(alpha/(alpha+get_unigram_probability(token))) for token, dist in tokens])\n",
    "        return np.mean([((alpha/(alpha+get_unigram_probability(token)))/prob_sum) * \n",
    "                func_feature(token, *args) for token, dist in tokens])\n",
    "    prob_sum = np.sum([(alpha/(alpha+get_unigram_probability(token))) for token in tokens])\n",
    "    return np.mean([((alpha/(alpha+get_unigram_probability(token)))/prob_sum) * \n",
    "                func_feature(token, *args) for token in tokens])\n",
    "\n",
    "agg_feat_num_weighted_average_medium = lambda target, func_feature, *args: \\\n",
    "                        agg_ctx_feat_num_weighted_average(target, func_feature, 0.0001, *args)\n",
    "\n",
    "def agg_ctx_feat_num_distance(tokens, func_feature, *args):\n",
    "    if all(isinstance(tpl, tuple) for tpl in tokens):\n",
    "        if len(tokens)==1:\n",
    "            return np.mean([func_feature(token, *args) for token, dist in tokens])\n",
    "        dist_sum = np.sum(dist for token, dist in tokens)\n",
    "        return np.sum([(func_feature(token, *args) * ((dist_sum-dist)/dist_sum)) \n",
    "                    for token, dist in tokens])\n",
    "    return np.mean([func_feature(token, *args) for token in tokens])\n",
    "\n",
    "def agg_ctx_feat_num_median(tokens, func_feature, *args):\n",
    "    if all(isinstance(tpl, tuple) for tpl in tokens):\n",
    "        return np.median([func_feature(token, *args) for token, dist in tokens])\n",
    "    return np.median([func_feature(token, *args) for token in tokens])\n",
    "\n",
    "def agg_ctx_feat_num_max(tokens, func_feature, *args):\n",
    "    if all(isinstance(tpl, tuple) for tpl in tokens):\n",
    "        return np.max([func_feature(token, *args) for token, dist in tokens])\n",
    "    return np.max([func_feature(token, *args) for token in tokens])\n",
    "\n",
    "def agg_ctx_feat_num_min(tokens, func_feature, *args):\n",
    "    if all(isinstance(tpl, tuple) for tpl in tokens):\n",
    "        return np.min([func_feature(token, *args) for token, dist in tokens])\n",
    "    return np.min([func_feature(token, *args) for token in tokens])\n",
    "\n",
    "def agg_ctx_feat_num_sum(tokens, func_feature, *args):\n",
    "    if all(isinstance(tpl, tuple) for tpl in tokens):\n",
    "        return np.sum([func_feature(token, *args) for token, dist in tokens])\n",
    "    return np.sum([func_feature(token, *args) for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agg_default = [Aggregation('mean', agg_ctx_feat_num_average)]\n",
    "agg_distance = [Aggregation('dist', agg_ctx_feat_num_distance)]\n",
    "agg_weighted = [Aggregation('weighted', agg_feat_num_weighted_average_medium)]\n",
    "aggs_small = [Aggregation('mean', agg_ctx_feat_num_average), Aggregation('max', agg_ctx_feat_num_max)]\n",
    "aggs_all = [Aggregation('mean', agg_ctx_feat_num_average), Aggregation('median', agg_ctx_feat_num_median),\n",
    "            Aggregation('max', agg_ctx_feat_num_max), Aggregation('min', agg_ctx_feat_num_min)]\n",
    "           #Aggregation('weighted_mean', agg_ctx_feat_num_weighted_average_medium)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aggs = agg_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def concat_feature_datasets(*args):\n",
    "    zipped = zip(*args)\n",
    "    concat_features = []\n",
    "    for dataset in zipped:\n",
    "        df_train = None\n",
    "        df_test = None\n",
    "        ctxs = []\n",
    "        fcs = []\n",
    "        aggs = []\n",
    "        for tpl in dataset:\n",
    "            if not fcs:\n",
    "                df_train = tpl.train.copy()\n",
    "                df_test = tpl.test.copy()\n",
    "            else:\n",
    "                df_train = pd.concat([df_train, tpl.train.copy()], axis = 1)\n",
    "                df_test = pd.concat([df_test, tpl.test.copy()], axis = 1)\n",
    "            ctxs.append(tpl.context)\n",
    "            fcs.append(tpl.fc)\n",
    "            aggs.append(tpl.agg)\n",
    "        concat_features.append(ContextFeatureDataset(tpl.name, ctxs, fcs, aggs,\n",
    "                    df_train.loc[:,~df_train.columns.duplicated()], \n",
    "                    df_test.loc[:,~df_test.columns.duplicated()]))\n",
    "    return concat_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Context Definition and Extraction\n",
    "Here we compute different kinds of context definitions. For example, as a baseline we extract all tokens from the sentence except the target. A second approach is to use a n preceeding or n succeding tokens, or a combined window apporach were we extract n tokens preceeding and succeding of the target. A more sophisticated apporach involves dependency parsing of the sentence and applying different extraction heuristics. Finally we also implement a context extraction approach exploting FrameNet semantic parsing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Context Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.parse.corenlp import *\n",
    "import os\n",
    "from functools import lru_cache\n",
    "\n",
    "# First make sure that the StanfordCoreNLP Server is running under port 9010\n",
    "# cd to stanfordCoreNLP directory\n",
    "# java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9010 -timeout 15000\n",
    "parser = CoreNLPDependencyParser(url='http://localhost:9010/')\n",
    "\n",
    "with open(\"resources/dictionaries/stopwords_en.txt\", encoding=\"utf8\") as file:\n",
    "    content = [line.strip().lower() for line in file.readlines()]\n",
    "    stop_words = set(content)\n",
    "    \n",
    "def overlaps(start1, end1, start2, end2):\n",
    "    return bool(range(max(start1, start2), min(end1, end2)+1))\n",
    "\n",
    "def post_process_ctx(context, filtering=True):\n",
    "    return [token for token in context if \n",
    "            (token.isalnum() and (not filtering\n",
    "        or preprocess_target(token).lower() not in stop_words))]\n",
    "\n",
    "def preprocess_target(target):\n",
    "    return target.strip()\n",
    "\n",
    "def target_index_char_based(start, end, ctx_tokens):\n",
    "    size = np.sum([len(token) for token in ctx_tokens]) + len(ctx_tokens)\n",
    "    target_pos = (start + end) / 2\n",
    "    target_pos_rel = target_pos / size\n",
    "    return int(target_pos_rel * len(post_process_ctx(ctx_tokens)))\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def targets_with_index(start, end, context):\n",
    "    curr_pos = 0\n",
    "    targets = []\n",
    "    j = 0\n",
    "    w = 0\n",
    "    curr_split = ''\n",
    "    ctx_split = context.split()\n",
    "    whitespaces = re.findall('\\s+', context)\n",
    "    num_whitespaces = [len(token) for token in whitespaces]\n",
    "    num_whitespaces.append(1)\n",
    "    tokens = word_tokenize(context)\n",
    "    tokens = ['\"' if token not in context else token for token in tokens]\n",
    "    for index, token in enumerate(tokens, 1):\n",
    "        targets.append((token, index, curr_pos, (curr_pos + len(token))))\n",
    "        curr_pos += len(token)\n",
    "        curr_split += token\n",
    "        if ctx_split[j] == curr_split:\n",
    "            curr_pos += num_whitespaces[w]\n",
    "            j += 1\n",
    "            w += 1\n",
    "            curr_split = ''\n",
    "    vals = [(target[0], target[1]) for target in targets \\\n",
    "            if overlaps(start, end, target[2], target[3])]\n",
    "    return [val for val in vals if val[0] != '\"']\n",
    "\n",
    "from joblib import Memory\n",
    "memory = Memory(location='resources/dependency-cache', verbose=0)\n",
    "@memory.cache\n",
    "def dependency_parse_with_root(sentence):\n",
    "    try:\n",
    "        dependency_parser = parser.raw_parse(sentence)\n",
    "        dependencies = []\n",
    "        parsetree = list(dependency_parser)[0]\n",
    "        for index, node in parsetree.nodes.items():\n",
    "            for relation, dependant in parsetree.nodes[index]['deps'].items():\n",
    "                for dep in dependant:\n",
    "                    triple = ((node['word'], index), relation, \\\n",
    "                              (parsetree.nodes[dep]['word'], dep))\n",
    "                    dependencies.append(triple)\n",
    "        return dependencies\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def dependency_parse(sentence):\n",
    "    dependencies = dependency_parse_with_root(sentence)\n",
    "    filtered_dependencies = [triple for triple in dependencies if triple[1] != 'ROOT']\n",
    "    return filtered_dependencies\n",
    "\n",
    "def ctx_extraction_sentence(context, target):\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    if target in ctx_tokens:\n",
    "        ctx_tokens.remove(target)\n",
    "    return ctx_tokens\n",
    "\n",
    "def ctx_extraction_sentence_filtered(context, target, start, end, filtering = True):\n",
    "    context = context[:start] + context[end:]\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    post_ctx_tokens = post_process_ctx(ctx_tokens, filtering)\n",
    "    return ctx_tokens\n",
    "\n",
    "def ctx_extraction_hit(context, filtering = True):\n",
    "    hit_tokens = [token for sentence in context for token in word_tokenize(sentence)]\n",
    "    post_ctx_tokens = post_process_ctx(hit_tokens, filtering)\n",
    "    return post_ctx_tokens\n",
    "\n",
    "def ctx_extraction_window_pre_n(context, target, start, end, \n",
    "                            filtering = True, n = 3, dist = True):\n",
    "    target = preprocess_target(target)\n",
    "    ctx_tokens = word_tokenize(context[:start])\n",
    "    post_ctx_tokens = post_process_ctx(ctx_tokens, filtering)\n",
    "    return [(elem, index) for index, elem in zip(range(n, 0, -1), post_ctx_tokens[-n:])] \\\n",
    "                if dist else post_ctx_tokens[-n:]\n",
    "\n",
    "def ctx_extraction_window_suc_n(context, target, start, end, \n",
    "                            filtering = True, n = 3, dist = True):\n",
    "    target = preprocess_target(target)\n",
    "    ctx_tokens = word_tokenize(context[end:])\n",
    "    post_ctx_tokens = post_process_ctx(ctx_tokens, filtering)\n",
    "    return [(elem, index) for index, elem in zip(range(1, (n+1)), post_ctx_tokens[:n])] \\\n",
    "                if dist else post_ctx_tokens[:n]\n",
    "\n",
    "def ctx_extraction_window_pre_suc_n(context, target, start, end, \n",
    "                                filtering = True, n = 3, dist = True):\n",
    "    ctx_tokens_pre = ctx_extraction_window_pre_n(context, target, start, end, filtering, n, dist)\n",
    "    ctx_tokens_suc = ctx_extraction_window_suc_n(context, target, start, end, filtering, n, dist)\n",
    "    ctx_tokens_pre.extend(ctx_tokens_suc)\n",
    "    return ctx_tokens_pre\n",
    "\n",
    "def ctx_extraction_dep_in(context, target, start, end, dist = True):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    selec_tuples = list(set([triple for triple in triples \\\n",
    "                if triple[2] in targets and triple[0] not in targets]))\n",
    "    return [(triple[0][0], np.abs(triple[0][1]-triple[2][1])) for triple in selec_tuples] if dist \\\n",
    "                else [triple[0][0] for triple in selec_tuples]\n",
    "\n",
    "def ctx_extraction_dep_out(context, target, start, end, dist = True):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    selec_tuples = list(set([triple for triple in triples \\\n",
    "                if triple[0] in targets and triple[2] not in targets]))\n",
    "    return [(triple[2][0], np.abs(triple[0][1]-triple[2][1])) for triple in selec_tuples] if dist \\\n",
    "                else [triple[2][0] for triple in selec_tuples]\n",
    "\n",
    "def ctx_extraction_dep_in_out(context, target, start, end, dist = True):\n",
    "    ctx_tokens_in = ctx_extraction_dep_in(context, target, start, end, dist)\n",
    "    ctx_tokens_out = ctx_extraction_dep_out(context, target, start, end, dist)\n",
    "    ctx_tokens_in.extend(ctx_tokens_out)\n",
    "    return list(set(ctx_tokens_in))\n",
    "\n",
    "def ctx_extraction_dep_recu_in_n_steps(context, target, start, \n",
    "                            end, n = 2, dist = True):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    result_tokens = []\n",
    "    curr_target = targets\n",
    "    for step in range(0, n):\n",
    "        step_result = [triple[0] for triple in triples \n",
    "                       if triple[2] in curr_target]\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "    mean_target_index = np.mean([tgt[1] for tgt in targets])\n",
    "    unique = list(set([result for result in result_tokens if result not in targets]))\n",
    "    return [(token[0], np.abs(token[1]-mean_target_index))\n",
    "                    for token in unique] if dist \\\n",
    "                else [token[0] for token in unique]\n",
    "\n",
    "def ctx_extraction_dep_recu_out_n_steps(context, target, start, \n",
    "                        end, n = 2, dist = True):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    result_tokens = []\n",
    "    curr_target = targets\n",
    "    for step in range(0, n):\n",
    "        step_result = [triple[2] for triple in triples \n",
    "                       if triple[0] in curr_target]\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "    mean_target_index = np.mean([tgt[1] for tgt in targets])\n",
    "    unique = list(set([result for result in result_tokens if result not in targets]))\n",
    "    return [(token[0], np.abs(token[1]-mean_target_index))\n",
    "                    for token in unique] if dist \\\n",
    "                else [token[0] for token in unique]\n",
    "\n",
    "def ctx_extraction_dep_recu_in_out_n_steps(context, target, start, \n",
    "                            end, n = 2, dist = True):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    result_tokens = []\n",
    "    curr_target = targets\n",
    "    for step in range(0, n):\n",
    "        step_result = [triple[2] for triple in triples \n",
    "                       if triple[0] in curr_target]\n",
    "        step_result_out = [triple[0] for triple in triples \n",
    "                       if triple[2] in curr_target]\n",
    "        step_result.extend(step_result_out)\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "    mean_target_index = np.mean([tgt[1] for tgt in targets])\n",
    "    unique = list(set([result for result in result_tokens if result not in targets]))\n",
    "    return [(token[0], np.abs(token[1]-mean_target_index))\n",
    "                    for token in unique] if dist \\\n",
    "                else [token[0] for token in unique]\n",
    "\n",
    "def ctx_extraction_dep_recu_in_cover(context, target, start, \n",
    "                        end, cover = 0.1, dist = True):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    ctx_tokens_post = post_process_ctx(ctx_tokens)\n",
    "    result_tokens = []\n",
    "    curr_target = targets\n",
    "    curr_cover = 0\n",
    "    while curr_cover < cover:\n",
    "        step_result = [triple[0] for triple in triples \n",
    "                       if triple[2] in curr_target]\n",
    "        if set(step_result) == set(curr_target):\n",
    "                break\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "        curr_cover = len(result_tokens) / len(ctx_tokens_post)\n",
    "    mean_target_index = np.mean([tgt[1] for tgt in targets])\n",
    "    unique = list(set([result for result in result_tokens if result not in targets]))\n",
    "    return [(token[0], np.abs(token[1]-mean_target_index))\n",
    "                    for token in unique] if dist \\\n",
    "                else [token[0] for token in unique]\n",
    "\n",
    "def ctx_extraction_dep_recu_out_cover(context, target, start, \n",
    "                        end, cover = 0.1, dist = True):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    ctx_tokens_post = post_process_ctx(ctx_tokens)\n",
    "    result_tokens = []\n",
    "    curr_target = targets\n",
    "    curr_cover = 0\n",
    "    while curr_cover < cover:\n",
    "        step_result = [triple[2] for triple in triples \n",
    "                       if triple[0] in curr_target]\n",
    "        if set(step_result) == set(curr_target):\n",
    "                break\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "        curr_cover = len(result_tokens) / len(ctx_tokens_post)\n",
    "    mean_target_index = np.mean([tgt[1] for tgt in targets])\n",
    "    unique = list(set([result for result in result_tokens if result not in targets]))\n",
    "    return [(token[0], np.abs(token[1]-mean_target_index))\n",
    "                    for token in unique] if dist \\\n",
    "                else [token[0] for token in unique]\n",
    "\n",
    "def ctx_extraction_dep_recu_in_out_cover(context, target, start,\n",
    "                        end, cover = 0.1, dist = True):\n",
    "    target = preprocess_target(target)\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    ctx_tokens = word_tokenize(context)\n",
    "    ctx_tokens_post = post_process_ctx(ctx_tokens)\n",
    "    result_tokens = []\n",
    "    curr_target = targets\n",
    "    curr_cover = 0\n",
    "    while curr_cover < cover:\n",
    "        step_result = [triple[2] for triple in triples \n",
    "                       if triple[0] in curr_target]\n",
    "        step_result_out = [triple[0] for triple in triples \n",
    "                       if triple[2] in curr_target]\n",
    "        step_result.extend(step_result_out)\n",
    "        if set(step_result) == set(curr_target):\n",
    "                break\n",
    "        curr_target = list(set(step_result))\n",
    "        result_tokens.extend(step_result)\n",
    "        curr_cover = len(result_tokens) / len(ctx_tokens_post)\n",
    "    mean_target_index = np.mean([tgt[1] for tgt in targets])\n",
    "    unique = list(set([result for result in result_tokens if result not in targets]))\n",
    "    return [(token[0], np.abs(token[1]-mean_target_index))\n",
    "                    for token in unique] if dist \\\n",
    "                else [token[0] for token in unique]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Normally, the land will be passed down by future generations in a way \" + \\\n",
    "             \"that recognizes the community's traditional connection to that country .\"\n",
    "target = 'passed'\n",
    "\n",
    "print('ctx_etraction_all:')\n",
    "print(ctx_extraction_sentence_filtered(sentence, target, 28, 34,))\n",
    "\n",
    "print('ctx_extraction_window_pre_n:')\n",
    "print(ctx_extraction_window_pre_n(sentence, \"Normally\", 0, 8, filtering=False))\n",
    "print(ctx_extraction_window_pre_n(sentence, \"the\", 11, 14, filtering=False))\n",
    "print(ctx_extraction_window_pre_n(sentence, \"land\", 15, 19, filtering=False))\n",
    "print(ctx_extraction_window_pre_n(sentence, \"to\", 126, 128, filtering=False))\n",
    "print(ctx_extraction_window_pre_n(sentence, target, 28, 34, n = 5, filtering=False))\n",
    "\n",
    "print('ctx_extraction_window_suc_n:')\n",
    "print(ctx_extraction_window_suc_n(sentence, \"country\", 135, 142, filtering=False))\n",
    "print(ctx_extraction_window_suc_n(sentence, \"to\", 126, 128, filtering=False))\n",
    "print(ctx_extraction_window_suc_n(sentence, \"connection\", 115, 125, filtering=False))\n",
    "print(ctx_extraction_window_suc_n(sentence, \"community\", 91, 100, n = 5, filtering=False))\n",
    "\n",
    "print('ctx_extraction_window_pre_suc_n:')\n",
    "print(ctx_extraction_window_pre_suc_n(sentence, \"passed\", 28, 34, filtering=False))\n",
    "print(ctx_extraction_window_pre_suc_n(sentence, \"the\", 11, 14, filtering=False))\n",
    "print(ctx_extraction_window_pre_suc_n(sentence, \"to\", 127, 129, filtering=False))\n",
    "\n",
    "print('ctx_extraction_dep_in:')\n",
    "print(ctx_extraction_dep_in(sentence, \"land\", 15, 19))\n",
    "\n",
    "print('ctx_extraction_dep_out:')\n",
    "print(ctx_extraction_dep_out(sentence, target, 28, 34))\n",
    "print(ctx_extraction_dep_out(sentence, \"land\", 15, 19))\n",
    "\n",
    "print('ctx_extraction_dep_in_out:')\n",
    "print(ctx_extraction_dep_in_out(sentence, \"land\", 15, 19))\n",
    "\n",
    "print('ctx_extraction_dep_recu_in_n_steps:')\n",
    "print(ctx_extraction_dep_recu_in_n_steps(sentence, \"the\", 11, 14, n = 3))\n",
    "\n",
    "print('ctx_extraction_dep_recu_out_n_steps:')\n",
    "print(ctx_extraction_dep_recu_out_n_steps(sentence, \"the\", 11, 14))\n",
    "\n",
    "print('ctx_extraction_dep_recu_in_out_n_steps:')\n",
    "print(ctx_extraction_dep_recu_in_out_n_steps(sentence, \"the\", 11, 14))\n",
    "\n",
    "print('ctx_extraction_dep_recu_in_cover:')\n",
    "print(ctx_extraction_dep_recu_in_cover(sentence, \"the\", 11, 14, cover=0.1))\n",
    "\n",
    "print('ctx_extraction_dep_recu_out_cover:')\n",
    "print(ctx_extraction_dep_recu_out_cover(sentence, \"the\", 11, 14, cover=0.1))\n",
    "\n",
    "print('ctx_extraction_dep_recu_in_out_cover:')\n",
    "print(ctx_extraction_dep_recu_in_out_cover(sentence, \"the\", 11, 14, cover=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Context Extraction\n",
    "\n",
    "After we defined all the context extraction approaches, we can apply them on the actual dataset. To do so, we first extract all the distinct sentences from the actual training set and create a new dataframe containing only the sentence ids, the sentence, the target and all the computed contexts. This also makes it easier to integrate context extraction functions implemented in other languages. Afterwards we can compute the context features and join them back with the target features dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Context = namedtuple('Context', 'name, params, func')\n",
    "ContextFeatureCategory = namedtuple('ContextFeatureCategory', 'name, func')\n",
    "ContextDataset = namedtuple('ContextDataset', 'name, context, train, test')\n",
    "ContextFeatureDataset = namedtuple('ContextFeatureDataset', 'name, context, fc, agg, train, test')\n",
    "ctx_fcs = []\n",
    "ctx_feature_datasets = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.2.1) Extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ctx_window(dataframe, n, filtering, window_func):\n",
    "    df = dataframe.copy()\n",
    "    df['context'] = df.apply(lambda columns : \n",
    "                window_func(columns['sentence'], columns['target'], \\\n",
    "                columns['start'], columns['end'],  n = n, filtering = filtering), axis = 1)\n",
    "    return df\n",
    "\n",
    "ctx_window_pre_2_nf = Context('ctx_window_pre_n', {'n':2, 'filtering':False}, \\\n",
    "                              lambda dataframe : ctx_window(dataframe, 2, False, ctx_extraction_window_pre_n))\n",
    "ctx_window_pre_2_f = Context('ctx_window_pre_n', {'n':2, 'filtering':True}, \\\n",
    "                             lambda dataframe : ctx_window(dataframe, 2, True, ctx_extraction_window_pre_n))\n",
    "ctx_window_suc_n_2_nf = Context('ctx_window_suc_n',  {'n':2, 'filtering':False}, \\\n",
    "                            lambda dataframe : ctx_window(dataframe, 2, False, ctx_extraction_window_suc_n))\n",
    "ctx_window_pre_suc_n_2_nf = Context('ctx_window_pre_suc_n',  {'n':2, 'filtering':False}, \\\n",
    "                            lambda dataframe : ctx_window(dataframe, 2, False, ctx_extraction_window_pre_suc_n))\n",
    "ctx_window_pre_suc_n_5_f = Context('ctx_window_pre_suc_n',  {'n':2, 'filtering':True}, \\\n",
    "                            lambda dataframe : ctx_window(dataframe, 5, True, ctx_extraction_window_pre_suc_n))\n",
    "\n",
    "\n",
    "\n",
    "def ctx_dependency(dataframe, filtering, dep_func):\n",
    "    df = dataframe.copy()\n",
    "    df['context'] = df.apply(lambda columns : \n",
    "            dep_func(columns['sentence'], columns['target'], \\\n",
    "            columns['start'], columns['end']), axis = 1)\n",
    "    return df\n",
    "\n",
    "ctx_dep_in_2_nf = Context('ctx_dep_in', {'n':2, 'filtering':False}, \\\n",
    "                              lambda dataframe : ctx_dependency(dataframe, False, ctx_extraction_dep_in))\n",
    "ctx_dep_out_2_nf = Context('ctx_dep_out', {'n':2, 'filtering':False}, \\\n",
    "                              lambda dataframe : ctx_dependency(dataframe, False, ctx_extraction_dep_out))\n",
    "ctx_dep_in_out_2_nf = Context('ctx_dep_in_out', {'n':2, 'filtering':False}, \\\n",
    "                              lambda dataframe : ctx_dependency(dataframe, False, ctx_extraction_dep_in_out))\n",
    "\n",
    "\n",
    "\n",
    "def ctx_dependency_recu_steps(dataframe, n, filtering, dep_func):\n",
    "    df = dataframe.copy()\n",
    "    df['context']  = df.apply(lambda columns : \n",
    "                dep_func(columns['sentence'], columns['target'], \\\n",
    "                columns['start'], columns['end'], n=n), axis = 1)\n",
    "    return df\n",
    "\n",
    "ctx_dep_rec_in_2_nf = Context('ctx_dep_rec_in_n', {'n':2, 'filtering':False}, \\\n",
    "                lambda dataframe : ctx_dependency_recu_steps(dataframe, 2, False, ctx_extraction_dep_recu_in_n_steps))\n",
    "ctx_dep_rec_out_2_nf = Context('ctx_dep_rec_out_n', {'n':2, 'filtering':False}, \\\n",
    "                lambda dataframe : ctx_dependency_recu_steps(dataframe, 2, False, ctx_extraction_dep_recu_out_n_steps))\n",
    "ctx_dep_rec_in_out_2_nf = Context('ctx_dep_rec_in_out_n', {'n':2, 'filtering':False}, \\\n",
    "                lambda dataframe : ctx_dependency_recu_steps(dataframe, 2, False, ctx_extraction_dep_recu_in_out_n_steps))\n",
    "\n",
    "\n",
    "\n",
    "def ctx_dependency_recu_cover(dataframe, cover, filtering, dep_func):\n",
    "    df = dataframe.copy()\n",
    "    df['context']  = df.apply(lambda columns : \n",
    "                dep_func(columns['sentence'], columns['target'], \\\n",
    "                columns['start'], columns['end'], cover=cover), axis = 1)\n",
    "    return df\n",
    "\n",
    "ctx_dep_rec_in_02_nf = Context('ctx_dep_rec_in_02', {'cover': 0.2, 'filtering':False}, \\\n",
    "                lambda dataframe : ctx_dependency_recu_cover(dataframe, 0.2, False, ctx_extraction_dep_recu_in_cover))\n",
    "ctx_dep_rec_out_02_nf = Context('ctx_dep_rec_out_02', {'cover': 0.2, 'filtering':False}, \\\n",
    "                lambda dataframe : ctx_dependency_recu_cover(dataframe, 0.2, False, ctx_extraction_dep_recu_out_cover))\n",
    "ctx_dep_rec_in_out_02_nf = Context('ctx_dep_rec_in_out_02', {'cover': 0.2, 'filtering':False}, \\\n",
    "                lambda dataframe : ctx_dependency_recu_cover(dataframe, 0.2, False, ctx_extraction_dep_recu_in_out_cover))\n",
    "\n",
    "\n",
    "\n",
    "def ctx_sentence(dataframe, filtering):\n",
    "    df = dataframe.copy()\n",
    "    df['context']  = df.apply(lambda columns : \n",
    "                ctx_extraction_sentence_filtered(columns['sentence'], columns['target'], \\\n",
    "                columns['start'], columns['end'], filtering=filtering), axis = 1)\n",
    "    return df\n",
    "\n",
    "ctx_sentence_nf = Context('ctx_sentence', {'filtering':False}, lambda dataframe : ctx_sentence(dataframe, False))\n",
    "\n",
    "\n",
    "\n",
    "def ctx_hit(dataframe, filtering):\n",
    "    df = dataframe.copy()\n",
    "    df = df.join(df.groupby('id')['sentence'].apply(lambda sentences : \\\n",
    "                    tuple(ctx_extraction_hit(list(set(sentences))))), on='id', rsuffix='_hits')\n",
    "    df['sentence_hits'] = df.sentence_hits.apply(lambda hits : list(hits))\n",
    "    df.rename(columns={'sentence_hits':'context'}, inplace=True)\n",
    "    return df\n",
    "\n",
    "ctx_hit_nf = Context('ctx_sentence', {'filtering':False}, lambda dataframe : ctx_hit(dataframe, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.2.2) Context Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "tbl = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "                      if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(tbl)\n",
    "\n",
    "def preprocess_ctx(context):\n",
    "    if all(isinstance(tpl, tuple) for tpl in context):\n",
    "        stripped = [(token.strip().lower(), dist) for token, dist in context]\n",
    "        return [(token, dist) for token, dist in stripped if remove_punctuation(token)]\n",
    "    stripped = [token.strip().lower() for token in context]\n",
    "    return [token for token in stripped if remove_punctuation(token)]\n",
    "\n",
    "def preprocess_ctx_df(dataframe):\n",
    "    df = dataframe.copy()\n",
    "    df['p_context_dist'] = df.context.apply(lambda context : preprocess_ctx(context))\n",
    "    df['p_context'] = df.context.apply(lambda context : [token for token, dist in preprocess_ctx(context)])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "contexts = [ctx_window_pre_suc_n_5_f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctx_datasets = [ContextDataset(ds.name, ctx, preprocess_ctx_df(ctx.func(ds.train.loc[:30,])), \n",
    "                preprocess_ctx_df(ctx.func(ds.test.loc[:30,])))\n",
    "                for ctx in contexts\n",
    "                for ds in datasets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Context Features\n",
    "After defining all the context definitions and extracting the different kinds of contexts from the sentence, we compute features on the context words. Therefore we first define which of the precomputed contexts to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4.1) Context-Complexity Features\n",
    "Here we compute features that measure the complexity of the extracted context itself. These features are divded into two categories. First, we compute the most important target features as found in the other notebook on feature importance on the context. The target features are already adapted to MWE, which makes it straightforward to apply them to context of any length and apply proper aggregation. Second, we compute features that are computed on the context alone as, for example, several traditional readability metrics and the number of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordmodel import Word\n",
    "\n",
    "words_mrc_database = {}\n",
    "with open(\"resources/mrc-database/mrc2.dct\", encoding=\"utf8\") as file:\n",
    "    for index, line in enumerate(file):\n",
    "        line = line.strip()\n",
    "        word, phon, dphon, stress = line[51:].split('|')\n",
    "        w = Word(\n",
    "                wid = index,\n",
    "                nlet = int(line[0:2]),\n",
    "                nphon = int(line[2:4]),\n",
    "                nsyl = int(line[4]),\n",
    "                kf_freq = int(line[5:10]),\n",
    "                kf_ncats = int(line[10:12]),\n",
    "                kf_nsamp = int(line[12:15]),\n",
    "                tl_freq = int(line[15:21]),\n",
    "                brown_freq = int(line[21:25]),\n",
    "                fam = int(line[25:28]),\n",
    "                conc = int(line[28:31]),\n",
    "                imag = int(line[31:34]),\n",
    "                meanc = int(line[34:37]),\n",
    "                meanp = int(line[37:40]),\n",
    "                aoa = int(line[40:43]),\n",
    "                tq2 = line[43],\n",
    "                wtype = line[44],\n",
    "                pdwtype = line[45],\n",
    "                alphasyl = line[46],\n",
    "                status = line[47],\n",
    "                var = line[48],\n",
    "                cap = line[49],\n",
    "                irreg = line[50],\n",
    "                word=word,\n",
    "                phon=phon,\n",
    "                dphon=dphon,\n",
    "                stress=stress)\n",
    "        words_mrc_database[w.word.strip().lower()] = w\n",
    "\n",
    "def mrc_database(target, func, missing_val):\n",
    "    word = words_mrc_database.get(target.strip().lower())\n",
    "    val = func(word) if word else missing_val\n",
    "    return val if val != 0 else missing_val\n",
    "\n",
    "word_concreteness = {}\n",
    "with open(\"resources/word-freq-dumps/concreteness_brysbaert_et_al.txt\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        word, bigram, conc_m, conc_sd, \\\n",
    "        unknown, total, percent_known, \\\n",
    "        subtlex, dom_pos = line.split('\\t')\n",
    "        word_concreteness[word.strip()] = float(conc_m)\n",
    "\n",
    "word_freq_wiki = {}\n",
    "freq_sum_wiki = 0\n",
    "with open(\"resources/word-freq-dumps/enwiki-20150602-words-frequency.txt\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        word, freq = line.partition(\" \")[::2]\n",
    "        word_freq_wiki[word.strip()] = int(freq)\n",
    "        freq_sum_wiki+=int(freq)\n",
    "        \n",
    "def get_dict_count(target, freqs):\n",
    "    return freqs.get(target.strip().lower(), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ctx_features_context_complexity_from_target(dataframe, agg):\n",
    "    df = dataframe.copy()\n",
    "    df['ctx_length'] = df.p_context.apply(lambda context : agg(context, len))\n",
    "    df['ctx_freq_wiki'] = df.p_context.apply(lambda context : agg(context, get_dict_count, word_freq_wiki))\n",
    "    df['ctx_mrc_fam'] = df.p_context.apply(lambda target : agg(target, mrc_database, lambda word : word.fam, 400))\n",
    "    df['ctx_mrc_conc'] = df.p_context.apply(lambda target : agg(target, mrc_database, lambda word : word.conc, 400))\n",
    "    df['ctx_mrc_imag'] = df.p_context.apply(lambda target : agg(target, mrc_database, lambda word : word.imag, 400))\n",
    "    df['ctx_mrc_meanc'] = df.p_context.apply(lambda target : agg(target, mrc_database, lambda word : word.meanc, 400))\n",
    "    df['ctx_concreteness'] = df.p_context.apply(lambda target : agg(target, \\\n",
    "                                                lambda target : word_concreteness.get(target, 2.5)))\n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "ctx_fc_context_complexity_from_target = ContextFeatureCategory('context_complexity_from_target', \\\n",
    "                                ctx_features_context_complexity_from_target)\n",
    "feature_categories.append(ctx_fc_context_complexity_from_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctx_datasets_fc_context_complexity_from_target = [ContextFeatureDataset(ctx_ds.name, ctx_ds.context, \n",
    "        ctx_fc_context_complexity_from_target, agg, ctx_fc_context_complexity_from_target.func(ctx_ds.train, agg.agg),\n",
    "        ctx_fc_context_complexity_from_target.func(ctx_ds.test, agg.agg)) \n",
    "        for ctx_ds in ctx_datasets for agg in aggs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textatistic import Textatistic\n",
    "    \n",
    "def ctx_features_context_complexity_from_context(dataframe, agg):\n",
    "    df = dataframe.copy()\n",
    "    df['ctx_len_chars'] = df.p_context.apply(lambda context : np.sum([len(word) for word in context]))\n",
    "    df['ctx_len_words'] = df.p_context.apply(lambda context : len(context))\n",
    "    df['ctx_rb_dalechall_score'] = df.p_context.apply(lambda context : \\\n",
    "                                    Textatistic(' '.join(context) + '.').dalechall_score)\n",
    "    df['ctx_rb_flesch_score'] = df.p_context.apply(lambda context : \\\n",
    "                                    Textatistic(' '.join(context) + '.').flesch_score)\n",
    "    df['ctx_rb_fleschkincaid_score'] = df.p_context.apply(lambda context : \\\n",
    "                                    Textatistic(' '.join(context) + '.').fleschkincaid_score)\n",
    "    df['ctx_rb_gunningfog_score'] = df.p_context.apply(lambda context : \\\n",
    "                                    Textatistic(' '.join(context) + '.').gunningfog_score)\n",
    "    df['ctx_rb_polysyblword_count'] = df.p_context.apply(lambda context : \\\n",
    "                                    Textatistic(' '.join(context) + '.').polysyblword_count)\n",
    "    df['ctx_rb_smog_score'] = df.p_context.apply(lambda context : \\\n",
    "                                    Textatistic(' '.join(context) + '.').smog_score)\n",
    "    df['ctx_rb_sybl_count'] = df.p_context.apply(lambda context : \\\n",
    "                                    Textatistic(' '.join(context) + '.').sybl_count)\n",
    "    df['rb_sybl_count_ratio'] = df.p_context.apply(lambda context : \\\n",
    "                                    Textatistic(' '.join(context) + '.').sybl_count / len(context))\n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "ctx_fc_context_complexity_from_context = ContextFeatureCategory('context_complexity_from_context', \\\n",
    "                                ctx_features_context_complexity_from_context)\n",
    "feature_categories.append(ctx_fc_context_complexity_from_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_datasets_fc_context_complexity_from_context = [ContextFeatureDataset(ctx_ds.name, ctx_ds.context, \n",
    "        ctx_fc_context_complexity_from_context, agg, ctx_fc_context_complexity_from_context.func(ctx_ds.train, agg.agg),\n",
    "        ctx_fc_context_complexity_from_context.func(ctx_ds.test, agg.agg)) \n",
    "        for ctx_ds in ctx_datasets for agg in aggs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_datasets_fc_context_complexity_from_context[1].train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_fc_context_complexity = ContextFeatureCategory('context_complexity', \\\n",
    "                        [ctx_fc_context_complexity_from_target, ctx_fc_context_complexity_from_context])\n",
    "ctx_datasets_fc_context_complexity = [ContextFeatureDataset(ds.name, ds.context, ctx_fc_context_complexity, ds.agg,\n",
    "            ds.train, ds.test) for ds in concat_feature_datasets(ctx_datasets_fc_context_complexity_from_target, \n",
    "                            ctx_datasets_fc_context_complexity_from_context)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_datasets_fc_context_complexity[0].train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4.2) Context-Target Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from joblib import Memory\n",
    "memory = Memory(location='resources/dependency-cache', verbose=0)\n",
    "@memory.cache\n",
    "def dependency_parse_with_root(sentence):\n",
    "    try:\n",
    "        dependency_parser = dependencyParser.raw_parse(sentence)\n",
    "        dependencies = []\n",
    "        parsetree = list(dependency_parser)[0]\n",
    "        for index, node in parsetree.nodes.items():\n",
    "            for relation, dependant in parsetree.nodes[index]['deps'].items():\n",
    "                for dep in dependant:\n",
    "                    triple = ((node['word'], index), relation, \\\n",
    "                              (parsetree.nodes[dep]['word'], dep))\n",
    "                    dependencies.append(triple)\n",
    "        return dependencies\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def dependency_parse(sentence):\n",
    "    dependencies = dependency_parse_with_root(sentence)\n",
    "    filtered_dependencies = [triple for triple in dependencies if triple[1] != 'root']\n",
    "    return filtered_dependencies\n",
    "\n",
    "\n",
    "def dep_dist_to_head(target, start, end, context):\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    return np.nan_to_num(np.mean([np.abs(triple[0][1] - triple[2][1])-1 \n",
    "                                for triple in triples if triple[2] in targets]))\n",
    "\n",
    "def dep_dist_to_root(target, start, end, context):\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse_with_root(context)\n",
    "    root_nodes = list(filter(lambda triple : triple[1] == 'root' , triples))\n",
    "    if root_nodes: \n",
    "        root_node = root_nodes[0]\n",
    "    else:\n",
    "        return 0\n",
    "    dist = np.nan_to_num(np.mean([np.abs(root_node[2][1] - triple[2][1])-1 \n",
    "                                for triple in triples if triple[2] in targets]))\n",
    "    return dist if dist != -1 else 0\n",
    "\n",
    "def dep_relation_to_head(target, start, end, context):\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse_with_root(context)\n",
    "    relations = [triple[1] for triple in triples if triple[2] in targets]\n",
    "    return relations[0] if len(relations) == 1 else 'misc'\n",
    "    \n",
    "\n",
    "def dep_head_word_len(target, start, end, context):\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    return np.nan_to_num(np.mean([len(triple[0][0]) \n",
    "        for triple in triples if triple[2] in targets]))\n",
    "\n",
    "def dep_num_dependents(target, start, end, context):\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse_with_root(context)\n",
    "    return len([triple[1] for triple in triples if triple[0] in targets])\n",
    "\n",
    "def dep_max_num_dependents(context):\n",
    "    triples = dependency_parse_with_root(context)\n",
    "    most = Counter([triple[0][0] for triple in triples]).most_common(1)\n",
    "    return most[0][1] if most else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " df['dep_dist_to_head'] = df[['target', 'start', 'end', 'sentence']].apply(lambda vals : \n",
    "                                                                            dep_dist_to_head(*vals), axis=1)\n",
    "    df['dep_dist_to_root'] = df[['target', 'start', 'end', 'sentence']].apply(lambda vals : \n",
    "                                                                            dep_dist_to_root(*vals), axis=1)\n",
    "    df['dep_dist_to_root_norm'] = df[['dep_dist_to_root', 'sentence']].apply(lambda vals : \\\n",
    "                                                        float(vals[0]) / (len(word_tokenize(vals[1]))-1), axis=1)\n",
    "    df['dep_relation_to_head'] = df[['target', 'start', 'end', 'sentence']].apply(lambda vals : \\\n",
    "                                                                dep_relation_to_head(*vals), axis = 1)\n",
    "    df['dep_num_dependents'] = df[['target', 'start', 'end', 'sentence']].apply(lambda vals : \\\n",
    "                                                                        dep_num_dependents(*vals), axis = 1)\n",
    "    df['dep_max_num_dependents'] = df.sentence.apply(lambda sentence : dep_max_num_dependents(sentence))\n",
    "    df['dep_num_dependents_norm'] = df.dep_num_dependents / df.dep_max_num_dependents\n",
    "    df['dep_head_word_len'] = df[['target', 'start', 'end', 'sentence']].apply(lambda vals : \\\n",
    "                                                                        dep_head_word_len(*vals), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Readability Measures\n",
    "Here we implement some of the most popular and well-known historical readability measures. Most of them need multiple sentences to compute them properly, however, we will apply them on the extracted context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>rb_dalechall_score</th>\n",
       "      <th>rb_flesch_score</th>\n",
       "      <th>rb_fleschkincaid_score</th>\n",
       "      <th>rb_gunningfog_score</th>\n",
       "      <th>rb_polysyblword_count</th>\n",
       "      <th>rb_smog_score</th>\n",
       "      <th>rb_sybl_count</th>\n",
       "      <th>rb_sybl_count_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>passed</td>\n",
       "      <td>[land, future, generations, recognizes]</td>\n",
       "      <td>15.677400</td>\n",
       "      <td>-8.725</td>\n",
       "      <td>15.470000</td>\n",
       "      <td>21.600000</td>\n",
       "      <td>2</td>\n",
       "      <td>11.208143</td>\n",
       "      <td>10</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>land</td>\n",
       "      <td>[passed, future, generations]</td>\n",
       "      <td>14.311967</td>\n",
       "      <td>6.390</td>\n",
       "      <td>13.113333</td>\n",
       "      <td>14.533333</td>\n",
       "      <td>1</td>\n",
       "      <td>8.841846</td>\n",
       "      <td>7</td>\n",
       "      <td>2.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>future</td>\n",
       "      <td>[land, passed, generations, recognizes, commun...</td>\n",
       "      <td>13.358500</td>\n",
       "      <td>-1.280</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>13.023867</td>\n",
       "      <td>12</td>\n",
       "      <td>2.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>future generations</td>\n",
       "      <td>[land, passed, recognizes, community, traditio...</td>\n",
       "      <td>13.358500</td>\n",
       "      <td>15.640</td>\n",
       "      <td>12.320000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>13.023867</td>\n",
       "      <td>11</td>\n",
       "      <td>2.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>generations</td>\n",
       "      <td>[land, passed, future, recognizes, community, ...</td>\n",
       "      <td>14.460767</td>\n",
       "      <td>17.445</td>\n",
       "      <td>12.316667</td>\n",
       "      <td>22.400000</td>\n",
       "      <td>3</td>\n",
       "      <td>13.023867</td>\n",
       "      <td>13</td>\n",
       "      <td>2.166667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               target                                            context  \\\n",
       "0              passed            [land, future, generations, recognizes]   \n",
       "1                land                      [passed, future, generations]   \n",
       "2              future  [land, passed, generations, recognizes, commun...   \n",
       "3  future generations  [land, passed, recognizes, community, traditio...   \n",
       "4         generations  [land, passed, future, recognizes, community, ...   \n",
       "\n",
       "   rb_dalechall_score  rb_flesch_score  rb_fleschkincaid_score  \\\n",
       "0           15.677400           -8.725               15.470000   \n",
       "1           14.311967            6.390               13.113333   \n",
       "2           13.358500           -1.280               14.680000   \n",
       "3           13.358500           15.640               12.320000   \n",
       "4           14.460767           17.445               12.316667   \n",
       "\n",
       "   rb_gunningfog_score  rb_polysyblword_count  rb_smog_score  rb_sybl_count  \\\n",
       "0            21.600000                      2      11.208143             10   \n",
       "1            14.533333                      1       8.841846              7   \n",
       "2            26.000000                      3      13.023867             12   \n",
       "3            26.000000                      3      13.023867             11   \n",
       "4            22.400000                      3      13.023867             13   \n",
       "\n",
       "   rb_sybl_count_ratio  \n",
       "0             2.500000  \n",
       "1             2.333333  \n",
       "2             2.400000  \n",
       "3             2.200000  \n",
       "4             2.166667  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textatistic import Textatistic\n",
    "\n",
    "df_context['rb_dalechall_score'] = df_context.context.apply(lambda context : \\\n",
    "                                                Textatistic(' '.join(context) + '.').dalechall_score)\n",
    "df_context['rb_flesch_score'] = df_context.context.apply(lambda context : \\\n",
    "                                                Textatistic(' '.join(context) + '.').flesch_score)\n",
    "df_context['rb_fleschkincaid_score'] = df_context.context.apply(lambda context : \\\n",
    "                                                Textatistic(' '.join(context) + '.').fleschkincaid_score)\n",
    "df_context['rb_gunningfog_score'] = df_context.context.apply(lambda context : \\\n",
    "                                                Textatistic(' '.join(context) + '.').gunningfog_score)\n",
    "df_context['rb_polysyblword_count'] = df_context.context.apply(lambda context : \\\n",
    "                                                Textatistic(' '.join(context) + '.').polysyblword_count)\n",
    "df_context['rb_smog_score'] = df_context.context.apply(lambda context : \\\n",
    "                                                Textatistic(' '.join(context) + '.').smog_score)\n",
    "df_context['rb_sybl_count'] = df_context.context.apply(lambda context : \\\n",
    "                                                Textatistic(' '.join(context) + '.').sybl_count)\n",
    "df_context['rb_sybl_count_ratio'] = df_context.context.apply(lambda context : \\\n",
    "                                                Textatistic(' '.join(context) + '.').sybl_count / len(context))\n",
    "\n",
    "df_context[['target', 'context', 'rb_dalechall_score', 'rb_flesch_score', 'rb_fleschkincaid_score', \\\n",
    "            'rb_gunningfog_score', 'rb_polysyblword_count', 'rb_smog_score', 'rb_sybl_count', 'rb_sybl_count_ratio']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Linguistic Features\n",
    "Here we implement some of the most popular and well-known historical readability measures. Most of them need multiple sentences to compute them properly, however, we will apply them on the extracted context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_context['context'] = df_context['ctx_extraction_window_pre_suc_n']\n",
    "\n",
    "df_context['ctx_num_tokens'] = df_context.context.apply(lambda context : len(context))\n",
    "df_context['ctx_avg_length'] = df_context.context.apply(lambda context : agg_ctx_feat_num_average(context, len))\n",
    "df_context['ctx_avg_word_freq_wiki'] = df_context.context.apply(lambda context : \\\n",
    "                                                    agg_ctx_feat_num_average(context, get_dict_count, word_freq_wiki))\n",
    "df_context.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Result = namedtuple('Result', 'dataset, fc, agg, measure')\n",
    "Dataset = namedtuple('Dataset', 'name, train, test')\n",
    "FeatureDataset = namedtuple('FeatureDataset', 'name, fc, agg, train, test')\n",
    "FeatureCategory = namedtuple('FeatureCategory', 'name, func')\n",
    "Feature = namedtuple('Feature', 'name, fc_name, train, test')\n",
    "Metric = namedtuple('Metric', 'name, func')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_labels_for_binary_df(dataframe):\n",
    "    df = dataframe.copy()\n",
    "    df = df.drop(['id', 'sentence', 'target', 'nat', 'non_nat', \n",
    "                  'nat_marked', 'non_nat_marked', 'prob', 'start', \n",
    "                  'end', 'p_target', 'lemma', 'p_lemma', 'pos_tags', 'pos_tags_pt'], axis = 1)\n",
    "    return df\n",
    "\n",
    "def remove_labels_for_regr_df(dataframe):\n",
    "    df = dataframe.copy()\n",
    "    df = df.drop(['id', 'sentence', 'target', 'nat', 'non_nat', \n",
    "                  'nat_marked', 'non_nat_marked', 'binary', 'start', \n",
    "                  'end', 'p_target', 'lemma', 'p_lemma', 'pos_tags', 'pos_tags_pt'], axis = 1)\n",
    "    return df\n",
    "    \n",
    "def transform_feat_to_num(train, test):\n",
    "    train_copy = train.copy()\n",
    "    test_copy = test.copy()\n",
    "    train_copy = train_copy.replace(np.inf, 0)\n",
    "    train_copy = train_copy.replace(np.nan, 0)\n",
    "    test_copy = test_copy.replace(np.inf, 0)\n",
    "    test_copy = test_copy.replace(np.nan, 0)\n",
    "    shape_train = train.shape\n",
    "    shape_test = test.shape\n",
    "    df = train_copy.append(test_copy, ignore_index=True)\n",
    "    df = pd.get_dummies(df)\n",
    "    return (df.loc[0:(shape_train[0]-1),], \n",
    "            df.loc[shape_train[0]:df.shape[0],])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn import model_selection\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "def get_majority_class_prediction(train, test):\n",
    "    dummy = DummyClassifier(strategy='most_frequent', \n",
    "                            random_state=None, constant=None)\n",
    "    x_train = train.loc[:, train.columns != 'binary']\n",
    "    y_train = train.binary.values\n",
    "    x_test = test\n",
    "    y_test = test.binary.values\n",
    "    dummy.fit(x_train, y_train)\n",
    "    prediction = dummy.predict(x_test)\n",
    "    f1score = f1_score(y_test, prediction)\n",
    "    return f1score\n",
    "\n",
    "def always_complex_prediction(train, test):\n",
    "    y_test = test.binary.values\n",
    "    prediction = [1 for val in y_test]\n",
    "    f1score = precision_recall_fscore_support(y_test, prediction)\n",
    "    return f1score\n",
    "\n",
    "def svm(train, test):\n",
    "    print('average_classification')\n",
    "    x_train = train.loc[:, train.columns != 'binary']\n",
    "    y_train = train.binary.values\n",
    "    x_test = test.loc[:, test.columns != 'binary']\n",
    "    y_test = test.binary.values\n",
    "    seed = 7\n",
    "    #knn = MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto',\n",
    "     #  beta_1=0.9, beta_2=0.999, early_stopping=False,\n",
    "    #   epsilon=1e-08, hidden_layer_sizes=(5, 100), learning_rate='constant',\n",
    "    #   learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
    "    #   nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
    "    #   solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
    "    #   warm_start=False)\n",
    "    knn = svm.SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "        decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
    "        max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "        tol=0.001, verbose=False)\n",
    "    knn.fit(x_train, y_train) \n",
    "    prediction = knn.predict(x_test)\n",
    "    f1score = f1_score(y_test, prediction)\n",
    "    #kfold = model_selection.KFold(n_splits=2, random_state=seed)\n",
    "    #cv_results = model_selection.cross_val_score(knn, x_train, y_train, cv=kfold, scoring=make_scorer(f1_score))\n",
    "    return f1score\n",
    "\n",
    "def xgboost(train, test):\n",
    "    x_train = train.loc[:, train.columns != 'binary']\n",
    "    y_train = train.binary.values\n",
    "    x_test = test.loc[:, test.columns != 'binary']\n",
    "    y_test = test.binary.values\n",
    "    xgtrain = xgb.DMatrix(x_train.values, label=y_train)\n",
    "    xgtest = xgb.DMatrix(x_test.values, label=y_test)\n",
    "    xg_test_x = xgb.DMatrix(x_test.values)\n",
    "    param = {'max_depth': 30, 'eta': 1, 'silent': 1, 'objective': 'binary:logistic',  'n_estimators':5000}\n",
    "    evallist = [(xgtest, 'eval'), (xgtrain, 'train')]\n",
    "    num_round = 70\n",
    "    bst = xgb.train(param, xgtrain, num_round, evallist)\n",
    "    prediction = bst.predict(xg_test_x)\n",
    "    prediction_binary = list(map(lambda val: 1 if val>0.5 else 0, prediction))\n",
    "    f1score = precision_recall_fscore_support(y_test, prediction_binary)\n",
    "    return f1score\n",
    "\n",
    "def xgboost_with_bst(train, test):\n",
    "    x_train = train.loc[:, train.columns != 'binary']\n",
    "    y_train = train.binary.values\n",
    "    x_test = test.loc[:, test.columns != 'binary']\n",
    "    y_test = test.binary.values\n",
    "    xgtrain = xgb.DMatrix(x_train.values, label=y_train, feature_names=x_train.columns.values)\n",
    "    xgtest = xgb.DMatrix(x_test.values, label=y_test, feature_names=x_test.columns.values)\n",
    "    xg_test_x = xgb.DMatrix(x_test.values, feature_names=x_test.columns.values)\n",
    "    param = {'max_depth': 30, 'eta': 1, 'silent': 1, 'objective': 'binary:logistic',  'n_estimators':5000}\n",
    "    evallist = [(xgtest, 'eval'), (xgtrain, 'train')]\n",
    "    num_round = 70\n",
    "    bst = xgb.train(param, xgtrain, num_round, evallist)\n",
    "    prediction = bst.predict(xg_test_x)\n",
    "    prediction_binary = list(map(lambda val: 1 if val>0.5 else 0, prediction))\n",
    "    f1score = precision_recall_fscore_support(y_test, prediction_binary)\n",
    "    return f1score, bst\n",
    "\n",
    "def adaboost(train, test):\n",
    "    x_train = train.loc[:, train.columns != 'binary']\n",
    "    y_train = train.binary.values\n",
    "    x_test = test.loc[:, test.columns != 'binary']\n",
    "    y_test = test.binary.values\n",
    "    adab = AdaBoostClassifier(base_estimator=None, n_estimators=5000, \n",
    "                          learning_rate=1.0, algorithm='SAMME.R',\n",
    "                          random_state=None)\n",
    "    adab.fit(x_train, y_train) \n",
    "    prediction = adab.predict(x_test)\n",
    "    f1score = precision_recall_fscore_support(y_test, prediction)\n",
    "    return f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = [Result(fs, fs.fc, fs.agg,\n",
    "    xgboost(*transform_feat_to_num(remove_labels_for_binary_df(fs.train), \n",
    "        remove_labels_for_binary_df(fs.test)))) for fs in all_fc_datasets]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
