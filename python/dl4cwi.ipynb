{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1.1) Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.wsd import lesk\n",
    "import nltk\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from collections import defaultdict\n",
    "\n",
    "Dataset = namedtuple('Dataset', 'name, train, test')\n",
    "ModelEmbed = namedtuple('ModelEmbed', 'type, name, dimension, corpus, model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "columns = ['id', 'sentence', \"start\", \"end\", \"target\", \n",
    "           \"nat\", \"non_nat\", \"nat_marked\", \"non_nat_marked\", \"binary\", \"prob\"]\n",
    "\n",
    "def load_df(path, d_type, header):\n",
    "    df = pd.read_csv(path, header=header, sep = \"\\t\")\n",
    "    if len(df.columns) == len(columns):\n",
    "        df.columns = columns\n",
    "    if d_type == 'word':\n",
    "        df = df.loc[df.target.map(lambda target : len(word_tokenize(target)))<=1,]\n",
    "    elif d_type == 'phrase':\n",
    "        df = df.loc[df.target.map(lambda target : len(word_tokenize(target)))>1,]\n",
    "    return df\n",
    "\n",
    "def load_datasets(names, train_name, test_name, type_train = None, type_test = None, header=None):\n",
    "    MAIN_PATH_DATASET = \"../cwishareddataset/traindevset/english/\"\n",
    "    datasets = [Dataset(name, load_df(MAIN_PATH_DATASET + name + '_' + train_name + '.tsv', type_train, header),\n",
    "                              load_df(MAIN_PATH_DATASET + name + '_' + test_name + '.tsv', type_test, header))\n",
    "                              for name in names]\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1.2) Load Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "MAIN_PATH = 'D:/workspace_python/CoWoReId/python/resources/word-embeddings/'\n",
    "\n",
    "def get_embedding_models():\n",
    "    glove_defs = [#Model('glove', 'glove.42B.300d.txt', 300, 'cc42B', None),  \n",
    "                  #Model('glove', 'glove.840B.300d.txt', 300, 'cc840B', None), \n",
    "                  Model('glove', 'glove.6B.50d.txt', 50, 'wikipedia+gigaword5', None)] \n",
    "                  #Model('glove', 'glove.6B.100d.txt',100, 'wikipedia+gigaword5', None),\n",
    "                  #Model('glove', 'glove.6B.200d.txt', 200, 'wikipedia+gigaword5', None), \n",
    "                  #Model('glove', 'glove.6B.300d.txt', 300, 'wikipedia+gigaword5', None),\n",
    "                  #Model('glove', 'glove.twitter.27B.25d.txt', 25, 'twitter', None)]\n",
    "                 #Model('glove', 'glove.twitter.27B.50d.txt', 50, 'twitter', None),\n",
    "                  #Model('glove', 'glove.twitter.27B.100d.txt', 100, 'twitter', None),\n",
    "                  #Model('glove', 'glove.twitter.27B.200d.txt', 200, 'twitter', None)]\n",
    "\n",
    "    models = []\n",
    "    for model in glove_defs:\n",
    "        glove_file = datapath(MAIN_PATH + model.name)\n",
    "        tmp_file = get_tmpfile(model.name + '-temp')\n",
    "        glove2word2vec(glove_file, tmp_file)\n",
    "        vecs = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "        models.append(ModelEmbed(model.type, model.name, model.dimension, model.corpus, vecs))\n",
    "        print('load model : {}'.format(model.name))\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2.1) Preprocessing\n",
    "Here we present all the code to preprocess the data stored in a dataframe into a proper representation that can be used in sequence tagging models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem.wordnet import *\n",
    "from functools import lru_cache\n",
    "from utils import penn_to_wn\n",
    "import re\n",
    "import unicodedata\n",
    "import sys\n",
    "from collections import Counter\n",
    "from ngram_representation import missing_strat_random\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "wordNetLemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tbl = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "                      if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "def overlaps(start1, end1, start2, end2):\n",
    "    return bool(range(max(start1, start2), min(end1, end2)+1))\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(tbl)\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def targets_with_index(start, end, context):\n",
    "    curr_pos = 0\n",
    "    targets = []\n",
    "    j = 0\n",
    "    w = 0\n",
    "    curr_split = ''\n",
    "    ctx_split = context.split()\n",
    "    whitespaces = re.findall('\\s+', context)\n",
    "    num_whitespaces = [len(token) for token in whitespaces]\n",
    "    num_whitespaces.append(1)\n",
    "    tokens = word_tokenize(context)\n",
    "    tokens = ['\"' if token not in context else token for token in tokens]\n",
    "    for index, token in enumerate(tokens, 1):\n",
    "        targets.append((token, index, curr_pos, (curr_pos + len(token))))\n",
    "        curr_pos += len(token)\n",
    "        curr_split += token\n",
    "        if ctx_split[j] == curr_split:\n",
    "            curr_pos += num_whitespaces[w]\n",
    "            j += 1\n",
    "            w += 1\n",
    "            curr_split = ''\n",
    "    vals = [(target[0], target[1]) for target in targets \\\n",
    "            if overlaps(start, end, target[2], target[3])]\n",
    "    return [val for val in vals if val[0] != '\"']\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def all_tokens_with_index(context):\n",
    "    '''\n",
    "    Receives a sentence denoted by context and applies tokenization\n",
    "    on the input. Each token is annotated with its word index starting\n",
    "    from 1 and the corresponding start and end character positions of \n",
    "    the word. Also applies some strategies to handle unproper formated\n",
    "    input sentence string such as removing additional whitespaces and \n",
    "    quotation marks that otherwise change the actual character start\n",
    "    and end positions. All results are cached in case it has to be computed\n",
    "    multiple times for the same sentence.\n",
    "    '''\n",
    "    curr_pos = 0\n",
    "    targets = []\n",
    "    j = 0\n",
    "    w = 0\n",
    "    curr_split = ''\n",
    "    ctx_split = context.split()\n",
    "    whitespaces = re.findall('\\s+', context)\n",
    "    num_whitespaces = [len(token) for token in whitespaces]\n",
    "    num_whitespaces.append(1)\n",
    "    tokens = word_tokenize(context)\n",
    "    tokens = ['\"' if token not in context else token for token in tokens]\n",
    "    for index, token in enumerate(tokens, 1):\n",
    "        targets.append((token, index, curr_pos, (curr_pos + len(token))))\n",
    "        curr_pos += len(token)\n",
    "        curr_split += token\n",
    "        if ctx_split[j] == curr_split:\n",
    "            curr_pos += num_whitespaces[w]\n",
    "            j += 1\n",
    "            w += 1\n",
    "            curr_split = ''\n",
    "    return [val for val in targets if val[0] != '\"']\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def wordnet_pos_tagging(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    return nltk.pos_tag(tokens)\n",
    "\n",
    "def pos_tags(start, end, sentence):\n",
    "    wordPOSPairs = wordnet_pos_tagging(sentence)\n",
    "    targets_index = targets_with_index(start, end, sentence)\n",
    "    results = [wordPOSPairs[tpl[1]-1][1] for tpl in targets_index]\n",
    "    filtered_results = [result for result in results \n",
    "                        if remove_punctuation(result).strip() and result != 'POS']\n",
    "    return filtered_results if len(filtered_results) > 0 else None\n",
    "\n",
    "def wordnet_lemma(target, pos):\n",
    "    tokens = nltk.word_tokenize(target)\n",
    "    if pos:\n",
    "        pos = [penn_to_wn(poss) if penn_to_wn(poss) else 'n' for poss in pos]\n",
    "        lemmas = [wordNetLemmatizer.lemmatize(token, poss)\n",
    "                     for token, poss in zip(tokens, pos)]\n",
    "        return ' '.join(lemmas)\n",
    "    return target\n",
    "\n",
    "def preprocessing(dataframe):\n",
    "    df = dataframe.copy()\n",
    "    df['sentence'] = df.sentence.apply(lambda sent : sent.replace(\"''\", \"``\"))\n",
    "    df['p_target'] = df.target.apply(lambda target : target.strip().lower())\n",
    "    df['pos_tags'] = df[['start', 'end', 'sentence']].apply(lambda vals : pos_tags(*vals), axis = 1)\n",
    "    df['pos_tags_pt'] = df.pos_tags.apply(lambda pos : [penn_to_wn(poss) if penn_to_wn(poss) else 'n' for poss in pos] \n",
    "                                          if pos else [])\n",
    "    df['lemma'] = df[['target', 'pos_tags']].apply(lambda vals : wordnet_lemma(*vals), axis = 1)\n",
    "    df['p_lemma'] = df.lemma.apply(lambda lemma : lemma.strip().lower())\n",
    "    return df\n",
    "\n",
    "def build_vocabulary(sentences, embedding, dimension, \n",
    "                     missing='unique', provided = ['s_target', 'e_target']):\n",
    "    '''\n",
    "    Based on a list of sentences which are themselve represented\n",
    "    as a list of words, constructs a vocabulary of the words contained\n",
    "    and assigns unique indicies to the words. In particular, it returns \n",
    "    a map of indices to their words, a map of words to their indices\n",
    "    and based on the provided embedding model an embedding matrix\n",
    "    for the constructed vocabulary. For missing vocabulary, it \n",
    "    constructs a random embedding and a proper index is missing parameter\n",
    "    is set to 'unique', otherwise if it is set to 'equal' it creates\n",
    "    a random embedding for one special UNK embedding and neglects missing\n",
    "    vocabulary in the built index. All tokens in the 'provided' list,\n",
    "    receive under 'equal' mode still individual random embeddings.\n",
    "    '''\n",
    "    if missing not in ['unique', 'equal']:\n",
    "        raise ValueError(\"Parameter missing must be either 'equal' or 'unique'\")\n",
    "    all_words = [word for sentence in sentences for word in sentence]\n",
    "    print('# Words : {}'.format(len(all_words)))\n",
    "    counter = Counter(all_words)\n",
    "    index = 1\n",
    "    word2index = {}\n",
    "    for (word, count) in counter.most_common():\n",
    "        if (missing=='unique' or word in embedding.vocab):\n",
    "            word2index[word] = index\n",
    "            index += 1\n",
    "    word2index['_pad_'] = 0\n",
    "    if missing == 'equal':\n",
    "        word2index['_unk_'] = len(word2index)\n",
    "        for token in provided:\n",
    "            word2index[token] = len(word2index)\n",
    "    index2word = {index : word for word, index in word2index.items()}\n",
    "    vocab_size = len(word2index)\n",
    "    print('# Vocab : {}'.format(vocab_size))\n",
    "    embedding_matrix = np.zeros(((vocab_size), dimension))\n",
    "    embedding_matrix[0] = missing_strat_random('_pad_', dimension)\n",
    "    missing_embed_words = []\n",
    "    for word, index in word2index.items():\n",
    "        if word in embedding.vocab:\n",
    "            embedding_matrix[index] = embedding[word]\n",
    "        else:\n",
    "            embedding_matrix[index] = missing_strat_random(word, dimension)\n",
    "            missing_embed_words.append(word)\n",
    "    missing_embed_count = len(missing_embed_words)\n",
    "    print('# Words missing embedding : {}'.format(missing_embed_count))\n",
    "    print('Embedding shape : {}'.format(embedding_matrix.shape))\n",
    "    return word2index, index2word, embedding_matrix\n",
    "\n",
    "\n",
    "def build_char_vocabulary(sentences, embedding, dimension, \n",
    "                          missing='unique', provided = ['_']):\n",
    "    '''\n",
    "    Based on a list of sentences which are themselve represented\n",
    "    as a list of words, constructs a character vocabulary and provides\n",
    "    a mapping of unique indices to the found characters, a mapping of\n",
    "    the characters to their indicies and a character embedding matrix\n",
    "    where the i-th row represents the character embedding of the character\n",
    "    with index i. This is based on a provided character embedding, represented\n",
    "    as a dictionary. Provided tokens will be added as a single char to\n",
    "    the vocabulary.\n",
    "    '''\n",
    "    if missing not in ['unique', 'equal']:\n",
    "        raise ValueError(\"Parameter missing must be either 'equal' or 'unique'\")\n",
    "    all_chars = [char for sentence in sentences \n",
    "                 for word in sentence for char in word]\n",
    "    all_chars.extend(provided)\n",
    "    print('# Chars : {}'.format(len(all_chars)))\n",
    "    counter = Counter(all_chars)\n",
    "    index = 1\n",
    "    char2index = {}\n",
    "    for (char, count) in counter.most_common():\n",
    "        if (missing=='unique' or char in embedding.keys()):\n",
    "            char2index[char] = index\n",
    "            index += 1\n",
    "    char2index['_pad_'] = 0\n",
    "    if missing == 'equal':\n",
    "        char2index['_unk_'] = len(char2index)\n",
    "    index2char = {index : char for char, index in char2index.items()}\n",
    "    vocab_size = len(char2index)\n",
    "    print('# Vocab (chars) : {}'.format(vocab_size))\n",
    "    embedding_matrix = np.zeros(((vocab_size), dimension))\n",
    "    embedding_matrix[0] = missing_strat_random('_pad_', dimension)\n",
    "    missing_embed_chars = []\n",
    "    for char, index in char2index.items():\n",
    "        if char in embedding.keys():\n",
    "            embedding_matrix[index] = embedding[char]\n",
    "        else:\n",
    "            embedding_matrix[index] = missing_strat_random(char, dimension)\n",
    "            missing_embed_chars.append(char)\n",
    "    missing_embed_count = len(missing_embed_chars)\n",
    "    print('# Chars missing embedding : {}'.format(missing_embed_count))\n",
    "    print('Embedding shape : {}'.format(embedding_matrix.shape))\n",
    "    return char2index, index2char, embedding_matrix\n",
    "\n",
    "\n",
    "def compute_character_embeddings(embedding):\n",
    "    '''\n",
    "    Computes a character embedding as a dictionary of word to its\n",
    "    embedding based on a gensim word embedding. For each character,\n",
    "    averages the word embeddings containing the character as an\n",
    "    approximation to character-level embeddings.\n",
    "    '''\n",
    "    chars = {}\n",
    "    for word, vocab in embedding.vocab.items():\n",
    "        vector = embedding[word]\n",
    "        for char in word:\n",
    "            if ord(char)<128:\n",
    "                if char in chars:\n",
    "                    chars[char] = (chars[char][0]+vector, \n",
    "                                   chars[char][1]+1)\n",
    "                else:\n",
    "                    chars[char] = (vector, 1)\n",
    "    for char, (vector, num) in chars.items():\n",
    "        chars[char] = np.round(vector/num, 6).tolist()\n",
    "    return chars\n",
    "\n",
    "def forward_transformation_v1(dataframe, lowercase = True, filter_punc = True, filtering = \"a132\"):\n",
    "    '''\n",
    "    Transforms the provided dataframe rows into a representation\n",
    "    to be used in a sequence classifier. For each sentence in the\n",
    "    dataset, returns the sentence id, the sentence as a list of \n",
    "    words based on tokenization, the binary label for each word, \n",
    "    the probability label for each word and a list of (start, end)\n",
    "    tuples representing the start and end positions of the words \n",
    "    in the sentence word list. This can be used to map back the\n",
    "    predictions later. This function should be used if classification\n",
    "    is done on word level.\n",
    "    '''\n",
    "    grouped = dataframe.groupby('sentence').apply(lambda row : \n",
    "                        {'sent_id' : list(set(row['sent_id']))[0],\n",
    "                         'sentence' : list(set(row['sentence']))[0], \n",
    "                         'tags': [tag for tag in zip(row['target'], \n",
    "                            row['start'], row['end'], row['binary'], row['prob'])]})\n",
    "    sentence_ids = []\n",
    "    sentences = []\n",
    "    binaries = []\n",
    "    probabilities = []\n",
    "    start_ends = []\n",
    "    for vals in grouped:\n",
    "        sent_id = vals['sent_id']\n",
    "        sentence = vals['sentence']\n",
    "        tags = vals['tags']\n",
    "        tags_without_labels = [(word, start, end) for word, start, end, binary, prob in tags]\n",
    "        all_tokens = all_tokens_with_index(sentence)\n",
    "        sent_repr = [(word, start, end, tags[tags_without_labels.index((word, start, end))][3],\n",
    "                     tags[tags_without_labels.index((word, start, end))][4])\n",
    "           if (word, start, end) in tags_without_labels \n",
    "          else (word, start, end, 0, 0.0) for word, index, start, end in all_tokens]\n",
    "        if lowercase:\n",
    "            sent_repr = [(word.lower(), start, end, binary, prob) \n",
    "                         for word, start, end, binary, prob in sent_repr]\n",
    "        if filter_punc:\n",
    "            sent_repr = list(filter(lambda vals : remove_punctuation(vals[0]), sent_repr))\n",
    "        if filtering:\n",
    "            sent_repr = list(filter(lambda vals : vals[0] != \"'s\", sent_repr))\n",
    "            sent_repr = list(filter(lambda vals : vals[0] != \"``\", sent_repr))\n",
    "        sentence_ids.append(sent_id)\n",
    "        sentences.append([vals[0] for vals in sent_repr])\n",
    "        binaries.append([vals[3] for vals in sent_repr])\n",
    "        probabilities.append([vals[4] for vals in sent_repr])\n",
    "        start_ends.append([(vals[1], vals[2]) for vals in sent_repr])\n",
    "    return sentence_ids, sentences, start_ends, binaries, probabilities\n",
    "\n",
    "\n",
    "def forward_transformation_v2(dataframe, start_tag = 's_target', end_tag = 'e_target',\n",
    "                                   lowercase = True, filter_punc = True, filtering = \"a132\"):\n",
    "    '''\n",
    "    Transforms the provided dataframe rows into a representation\n",
    "    to be used in a sequence classifier. Here, each row in the dataframe\n",
    "    is transformed into one instance to classify and each target in a \n",
    "    sentence is tagged by surrounding it with the provided start and\n",
    "    end tag. Hence, the whole sequence is classified at once. Returns\n",
    "    the transformed sentences (one for each row in the dataframe), the\n",
    "    corresponding binary and probabilitiy label.\n",
    "    '''\n",
    "    sentences = []\n",
    "    binaries = []\n",
    "    probabilities = []\n",
    "    for index, row in dataframe.iterrows():\n",
    "        sentence = row['sentence']\n",
    "        start = row['start']\n",
    "        end = row['end']\n",
    "        sentence = sentence[:start] + start_tag + ' ' + sentence[start:end] + \\\n",
    "                             ' ' + end_tag + sentence[end:]\n",
    "        if lowercase:\n",
    "            sentence = sentence.lower()\n",
    "        sent_repr = all_tokens_with_index(sentence)\n",
    "        if filter_punc:\n",
    "            sent_repr = list(filter(lambda vals : remove_punctuation(vals[0]), sent_repr))\n",
    "        if filtering:\n",
    "            sent_repr = list(filter(lambda vals : vals[0] != \"'s\", sent_repr))\n",
    "            sent_repr = list(filter(lambda vals : vals[0] != \"``\", sent_repr))\n",
    "        sentences.append([word for word, index, start, end in sent_repr])\n",
    "        binaries.append(row['binary'])\n",
    "        probabilities.append(row['prob'])\n",
    "    return sentences, binaries, probabilities\n",
    "\n",
    "\n",
    "def merge_train_test_dataset(dataset):\n",
    "    '''\n",
    "    Computes the training and test set size as (1) the\n",
    "    number of rows and (2) the number of unique sentences\n",
    "    contained. Merges the training and test set and\n",
    "    computes the same values for the merged dataframe.\n",
    "    This can be used before creating the vocabulary,\n",
    "    to also not miss vocabulary contained solely in the\n",
    "    test set. Before training, the dataset has to be split\n",
    "    up again based on the provided numbers.\n",
    "    '''\n",
    "    # Compute num rows and sents for train and test\n",
    "    train_num_rows = dataset.train.shape[0]\n",
    "    train_num_sents = len(list(set(dataset.train.sentence.values.tolist())))\n",
    "    test_num_rows = dataset.test.shape[0]\n",
    "    test_num_sents = len(list(set(dataset.test.sentence.values.tolist())))\n",
    "    # Merge dataframes and compute same values\n",
    "    dataset_merged = dataset.train.append(dataset.test)\n",
    "    dataset_merged['sent_id'] = dataset_merged.groupby('sentence').ngroup()\n",
    "    dataset_num_rows = dataset_merged.shape[0]\n",
    "    dataset_num_sents = len(list(set(dataset_merged.sentence.values.tolist())))\n",
    "    print('----------------------')\n",
    "    print('# Rows train : {}'.format(train_num_rows))\n",
    "    print('# Rows test : {}'.format(test_num_rows))\n",
    "    print('# Rows dataset : {}'.format(dataset_num_rows))\n",
    "    print('----------------------')\n",
    "    print('# Sents train : {}'.format(train_num_sents))\n",
    "    print('# Sents test : {}'.format(test_num_sents))\n",
    "    print('# Sents dataset : {}'.format(dataset_num_sents))\n",
    "    print('----------------------')\n",
    "    return dataset_merged, train_num_rows, test_num_rows, \\\n",
    "                dataset_num_rows, train_num_sents, test_num_sents, \\\n",
    "                dataset_num_sents\n",
    "        \n",
    "def split_train_test_dataset(dataset, index):\n",
    "    '''\n",
    "    Splits the given dataset (here the list of sentences represented\n",
    "    as list of words, binary label lists etc.) into train and test\n",
    "    based on the given index. Depending on the used sequence representation\n",
    "    (v1 or v2), the index should be either the number of training sentences \n",
    "    (unique) or the number of training rows.\n",
    "    '''\n",
    "    train = dataset[:index]\n",
    "    test = dataset[index:]\n",
    "    print('Training set length : {}'.format(len(train)))\n",
    "    print('Test set length : {}'.format(len(test)))\n",
    "    return train, test\n",
    "        \n",
    "def sentence_max_length(sentences):\n",
    "    sent_lens = [len(sentence) for sentence in sentences]\n",
    "    sent_max_length = np.max(sent_lens)\n",
    "    print('Max length sentence : {}'.format(sent_max_length))\n",
    "    return sent_max_length\n",
    "\n",
    "def pad_encode_sequences(sentences, word2index, missing, max_length):\n",
    "    '''\n",
    "    Encodes a list of sentences (a sentence represented as a list of words)\n",
    "    into their corresponding integer index based on the provided dictionary.\n",
    "    For missing tokens in the dictionary, applies the provided 'missing' value.\n",
    "    Afterwards, applies padding to the sentences and uses 'max_length' as the\n",
    "    maximum overall sentence length.\n",
    "    '''\n",
    "    words_with_indices = [[word2index.get(word, missing)\n",
    "                        for word in sent] for sent in sentences]\n",
    "    pad_val = word2index['_pad_']\n",
    "    return pad_sequences(maxlen=max_length, \\\n",
    "            sequences=words_with_indices, padding=\"post\", value=pad_val)\n",
    "\n",
    "def pad_binaries_probs(binaries, probabilities, max_length):\n",
    "    '''\n",
    "    Applies padding to binary labels and the probabilities.\n",
    "    Parameter 'max_length' is used as the maximum length\n",
    "    to pad the two provided lists to.\n",
    "    '''\n",
    "    binary_padded = pad_sequences(maxlen=max_length, sequences=binaries, padding=\"post\", value=0)\n",
    "    prob_padded = pad_sequences(maxlen=max_length, sequences=probabilities, padding=\"post\", value=0, dtype=\"float\")\n",
    "    return binary_padded, prob_padded\n",
    "\n",
    "def pad_chars(sentences, char2index, sent_max_length, char_max_length):\n",
    "    words_as_chars_indices = []\n",
    "    for sent in sentences:\n",
    "        sent_repr = []\n",
    "        for sent_index in range(sent_max_length):\n",
    "            words_repr = []\n",
    "            for word_index in range(char_max_length):\n",
    "                try:\n",
    "                    words_repr.append(char2index[sent[sent_index][word_index]])\n",
    "                except:\n",
    "                    words_repr.append(char2index.get('_pad_'))\n",
    "            sent_repr.append(words_repr)\n",
    "        words_as_chars_indices.append(sent_repr)\n",
    "    return words_as_chars_indices\n",
    "\n",
    "def ngram_prediction_agg_majority_vote(predictions):\n",
    "    positive_sum = np.sum(predictions)\n",
    "    ratio = positive_sum / len(predictions)\n",
    "    return int(ratio + 0.5)\n",
    "\n",
    "def ngram_prediction_agg_max(predictions):\n",
    "    return np.max(predictions)\n",
    "\n",
    "def ngram_prediction_agg_begin(predictions):\n",
    "    return predictions[0]\n",
    "\n",
    "def ngram_prediction_agg_end(predictions):\n",
    "    return predictions[-1]\n",
    "\n",
    "def backward_transformation_v1(dataset, sent_ids, test_words_padded, \n",
    "                    index2word, test_start_end, final_predictions):\n",
    "    '''\n",
    "    Based on the dataset, the list of sentence_ids, the padded words\n",
    "    from the test set, the start and end tuple list of the test set and\n",
    "    the produced v1 predictions, maps the predictions back to original\n",
    "    representation (from number of sentences to number of rows) and\n",
    "    computes the F1-score between predictions and labels of the orignal\n",
    "    representation. For n-gram targets, applies an aggregation strategy.\n",
    "    '''\n",
    "    ngram_prediction_agg = ngram_prediction_agg_max\n",
    "    results = []\n",
    "    num_missing_match = []\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    count=0\n",
    "    sent_count = 0\n",
    "    for ind, sent in enumerate(test_words_padded):\n",
    "        sent_id = sent_ids[ind]\n",
    "        ses = test_start_end[ind]\n",
    "        preds = final_predictions[ind]\n",
    "        selected = dataset.loc[dataset.sent_id==sent_id,]\n",
    "        targets = selected.target.values.tolist()\n",
    "        start_ends = list(zip(selected.start.values.tolist(), selected.end.values.tolist()))\n",
    "        binary_y = selected.binary.values.tolist()\n",
    "        for label_index, (start, end) in enumerate(start_ends):\n",
    "            matching_indices = [i for i, (s, e) in enumerate(ses) if overlaps(start, end, s, e)]\n",
    "            if not matching_indices:\n",
    "                num_missing_match.append((sent_id, (start,end), ses, sent))\n",
    "                prediction = 1\n",
    "            matching_predictions = [preds[i] for i in matching_indices]\n",
    "            if len(matching_predictions)>1:\n",
    "                prediction = ngram_prediction_agg(matching_predictions)\n",
    "            else:\n",
    "                if matching_indices:\n",
    "                    prediction = matching_predictions[0]\n",
    "            matching_labels = binary_y[label_index]\n",
    "            all_labels.append(matching_labels)\n",
    "            all_predictions.append(prediction)\n",
    "    f1 = f1_score(all_labels, all_predictions)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_datasets(datasets):\n",
    "    return [Dataset(ds.name, preprocessing(ds.train), \n",
    "                             preprocessing(ds.test)) \n",
    "                             for ds in datasets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2.2) DL Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2.2.1) Utility Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define some Keras Metric classes, in order to introduce some custom metrics during training. Basically, we employ F1 and compute the F1 on the training and on the validation set after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import keras.callbacks\n",
    "\n",
    "class MetricsV2(keras.callbacks.Callback):\n",
    "    def __init__(self, training_data, show_test_data=True):\n",
    "        self.train_scores = []\n",
    "        self.test_scores = []\n",
    "        self.training_data = training_data\n",
    "        self.show_test_data = show_test_data\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        # Compute the F1-score for the test data\n",
    "        test_label = self.validation_data[1]\n",
    "        test_label = np.array(test_label)\n",
    "        test_label = np.argmax(test_label, axis = 1)\n",
    "        test_predict = np.asarray(self.model.predict(self.validation_data[0]))\n",
    "        test_predict = np.argmax(test_predict, axis = 1)\n",
    "        test_scores = precision_recall_fscore_support(test_label, test_predict)\n",
    "        self.test_scores.append(test_scores)\n",
    "        # Compute the F1-score for the training data\n",
    "        train_label = self.training_data[1]\n",
    "        train_label = np.array(train_label)\n",
    "        train_label = np.argmax(train_label, axis = 1)\n",
    "        train_predict = np.asarray(self.model.predict(self.training_data[0]))\n",
    "        train_predict = np.argmax(train_predict, axis = 1)\n",
    "        train_scores = precision_recall_fscore_support(train_label, train_predict)\n",
    "        self.train_scores.append(train_scores)\n",
    "        print('Training P : {}, R : {}, F1 : {}'.format(train_scores[0][1], \\\n",
    "                                train_scores[1][1], train_scores[2][1]))\n",
    "        print('Testing P : {}, R : {}, F1 : {}'.format(test_scores[0][1], \\\n",
    "                                test_scores[1][1], test_scores[2][1]))\n",
    "        if self.show_test_data:\n",
    "            print('--------------------Targets-------------------------')\n",
    "            print(test_label)\n",
    "            print('--------------------Predictions-------------------------')\n",
    "            print(test_predict)\n",
    "    \n",
    "class MetricsV1(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, training_data, show_test_data=True):\n",
    "        self.train_scores = []\n",
    "        self.test_scores = []\n",
    "        self.training_data = training_data\n",
    "        self.show_test_data = show_test_data\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        # Compute the F1-score for the test data\n",
    "        test_label = self.validation_data[1]\n",
    "        test_label = np.array(test_label)\n",
    "        test_shape = test_label.shape\n",
    "        test_label = test_label.reshape((test_shape[0]*test_shape[1], test_shape[2]))\n",
    "        test_label = np.argmax(test_label, axis = 1)\n",
    "        test_predict = np.asarray(self.model.predict(self.validation_data[0]))\n",
    "        test_predict = test_predict.reshape((test_shape[0]*test_shape[1]), test_shape[2])\n",
    "        test_predict = np.argmax(test_predict, axis = 1)\n",
    "        test_scores = precision_recall_fscore_support(test_label, test_predict)\n",
    "        self.test_scores.append(test_scores)\n",
    "        # Compute the F1-score for the training data\n",
    "        train_label = self.training_data[1]\n",
    "        train_label = np.array(train_label)\n",
    "        train_shape = train_label.shape\n",
    "        train_label = train_label.reshape((train_shape[0]*train_shape[1], train_shape[2]))\n",
    "        train_label = np.argmax(train_label, axis = 1)\n",
    "        train_predict = np.asarray(self.model.predict(self.training_data[0]))\n",
    "        train_predict = train_predict.reshape((train_shape[0]*train_shape[1]), train_shape[2])\n",
    "        train_predict = np.argmax(train_predict, axis = 1)\n",
    "        train_scores = precision_recall_fscore_support(train_label, train_predict)\n",
    "        self.train_scores.append(train_scores)\n",
    "        print('Training P : {}, R : {}, F1 : {}'.format(train_scores[0][1], \\\n",
    "                                train_scores[1][1], train_scores[2][1]))\n",
    "        print('Testing P : {}, R : {}, F1 : {}'.format(test_scores[0][1], \\\n",
    "                                test_scores[1][1], test_scores[2][1]))\n",
    "        if self.show_test_data:\n",
    "            print('--------------------Targets-------------------------')\n",
    "            print(test_label)\n",
    "            print('--------------------Predictions-------------------------')\n",
    "            print(test_predict)\n",
    "            \n",
    "class MetricsV1Char(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, training_data, show_test_data=True):\n",
    "        self.train_scores = []\n",
    "        self.test_scores = []\n",
    "        self.training_data = training_data\n",
    "        self.show_test_data = show_test_data\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        # Compute the F1-score for the test data\n",
    "        test_label = self.validation_data[2]\n",
    "        test_label = np.array(test_label)\n",
    "        test_shape = test_label.shape\n",
    "        test_label = test_label.reshape((test_shape[0]*test_shape[1], test_shape[2]))\n",
    "        test_label = np.argmax(test_label, axis = 1)\n",
    "        test_predict = np.asarray(self.model.predict([self.validation_data[0], self.validation_data[1]]))\n",
    "        print(test_predict.shape)\n",
    "        test_predict = test_predict.reshape((test_shape[0]*test_shape[1]), test_shape[2])\n",
    "        test_predict = np.argmax(test_predict, axis = 1)\n",
    "        test_scores = precision_recall_fscore_support(test_label, test_predict)\n",
    "        self.test_scores.append(test_scores)\n",
    "        # Compute the F1-score for the training data\n",
    "        train_label = self.training_data[1]\n",
    "        train_label = np.array(train_label)\n",
    "        train_shape = train_label.shape\n",
    "        train_label = train_label.reshape((train_shape[0]*train_shape[1], train_shape[2]))\n",
    "        train_label = np.argmax(train_label, axis = 1)\n",
    "        train_predict = np.asarray(self.model.predict(self.training_data[0]))\n",
    "        train_predict = train_predict.reshape((train_shape[0]*train_shape[1]), train_shape[2])\n",
    "        train_predict = np.argmax(train_predict, axis = 1)\n",
    "        train_scores = precision_recall_fscore_support(train_label, train_predict)\n",
    "        self.train_scores.append(train_scores)\n",
    "        print('Training P : {}, R : {}, F1 : {}'.format(train_scores[0][1], \\\n",
    "                                train_scores[1][1], train_scores[2][1]))\n",
    "        print('Testing P : {}, R : {}, F1 : {}'.format(test_scores[0][1], \\\n",
    "                                test_scores[1][1], test_scores[2][1]))\n",
    "        if self.show_test_data:\n",
    "            print('--------------------Targets-------------------------')\n",
    "            print(test_label)\n",
    "            print('--------------------Predictions-------------------------')\n",
    "            print(test_predict)\n",
    "            \n",
    "\n",
    "class MetricsElmoV1(keras.callbacks.Callback):\n",
    "    '''\n",
    "    Make sure to use 'sparse_categorical_crossentropy' as the loss function.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, training_data, batch_size, show_test_data=True):\n",
    "        self.train_scores = []\n",
    "        self.test_scores = []\n",
    "        self.training_data = training_data\n",
    "        self.show_test_data = show_test_data\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        # Compute the F1-score for the test data\n",
    "        test_label = self.validation_data[1]\n",
    "        test_label = np.array(test_label)\n",
    "        test_shape = test_label.shape\n",
    "        print('ELMo Metric Test Prediction')\n",
    "        print(test_label.shape)\n",
    "        test_label = test_label.reshape((test_shape[0]*test_shape[1]))\n",
    "        test_predict = prediction_elmo_model(self.validation_data[0], self.model, self.batch_size)\n",
    "        print(test_predict.shape)\n",
    "        test_predict = test_predict.reshape((test_shape[0]*test_shape[1]))\n",
    "        test_scores = precision_recall_fscore_support(test_label, test_predict)\n",
    "        self.test_scores.append(test_scores)\n",
    "        # Compute the F1-score for the training data\n",
    "        train_label = self.training_data[1]\n",
    "        train_label = np.array(train_label)\n",
    "        train_shape = train_label.shape\n",
    "        train_label = train_label.reshape((train_shape[0]*train_shape[1]))\n",
    "        print('ELMo Metric Training Prediction')\n",
    "        train_predict = prediction_elmo_model(self.training_data[0], self.model, self.batch_size)\n",
    "        train_predict = train_predict.reshape((train_shape[0]*train_shape[1]))\n",
    "        train_scores = precision_recall_fscore_support(train_label, train_predict)\n",
    "        self.train_scores.append(train_scores)\n",
    "        print('Training P : {}, R : {}, F1 : {}'.format(train_scores[0][1], \\\n",
    "                                train_scores[1][1], train_scores[2][1]))\n",
    "        print('Testing P : {}, R : {}, F1 : {}'.format(test_scores[0][1], \\\n",
    "                                test_scores[1][1], test_scores[2][1]))\n",
    "        if self.show_test_data:\n",
    "            print('--------------------Targets-------------------------')\n",
    "            print(test_label)\n",
    "            print('--------------------Predictions-------------------------')\n",
    "            print(test_predict)\n",
    "            \n",
    "class MetricsElmoWembedsV1(keras.callbacks.Callback):\n",
    "    '''\n",
    "    Make sure to use 'sparse_categorical_crossentropy' as the loss function.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, training_data, batch_size, show_test_data=True):\n",
    "        self.train_scores = []\n",
    "        self.test_scores = []\n",
    "        self.training_data = training_data\n",
    "        self.show_test_data = show_test_data\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        # Compute the F1-score for the test data\n",
    "        test_label = self.validation_data[2]\n",
    "        test_label = np.array(test_label)\n",
    "        test_shape = test_label.shape\n",
    "        print('ELMo Metric Test Prediction')\n",
    "        print(test_label.shape)\n",
    "        test_label = test_label.reshape((test_shape[0]*test_shape[1]))\n",
    "        print(test_label.shape)\n",
    "        print(np.array(self.validation_data).shape)\n",
    "        test_predict = prediction_elmo_wembeds_model([self.validation_data[0], \n",
    "                                    self.validation_data[1]], self.model, self.batch_size)\n",
    "        print('Test predictions')\n",
    "        print(test_predict.shape)\n",
    "        test_predict = test_predict.reshape((test_shape[0]*test_shape[1]))\n",
    "        print('shape test predict {}'.format(test_predict.shape))\n",
    "        print('shape test label {}'.format(len(test_label)))\n",
    "        test_scores = precision_recall_fscore_support(test_label, test_predict)\n",
    "        self.test_scores.append(test_scores)\n",
    "        # Compute the F1-score for the training data\n",
    "        train_label = self.training_data[1]\n",
    "        train_label = np.array(train_label)\n",
    "        train_shape = train_label.shape\n",
    "        print('training : {}'.format(train_shape))\n",
    "        train_label = train_label.reshape((train_shape[0]*train_shape[1]))\n",
    "        print('ELMo Metric Training Prediction')\n",
    "        train_predict = prediction_elmo_wembeds_model(self.training_data[0], self.model, self.batch_size)\n",
    "        train_predict = train_predict.reshape((train_shape[0]*train_shape[1]))\n",
    "        train_scores = precision_recall_fscore_support(train_label, train_predict)\n",
    "        self.train_scores.append(train_scores)\n",
    "        print('Training P : {}, R : {}, F1 : {}'.format(train_scores[0][1], \\\n",
    "                                train_scores[1][1], train_scores[2][1]))\n",
    "        print('Testing P : {}, R : {}, F1 : {}'.format(test_scores[0][1], \\\n",
    "                                test_scores[1][1], test_scores[2][1]))\n",
    "        if self.show_test_data:\n",
    "            print('--------------------Targets-------------------------')\n",
    "            print(test_label)\n",
    "            print('--------------------Predictions-------------------------')\n",
    "            print(test_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ELMo based models require the prediction to use the same batch size as in training. This function computes predictions based on the provided batch size and handles test sets which sizes are not divisible by the batch size without a remaining part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_elmo_model(data, model, batch_size):\n",
    "    '''\n",
    "    Returns the predictions of an ELMo Keras model for the\n",
    "    provided data and batch_size. If required, extends the\n",
    "    data to make it divisible by batch_size in order to\n",
    "    work with the ELMo model and cuts of the extension.\n",
    "    Creates predictions iteratively for batch_size large\n",
    "    fractions of the provided data.\n",
    "    '''\n",
    "    data = np.array(data.copy())\n",
    "    # Ensure that the data has number of samples\n",
    "    # which is divisible by the batch size\n",
    "    add_index = data.shape[0] % batch_size\n",
    "    data_ext = np.concatenate((data, data[:add_index]))\n",
    "    # Get the predictions of the elmo model on the \n",
    "    # extended data\n",
    "    predictions = None\n",
    "    print('ELMo Prediction')\n",
    "    print('Data shape {}'.format(data.shape))\n",
    "    print('Data ext. shape {}'.format(data_ext.shape))\n",
    "    num_samples = data_ext.shape[0]\n",
    "    for index in range(0, int((num_samples/batch_size))):\n",
    "        print((index*batch_size),((index+1)*batch_size))\n",
    "        test_words_batch = data_ext[(index*batch_size):((index+1)*batch_size)]\n",
    "        prediction = model.predict(test_words_batch)\n",
    "        final_prediction = np.argmax(prediction, axis = -1)\n",
    "        if predictions is None:\n",
    "            predictions = final_prediction\n",
    "        else:\n",
    "            predictions = np.concatenate((predictions, final_prediction))\n",
    "    return predictions[:data.shape[0]]\n",
    "\n",
    "def prediction_elmo_wembeds_model(data, model, batch_size):\n",
    "    '''\n",
    "    Returns the predictions of an ELMo Keras model for the\n",
    "    provided data and batch_size. If required, extends the\n",
    "    data to make it divisible by batch_size in order to\n",
    "    work with the ELMo model and cuts of the extension.\n",
    "    Creates predictions iteratively for batch_size large\n",
    "    fractions of the provided data.\n",
    "    '''\n",
    "    print('input shape to elmo predictor {} {}'.format(data[0].shape, data[1].shape))\n",
    "    elmo_data = data[0].copy()\n",
    "    integer_data = data[1].copy()\n",
    "    # Ensure that the data has number of samples\n",
    "    # which is divisible by the batch size\n",
    "    add_index = elmo_data.shape[0] % batch_size\n",
    "    elmo_data_ext = np.concatenate((elmo_data, elmo_data[:add_index]))\n",
    "    integer_data_ext = np.concatenate((integer_data, integer_data[:add_index]))\n",
    "    # Get the predictions of the elmo model on the \n",
    "    # extended data\n",
    "    predictions = None\n",
    "    print('ELMo Prediction')\n",
    "    print('Data shape elmo {}'.format(elmo_data.shape))\n",
    "    print('Data shape integer {}'.format(integer_data.shape))\n",
    "    print('Data ext. elmo shape {}'.format(elmo_data_ext.shape))\n",
    "    print('Data ext. integer shape {}'.format(integer_data_ext.shape))\n",
    "    num_samples = elmo_data_ext.shape[0]\n",
    "    for index in range(0, int((num_samples/batch_size))):\n",
    "        print((index*batch_size),((index+1)*batch_size))\n",
    "        test_words_batch_elmo = elmo_data_ext[(index*batch_size):((index+1)*batch_size)]\n",
    "        test_words_batch_integer = integer_data_ext[(index*batch_size):((index+1)*batch_size)]\n",
    "        prediction = model.predict([test_words_batch_elmo, test_words_batch_integer])\n",
    "        final_prediction = np.argmax(prediction, axis = -1)\n",
    "        if predictions is None:\n",
    "            predictions = final_prediction\n",
    "        else:\n",
    "            predictions = np.concatenate((predictions, final_prediction))\n",
    "    return predictions[:elmo_data.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'check': {'configs': [Configuration(name='ELMo-r-BiLSTM-e', params={'epoch': 1}, train_scores=[(array([0.95951417, 0.23943662]), array([0.81443299, 0.62962963]), array([0.88104089, 0.34693878]), array([291,  27], dtype=int64))], test_scores=[(array([0.95      , 0.21212121]), array([0.42222222, 0.875     ]), array([0.58461538, 0.34146341]), array([90, 16], dtype=int64))], f1=0.6)],\n",
       "  'best': Configuration(name='ELMo-r-BiLSTM-e', params={'epoch': 1}, train_scores=[(array([0.95951417, 0.23943662]), array([0.81443299, 0.62962963]), array([0.88104089, 0.34693878]), array([291,  27], dtype=int64))], test_scores=[(array([0.95      , 0.21212121]), array([0.42222222, 0.875     ]), array([0.58461538, 0.34146341]), array([90, 16], dtype=int64))], f1=0.6)}}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elmo_r_bilstm_e_params_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2.2.2) Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using C:\\Users\\Studio\\AppData\\Local\\Temp\\tfhub_modules to cache modules.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from keras import backend as K\n",
    "\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "\n",
    "elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.tables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model dataset[0] elmo batch=2 epoch=1 f1=0.7632978723404255\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Input\n",
    "from keras.layers.merge import add, concatenate\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, \\\n",
    "        Bidirectional, Lambda, average, SpatialDropout1D, Reshape, \\\n",
    "        Flatten, Conv2D, MaxPooling2D, Conv1D, GlobalMaxPooling1D\n",
    "from attention_keras import AttentionLayer\n",
    "from keras_contrib.layers import CRF\n",
    "\n",
    "def model_v1_LSTM(sent_max_length, vocab_size, dimension, embedding, units=40):\n",
    "    in_seq = Input(shape=(sent_max_length,), name='in')\n",
    "    embed = Embedding(input_dim=vocab_size, output_dim=dimension, \\\n",
    "                  weights=[embedding], input_length=sent_max_length, name='em')(in_seq)\n",
    "    drop = Dropout(0.1, name='dr')(embed)\n",
    "    lstm = LSTM(units=units, return_sequences=True, recurrent_dropout=0.1, name='ls')(drop)\n",
    "    output = TimeDistributed(Dense(2, activation=\"softmax\", name='d'), name='td')(lstm)\n",
    "    return Model(in_seq, output)\n",
    "\n",
    "def model_v1_BiLSTM(sent_max_length, vocab_size, dimension, embedding, units=20):\n",
    "    in_seq = Input(shape=(sent_max_length,), name='in')\n",
    "    embed = Embedding(input_dim=vocab_size, output_dim=dimension, \\\n",
    "                  weights=[embedding], input_length=sent_max_length, name='em')(in_seq)\n",
    "    drop = Dropout(0.1, name='dr')(embed)\n",
    "    lstm = Bidirectional(LSTM(units=units, return_sequences=True, recurrent_dropout=0.1, name='ls'), name='bi')(drop)\n",
    "    output = TimeDistributed(Dense(2, activation=\"softmax\", name='ds'), name='ts')(lstm)\n",
    "    return Model(in_seq, output)\n",
    "\n",
    "def model_v1_char_BiLSTM(sent_max_length, char_max_length, word_embedding, \n",
    "                        char_embedding, units_char=25, units_word=40):\n",
    "    # (1.1) Word input parameters\n",
    "    word_vocab_size = word_embedding.shape[0]\n",
    "    word_dimension = word_embedding.shape[1]\n",
    "    # (1.2) Word embedding layers\n",
    "    word_in = Input(shape=(sent_max_length,), name='inw')\n",
    "    word_embed = Embedding(input_dim=word_vocab_size, output_dim=word_dimension, \\\n",
    "                      weights=[word_embedding], input_length=sent_max_length, name='emw')(word_in)\n",
    "    # (2.1) Char input parameters\n",
    "    char_vocab_size = char_embedding.shape[0]\n",
    "    char_dimension = char_embedding.shape[1]\n",
    "    # (2.2) Char embedding layers\n",
    "    char_in = Input(shape=(sent_max_length, char_max_length), name='inc')\n",
    "    char_embed = TimeDistributed(Embedding(input_dim=char_vocab_size, output_dim=char_dimension, \\\n",
    "                        weights=[char_embedding], input_length=char_max_length, \\\n",
    "                        mask_zero=True,name='emc'), name='tsce')(char_in)\n",
    "    char_lstm = TimeDistributed(Bidirectional(LSTM(units=units_char, return_sequences=False, \n",
    "                                recurrent_dropout=0.1,name='lsc'), name='bic'),name='tscls')(char_embed)\n",
    "    concatenation = concatenate([word_embed, char_lstm], name='co')\n",
    "    drop1 = SpatialDropout1D(0.3, name='sp1')(concatenation)\n",
    "    lstm = Bidirectional(LSTM(units=units_word, return_sequences=True,\\\n",
    "                            recurrent_dropout=0.1, name='ls'), name='bi')(drop1)\n",
    "    drop2 = SpatialDropout1D(0.3, name='sp2')(lstm)\n",
    "    out = TimeDistributed(Dense(2, activation=\"softmax\", name='ds'), name='ts')(drop2) \n",
    "    return Model([word_in, char_in], out)\n",
    "\n",
    "\n",
    "def model_v1_ElMo_residual_BiLSTM(sent_max_length, batch_size):\n",
    "    def ElmoEmbedding(x):\n",
    "        return elmo_model(inputs={\n",
    "                                \"tokens\": tf.squeeze(tf.cast(x, tf.string)),\n",
    "                                \"sequence_len\": tf.constant(batch_size*[sent_max_length])\n",
    "                          },\n",
    "                          signature=\"tokens\",\n",
    "                          as_dict=True)[\"elmo\"]\n",
    "    in_seq = Input(shape=(sent_max_length,), dtype=tf.string, name='in')\n",
    "    embed = Lambda(ElmoEmbedding, output_shape=(sent_max_length, 1024), name='la')(in_seq)\n",
    "    lstm_1 = Bidirectional(LSTM(units=512, return_sequences=True,\n",
    "                           recurrent_dropout=0.2, dropout=0.2, name='ls1'), name='bi1')(embed)\n",
    "    lstm_2 = Bidirectional(LSTM(units=512, return_sequences=True,\n",
    "                               recurrent_dropout=0.2, dropout=0.2, name='ls2'), name='bi2')(lstm_1)\n",
    "    lstm_1 = add([lstm_1, lstm_2])\n",
    "    out = TimeDistributed(Dense(2, activation=\"softmax\", name='ds'), name='ts')(lstm_1)\n",
    "    return Model(in_seq, out)\n",
    "\n",
    "def model_v1_ElMo_residual_BiLSTM_h(sent_max_length, batch_size):\n",
    "    def ElmoEmbedding(x):\n",
    "        return elmo_model(inputs={\n",
    "                                \"tokens\": tf.squeeze(tf.cast(x, tf.string)),\n",
    "                                \"sequence_len\": tf.constant(batch_size*[sent_max_length])\n",
    "                          },\n",
    "                          signature=\"tokens\",\n",
    "                          as_dict=True)[\"elmo\"]\n",
    "    in_seq = Input(shape=(sent_max_length,), dtype=tf.string, name='in')\n",
    "    embed = Lambda(ElmoEmbedding, output_shape=(sent_max_length, 1024), name='la')(in_seq)\n",
    "    lstm_1 = Bidirectional(LSTM(units=512, return_sequences=True,\n",
    "                           recurrent_dropout=0.2, dropout=0.2, name='ls1'), name='bi1')(embed)\n",
    "    lstm_2 = Bidirectional(LSTM(units=512, return_sequences=True,\n",
    "                               recurrent_dropout=0.2, dropout=0.2, name='ls2'), name='bi2')(lstm_1)\n",
    "    lstm_1 = add([lstm_1, lstm_2])\n",
    "    concat = concatenate([lstm_1, embed], name='co')\n",
    "    out = TimeDistributed(Dense(2, activation=\"softmax\", name='ds'), name='ts')(concat)\n",
    "    return Model(in_seq, out)\n",
    "\n",
    "def model_v1_ElMo_residual_BiLSTM_wembeds(sent_max_length, batch_size, \n",
    "                                    vocab_size, dimension, embedding):\n",
    "    def ElmoEmbedding(x):\n",
    "        return elmo_model(inputs={\n",
    "                                \"tokens\": tf.squeeze(tf.cast(x, tf.string)),\n",
    "                                \"sequence_len\": tf.constant(batch_size*[sent_max_length])\n",
    "                          },\n",
    "                          signature=\"tokens\",\n",
    "                          as_dict=True)[\"elmo\"]\n",
    "    # ELMo embedding input\n",
    "    in_elmo = Input(shape=(sent_max_length,), dtype=tf.string, name='ine')\n",
    "    embed_elmo = Lambda(ElmoEmbedding, output_shape=(sent_max_length, 1024), name='la')(in_elmo)\n",
    "    # Word embedding input\n",
    "    in_w = Input(shape=(sent_max_length,), name='inw')\n",
    "    embed_w = Embedding(input_dim=vocab_size, output_dim=dimension, \\\n",
    "                  weights=[embedding], input_length=sent_max_length, name='em')(in_w)\n",
    "    drop = Dropout(0.1, name='dr')(embed_w)\n",
    "    # Concatenate context-independent (pretrained wembed) \n",
    "    # and context-dependent (ELMo) embeddings\n",
    "    concat = concatenate([drop, embed_elmo], name='co')\n",
    "    lstm_1 = Bidirectional(LSTM(units=512, return_sequences=True,\n",
    "                           recurrent_dropout=0.2, dropout=0.2, name='ls1'), name='bi1')(concat)\n",
    "    lstm_2 = Bidirectional(LSTM(units=512, return_sequences=True,\n",
    "                               recurrent_dropout=0.2, dropout=0.2, name='ls2'), name='bi2')(lstm_1)\n",
    "    lstm_1 = add([lstm_1, lstm_2])\n",
    "    out = TimeDistributed(Dense(2, activation=\"softmax\", name='ds'), name='ts')(lstm_1)\n",
    "    return Model([in_elmo, in_w], out)\n",
    "\n",
    "def model_v1_BiLSTM_CRF(sent_max_length, vocab_size, dimension, embedding, units):\n",
    "    in_seq = Input(shape=(sent_max_length,), name='in')\n",
    "    embed = Embedding(input_dim=vocab_size, output_dim=dimension, \\\n",
    "                  weights=[embedding], input_length=sent_max_length, name='em')(in_seq)\n",
    "    drop = Dropout(0.1, name='dr')(embed)\n",
    "    lstm = Bidirectional(LSTM(units=units, return_sequences=True, recurrent_dropout=0.1, name='ls'), name='bi')(drop)\n",
    "    dense = Dense(30, activation=\"relu\", name='ds')(lstm)\n",
    "    crf = CRF(2, learn_mode='marginal', name='crf')\n",
    "    output = crf(dense)\n",
    "    return Model(in_seq, output)\n",
    "\n",
    "def model_v2_LSTM(sent_max_length, vocab_size, dimension, embedding, units=40):\n",
    "    in_seq = Input(shape=(sent_max_length,))\n",
    "    embed = Embedding(input_dim=vocab_size, output_dim=dimension, \\\n",
    "                  weights=[embedding], input_length=sent_max_length)(in_seq)\n",
    "    drop = Dropout(0.1)(embed)\n",
    "    lstm = LSTM(units=units, return_sequences=False, recurrent_dropout=0.1)(drop)\n",
    "    output = Dense(2, activation=\"softmax\")(lstm)\n",
    "    return Model(in_seq, output)\n",
    "\n",
    "def model_v2_BiLSTM(sent_max_length, vocab_size, dimension, embedding, units=20):\n",
    "    in_seq = Input(shape=(sent_max_length,))\n",
    "    embed = Embedding(input_dim=vocab_size, output_dim=dimension, \\\n",
    "                  weights=[embedding], input_length=sent_max_length)(in_seq)\n",
    "    drop = Dropout(0.1)(embed)\n",
    "    lstm = Bidirectional(LSTM(units=units, return_sequences=False, recurrent_dropout=0.1))(drop)\n",
    "    output = Dense(2, activation=\"softmax\")(lstm)\n",
    "    return Model(in_seq, output)\n",
    "\n",
    "def model_v2_attention_BiLSTM(sent_max_length, vocab_size, dimension, embedding, units=20):\n",
    "    in_seq = Input(shape=(sent_max_length,), name='in')\n",
    "    embed = Embedding(input_dim=vocab_size, output_dim=dimension, \\\n",
    "                  weights=[embedding], input_length=sent_max_length, name='em')(in_seq)\n",
    "    drop = Dropout(0.1, name='dr')(embed)\n",
    "    lstm = Bidirectional(LSTM(units=units, return_sequences=True, recurrent_dropout=0.1, name='ls'), name='bi')(drop)\n",
    "    attended = AttentionLayer(name='at')(lstm)\n",
    "    output = Dense(2, activation=\"softmax\", name='ds')(attended)\n",
    "    return Model(in_seq, output)\n",
    "\n",
    "def model_v2_simple_CNN(sent_max_length, vocab_size, dimension, embedding):\n",
    "    in_seq = Input(shape=(sent_max_length,))\n",
    "    embed = Embedding(input_dim=vocab_size, output_dim=dimension, \\\n",
    "                  weights=[embedding], input_length=sent_max_length)(in_seq)\n",
    "    embed_reshaped = Reshape((sent_max_length, dimension, 1))(embed)\n",
    "    conv2d = Conv2D(100, (5, dimension), activation='relu')(embed_reshaped)\n",
    "    max2d = MaxPooling2D((sent_max_length - 5 + 1, 1))(conv2d)\n",
    "    flatten = Flatten()(max2d)\n",
    "    drop = Dropout(0.5)(flatten)\n",
    "    out = Dense(2, activation='softmax')(drop)\n",
    "    return Model(in_seq, out)\n",
    "\n",
    "def model_v2_adv_CNN(sent_max_length, vocab_size, dimension, embedding, **kwargs):\n",
    "    in_seq = Input(shape=(sent_max_length,), name='in')\n",
    "    embed = Embedding(input_dim=vocab_size, output_dim=dimension, \\\n",
    "                      weights=[embedding], input_length=sent_max_length, name='em')(in_seq)\n",
    "    embed_reshaped = Reshape((sent_max_length, dimension, 1), name='re')(embed)\n",
    "    # First convolution and pooling layer\n",
    "    conv_1 = Conv1D(100, 5, activation='relu', name='cv1')(embed)\n",
    "    pool_1 = GlobalMaxPooling1D(name='po1')(conv_1)\n",
    "    #pool_1 = MaxPooling2D((sent_max_length - 5 + 1, 1))(conv_1)\n",
    "    # Second convolution and pooling layer\n",
    "    conv_2 = Conv1D(100, 4, activation='relu', name='cv2')(embed)\n",
    "    pool_2 = GlobalMaxPooling1D(name='po2')(conv_2)\n",
    "    #pool_2 = MaxPooling2D((sent_max_length - 4 + 1, 1))(conv_2)\n",
    "    # Third convolution and pooling layer\n",
    "    conv_3 = Conv1D(100, 3, activation='relu', name='cv3')(embed)\n",
    "    pool_3 = GlobalMaxPooling1D(name='po3')(conv_3)\n",
    "    #pool_3 = MaxPooling2D((sent_max_length - 3 + 1, 1))(conv_3)\n",
    "    # Concatenate all three layers\n",
    "    concat = concatenate([pool_1, pool_2, pool_3], name='co')\n",
    "    # Flatten concatenation\n",
    "    #flatten = Flatten()(concat)\n",
    "    drop = Dropout(0.5, name='dr')(concat)\n",
    "    out = Dense(2, activation='softmax', name='ds')(drop)\n",
    "    return Model(in_seq, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2.2.3) Models preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "def preparation_v1(dataset, model_func, embedding, epochs, batch_size, **kwargs):\n",
    "    '''\n",
    "    Provides wrapping of preprocessing and postprocessing\n",
    "    for the first variant of sequence classification v1 \n",
    "    (where complexity of each word is predicted individually)\n",
    "    based on the provided dataset, embedding and model_func.\n",
    "    The model_func is a function that should accept values\n",
    "    specifying the embedding layer (max_sentence_length, \n",
    "    vocab_dimension, embedding_weights) and return the \n",
    "    constructed Keras model instance. Returns the final test \n",
    "    F1-score, F1-scores of training and F1-scores\n",
    "    of test sets for each epoch.\n",
    "    '''\n",
    "    # Merge the train and test set in the dataset\n",
    "    dataframe_merged, train_num_rows, test_num_rows, \\\n",
    "                dataset_num_rows, train_num_sents, test_num_sents, \\\n",
    "                dataset_num_sents = merge_train_test_dataset(dataset)\n",
    "    # Forward transform the data represented in a dataframe into\n",
    "    # a sequence representation for classifying each word individually\n",
    "    sentence_ids, sentences, start_ends, \\\n",
    "        binaries, probabilties = forward_transformation_v1(dataframe_merged)\n",
    "    # Build the vocbulary and embedding\n",
    "    word2index, index2word, word_embedding = build_vocabulary(sentences, \\\n",
    "                                embedding, embedding.vector_size, missing='unique')\n",
    "    # Pad the sentences and labels\n",
    "    sent_max_length = sentence_max_length(sentences)\n",
    "    words_padded = pad_encode_sequences(sentences, word2index, '', sent_max_length)\n",
    "    binaries_padded, probabilities_padded = pad_binaries_probs(binaries, probabilties, sent_max_length)\n",
    "    binaries_padded_categorical = [to_categorical(clazz, num_classes=2) for clazz in binaries_padded]\n",
    "    # Instantiate the model\n",
    "    model = model_func(sent_max_length, len(word2index), embedding.vector_size, word_embedding, **kwargs)\n",
    "    model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "    # Split merged data into train and test again\n",
    "    train_words_padded, test_words_padded = split_train_test_dataset(words_padded, train_num_sents)\n",
    "    train_binaries_padded_cat, test_binaries_padded_cat = split_train_test_dataset(\\\n",
    "                    binaries_padded_categorical, train_num_sents)\n",
    "    print(len(train_words_padded))\n",
    "    print(len(test_words_padded))\n",
    "    # Fit the model with a custom metric to obtain the F1-scores\n",
    "    metrics_v1 = MetricsV1((train_words_padded, np.array(train_binaries_padded_cat)))\n",
    "    history = model.fit(train_words_padded, np.array(train_binaries_padded_cat),\n",
    "                    batch_size=batch_size, epochs=epochs, validation_data = (test_words_padded, \n",
    "                    np.array(test_binaries_padded_cat)), verbose=1, callbacks=[metrics_v1])\n",
    "    # Use trained model to predict the test set\n",
    "    predictions = model.predict(test_words_padded)\n",
    "    final_predictions = np.argmax(predictions, axis = 2)\n",
    "    train_ses, test_ses = split_train_test_dataset(start_ends, train_num_sents)\n",
    "    train_sent_ids, test_sent_ids = split_train_test_dataset(sentence_ids, train_num_sents)\n",
    "    # Transform the sequence prediction back into the format which we\n",
    "    # had in the dataframe and compute the F1-score on that representation\n",
    "    f1 = backward_transformation_v1(dataframe_merged, test_sent_ids, \\\n",
    "                    test_words_padded, index2word, test_ses, final_predictions)\n",
    "    return f1, metrics_v1.train_scores, metrics_v1.test_scores\n",
    "\n",
    "\n",
    "def preparation_char_v1(dataset, w_embedding, c_embedding, epochs, batch_size, **kwargs):\n",
    "    '''\n",
    "    Provides wrapping of preprocessing and postprocessing\n",
    "    for the first variant of sequence classification v1 \n",
    "    (where complexity of each word is predicted individually)\n",
    "    based on the provided dataset, embedding and model_func.\n",
    "    The model_func is a function that should accept values\n",
    "    specifying the embedding layer (max_sentence_length, \n",
    "    vocab_dimension, embedding_weights) and return the \n",
    "    constructed Keras model instance. Returns the final test \n",
    "    F1-score, F1-scores of training and F1-scores\n",
    "    of test sets for each epoch.\n",
    "    '''\n",
    "    # Merge the train and test set in the dataset\n",
    "    dataframe_merged, train_num_rows, test_num_rows, \\\n",
    "                dataset_num_rows, train_num_sents, test_num_sents, \\\n",
    "                dataset_num_sents = merge_train_test_dataset(dataset)\n",
    "    # Forward transform the data represented in a dataframe into\n",
    "    # a sequence representation for classifying each word individually\n",
    "    sentence_ids, sentences, start_ends, \\\n",
    "        binaries, probabilties = forward_transformation_v1(dataframe_merged, lowercase=False)\n",
    "    # Build the vocbulary and embedding\n",
    "    word2index, index2word, word_embedding = build_vocabulary(sentences, \\\n",
    "                                w_embedding, w_embedding.vector_size, missing='unique')\n",
    "    char_dimension = len(list(c_embedding.values())[0])\n",
    "    char2index, index2char, char_embedding = build_char_vocabulary(sentences, \\\n",
    "                                c_embedding, char_dimension, missing='unique')\n",
    "    # Pad the sentences and labels\n",
    "    sent_max_length = sentence_max_length(sentences)\n",
    "    words_padded = pad_encode_sequences(sentences, word2index, '', sent_max_length)\n",
    "    binaries_padded, probabilities_padded = pad_binaries_probs(binaries, probabilties, sent_max_length)\n",
    "    binaries_padded_categorical = [to_categorical(clazz, num_classes=2) for clazz in binaries_padded]\n",
    "    # Pad char sequences\n",
    "    char_max_length = 11\n",
    "    chars_padded = pad_chars(sentences, char2index, sent_max_length, char_max_length)\n",
    "    # Instantiate the model\n",
    "    model = model_v1_char_BiLSTM(sent_max_length, char_max_length, word_embedding, char_embedding, **kwargs)\n",
    "    adam = optimizers.Adam(clipnorm=1.)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "    # Split merged data into train and test again\n",
    "    train_words_padded, test_words_padded = split_train_test_dataset(words_padded, train_num_sents)\n",
    "    train_chars_padded, test_chars_padded = split_train_test_dataset(chars_padded, train_num_sents)\n",
    "    train_chars_reshaped = np.array(train_chars_padded).reshape(len(train_chars_padded), \\\n",
    "                                    sent_max_length, char_max_length)\n",
    "    test_chars_reshaped = np.array(test_chars_padded).reshape(len(test_chars_padded), \\\n",
    "                                    sent_max_length, char_max_length)\n",
    "    train_binaries_padded_cat, test_binaries_padded_cat = split_train_test_dataset(\\\n",
    "                    binaries_padded_categorical, train_num_sents)\n",
    "    print(len(train_words_padded))\n",
    "    print(len(test_words_padded))\n",
    "    # Fit the model with a custom metric to obtain the F1-scores\n",
    "    metrics_v1 = MetricsV1Char(([train_words_padded, train_chars_reshaped], np.array(train_binaries_padded_cat)))\n",
    "    history = model.fit([train_words_padded, train_chars_reshaped], np.array(train_binaries_padded_cat),\n",
    "                    batch_size=batch_size, epochs=epochs, validation_data = ([test_words_padded, test_chars_reshaped], \n",
    "                    np.array(test_binaries_padded_cat)), verbose=1, callbacks=[metrics_v1])\n",
    "    # Use trained model to predict the test set\n",
    "    predictions = model.predict([test_words_padded, test_chars_reshaped])\n",
    "    final_predictions = np.argmax(predictions, axis = -1)\n",
    "    train_ses, test_ses = split_train_test_dataset(start_ends, train_num_sents)\n",
    "    train_sent_ids, test_sent_ids = split_train_test_dataset(sentence_ids, train_num_sents)\n",
    "    # Transform the sequence prediction back into the format which we\n",
    "    # had in the dataframe and compute the F1-score on that representation\n",
    "    f1 = backward_transformation_v1(dataframe_merged, test_sent_ids, \\\n",
    "                     test_words_padded, index2word, test_ses, final_predictions)\n",
    "    return f1, metrics_v1.train_scores, metrics_v1.test_scores\n",
    "\n",
    "\n",
    "def preparation_elmo_v1(dataset, embedding, model_func, epochs):\n",
    "    # Merge the train and test set in the dataset\n",
    "    dataframe_merged, train_num_rows, test_num_rows, \\\n",
    "                dataset_num_rows, train_num_sents, test_num_sents, \\\n",
    "                dataset_num_sents = merge_train_test_dataset(dataset)\n",
    "    # Forward transform the data represented in a dataframe into\n",
    "    # a sequence representation for classifying each word individually\n",
    "    sentence_ids, sentences, start_ends, \\\n",
    "            binaries, probabilties = forward_transformation_v1(dataframe_merged)\n",
    "    # Build the vocbulary and embedding\n",
    "    word2index, index2word, word_embedding = build_vocabulary(sentences, \\\n",
    "                                    embedding, embedding.vector_size, missing='unique')\n",
    "    # Pad the sentences (for elmo we need the actual words, so we\n",
    "    # first map it to integers, pad the integer sequence and map \n",
    "    #it back to string) and labels\n",
    "    sent_max_length = sentence_max_length(sentences)\n",
    "    words_padded = pad_encode_sequences(sentences, word2index, '', sent_max_length)\n",
    "    words_padded = [[index2word[index] for index in sentence] for sentence in words_padded]\n",
    "    binaries_padded, probabilities_padded = pad_binaries_probs(binaries, probabilties, sent_max_length)\n",
    "    # Split merged data into train and test again\n",
    "    train_words_padded, test_words_padded = split_train_test_dataset(words_padded, train_num_sents)\n",
    "    # We use sparse_categorical_crossentropy so we not require the\n",
    "    # binaries mapped to categorical representation, but we reshape them\n",
    "    train_binaries_padded, test_binaries_padded = split_train_test_dataset(binaries_padded, train_num_sents)\n",
    "    train_binaries_padded = train_binaries_padded.reshape((train_binaries_padded.shape[0], \\\n",
    "                                                              train_binaries_padded.shape[1], 1))\n",
    "    test_binaries_padded = test_binaries_padded.reshape((test_binaries_padded.shape[0], \\\n",
    "                                                              test_binaries_padded.shape[1], 1))\n",
    "    # Make sure the training sample size is divisible by the batch size\n",
    "    batch_size = 2\n",
    "    training_extension = np.array(train_words_padded).shape[0] % batch_size\n",
    "    print('training extension {}'.format(training_extension))\n",
    "    print('shape train words {}'.format(np.array(train_words_padded).shape))\n",
    "    train_words_padded.append(train_words_padded[-training_extension])\n",
    "    print('shape train words extension {}'.format(np.array(train_words_padded).shape))\n",
    "    train_binaries_padded_ext = train_binaries_padded[-training_extension].reshape(training_extension, \\\n",
    "                                train_binaries_padded.shape[1], train_binaries_padded.shape[2])\n",
    "    print('shape train binaries extension {}'.format(np.array(train_binaries_padded_ext).shape))\n",
    "    print('shape train binaries {}'.format(np.array(train_binaries_padded).shape))\n",
    "    train_binaries_padded = np.concatenate((train_binaries_padded, train_binaries_padded_ext))\n",
    "    print('shape train binaries {}'.format(np.array(train_binaries_padded).shape))\n",
    "    # Make sure the test sample size is divisible by the batch size\n",
    "    test_cutoff = test_num_sents - (np.array(test_words_padded).shape[0] % batch_size)\n",
    "    print('test cutoff {}'.format(test_cutoff))\n",
    "    test_words_padded_cut = test_words_padded[:test_cutoff]\n",
    "    print('shape test words cut {}'.format(np.array(test_words_padded_cut).shape))\n",
    "    print('shape test words {}'.format(np.array(test_words_padded).shape))\n",
    "    test_binaries_padded_cut = test_binaries_padded[:test_cutoff]\n",
    "    print(np.array(train_binaries_padded).shape)\n",
    "    print(np.array(test_binaries_padded).shape)\n",
    "    # Fit the model with a custom metric to obtain the F1-scores\n",
    "    model = model_func(sent_max_length, batch_size)\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "    # Use custom ELMo metric to evaluate F1-score after each epoch\n",
    "    metrics_elmo_v1 = MetricsElmoV1((np.array(train_words_padded), \\\n",
    "                                     np.array(train_binaries_padded)), batch_size)\n",
    "    history = model.fit(np.array(train_words_padded), np.array(train_binaries_padded),\n",
    "                         batch_size=batch_size, epochs=epochs, validation_data = (np.array(test_words_padded_cut), \n",
    "                        np.array(test_binaries_padded_cut)), verbose=1, callbacks=[metrics_elmo_v1])\n",
    "    # Use trained model to predict the test set\n",
    "    train_words_padded, test_words_padded = split_train_test_dataset(words_padded, train_num_sents)\n",
    "    train_binaries_padded, test_binaries_padded = split_train_test_dataset(binaries_padded, train_num_sents)\n",
    "    prediction = prediction_elmo_model(test_words_padded, model, batch_size)\n",
    "    test_words_padded = test_words_padded\n",
    "    print('test words reloaded shape {}'.format(np.array(test_words_padded).shape))\n",
    "    preds = prediction\n",
    "    # Transform the sequence prediction back into the format which we\n",
    "    # had in the dataframe and compute the F1-score on that representation\n",
    "    train_ses, test_ses = split_train_test_dataset(start_ends, train_num_sents)\n",
    "    train_sent_ids, test_sent_ids = split_train_test_dataset(sentence_ids, train_num_sents)\n",
    "    f1 = backward_transformation_v1(dataframe_merged, test_sent_ids, \\\n",
    "                    test_words_padded, index2word, test_ses, preds)\n",
    "    return f1, metrics_elmo_v1.train_scores, metrics_elmo_v1.test_scores\n",
    "\n",
    "\n",
    "def compute_ELMo_train_test_extensions(train_words_padded, train_binaries_padded, \\\n",
    "                                test_words_padded, test_binaries_padded, batch_size, test_num_sents):\n",
    "    training_extension = np.array(train_words_padded).shape[0] % batch_size\n",
    "    print('training extension {}'.format(training_extension))\n",
    "    print('shape train words {}'.format(np.array(train_words_padded).shape))\n",
    "    train_words_padded.append(train_words_padded[-training_extension])\n",
    "    print('shape train words extension {}'.format(np.array(train_words_padded).shape))\n",
    "    train_binaries_padded_ext = train_binaries_padded[-training_extension].reshape(training_extension, \\\n",
    "                                train_binaries_padded.shape[1], train_binaries_padded.shape[2])\n",
    "    print('shape train binaries extension {}'.format(np.array(train_binaries_padded_ext).shape))\n",
    "    print('shape train binaries {}'.format(np.array(train_binaries_padded).shape))\n",
    "    train_binaries_padded = np.concatenate((train_binaries_padded, train_binaries_padded_ext))\n",
    "    print('shape train binaries {}'.format(np.array(train_binaries_padded).shape))\n",
    "    # Make sure the test sample size is divisible by the batch size\n",
    "    test_cutoff = test_num_sents - (np.array(test_words_padded).shape[0] % batch_size)\n",
    "    print('test cutoff {}'.format(test_cutoff))\n",
    "    test_words_padded_cut = test_words_padded[:test_cutoff]\n",
    "    print('shape test words cut {}'.format(np.array(test_words_padded_cut).shape))\n",
    "    print('shape test words {}'.format(np.array(test_words_padded).shape))\n",
    "    test_binaries_padded_cut = test_binaries_padded[:test_cutoff]\n",
    "    print(np.array(train_binaries_padded).shape)\n",
    "    print(np.array(test_binaries_padded_cut).shape)\n",
    "    return train_words_padded, train_binaries_padded, \\\n",
    "                test_words_padded_cut, test_binaries_padded_cut\n",
    "\n",
    "def preparation_elmo_v1_wembeds(dataset, embedding, epochs):\n",
    "    # Merge the train and test set in the dataset\n",
    "    dataframe_merged, train_num_rows, test_num_rows, \\\n",
    "                dataset_num_rows, train_num_sents, test_num_sents, \\\n",
    "                dataset_num_sents = merge_train_test_dataset(dataset)\n",
    "    # Forward transform the data represented in a dataframe into\n",
    "    # a sequence representation for classifying each word individually\n",
    "    sentence_ids, sentences, start_ends, \\\n",
    "            binaries, probabilties = forward_transformation_v1(dataframe_merged)\n",
    "    # Build the vocbulary and embedding\n",
    "    word2index, index2word, word_embedding = build_vocabulary(sentences, \\\n",
    "                                    embedding, embedding.vector_size, missing='unique')\n",
    "    # Pad the sentences (for elmo we need the actual words, so we\n",
    "    # first map it to integers, pad the integer sequence and map \n",
    "    #it back to string) and labels\n",
    "    sent_max_length = sentence_max_length(sentences)\n",
    "    words_padded_integer = pad_encode_sequences(sentences, word2index, '', sent_max_length)\n",
    "    words_padded_elmo = [[index2word[index] for index in sentence] for sentence in words_padded_integer]\n",
    "    binaries_padded, probabilities_padded = pad_binaries_probs(binaries, probabilties, sent_max_length)\n",
    "    # Split merged data into train and test again\n",
    "    train_words_padded_integer, test_words_padded_integer = split_train_test_dataset(words_padded_integer, \\\n",
    "                                                                        train_num_sents)\n",
    "    train_words_padded_elmo, test_words_padded_elmo = split_train_test_dataset(words_padded_elmo, \\\n",
    "                                                                        train_num_sents)\n",
    "    # We use sparse_categorical_crossentropy so we not require the\n",
    "    # binaries mapped to categorical representation, but we reshape them\n",
    "    train_binaries_padded, test_binaries_padded = split_train_test_dataset(binaries_padded, train_num_sents)\n",
    "    train_binaries_padded = train_binaries_padded.reshape((train_binaries_padded.shape[0], \\\n",
    "                                                              train_binaries_padded.shape[1], 1))\n",
    "    test_binaries_padded = test_binaries_padded.reshape((test_binaries_padded.shape[0], \\\n",
    "                                                              test_binaries_padded.shape[1], 1))\n",
    "    # Make sure the training sample size is divisible by the batch size\n",
    "    batch_size = 2\n",
    "    \n",
    "    \n",
    "    train_words_padded_elmo, train_binaries_padded_elmo, test_words_padded_cut_elmo, \\\n",
    "            test_binaries_padded_cut_elmo = compute_ELMo_train_test_extensions(train_words_padded_elmo, \\\n",
    "                    train_binaries_padded, test_words_padded_elmo, test_binaries_padded, batch_size, test_num_sents)\n",
    "    print(type(words_padded_integer.tolist()))\n",
    "    print(type(words_padded_elmo))\n",
    "    train_words_padded_int, train_binaries_padded_int, test_words_padded_cut_int, \\\n",
    "            test_binaries_padded_cut_int = compute_ELMo_train_test_extensions(train_words_padded_integer.tolist(), \\\n",
    "                    train_binaries_padded, test_words_padded_integer.tolist(), test_binaries_padded, batch_size, test_num_sents)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Fit the model with a custom metric to obtain the F1-scores\n",
    "    model = model_v1_ElMo_residual_BiLSTM_wembeds(sent_max_length, batch_size, len(word2index), \\\n",
    "                                                  embedding.vector_size, word_embedding)\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "    # Use custom ELMo metric to evaluate F1-score after each epoch\n",
    "    metrics_elmo_v1 = MetricsElmoWembedsV1(([np.array(train_words_padded_elmo), np.array(train_words_padded_int)], \\\n",
    "                                     np.array(train_binaries_padded_elmo)), batch_size)\n",
    "    print(train_words_padded_elmo)\n",
    "    print(train_words_padded_int)\n",
    "    # Use multi input here based on ELMo padded word strings of the sentence\n",
    "    # and integer encoded padded words of the sentence\n",
    "    history = model.fit([np.array(train_words_padded_elmo), np.array(train_words_padded_int)], \n",
    "                        np.array(train_binaries_padded_elmo), batch_size=batch_size, epochs=epochs, \n",
    "                        validation_data = ([np.array(test_words_padded_cut_elmo), np.array(test_words_padded_cut_int)], \n",
    "                        np.array(test_binaries_padded_cut_elmo)), verbose=1, callbacks=[metrics_elmo_v1])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Use trained model to predict the test set\n",
    "    train_words_padded_integer, test_words_padded_integer = split_train_test_dataset(words_padded_integer, \\\n",
    "                                                                        train_num_sents)\n",
    "    train_words_padded_elmo, test_words_padded_elmo = split_train_test_dataset(words_padded_elmo, \\\n",
    "                                                                        train_num_sents)\n",
    "    train_binaries_padded, test_binaries_padded = split_train_test_dataset(binaries_padded, train_num_sents)\n",
    "    prediction = prediction_elmo_wembeds_model([np.array(test_words_padded_elmo), \\\n",
    "                                    np.array(test_words_padded_integer)], model, batch_size)\n",
    "    test_words_padded = test_words_padded_integer\n",
    "    preds = prediction\n",
    "    # Transform the sequence prediction back into the format which we\n",
    "    # had in the dataframe and compute the F1-score on that representation\n",
    "    train_ses, test_ses = split_train_test_dataset(start_ends, train_num_sents)\n",
    "    train_sent_ids, test_sent_ids = split_train_test_dataset(sentence_ids, train_num_sents)\n",
    "    f1 = backward_transformation_v1(dataframe_merged, test_sent_ids, \\\n",
    "                    test_words_padded, index2word, test_ses, preds)\n",
    "    return f1, metrics_elmo_v1.train_scores, metrics_elmo_v1.test_scores\n",
    "\n",
    "   \n",
    "def preparation_v2(dataset, model_func, embedding, epochs, batch_size, **kwargs):\n",
    "    '''\n",
    "    Provides wrapping of preprocessing and postprocessing\n",
    "    for the second variant of sequence classification v1 \n",
    "    (where a target is marked in the sentence and the overall\n",
    "    sentence is classified) based on the provided dataset,\n",
    "    embedding and model_func.The model_func is a function \n",
    "    that should accept values specifying the embedding layer\n",
    "    (max_sentenc_length, vocab_dimension, embedding_weights) \n",
    "    and return the constructed Keras model instance. Returns\n",
    "    the final test F1-score, F1-scores of training and F1-scores\n",
    "    of test sets for each epoch.\n",
    "    '''\n",
    "    # Merge the train and test set in the dataset\n",
    "    dataframe_merged, train_num_rows, test_num_rows, \\\n",
    "                dataset_num_rows, train_num_sents, test_num_sents, \\\n",
    "                dataset_num_sents = merge_train_test_dataset(dataset)\n",
    "    # Forward transform the data represented in a dataframe into\n",
    "    # a sequence representation where each target is marked in its sentence\n",
    "    sentences, binaries, probabilties = forward_transformation_v2(dataframe_merged)\n",
    "    # Build the vocbulary and embedding\n",
    "    word2index, index2word, word_embedding = build_vocabulary(sentences, \\\n",
    "                            embedding, embedding.vector_size, missing='unique')\n",
    "    # Pad the sentences and labels\n",
    "    sent_max_length = sentence_max_length(sentences)\n",
    "    words_padded = pad_encode_sequences(sentences, word2index, '', sent_max_length)\n",
    "    binaries_categorical = [to_categorical(clazz, num_classes=2) for clazz in binaries]\n",
    "    # Instantiate the model\n",
    "    model = model_func(sent_max_length, len(word2index), embedding.vector_size, word_embedding, **kwargs)\n",
    "    model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "    # Split merged data into train and test again\n",
    "    train_words_padded, test_words_padded = split_train_test_dataset(words_padded, train_num_rows)\n",
    "    train_binaries_categorical, test_binaries_categorical = split_train_test_dataset(binaries_categorical, train_num_rows)\n",
    "    # Fit the model with a custom metric to obtain the F1-scores\n",
    "    metrics_v2 = MetricsV2((train_words_padded, np.array(train_binaries_categorical)))\n",
    "    history = model.fit(train_words_padded, np.array(train_binaries_categorical),\n",
    "                    batch_size=batch_size, epochs=epochs, validation_data = (test_words_padded, \n",
    "                    np.array(test_binaries_categorical)), verbose=1, callbacks=[metrics_v2])\n",
    "    # Use trained model to predict the test set\n",
    "    predictions = model.predict(test_words_padded)\n",
    "    final_predictions = np.argmax(predictions, axis = 1)\n",
    "    targets = np.argmax(test_binaries_categorical, axis = 1)\n",
    "    f1 = f1_score(targets, final_predictions)\n",
    "    return f1, metrics_v2.train_scores, metrics_v2.test_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X.x CWI Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model : glove.6B.50d.txt\n"
     ]
    }
   ],
   "source": [
    "datasets = load_datasets(['Wikipedia', 'WikiNews', 'News'], 'Train', 'Dev', type_train='word', type_test='word')\n",
    "datasets = preprocess_datasets(datasets)\n",
    "models = get_embedding_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X.x Hyperparamter-Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "Configuration = namedtuple('Configuration', 'name, params, train_scores, test_scores, f1')\n",
    "\n",
    "def hyperparameter_opt_v1(dataset, model, embedding_models, \n",
    "                          batch_sizes, epochs, units):\n",
    "    configurations = []\n",
    "    t = time.process_time()\n",
    "    name = model.__name__\n",
    "    for batch_size in batch_sizes:\n",
    "        for epoch in epochs:\n",
    "            for embed_model in embedding_models:\n",
    "                for unit in units:\n",
    "                    if unit <= embed_model.model.vector_size:\n",
    "                        params = {'epoch' : epoch, 'bs' : \\\n",
    "                                batch_size, 'embed' : embed_model.name, 'unit' : unit}\n",
    "                        print('Optimization Testing : {}'.format(params))\n",
    "                        f1, train_f1s, test_f1s = preparation_v1(dataset, \\\n",
    "                            model, embed_model.model, epoch, batch_size, units=unit)\n",
    "                        config = Configuration(name, params, \\\n",
    "                                             train_f1s, test_f1s, f1)\n",
    "                        configurations.append(config)\n",
    "    elapsed_time = time.process_time() - t\n",
    "    print('Hyperparameter Tuning took {}'.format(elapsed_time))\n",
    "    return configurations, max(configurations, key = lambda c : c.f1)\n",
    "\n",
    "def hyperparameter_opt_v1_datasets(datasets, model, embedding_models, \n",
    "                        batch_sizes, epochs, units):\n",
    "    params_per_dataset = {}\n",
    "    for dataset in datasets:\n",
    "        configs, best = hyperparameter_opt_v1(dataset, model, embedding_models, \n",
    "                              batch_sizes, epochs, units)\n",
    "        params_per_dataset[dataset.name] = {'configs' : configs, 'best' : best}\n",
    "    return params_per_dataset\n",
    "\n",
    "def hyperparameter_opt_v2(dataset, model, embedding_models, \n",
    "                          batch_sizes, epochs, units):\n",
    "    configurations = []\n",
    "    t = time.process_time()\n",
    "    name = model.__name__\n",
    "    for batch_size in batch_sizes:\n",
    "        for epoch in epochs:\n",
    "            for embed_model in embedding_models:\n",
    "                for unit in units:\n",
    "                    if unit <= embed_model.model.vector_size:\n",
    "                        params = {'epoch' : epoch, 'bs' : \\\n",
    "                                batch_size, 'embed' : embed_model.name, 'unit' : unit}\n",
    "                        print('Optimization Testing : {}'.format(params))\n",
    "                        f1, train_f1s, test_f1s = preparation_v2(dataset, \\\n",
    "                            model, embed_model.model, epoch, batch_size, units=unit)\n",
    "                        config = Configuration(name, params, \\\n",
    "                                             train_f1s, test_f1s, f1)\n",
    "                        configurations.append(config)\n",
    "    elapsed_time = time.process_time() - t\n",
    "    print('Hyperparameter Tuning took {}'.format(elapsed_time))\n",
    "    return configurations, max(configurations, key = lambda c : c.f1)\n",
    "\n",
    "def hyperparameter_opt_v2_datasets(datasets, model, embedding_models, \n",
    "                        batch_sizes, epochs, units):\n",
    "    params_per_dataset = {}\n",
    "    for dataset in datasets:\n",
    "        configs, best = hyperparameter_opt_v2(dataset, model, embedding_models, \n",
    "                              batch_sizes, epochs, units)\n",
    "        params_per_dataset[dataset.name] = {'configs' : configs, 'best' : best}\n",
    "    return params_per_dataset\n",
    "\n",
    "def hyperparameter_opt_char_v1(dataset, embedding_models, \n",
    "                          batch_sizes, epochs, units_char, units_word):\n",
    "    configurations = []\n",
    "    t = time.process_time()\n",
    "    name = 'ca_bilstm_v1'\n",
    "    for batch_size in batch_sizes:\n",
    "        for epoch in epochs:\n",
    "            for embed_model in embedding_models:\n",
    "                for unit_char in units_char:\n",
    "                    for unit_word in units_word:\n",
    "                        params = {'epoch' : epoch, 'bs' : \\\n",
    "                                batch_size, 'embed' : embed_model.name, \n",
    "                                  'units_char' : unit_char, 'units_word' : unit_word}\n",
    "                        print('Optimization Testing : {}'.format(params))\n",
    "                        f1, train_f1s, test_f1s = preparation_char_v1(dataset, \\\n",
    "                            embed_model.model, compute_character_embeddings(embed_model.model), \n",
    "                            epoch, batch_size, units_char=unit_char, units_word=unit_word)\n",
    "                        config = Configuration(name, params, \\\n",
    "                                             train_f1s, test_f1s, f1)\n",
    "                        configurations.append(config)\n",
    "    elapsed_time = time.process_time() - t\n",
    "    print('Hyperparameter Tuning took {}'.format(elapsed_time))\n",
    "    return configurations, max(configurations, key = lambda c : c.f1)\n",
    "\n",
    "def hyperparameter_opt_char_v1_datasets(datasets, embedding_models, \n",
    "                        batch_sizes, epochs, units_char, units_word):\n",
    "    params_per_dataset = {}\n",
    "    for dataset in datasets:\n",
    "        configs, best = hyperparameter_opt_char_v1(dataset, embedding_models, \n",
    "                              batch_sizes, epochs, units_char, units_word)\n",
    "        params_per_dataset[dataset.name] = {'configs' : configs, 'best' : best}\n",
    "    return params_per_dataset\n",
    "\n",
    "def hyperparameter_opt_elmo_v1(dataset, model, embed_model, epochs):\n",
    "    configurations = []\n",
    "    t = time.process_time()\n",
    "    name = model.__name__\n",
    "    for epoch in epochs:\n",
    "        params = {'epoch' : epoch}\n",
    "        print('Optimization Testing : {}'.format(params))\n",
    "        f1, train_f1s, test_f1s = preparation_elmo_v1(dataset, \\\n",
    "            embed_model.model, model, epoch)\n",
    "        config = Configuration(name, params, \\\n",
    "                                train_f1s, test_f1s, f1)\n",
    "        configurations.append(config)\n",
    "    elapsed_time = time.process_time() - t\n",
    "    print('Hyperparameter Tuning took {}'.format(elapsed_time))\n",
    "    return configurations, max(configurations, key = lambda c : c.f1)\n",
    "\n",
    "def hyperparameter_opt_elmo_v1_datasets(datasets, model, embed_model, \n",
    "                        epochs):\n",
    "    params_per_dataset = {}\n",
    "    for dataset in datasets:\n",
    "        configs, best = hyperparameter_opt_elmo_v1(dataset, model, embed_model, \n",
    "                              epochs)\n",
    "        params_per_dataset[dataset.name] = {'configs' : configs, 'best' : best}\n",
    "    return params_per_dataset\n",
    "\n",
    "def hyperparameter_opt_elmo_wembeds_v1(dataset, embed_model, epochs):\n",
    "    configurations = []\n",
    "    t = time.process_time()\n",
    "    name = 'ELMo-r-BiLSTM-e'\n",
    "    for epoch in epochs:\n",
    "        params = {'epoch' : epoch}\n",
    "        print('Optimization Testing : {}'.format(params))\n",
    "        f1, train_f1s, test_f1s = preparation_elmo_v1_wembeds(dataset, \\\n",
    "            embed_model.model, epoch)\n",
    "        config = Configuration(name, params, \\\n",
    "                                train_f1s, test_f1s, f1)\n",
    "        configurations.append(config)\n",
    "    elapsed_time = time.process_time() - t\n",
    "    print('Hyperparameter Tuning took {}'.format(elapsed_time))\n",
    "    return configurations, max(configurations, key = lambda c : c.f1)\n",
    "\n",
    "def hyperparameter_opt_elmo_wembeds_v1_datasets(datasets, embed_model, \n",
    "                        epochs):\n",
    "    params_per_dataset = {}\n",
    "    for dataset in datasets:\n",
    "        configs, best = hyperparameter_opt_elmo_wembeds_v1(dataset, embed_model, \n",
    "                              epochs)\n",
    "        params_per_dataset[dataset.name] = {'configs' : configs, 'best' : best}\n",
    "    return params_per_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.1 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_models = models\n",
    "lstm_batch_sizes = [1]\n",
    "lstm_epochs = [1,2]\n",
    "lstm_units = [25, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_params_datasets = hyperparameter_opt_v1_datasets(datasets, model_v1_LSTM, \n",
    "                                            lstm_models, lstm_batch_sizes, lstm_epochs, lstm_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.2 BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_models = models\n",
    "bilstm_batch_sizes = [1]\n",
    "bilstm_epochs = [1,2]\n",
    "bilstm_units = [25, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Testing : {'epoch': 1, 'bs': 1, 'embed': 'glove.6B.50d.txt', 'unit': 25}\n",
      "----------------------\n",
      "# Rows train : 4833\n",
      "# Rows test : 606\n",
      "# Rows dataset : 5439\n",
      "----------------------\n",
      "# Sents train : 387\n",
      "# Sents test : 53\n",
      "# Sents dataset : 440\n",
      "----------------------\n",
      "# Words : 10722\n",
      "# Vocab : 3639\n",
      "# Words missing embedding : 160\n",
      "Embedding shape : (3639, 50)\n",
      "Max length sentence : 101\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "in (InputLayer)              (None, 101)               0         \n",
      "_________________________________________________________________\n",
      "em (Embedding)               (None, 101, 50)           181950    \n",
      "_________________________________________________________________\n",
      "dr (Dropout)                 (None, 101, 50)           0         \n",
      "_________________________________________________________________\n",
      "bi (Bidirectional)           (None, 101, 50)           15200     \n",
      "_________________________________________________________________\n",
      "ts (TimeDistributed)         (None, 101, 2)            102       \n",
      "=================================================================\n",
      "Total params: 197,252\n",
      "Trainable params: 197,252\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Training set length : 387\n",
      "Test set length : 53\n",
      "Training set length : 387\n",
      "Test set length : 53\n",
      "387\n",
      "53\n",
      "[[  35  676 1259   17 1260    7   18 1261 1262 1263 1264  677 1265  304\n",
      "   305  678    4    1 1266   10   25  226  679  137  680  453    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 138   37  139 1267    7    5   65  681    4 1268    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   5  682    7    5  683   12 1269  684    5 1270    2 1271  306 1272\n",
      "  1273    4    3    5  307    2  685 1274 1275    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   5   57   17    1  140    2    5    7    5   57  686  227   13    5\n",
      "    15    1 1276    5 1277  687  454  688  689 1278  454    4  690  454\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   5  691    6    1 1279 1280    7   94   13  692    2  228  455    4\n",
      "     7  308   16   39    2    1   47    2  179    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   5  309  456 1281   66   24   74   10 1282   58  229 1283  457  229\n",
      "  1284    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 230  458  693  459    9    5   51  309  180 1285 1286  694   67    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [1287  310  231  232    6    8  460  310    7    5  311  233 1288  461\n",
      "    16 1289 1290    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 141   83  114    3   95   19 1291    2   83    6  462  463   11    1\n",
      "  1292  115   17  695 1293    2   95    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  40 1294 1295   68  696    6  464   52    5 1296  697   41   75    9\n",
      "     5 1297    4 1298  312   10    5   42 1299  465   10    5  181  698\n",
      "  1300  699    1   39  466  234    3  313   20    1 1301   84  314  700\n",
      "    13    1 1302 1303    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  40    5 1304    7    5  467  235   67   21 1305   13  182  137   85\n",
      "    13    1  701    3    1  468    4 1306    3 1307    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 183    6 1308 1309  236    1  702 1310 1311   17  315    7    1   30\n",
      "     1  316    4    1   45 1312 1313 1314    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 183    6 1315 1316 1317 1318    5   57    7    5  140 1319   10 1320\n",
      "     2 1321  469 1322    2 1323   30 1324  142   76 1325 1326 1327    4\n",
      "   470    3  236    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 183    6  317    4 1328 1329 1330 1331    4    1   96  318 1332  318\n",
      "   237    9  238    3 1333    4 1334    3  471 1335    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 183    6    1  703  468  704 1336    1   47   59    5  319    2 1337\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 183    6    1  317    1   96    7    1  239    2  236 1338    3   26\n",
      "   184    1 1339  705   17  706  184    6  320    1   69   11 1340   14\n",
      "    52    1 1341  705    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 183    6    1   96  472  707  708  709  240   26   96   13    1 1342\n",
      "   473    2    1 1343 1344    2 1345 1346    3    1   70 1347 1348    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [1349   15 1350   12  116   32 1351    6    1  241  474   19  143   53\n",
      "   710  711    2   43  242  712    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 475  713    7    1   39 1352    2    1  321  243    2  476 1353 1354\n",
      "  1355    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [1356 1357 1358  185   48 1359  322  182 1360    9    5  477  714    4\n",
      "  1361    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 715  716 1362    7    5  144    2  717   41  715  323    6    1 1363\n",
      "     2    1 1364  718    4    1  316  315    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 478  719  720  479    7    5 1365 1366 1367    4 1368 1369   10 1370\n",
      "   719    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 324    9  721   11  145   20  722   17    1 1371 1372   77 1373    4\n",
      "  1374  480  723   77  722    4  724    4  325  481    8    1   42  186\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 725  326 1375 1376  725 1377  482 1378  726   35   33    8  324    9\n",
      "  1379  186   10  321 1380    3  726    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [1381  727    7    1  728 1382    2  727 1383   15    1  729  483    4\n",
      "   730    2 1384    4 1385    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  78 1386  117  327  146  328    6 1387   15    1  147    2  329   13\n",
      "   244    1  731  484    5  484  330   60 1388   61    1  245  148  244\n",
      "  1389    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  78  331    5  732 1390  187   16   97   62  485    7 1391    6  246\n",
      "     5 1392   20    5  332 1393   41   36 1394   62 1395    6 1396    1\n",
      "  1397  473  118  142  149   30    2    1 1398 1399   10   62  733    6\n",
      "   486  247   34  696    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  78 1400   37  119    3    1  487  467  734  120   69  248  188  150\n",
      "   328    6    1  488    3 1401    4   59   23  151   54  735    1  736\n",
      "    15 1402  333    1  489  121    5  737  738    1  736    3  333    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [1403  249  739 1404    6    1  490    2 1405    5  226 1406    6    1\n",
      "   490    2 1407  740 1408    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  86   51  122   34    5  491 1409    2 1410 1411   61 1412    3    1\n",
      "   334   49    1  307    2   25 1413   19  741  742   16    1 1414 1415\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  86   51  122    7   94    3    1  743    2  335  336 1416   13    1\n",
      "  1417   15   51 1418    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  86   51  122    7  143   63    1   51  492   47    2    1 1419  337\n",
      "     1   98  744  745 1420 1421    4  746 1422  152    1   47    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  86  122  747 1423 1424  716    1 1425   35   33    8    1  492   47\n",
      "   338    6   25 1426    7    1   99  339   47    3    1  743    2  335\n",
      "   336    4    1  748  339   47    3    1  250  749  750    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 493  751   34   46   74   11 1427   31    8 1428 1429   71 1430  493\n",
      "   751    4  752    4  494    1  753  754    2  493  189    8    5  755\n",
      "  1431    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [1432  190 1433 1434  238  185   48 1435    7   18  251 1436    4    1\n",
      "  1437  495    2  153    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 756 1438   23  191  757  188    2 1439    6    1 1440    3 1441  758\n",
      "    78  757    9 1442    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [1443 1444   68  496 1445 1446 1447 1448  497  252 1449 1450  252 1451\n",
      "    33    3  192    8 1452   68    9    5  496   50  498 1453    4  746\n",
      "   340   33  151   10   23 1454  123    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  58  229  456  759   28    5 1455  729   15   87  760  227   13    5\n",
      "   499  500  501  760 1456    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  35  149    1  502  503 1457   12    1  341    9 1458   55   72   32\n",
      "    24  342  761    1  502  503    7 1459   12   55  762  247   55   19\n",
      "    53   10    4   12   55   72   32  116   14  142    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 154   14    7  231  763   12    1  504    9  155   11 1460  764    3\n",
      "  1461  764  676   14 1462    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 154   32    8 1463    3    1 1464  343    8 1465  765  505    2  766\n",
      "     3    5   57   19   32    8 1466    8   10  688   57   17  690   57\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 154    1  344    2    1  767  156    9 1467  506   20    1  121   14\n",
      "     9  768 1468  507  508   73   24 1469    8    5   64  156   17  460\n",
      "     5  769  345  124 1470   14    7  253 1471    6 1472   77    1   37\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 154    1  254   88    7  231   74    6  509    1  770  157  346  158\n",
      "     2    5  347    3 1473    1  254 1474    7  143  771   10   26 1475\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 255   23  772   56  348   27 1476 1477    3    1 1478  773    1 1479\n",
      "  1480  193 1481    4 1482 1483 1484    3    1 1485    4    1 1486   67\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [1487    1   39 1488 1489    9    1 1490    2    5   65    2   51 1491\n",
      "   349    3    1  123    2  774  775  231  232    6    3    1  510    8\n",
      "    51  776  349  777 1492    2    1  350  511  256  257  349   49 1493\n",
      "  1494    3    1  351  510    4   11    5  352    2 1495  778  512    4\n",
      "   513  779 1496   12    1  780 1497    2    1  350 1498   45   30  258\n",
      "     2    1  339 1499  232    6    8    5   51  257  258   15    5  259\n",
      "   256   51  249    4  256   51  226  349    2    5  781 1500    4   15\n",
      "  1501 1502 1503]\n",
      " [  18 1504  100 1505    2    5  782 1506  783    5 1507   18 1508    4\n",
      "     5 1509  514  100    5 1510  514    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  18  159   15    5  241   12 1511 1512   17 1513  784   17  515 1514\n",
      "  1515   17 1516  785   72   24  353   53  754   12   18  194  260   72\n",
      "    24 1517    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  18  195  516  234   10  786  740 1518    7    5  160   71  517    3\n",
      "     5 1519   17  518  261    4   71  262    3   56  263  519   17  520\n",
      "     3   12 1520    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  18 1521  317 1522    1 1523    6  787    4    1  161 1524    2 1525\n",
      "  1526 1527  521   26    8    1  788 1528    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  18  789 1529   17 1530    7    5  311   71 1531 1532 1533 1534 1535\n",
      "   790  118    4 1536  791    6  354 1537    6   28 1538  355    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [1539  788   34    5  319    2 1540 1541  196  482 1542  197    3   25\n",
      "   792  356    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 264 1543 1544   12 1545    7    1 1546    2 1547 1548 1549 1550   21\n",
      "     7    5 1551  522    2    1 1552    2  179    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [1553    7  793    3    1  125 1554 1555  794 1556 1557 1558  795  796\n",
      "   796  794 1559    4 1560    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 797  798   35 1561  357 1562 1563    6 1564    1 1565    2 1566    4\n",
      "  1567 1568    1 1569 1570 1571    2 1572 1573    4  523  799    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   8  524 1574  240  797 1575  357   12   28 1576  355    8 1577 1578\n",
      "     2    1 1579    4   44  733  525  526    4  762    1  354   44  312\n",
      "     3  527  357    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   8    5  528  800    6   24  342   14    7  101 1580    6 1581 1582\n",
      "    55 1583  101  341 1584  265  529  474   17  358  515   24 1585    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   8    2  198  801  102 1586  359  103   19  360   52    1  361  362\n",
      "  1587    3  530    3   37 1588   87   70    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   8    2    1  704    2  802   75   27 1589  525 1590 1591    4 1592\n",
      "   803 1593    3    1   47    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   8  804    1 1594  199   75    3  472 1595    9 1596   11    1 1597\n",
      "   200    4 1598    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   8    1  531  266  805  806   13 1599  807  532    1  531  363    9\n",
      "   533    3 1600    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   8    1  267  364 1601  534    4 1602  187    3  365    1  366  364\n",
      "  1603  535    4   37   42  808 1604    3    1  366    2    1 1605    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  20    5  809    2   40 1606  367  119    1 1607   33   64  156    7\n",
      "  1608  810    5  811  144    2   64  104  812    3  813    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  20   40 1609    6  814  119    2  184    1 1610   79  368   34  369\n",
      "     4   72  536 1611    2 1612    5  499 1613    2  476    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  20   12  121    1   60  815    4  201  268    9 1614 1615 1616  338\n",
      "     6  816 1617    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  20    1 1618 1619  370   22  105   37 1620  817    3    1  103 1621\n",
      "   106  818  819    4  103 1622  106  818  819  269    4  321 1623  817\n",
      "     3    1  103  537  106  538  103 1624  106  538    4  103 1625  106\n",
      "   538  269    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  20    1  780  162    2    1   54    3 1626  230   89  820   27 1627\n",
      "     1 1628  230  458  693   69 1629   15 1630  202    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  20  270  539   14  821    1 1631    2 1632 1633 1634 1635    4   42\n",
      "  1636    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 822   88    7 1637  371  823  509  346  158 1638   17  332  271  158\n",
      "   157  271  158   17 1639    8    1  523 1640    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [1641    7    1 1642    2    1  824  825   33    8  826  827   17  828\n",
      "   829  830  272  372   21   27  540    3  373    1 1643  126    1  831\n",
      "   126    1 1644  828  829    4    1  792 1645    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 237   29   56 1646  203    1  832  697 1647    8    1   69   29  163\n",
      "    50 1648  541  107   25  833   20    1  542 1649   56   80    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 237  189    9  107  108    2    1  471  834 1650   21  835 1651   77\n",
      "   374    4 1652    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 237 1653   32  836  837   17 1654    2   23  163  375  127   50 1655\n",
      "    28   46   33    6  838    6  836   23  163   50   13  347    4  839\n",
      "     1  840    2   25 1656    3    1 1657    2    1  841 1658    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 124    2    1  842  543    1  273 1659    8   14 1660   21  544   14\n",
      "     6  376    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 843  545   79  377 1661  545   79  546    4 1662  545   79 1663 1664\n",
      "     5 1665 1666  844    2  766    3   26  140    3    1 1667 1668    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 204   55  180   18  194  260   55  358 1669  845    1  846    4  847\n",
      "   155    3  101  341  205    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 699    9  274 1670   15    1  848    2   26   29  465   12   36  547\n",
      "     5 1671  849   15   68    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [1672   19 1673  149   44   19   32   17   79  368 1674    6  850   31\n",
      "   851   17  852    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 378   28  548 1675    2  306    4 1676  243    4   19    2  117 1677\n",
      "    10 1678  742    3 1679   12   19  515 1680    4 1681    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 378 1682   41    1  379   20    1  271  549    7 1683    4  853    3\n",
      "  1684    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 238    3 1685  854  550  150    9   63   16    1 1686    3 1687    6\n",
      "   181    3   30  855    2    1  551  380 1688   41   22  856    1 1689\n",
      "   857  858 1690    1  551  380    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 859  142 1691   10   62 1692 1693   13  552  860   15  553  554 1694\n",
      "  1695    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 859    4  553 1696   12   44  312    6 1697    1 1698    2  552 1699\n",
      "   100  861    8   18 1700    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  11    1  381    2    1 1701  382 1702 1703  109    5  275  164    2\n",
      "   276  383  255    1 1704    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 384  126  384  126  862 1705   84  384    7    1  863    2  385    2\n",
      "     1 1706    2 1707   94    3    1  555  110    2  179    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 190    9 1708    6  386    4  556    1 1709    2  864    8    5 1710\n",
      "     3  865 1711  557    3  866  277    4 1712    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 190    9  387  388  864    3    4   10    1  115    2  153    3  558\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 190    9  238    3 1713    6  206 1714    4  867   15   23  207    6\n",
      "  1715 1716   41   22 1717   23  868    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 190    9  559   18  869    2    1   90    2   95    3    1   85  388\n",
      "  1718 1719  844   10  799    6    1  277    4  870    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 389    2  228  871    3  830  539    1  389    2  228  871    9   30\n",
      "     2    1  339 1720 1721    2  179    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [1722    2    1 1723 1724   22 1725   18  208 1726  390    1 1727 1728\n",
      "     4    9   10   26  205 1729  186   11   23  361    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [1730    7   32   18 1731 1732    8   14    7 1733    3    1  391  392\n",
      "    16 1734 1735    4 1736    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [1737   14    7 1738    3 1739  872 1740 1741  873  393 1742 1743 1744\n",
      "  1745 1746  874  875    4 1747    4    1  307    2   25 1748   19 1749\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [1750 1751   72 1752    1  876    2 1753 1754    4   72 1755    5  160\n",
      "  1756    6    1  278 1757   11 1758   23 1759    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 877  560    2   83  114  878 1760    3   95 1761   10    1  732    2\n",
      "    83    6  462  463   97  270  879    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [1762  880   15   58  279  111  394  279  379  279    4 1763  279   31\n",
      "     8  308  367 1764    4  520  279    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 116  395 1765    6  561  101  881  205    6 1766  561  101  205    6\n",
      "    24  342  882    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 120    1 1767  396 1768    5   65    2 1769    6 1770   10  562 1771\n",
      "   111    1 1772    2   25 1773  563    1 1774    2    1  320    2  277\n",
      "     1 1775    2 1776   10  391  114    1 1777    6 1778 1779    2 1780\n",
      "     4    1 1781    2    5 1782 1783  883    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 120  884    5  564  885    7  845   11   18  397 1784   21 1785    5\n",
      "  1786 1787  397  398   63    5  728  886    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 146  191    7   18 1788   16    1   47    2 1789    4   62  887    7\n",
      "     2 1790    4  192 1791    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [1792 1793    4  888 1794   28   87  889    3    5 1795  235   56 1796\n",
      "     4 1797 1798  890    3   21   44   91   39  891 1799    8    5  565\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 566 1800   16    1 1801 1802  567  892    1 1803 1804   27 1805  399\n",
      "     6 1806 1807    3 1808    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 566  568 1809    1  893    2    1 1810  128    4    1 1811  894 1812\n",
      "    12    5  319  139  400   15    5  895  896   61 1813  569  209   17\n",
      "     5  319  188  400  896   61 1814  569  209   73  570  464  203 1815\n",
      "   571  897    4   73  401    5 1816 1817  345 1818  898    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [1819   19 1820 1821   97  899 1822    2 1823 1824  900   49   35 1825\n",
      "     4   38 1826    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 228  455    7    5  901   94    3    1  402  403    2    1   47   20\n",
      "    18   88    2   40 1827  210 1828  129  157  346  158    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [1829   28  572  808   87  902   40  137  280 1830  404    4  903   40\n",
      "   405  406  185    3  281    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 282 1831  211  904    3 1832   15  905 1833    4  906  907   97    1\n",
      "   272  283  407 1834 1835    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 282  284 1836   16    1  908 1837  285   11 1838 1839  909   13  910\n",
      "   573  408  563 1840  155    5  910 1841    2  574   10  249  911  574\n",
      "  1842    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 186  324  103 1843    2    5  912  248    4  481  201  913  914    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 130  915  916    7    5  504    2  226 1844  917   33   16   30  918\n",
      "  1845    3    1 1846    2    1 1847    2 1848 1849    4 1850    3 1851\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 130   35  575    5 1852    2 1853  392  165  409    2 1854 1855    8\n",
      "     3   60 1856 1857 1858    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 130 1859    6   28 1860 1861 1862  165    4   14  536 1863   32 1864\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [1865 1866  919  286 1867  185  252 1868 1869  576 1870    9   30    2\n",
      "     1   29  286  920 1871    6 1872 1873    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 166  921    1  410    2    1 1874 1875   17   15    5 1876    2 1877\n",
      "    18  159   12  922    1  131  923    8    5  877   53  159   34    1\n",
      "   131 1878 1879    4    1  131  577 1880   48 1881   53   97    1 1882\n",
      "   411    5 1883 1884    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 505   19 1885    3 1886 1887 1888 1889    3    1 1890    2 1891    4\n",
      "  1892    3 1893 1894    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 505    2   31  167   81  212   58 1895 1896   21  578  168    4 1897\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [1898    1 1899    2 1900   38  249 1901    4 1902   11  119    2 1903\n",
      "   756  361  914   20    1 1904  924 1905    6  925  259  305    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 310   19   33  677   11    1 1906 1907    5  523    6   43  548  926\n",
      "  1908    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 310   28  105   37  233 1909    3 1910    4  142    3 1911    8   92\n",
      "     8  841  579 1912 1913 1914    4   30  233  776  380    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  10  580 1915  927    1  928  128    3    1    5  581    4 1916  927\n",
      "     1  767  929    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  10  580 1917    4 1918  578   18  706  110 1919    6    1  930 1920\n",
      "     2    5 1921  931 1922    7    1  110    2    5 1923  931    3 1924\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  10 1925  262    3    5   57    1 1926   19 1927  412    6    5    4\n",
      "   689    5  932  933  934   61    5  748  933    8   10   39   38  765\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  10   12   70    1   80    9  132    6   38  332  582   49   11    1\n",
      "   125   70    1  413   59   46 1928    4  414  142   14    9  133   11\n",
      "     1  388   26  121   20    1  415    2    1  908   80    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 918  583    2  787   28   45   46  199    3  935   41   44  936    3\n",
      "     1  402 1929    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  16 1930    6 1931    1 1932  937    9  938   11    1  226  490    2\n",
      "  1933    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  16 1934    6 1935    1 1936  107   43  318    3 1937   13    1  939\n",
      "     2    1  169    2  940   41   93   59  941 1938    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 399    9  360    6  416    3 1939   11    1 1940 1941  942    2  584\n",
      "   942 1942 1943   84  584    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 943 1944 1945  943 1946   17 1947  477 1948  326 1949    7   30    2\n",
      "     1 1950 1951    2 1952    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 331   18  195 1953    8    1 1954  944   10  518 1955    7   35    5\n",
      "  1956 1957    3  518  519    4   56    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  22   35  945    8    1  946    2    1 1958  134   23 1959   11 1960\n",
      "  1961 1962    3  947    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  22 1963   12   50 1964   45    6 1965    5  755 1966    4 1967   60\n",
      "  1968   23 1969   20    1 1970   22 1971   11   23  866   50   76   22\n",
      "    39  948    6 1972   13   60  949 1973    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  22  950    1 1974    4 1975  951    4 1976   15 1977    1  952  951\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  22 1978   40    1  585  144  586  573   16   18 1979    3    1  585\n",
      "   587 1980 1981   11  906  907    5  417  953    2  211    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  22  533  584 1982    6  197    3 1983  820  954    1  287    2 1984\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  22   91 1985  955  488  370 1986 1987  333    4 1988  956   10 1989\n",
      "   202    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  22  588   23  785    6 1990    1 1991    2    1 1992 1993   11 1994\n",
      "     5 1995   52  589  957    4 1996   22 1997  678    8    5  832   16\n",
      "     1 1998    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  22  958   10  959   30   54  204 1999    6    1 2000 2001    2    1\n",
      "   960   41   22 2002    1 2003  590 2004    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  22  591  132 2005    6  873   41   22 2006  186 2007 2008   45    6\n",
      "    24  721   11   23  881  103   76  264  577  481 2009  418  186    4\n",
      "   132  390  324   15    5  201  361    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  22  591  132    6  416 2010   15    1  213 2011    3    1  276    2\n",
      "    85    6  181   97  592 2012    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  22    9  946    2    1 2013  961 2014 2015    2    1  251  961 2016\n",
      "  2017    4  419    2    1  277  962    2   95 2018    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  22    9 2019    1 2020 2021    2    1  169    2  963    3  135  288\n",
      "  2022    3   12  593  134   22 2023    6  246  108    1  593    2  495\n",
      "     2  153    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  22    9  964    6    5 2024 2025   41   22   73 2026    5  272 2027\n",
      "  2028    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  22    9    1 2029   15  965 2030 2031    2 2032  277    4  386  153\n",
      "  2033    4    1  966    2   25   99 2034  558    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  22  967  108    3    1 2035    8  968    4   22  594  395  595  247\n",
      "    22   59  285    1  473  204    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  62 2036    1  596  597    9    1 2037  969   96    3    1  468    3\n",
      "  2038    4  970    4  105    1 2039  971    3  970    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [2040  598  574 2041   11    1 2042  112 2043    3  170  289  588   12\n",
      "   599  112   59 2044    1  777  168 2045    2    1  420 2046 2047   16\n",
      "    21  972  128  973 2048    1 2049    2  974  975    3   25 2050    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  23 2051    7    1 2052  976    3 2053  154   23   39  466  189    7\n",
      "     1  977  976    3 2054    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  23 2055  163   50    4 2056 2057 2058  978 2059   15  375 2060    3\n",
      "     5  548 2061 2062    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 979    4  600 2063  738    1  364    6 2064    1 2065    2 2066 2067\n",
      "    21   27 2068   45 2069  214    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 979    4  600  911    1  971   12   70   15  421 2070   71    9 2071\n",
      "    10  587   13 2072    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 127   14    9   32  134    1 2073   76  462  463   93  251 2074    4\n",
      "  2075 2076 2077  109   60 2078 2079   12   75 2080 2081 2082   10    1\n",
      "  2083    2  141   83  114    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 127   31  215   27  353    6  526  124   44 2084 2085   79  308  367\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 127    1  392  165    2  130   19  422    3   12  102 2086    6 2087\n",
      "    16    5 2088  980   20    1  601    2   87  916    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 127  981    3 2089 2090  216  165    1  982   27   32 2091   52 2092\n",
      "  2093   49  107  108    2    5  309 2094 2095    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 149   55   19 2096   53   10  602   55  358 2097   12   14   48   32\n",
      "   983   24 2098   17  166  603    6 2099    1 2100    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 149  101 2101   34   46   53    8   18  423 2102   55   72 2103  312\n",
      "     6  290  362  264  424    3   39  685  984   19  353   53   21 2104\n",
      "   166 2105  291   16  371    1  424    6  290    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   3 2106  217    1  292 2107    1 2108    6   23 2109    1  417  985\n",
      "   604  139    8    5 2110    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   3 2111    1  207  132    6  986 2112   41  919  191  133    5 2113\n",
      "    10 2114    4  425 2115    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   3 2116  293  887  426    2    5 2117 2118 2119    4 2120 2121   62\n",
      "   191  132    3    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   3 2122   23  207  867    6   95    4   22  987   23  868   20  963\n",
      "   117  327    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   3 2123    6  988  989   71   27   32 2124   10  990  293  294    1\n",
      "   427  605  293 2125  171   63  427  605  293  161    6  991 2126    4\n",
      "  2127 2128   11  989    3  589  992    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   3 2129   36  133    1  993  110    4 2130    3  295 2131    4  214\n",
      "  2132   27  133    3 2133    1 2134    4 2135    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   3 2136    1  862  340 2137   84  314 2138 2139    9  994   11    1\n",
      "   513   90    2    1 2140    6  995    5  996  210 2141  129  428 2142\n",
      "   691    2    5 2143   21    9 2144   13    5  117 2145   13    1  692\n",
      "     2  455    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   3  997 2146 2147 2148   13    1 2149 2150    2  218  998  999  108\n",
      "     1 2151 2152   13    1 2153    2    1 2154 2155    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   3 2156  203    1 2157    2  419 2158 2159 2160 2161    1  320   12\n",
      "  2162 2163 2164    6 2165    3  161 2166  111    1  606    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   3 1000  146 2167    5  147    8  427 2168  965   84 2169    4 2170\n",
      "  2171    3   26 1001  607    5  147   12   36  429  133    5 2172    2\n",
      "  1002    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   3  289    1 1003    2   64  104 1004 2173   15    1  344    2   37\n",
      "  2174 2175  215  599  112    4 1005 1006   21   27 1007   11    1 2176\n",
      "     2    1 2177 2178  168 2179    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   3 2180  172  547   23  955 1008   10 2181  169   41   22    9   89\n",
      "  2182 1009    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   3  219  146  608    3    1 2183   56 1010 1011    4    1 2184   18\n",
      "  2185 2186    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   3  430  219 2187  360 2188 1012    6  479   11 2189   12   25 2190\n",
      "     4 2191 1013   73  988 2192    3 2193  479 1014  362    5 2194  801\n",
      "    10 1015 2195    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   3  430  198   14    9  220   12  172   73  839  213    6 2196   10\n",
      "     1  431  461 1016 1017    6 2197    6 2198   10    1  233  351   89\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   3  296   85    5   42 1018 2199 2200 2201    5 2202  218 2203 2204\n",
      "   280 2205  404    4  609  372  322  106 2206  129  610    1 2207    2\n",
      "  2208    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   3  135  611   22    9 2209   10    1  221  173   10  151 2210   10\n",
      "     1   56    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   3  393 2211   15 2212 2213 2214 2215    7  612 2216   10 2217  613\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   3   48 2218   22    9  387    6    1 2219 1019    2    1  702  614\n",
      "     2  153    4    3  182 2220    6    1  614    2 2221    2   12  614\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   3   48  174    5 2222    2  256 1020 1021   16  270  929 1022   11\n",
      "   332 2223    9  220   10    1  347    2    1  257  258    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   3  373  615 2224    5 2225   40  343  616    4   50   21 1023  354\n",
      "    40   42  617    4  824  269    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   3  373    6    1 2226 2227    3    1  175  158    2    1  126    7\n",
      "     1 1024  383    5 2228 2229 2230    3   21   44  197    1  419    4\n",
      "    23  207    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   3  373    6    1 1025 1026 2231 2232    4 2233 2234   31    8 2235\n",
      "  2236 2237    4 2238   19  432    6  297  167   81 2239 2240   81   31\n",
      "     8 2241 2242    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   3   58 1027  150   91   10  618    2    1  488  420  579  619   58\n",
      "   457    1 2243 2244    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   3   58    1 1028    2  130 2245  903 1029 2246  322 1030    3  620\n",
      "  2247  406   17 2248    3  111    1 2249  216  165    4    1 1031 2250\n",
      "     5 2251    2 2252 2253  781   61   39   38 2254 2255    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   3 2256  167   81   31    8 2257   17 2258    1 2259 2260    2    1\n",
      "   621 2261 2262    1  622    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   3  424    6  843 2263 2264  217 2265  429   12    5   57    7    1\n",
      "  2266 2267  140   10    1 1032    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   3 2268    6  113 1033   81 2269    5  297    2 2270 2271   15    5\n",
      "  2272 1034    7    5  278  167 2273   30    2    1 1035   33    3 2274\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   3  577    1  853 2275    4  376 2276 2277    3  433 2278  306 2279\n",
      "    49 2280    7  166 2281 2282    6    1 2283  957    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   3   38 1036    1 1037    2  168    3    1  434    2    5 2284  400\n",
      "  2285   12    8  281    8   14    7 1015   61    1  433  359  184    2\n",
      "   537 1038   14    7    5   64  156    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   3 2286   14    7  143 2287   15 2288 2289    3  623  453 2290   18\n",
      "     5  253  499 2291    3   58    2  113 2292    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   3    1 2293 2294    4  724    4    1 2295   69   14    9  432 2296\n",
      "    10    5  565    6  464   13    1  435    4   26 1039 2297  134    1\n",
      "   624  382   76    3 1040   14    9 2298    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   3    1  554   56 1041 1042   91    1 2299  147    2 1043 1044   10\n",
      "    21   36 1045    1  221  173   10  151 1046  234    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   3    1 1047   94   13    1  625 2300    2    1  842 2301   19 2302\n",
      "   365    2  292  179 2303   31    8    1  176 1048   84 2304   11  191\n",
      "  2305    1  176  421    2  236   11 2306    4    1  176 2307 2308   11\n",
      "  2309    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   3    1  175 2310  926   17 2311 2312 2313   13    1  626 1049    4\n",
      "     1  216 2314 2315   16 2316    6  626    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   3    1  121    2  854 1050  177  348   27  627   91   11  103   17\n",
      "  1051    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   3  752    5   64  156 1052 2317  628  209    7 2318    6 1053  168\n",
      "    11  974  975   20  178  121  120   25  897    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  14  629    5 1054    2 2319 2320   11    1  192  136 1055  436 2321\n",
      "  2322 2323 2324 2325 2326    4 1056   11 2327 2328    4    1 2329 2330\n",
      "  2331    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  14   59  369   45    3    1  267    2    1 2332   15 2333  281  267\n",
      "   369 2334  535    5  522   45 1057    3 2335  255   38 2336    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  14   34   46  630    3  532   11    1  952 2337  340    4 1058  193\n",
      "  1059    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  14   34   46 2338    6   24    5 2339  917   16    1  402 1060   17\n",
      "  1061 1060  184 2340 2341    2  753 2342 1062   40 2343   17 2344 2345\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  14   34 1057    1 2346    2   82  262   11  436    5 1063  631 1064\n",
      "     3 1065  476    3 1040    3 1066    4   35 2347 2348 2349    4 2350\n",
      "     3  865    1 2351 2352    3  947    4    1 2353    3  288    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  14    7  810 1030  996  406  428  902  118  185 2354 2355  280    7\n",
      "  2356    3 2357 2358    4 1067   11    1  437 2359  632 2360    3 2361\n",
      "   633    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  14    7    5  275 1039   12 1068  107 2362    6    1 1069 2363    2\n",
      "     1 2364 2365    4 2366 2367  261    2 2368 2369 2370 2371 2372 2373\n",
      "  2374 2375  782 2376 1070    2 2377 2378    4  274   13   14 2379   12\n",
      "     1 1070    2 2380 2381    1  379    2  607    9    5 2382  275 2383\n",
      "   255  222 1068    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  14    7 2384    6 2385   40    3 2386 2387  124   14 2388    1 2389\n",
      "     2    1 2390    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  14    7   35   92   33   10  298    1  863    2  208  634  269   31\n",
      "     8    1   56   80 2391 2392 2393   84 2394 1071 2395    3    1 1072\n",
      "     4 1073 2396 1071  163   80    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  14    7   20   26  590   12    1 2397 2398   12  552 2399 2400  553\n",
      "    16  171  134    1  381    2    1  136   36    7 2401 2402    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  14    7   33   16    5   92  575 2403 1028   12 1074  572  281  165\n",
      "    13    1  216  409    2    5  438 2404    4  982    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  14    7  171  223    2    1  287    2 1075    4    2    1   69  635\n",
      "   636  176 2405    4  778  779    2 1076    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  14    7  494    8  149 2406    7 2407    1  316  239    6   23 2408\n",
      "  2409    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  14    7    1  439  268    3  633    1 2410  439    3    1  250 1077\n",
      "     1  439  403    2    1 2411  924    4    1 2412  439  268    3    1\n",
      "    69   11 2413  770    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  14  880    8  200  305    8 1078    4  440 2414 2415    4    8  200\n",
      "   403    8    1 2416 2417    4 2418    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  14  143 2419   26  394   15 2420   25 2421 2422 2423    3    5  259\n",
      "   838    6 2424   25 1079    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  14 2425    3    4    9   29 1080    3 2426    3  393    8    5 2427\n",
      "  1081  637    8    5 2428    6    5 2429 2430    2 1081  637   63 2431\n",
      "  1082    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  14    9  604  139   71 1083    6 2432    1  164    8    5  487  334\n",
      "   383    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  14    9  199 2433  152    1  638    2    5  330  201    4   60  815\n",
      "   268   12    9  298   74   11    1 2434 2435  134  997    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  14    9 1080    6  875    4  874    3  532    4   14  109    1  299\n",
      "   637    2    1 1084  276  606    3 2436    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  14    9   94    3  438 1085    4    9  155   78  639  421 2437    1\n",
      "  1086 2438    2 1085    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  14    9    1   29 2439  235   10    1  701    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  14    9  422    3   25 2440    3   12 2441 2442  861   43 2443    8\n",
      "  2444    6 1087   17    5   89    1  968 1014    3   39 2445  694   67\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  25  438 2446 2447    6    5 1088  950   15 2448    2 2449  891  698\n",
      "  2450  189   11 2451 2452    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [2453 1027  150  300 2454 2455  170 1089  542    9    5  311  640  431\n",
      "   535    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [2456  172  238  182  137  558    3 2457 2458  550    7    5  467  311\n",
      "   640  431 2459    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [2460 2461 2462 2463    4 2464 2465  240 1032 2466    3    5   57    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 421 2467 2468 2469  296 1090 2470  300  497 2471    9   18  437 2472\n",
      "   195    4 1058    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  33   10   23 2473   10    1  385    3 2474  375    8 1091  237 2475\n",
      "    23   50   13 2476  308 2477   31    8  638  166 2478    8  200    8\n",
      "     6  995 1092 2479  365    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [2480 2481 1093 2482 2483 2484 1094  641 2485 2486  441  641 1095 2487\n",
      "  2488 2489 2490  482 2491  252 1096 2492    9    1 1097 1094 1098  641\n",
      "  1095    4  912 2493   10 2494  977    1 2495 2496  188    4 2497 2498\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 314 1078 1099    1 1100   13   62 1101 2499  228 2500    8   62  175\n",
      "   309   16    1 1101    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  83  114 2501   19    3  164    3    1 2502  695 2503   42  295 2504\n",
      "   295   95    4  153    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [2505  282    5  953    2  211   34  945    8  211 2506  304   25 2507\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 214  985  604  188    2  530    4   62 1098 2508 1102 2509    1  126\n",
      "   268   12    7  171   33    8    1 1102 2510    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [1041 1042  105   18  221  173   10  151 1046  234   10 1008 1043 1044\n",
      "     3    1   70    2 1103 2511    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  94    3  440  442    1 2512 2513   16    1  625    4 2514    1  477\n",
      "  2515  625 2516 2517    4 2518    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [2519   17    1  292 2520  115    1 2521    2  315    4  718    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [2522  128 2523    3  549  443   16  679 2524    6 2525 2526  642 2527\n",
      "   128   66   28 1104  157 2528 2529 2530    1  444 1105    1  443    2\n",
      "     1  345  434    4   19  643 2531   16  928    6 2532    8    7 1106\n",
      "     3    1  125 2533    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [2534 1107 2535 2536  106 2537  129  428   20    1 2538    4 1108 2539\n",
      "  2540  280 2541 2542  404  196 2543 1107 2544 2545  106 2546 2547  129\n",
      "   428    4 1108 2548 2549  280 2550 2551  404    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [2552   72   24   53  710  445  116  395  290   13 2553    2 2554 1109\n",
      "    79 2555  892   92   55   48  595  145    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [1110  521   12 1111  157 2556  210 2557  129  415    6 2558  644    4\n",
      "    75    7   79 2559    2  644 1103   20 2560 1111  157 2561 2562  210\n",
      "  2563 2564  129   10   60   61   37  119    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [2565 2566    7 1112   74    6  297   18 2567   15 1026   31    8 2568\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  39 2569    6  197  108    6   43 2570  127  124    1 2571    2  168\n",
      "   588  145    6   24  571  215    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  39    2 1050 2572  212 2573    2 2574 2575   31    8 2576 2577    3\n",
      "     5 1063  631 1064    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [1113 1114 2578    4 1114 2579   19   93    3    5   57    4  627 1113\n",
      "     9   60  536    6  410 2580    3    5   57   61    3  178   38  140\n",
      "  2581 2582   57    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  68  262 2583  629 2584  359  177    3 1115 2585 2586 2587  143 2588\n",
      "    11 2589 1116   21  516  973 2590 1117   43 2591    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  68  132    6  313    3 2592    4 2593   23 1118   20 1119 2594    4\n",
      "  1119 2595    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  68 1067    5 2596    2  645 2597 2598    4   96 2599    8   92    8\n",
      "  2600   10 2601 2602 2603    4  519 2604    3  247    9 2605  446    1\n",
      "    68  123   49  109   33    8   50  498  286   10   51   42   50    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  68 2606    6 2607    5 2608  465  152   37 2609    4   13  112  135\n",
      "  2610    1 2611   10    1  181 2612   11 2613 2614    9 2615    3    1\n",
      "    47   41   14 2616  330 1012    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 893 2617   64  104  812  299    3    1  267 1120    2   12 2618    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [2619 2620 2621  105    1 1121  162    2    1   67   10  230   89  872\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [2622   10    1  224 2623   16   37 2624   30   10   56    4   30   10\n",
      "   263   87 2625 2626  583    2    1  266 2627 1021 2628   87   70   15\n",
      "     1 2629 1122 2630    8    2 1123  793    6 2631   10    1 2632    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [2633   30  327    2  717    7  227   13 1124  264   13 2634    3    1\n",
      "  2635    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 643    1   83   72   24 2636  534    6  417 1125    3    5  723   12\n",
      "   521    1  646 1025  424    6   12  334    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [2637    1  598  444 2638  203 2639   19 2640   11  747 2641 2642   21\n",
      "    66   24  259  647   52 2643    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [2644   28 2645   12   23  123    7 1126    6 2646  986 2647   71  547\n",
      "     6  189   15 2648    3 1127    3  313    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [1112 2649   71   28  570   17 2650 1128  204   17   32   16   12  347\n",
      "    15   79 1129    2 2651   18  159 2652   13  290   45    6  526   12\n",
      "   242   16   43 2653  991    7   53   10 2654   44  594  395  116    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  13 2655  296  611   14    9  220   12   22   73  401    1 2656  495\n",
      "     2  153    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  13 1090  135  611  190    9  155    5 1130    2    1   90    2   95\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  13  377   48 1084 2657  387  418    8    1   42  206  648    2 2658\n",
      "  2659    5 2660 2661    3    1 2662    2    1   42 2663 1131    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  13  377   48  288   22  533  431  461 1016 1017    4  556    5  849\n",
      "    15 2664 2665 2666 2667    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  13  576  649  198    1 2668 2669 2670  282    8    1 2671 1097 1132\n",
      "     3    1   69    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  13 1133 1096 2672    1 2673  938    1  287    2 1075 2674    4 2675\n",
      "    98  555 1076 1134    4   38  634 2676  111    1  466 2677 2678    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  13   31  613   76   22 2679   23 2680    9 1004    4   23 2681    4\n",
      "  2682   27  999    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  13    1 1135 2683    1 2684 2685   10  609   88    7    1  543 2686\n",
      "    21    7   18 2687 2688   15    5  267 2689 2690  809 2691   17  210\n",
      "  2692    2 2693  543    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 414    1   67   59 1045    1 2694    2    1 2695    5  408  980  111\n",
      "    42 1132 2696 2697 2698    4  592 1136   27  387    6 2699    1 2700\n",
      "     2 2701   10    1 2702    2  161  619    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  30    2  113    9 2703  225   21    9  650    6 2704   33    8  651\n",
      "    84  301  225    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 136   34   82 2705  348  612 2706   11  177  100 2707    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 167  652    7    1 1003    2 1069   81  783  653   77 1137    4    5\n",
      "   622    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 167   81   15  653   12   28 2708    3   77  654    4 1138   19  253\n",
      "   208    3 1139    8   44   19   93  433 2709    3 2710    4  433  654\n",
      "     6 2711 2712    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [1140    1  254  399   16  326 2713  362  899  286 2714  232    6  575\n",
      "  2715    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  38 2716  212    1   86  122 2717    3    1   47  110   38  744 2718\n",
      "   372    1  356   58  376 2719    3    1  402    2    1  276 1141   86\n",
      "   122 2720   18 2721 1142  155 2722 2723   47   98 2724  745  275   15\n",
      "   803    3    1  276 2725    4    5  635 2726    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  38 1143   10  378  212  683 2727    4 2728 2729 2730   19   63 2731\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 118  480    2   58   33 2732  243   19 2733    4  102  116 2734 2735\n",
      "     6  644    4   43 2736    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 118    1  125  321 2737  172 2738   77  619    3    1  960    4    1\n",
      "  2739   15 2740   20    1 2741 2742    1 2743  394 2744   13   37  613\n",
      "     4    1  774  775 2745    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 118    1  119    1   80 2746    4  284    3 2747  582   31    8  302\n",
      "  2748  389    2  176  217    4  176  193    4  582    3 2749 2750 2751\n",
      "     4 2752    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 223    2  211  904  566  848 2753   16  371    1  687 2754  784    6\n",
      "  2755   25 1013    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 427  605  293  426    2 2756 2757   13  925  546 1066    3 2758 2759\n",
      "     4    9 2760    3 1144 2761 1145    3 2762 2763    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [2764 2765   98  525   76   44 2766    1 2767  115 1146   12   43 1147\n",
      "    34  401   30   15 2768 1109    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 650    6 1148  350 2769   27   45   33    6   24 2770    3 1100    4\n",
      "  1149    4    3  175  160 2771    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 650    6    1 2772    2 2773 2774 2775    3  506 2776    4 2777    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [2778    7    5  181   11 2779 2780 2781  155   78    5 1150 2782  148\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [2783 2784  419 1151   85 2785   12  384  126    4   25 2786   19 2787\n",
      "  2788 2789    1 1024 2790   52    5 1047 1152    6   58   71 2791    6\n",
      "  2792   14    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [2793   25 2794  655 1153 2795    5 2796 2797 1154   20  138 2798    5\n",
      "   522   12   59 1155   45   46 2799    3    1 1156    2  447 2800    4\n",
      "    12    2 2801 2802 2803    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [2804    4 2805  329  328    8    5 2806    3   38  448    2 1157    4\n",
      "     3    1  489  618  448    2  244    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 502 1158   72  100 2807    6    1  529 2808    3    5 2809  341  900\n",
      "   149    1 2810  159   34 2811  923    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 396  556   25  416 2812    3 1000    4 1022   25  299 2813   10 1122\n",
      "     3    1  562    3  289    1  175    2    1 1159  222  992    6  116\n",
      "   274   78 2814    4 2815    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 396    9    1   29  334    2 1159  416    6   28  299 2816   15    1\n",
      "   222  646    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 651   84  301  225    7    1   99   39 2817  225    3    1  860    2\n",
      "   301    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 651   84  301  225   34   46    1  636    2   98 2818   93  120    1\n",
      "   248   10  826    4    1 2819 2820 2821   41  330    2    1 2822 1160\n",
      "   284  164    3    1 2823  225    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [2824 1161   11   30  504    2 2825    7    1 2826 2827    2   58  391\n",
      "  2828 1161   11 2829 2830   45   11 1162    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  36  889    3    1 2831 2832  616  656  139 1093  959    5 2833    8\n",
      "     1 1154  469 1163    8   92    8 2834  656   10 2835    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  36 2836   12  108   15 1147 2837 2838   62   15  162    1 1164 1165\n",
      "  2839 2840    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  36   59  739    8  193 2841 1166    3 1167 1167  469    8 2842 2843\n",
      "     3  162    1 1164    4  348    3    5  263  541    4    5 2844   56\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  36 2845    3    1  131  261   20    1  487  136  657 2846  492    3\n",
      "  2847    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  36   91    1  245  148    3    1 2848  658    4    1 2849 2850   93\n",
      "  2851  227   13 2852  269   30    2  145  659   11 2853 2854   15 1168\n",
      "    36   59 2855   13 1010 1011    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  36  608    3  423 2856    5  198 2857   40    5 2858  855 2859 2860\n",
      "  2861    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  36 2862  192  343   20    1 2863 2864  169    2  940 2865 2866  660\n",
      "  2867    4   20    1  351  438  169 2868    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  36  240   13    5 1169  661    2 2869  111  177  114  589 2870 2871\n",
      "   990 2872  189    4  248    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 639 2873 2874  600 2875 2876  649  170 2877  405   48 1123    9    5\n",
      "  2878 2879  192 2880    4 2881    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 639 2882  524 2883 2884 2885 2886  546 1133 2887  137   48  289    9\n",
      "     5 2888  869    2  662   20    1  660    2  662    3 2889    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [1020 2890   20   26 2891 2892  821   13  405  135   85   14    9  220\n",
      "    12    1   29  257  258   73   24  540    3  305  213  954    1   47\n",
      "     2  213 2893    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 512    4 2894 2895 1074  250  749  750  169  491 2896    2 2897  335\n",
      "   336  169   86  122  939 2898 1170 2899  111    1  941 1171   20 2900\n",
      "   363 1172  663    4   86  122  161  664    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 102 1031   31    8  217  193 2901   28 1173   82  617   15  615   49\n",
      "  2902  410    1 2903  657    8    5 2904    6 2905   43   29   96 2906\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 218  411 2907   28  256  662 2908    3 2909    4   37  100  368  998\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [1174   12  131   70   36  608    3    5   42 2910 1175   67  485 2911\n",
      "    41   36   91    1  245  148 1170 2912  485 2913    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 150  426   13  170 1089  542    3    5 1171    3 2914  550   20    1\n",
      "   184    2 2915    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 150  105   23   99  551  380  142   15    1  857  858    3 2916    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [1176   14    9  220   13  182  807   12  146   59 2917 2918 2919    2\n",
      "   244    1  731  484  366    6 1175    8   44 2920    6 2921    5  567\n",
      "    63 2922    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  31  949  262    2  665    4  512 2923   28   46  629   13 1072  638\n",
      "     4 2924    2 1177  304    1   69    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [2925   27   99   15 2926  202    4  230   89  292 2927  175   15 2928\n",
      "   202    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 144  586   35 2929   16  909  568    4  282  948    6  180  211 1178\n",
      "  2930   15  144  586   49  585 2931   26   11 2932    1 2933 2934   10\n",
      "    43 2935 2936    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 172   59   18 1179   54    3  213 2937    8    1 1180   99 2938 2939\n",
      "  2940 2941 2942  333    4 2943  956    3 2944  370    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [ 172  132    6    1 2945 2946 2947    1 1145   89   10    1   42 2948\n",
      "  2949    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [  89  442  105   14   15 2950  202 2951  202 2952 2953    2   89   42\n",
      "  1018    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1 2954  500 2955    3  313 2956    1   68  123 2957    2   21   68\n",
      "   429  139 2958    1 1181 2959  107  102 2960 2961 2962 2963 2964   52\n",
      "   632    4 2965    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1 1182  666  606  805   33    8    1 2966 2967  666  370  286 1073\n",
      "  2968 2969 2970 2971   27    5  666 2972 2973   21    9 2974    3 1182\n",
      "     3    1 2975  287    2 2976    3 2977  442    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1 2978 2979    9 1183   13  377  252  174   15    1   29  162   20\n",
      "  2980 1142 2981   13  112  182  174    4    1  489  162   20 2982 2983\n",
      "    13 2984  430   85    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1   85 2985    2    1  222  266 2986 2987    4  396 2988    1  222\n",
      "   266  562   13  112  135   85    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1 2989   54    9    1   29   54    3   21    1   42 2990   11 2991\n",
      "   459 2992    9   74    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1  221  173   10  151  425    7   30    2    1  221  224    2 2993\n",
      "   494 2994   11    1  221    2 1184  425  632    4 2995 2996    6  374\n",
      "  2997    3    1 1184  425 1139    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1  437 2998 1185 2999    7    5 3000 1019  139  177 3001  660 3002\n",
      "  1185 3003  579  583   19   94    3    1 3004  480    2    1  250 1077\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1  229 3005 1186    9 1187    6   24  987   29   25   29  456    9\n",
      "  1188    6  246  164    3  813   16    1 3006 3007    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1  229  501  207    7    5  207    2 3008  759  298  936   11    1\n",
      "  3009 3010  115  587    4  261  825  110    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1   50  498  123  127    9   30   12   68 3011    6 3012  418   16\n",
      "   304   23  607   22  983 3013   12  934   61 3014  178 3015 3016  297\n",
      "    23  645   27 3017    5 1178    2  418    4  496   50    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1  251 3018 3019 3020 3021    1  147    3    5  261   11 3022 3023\n",
      "     3   21   36  107   62  833   20  314 3024    3  296 1065    4   20\n",
      "     1 3025  136    3  170    2   12   70    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1   80    9  630    3  667   11    1  714 3026  436    1 3027  217\n",
      "  3028    4    1 3029 3030 3031    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1 1189 3032    2  179    7   94    3    1  110  295    2    1 3033\n",
      "    13   18  356    2 3034 3035 3036 3037    4    7  432   30    2    1\n",
      "    39  208 1189  663    3  326 3038    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1 1190 3039    7    5  219  437  730  969 3040   56    4    1   99\n",
      "  3041    3    1 1190 3042  285    4  659   11    1 3043 3044    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1   90    9  449    3 3045  444  134  219   76    1  458 3046   15\n",
      "  3047 1116    9  107    5 3048   90    4    1 3049   37  444   27 3050\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1   90    9    1   29  351 1191  449   11    1 1192  385  559   13\n",
      "   322  430 3051   11 3052    2    1  962    2  115    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1 1193 3053    7 3054   11 1194 1195    4   14 3055   82    2    1\n",
      "  3056  506 3057    3  442    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1  206  363    7  647   52    1  125 3058    1  206  271 1131    1\n",
      "   206  734    4    1  206  273  913    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1 1196 1197 3059  173   35   33    8    1  668  173    7   18 3060\n",
      "   337   11    1 1196 1197 3061 3062    2  263    4  520  374 3063    6\n",
      "  3064 1179 1056    3   56    4 3065  263    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1  317 3066   12  236  967  450  108  120   23 3067    4  994  450\n",
      "     6  246 3068    2  247   22  429    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1 3069    2    1   90    7 3070   11    1 1191 3071    2  593    2\n",
      "     1  303  648    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1  453    9   29 3072   20 3073 3074    2 1192 3075 3076 3077    3\n",
      "  1198    4  214   14    9  231  199   13   98 3078 3079    3 1198    6\n",
      "  3080    3  393    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1 3081    9  214 3082   52    1  483    2 3083 3084   21    9    6\n",
      "  3085    8    1  299  383    2    1   97 3086   16 3087    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1 1199   35 3088 3089   10  573  407    4  563    2 3090  283 3091\n",
      "  3092 3093 3094  283 3095 3096  408 3097  283    4 3098 3099  408 3100\n",
      "   283    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1  483    7    2    1  233  700  123    4    9  540    3    1  669\n",
      "   624  382    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1 3101    2   58  617 1128   11  615  567    5 1200  890    5 3102\n",
      "    16    5 1200   11  193 1059    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1 1201   77    5  769   64  156    4    5  447 3103 3104  451  628\n",
      "   209   34   46 3105  768    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1  344    2 3106  534    6 3107  569  209    4    1 3108    2 3109\n",
      "  1124    3    1  376 3110 1156    2   64  104    3    1  669 1202  360\n",
      "   113  568   52 3111    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1  937    2 1203    9  294    3 3112   11 3113    1  554 3114    2\n",
      "  1203 3115    4 3116    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1  816 3117  273  922   14    5 3118 3119   16    1 3120 1204    2\n",
      "     1  201 1177    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1  883  127 3121  557   13 3122   15  249 3123 3124 3125 3126    4\n",
      "  3127 3128    1  920    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1   80    7  171 3129   11  302  616   21   35 3130    1 1205  359\n",
      "   340 3131  670    1 1205  327   10  478 1206 1118  302 3132    5 1207\n",
      "  3133 3134 1172    4  407 3135    6 1208    4  161  374    8   92    8\n",
      "    18 3136  870 1207    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1   29 3137 3138    2  399 3139 3140    6    1  624  382   41   44\n",
      "    27 3141    3 3142 3143    4  287  827 3144 3145    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1   29   80    9  671   16    1  137  300  667    6    1  451  300\n",
      "   667    4   74    1  302 3146  413    5 1035 1002 3147   16  436  657\n",
      "     3 3148  163    8   25  245 3149    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1   29 3150  658    2   18  195 3151  284  164    3 3152  758  672\n",
      "     1 3153    3 3154  118    1  119  180   14  353    6 3155 3156   76\n",
      "     1 1150 3157 1148 3158   13    6    1  435   20    1 3159 3160    4\n",
      "   109    1   29   33  160    6 3161 1036    8    5  148    3    5  181\n",
      "    17  239    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1   29  257  258    9  446  511    6   24    3    1   47    2  213\n",
      "    49   26   34 1151   46 3162   11    1 1208  385 1209   78 3163   64\n",
      "   109  303  648    2    1  250 3164    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1  125   70   36   91    1 3165 1166    2 3166 3167    4 3168 3169\n",
      "  3170    3  278 3171    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1 3172 3173    2 3174  306 3175    3    5  682 3176    8    5 1137\n",
      "  3177    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1  391  392   66 3178    6  117   88   11 3179 3180  331    5  491\n",
      "  3181 1210    4 3182   25 3183  652    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1 1121  668  224 1211  197   13  296 1029  289   16  435  185  500\n",
      "  3184    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1  354 3185   11   18  789   66   24  178  297    2  510  837  656\n",
      "    17  394 3186 3187 3188 3189  357 3190    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1  140    2    5   57    7    1   45  140   41    5 3191 1086 3192\n",
      "    13 1212   93    5 3193    4    5  673 3194    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1 3195    7    5 3196 3197 3198 3199  227   13 3200    2 3201  447\n",
      "   218    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1 1213 3202    6   40 3203  680    4 3204    3   18  834 3205    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1  878    2 3206  115   12 1214  452   66   32   24 3207  371   45\n",
      "  1215 3208    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1 3209 3210   12    1 1001   73  171  411    6   24    5 3211    8\n",
      "    23  191    9    4 1216    1 1009  823    6  246  450    3   17    6\n",
      "  3212  450    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1  307   19   79   60   61  814  406  497    3  281  154    1  447\n",
      "   218   48 3213  451  210 3214  129    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1 1217 1034    3  167   81    7  627    2  148 3215   77  654    4\n",
      "  1138    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1  441 1186    2   26 1218   34   46 3216   13 3217 3218 3219    3\n",
      "  1127  154   82 3220   27  337    1 1218  204   69  248  188    4  177\n",
      "    27  449    1   90 1174    3  219 1155  177   27  449    1   90    2\n",
      "     1 3221 3222    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1   39  772    2  113  215    9    1 3223   21    9  199    6   28\n",
      "     5  443    4 3224   92 1052    1  571  352    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1  541    9 1183    3 1219 1220   13  170  405   85    4   13 3225\n",
      "    13  135  649  198    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1 3226  459 3227    9  630   11 3228  674 3229  674   86  674    2\n",
      "  1221  446    3  219    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1  673 3230    2    1  356    7 3231   98 3232   16   93    1 3233\n",
      "     2  301    4   16 3234    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1   42 1222  413    9  133   11    1  388   13  138  300 3235   20\n",
      "     1  415    2    1 3236  302   80    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1 3237 1213 3238  187    5  367 3239    2 1144   49 3240   78 3241\n",
      "    15  626 1049 3242   13    1 3243    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1 3244   54  150  105    1 3245 3246 3247 3248 3249  187 3250   51\n",
      "     1  501  524    2    1 3251 3252    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1 3253 3254    2    1   54 1188    8  223    2    1 1173   92  294\n",
      "  3255 3256  162 3257    9 1223  618 3258  204  386    9  338    6 3259\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1  136 1023   82 1206 3260   15  993   10 1149  113   27  409   10\n",
      "  3261 3262 3263    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1  621 1224  143 3264    1  622  203    5 3265   31    8 3266   17\n",
      "   930    3   21  658   31   81   19  432 1033   81    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1  420 1129    9    6 3267    5  318   10   43  136 1199    1  192\n",
      "   136 1055   49    1 3268    9 1209 3269    6  212 3270    2 3271  343\n",
      "   773 3272    4 3273    2   50    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1   38  564  398    1 3274  398    7   63    1  720  398  124   25\n",
      "   885    7    1  131    8    1 3275  559  397  886  457   10    1 3276\n",
      "     2 3277   10 3278    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1 3279    2  141   83  114 3280    3   95    9 3281   11    5   65\n",
      "     2  208  141 3282  111    1 3283  141 3284 1225    1  898 3285 1226\n",
      "  3286    4    1 1227 3287  901 3288    8   92    8    1  141 1228  470\n",
      "   350 1227 3289   21  294    1  295  251  141 1228  470    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1 3290    9 1140  964   16   49 3291 3292   10 3293 3294 3295 3296\n",
      "  1229 3297  411  527    2    1 3298 3299 1229    4 3300   18 3301 3302\n",
      "   412    3    1  121    2 3303 3304  390 3305    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1 1230    2    1 3306  123  389  675  645    2 3307    2    1   90\n",
      "     2  176 1048    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1 1230    2    1 3308    7  409    2    5  364    4 3309 3310 3311\n",
      "  3312   11  365    2 3313    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1  131   70 3314 3315 1216  146    6   24    5  223    2 3316    4\n",
      "  3317  905 1225  366    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1   99  668  224 1211  197   16    1 3318 3319 3320 3321  196 3322\n",
      "   224   28   46  671   20    1 3323 1181  110    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1   67 3324   15  451  448    3   25   45   54    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1   67    9 3325   52  259 1231    8   60 1232   27 1223   15   18\n",
      "  3326  107   13  137  170  288 3327   12    1 1232    3 1062    4 3328\n",
      "    73   32   24 3329  164    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1 1233    7 1099    3 3330   21 3331    1  218    6  486 1234    6\n",
      "  3332   25 3333 3334   14 3335 3336    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1 1235    2  130    7   35  422    3    5   65    2  675    4 3337\n",
      "    18 3338 3339    6    1 1235    2 3340 1236    4    6    5 3341 3342\n",
      "  3343    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1 3344    2    5 3345 3346   22 3347    8    5 3348   49  109  835\n",
      "     3  375  120    1  292  471 3349 3350    2    1  669 1202    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1  655    2    1 1130  507  508    9  253  623    4 3351 3352 3353\n",
      "     2    1  675 1187    2    5 3354  623  156  345    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1  435  147    2  217 3355   10  580    7  612   91   11    5  565\n",
      "     8   19   39 3356 1051    3 3357 3358    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1 3359  337    5 3360 3361 3362 3363   93    5 1237    2  235    4\n",
      "     5 1237    2 3364    7   63    1  195    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1 3365    2  564 3366   52   18  397 3367    7   63    5  884 3368\n",
      "     4 3369   20 3370   30 3371    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1  216    2  130   35 3372 3373 3374 3375    1 3376   21 3377    1\n",
      "  3378 3379    2  441 3380    4  102 1236    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1  254 3381  100  323    6 3382   81 3383 3384 1217  653   49   21\n",
      "   578  621 1224    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1 3385    7  643  671   11    5  646   17   18 1238   32   11 3386\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]\n",
      " [   1 3387  195  323    6    5  160   71  517  711    2 3388    4   26\n",
      "   254    7 3389  771  196  234  323 3390    6    5  786  160   71  517\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x000000DD8DC3E4E0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1395, in __del__\n",
      "    if self._handle is not None and self._session._session is not None:\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 387 samples, validate on 53 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-3dfd03fa4234>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m bilstm_params_datasets = hyperparameter_opt_v1_datasets(datasets, model_v1_BiLSTM, \n\u001b[1;32m----> 2\u001b[1;33m                                     bilstm_models, bilstm_batch_sizes, bilstm_epochs, bilstm_units)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-60-7f5fbf0b6cda>\u001b[0m in \u001b[0;36mhyperparameter_opt_v1_datasets\u001b[1;34m(datasets, model, embedding_models, batch_sizes, epochs, units)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         configs, best = hyperparameter_opt_v1(dataset, model, embedding_models, \n\u001b[1;32m---> 29\u001b[1;33m                               batch_sizes, epochs, units)\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0mparams_per_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'configs'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'best'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mbest\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mparams_per_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-60-7f5fbf0b6cda>\u001b[0m in \u001b[0;36mhyperparameter_opt_v1\u001b[1;34m(dataset, model, embedding_models, batch_sizes, epochs, units)\u001b[0m\n\u001b[0;32m     15\u001b[0m                         \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'epoch'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'bs'\u001b[0m \u001b[1;33m:\u001b[0m                                 \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'embed'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0membed_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'unit'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0munit\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Optimization Testing : {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m                         \u001b[0mf1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_f1s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_f1s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreparation_v1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m                             \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membed_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0munit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m                         \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConfiguration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m                                              \u001b[0mtrain_f1s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_f1s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                         \u001b[0mconfigurations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-63-b941dddbe6ad>\u001b[0m in \u001b[0;36mpreparation_v1\u001b[1;34m(dataset, model_func, embedding, epochs, batch_size, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m     history = model.fit(train_words_padded, np.array(train_binaries_padded_cat),\n\u001b[0;32m     41\u001b[0m                     batch_size=batch_size, epochs=epochs, validation_data = (test_words_padded, \n\u001b[1;32m---> 42\u001b[1;33m                     np.array(test_binaries_padded_cat)), verbose=1, callbacks=[metrics_v1])\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[1;31m# Use trained model to predict the test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_words_padded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2657\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2658\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_make_callable_from_options'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2659\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[1;34m()\u001b[0m\n\u001b[0;32m    202\u001b[0m                     \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0muninitialized_vars\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m                     \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m     \u001b[1;31m# hack for list_devices() function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;31m# list_devices() function is not available under tensorflow r1.3.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1100\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1272\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1273\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1276\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1259\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1260\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1261\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m   1263\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1293\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1294\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1295\u001b[1;33m       \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1297\u001b[0m   \u001b[1;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bilstm_params_datasets = hyperparameter_opt_v1_datasets(datasets, model_v1_BiLSTM, \n",
    "                                    bilstm_models, bilstm_batch_sizes, bilstm_epochs, bilstm_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Configuration(name='model_v1_BiLSTM', params={'epoch': 2, 'bs': 1, 'embed': 'glove.6B.50d.txt', 'unit': 50}, train_scores=[(array([0.98231347, 0.68089765]), array([0.98432121, 0.65370468]), array([0.98331632, 0.66702413]), array([37184,  1903], dtype=int64)), (array([0.98624112, 0.77136515]), array([0.98891997, 0.73042564]), array([0.98757872, 0.75033738]), array([37184,  1903], dtype=int64))], test_scores=[(array([0.98093928, 0.65909091]), array([0.98229044, 0.64206642]), array([0.98161439, 0.65046729]), array([5082,  271], dtype=int64)), (array([0.98212883, 0.68965517]), array([0.98406139, 0.66420664]), array([0.98309416, 0.67669173]), array([5082,  271], dtype=int64))], f1=0.683111954459203)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilstm_params_datasets['Wikipedia']['best']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.3 A-BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_bilstm_models = models\n",
    "a_bilstm_batch_sizes = [32]\n",
    "a_bilstm_epochs = [4]\n",
    "a_bilstm_units = [25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Testing : {'epoch': 4, 'bs': 32, 'embed': 'glove.6B.50d.txt', 'unit': 25}\n",
      "----------------------\n",
      "# Rows train : 4833\n",
      "# Rows test : 606\n",
      "# Rows dataset : 5439\n",
      "----------------------\n",
      "# Sents train : 387\n",
      "# Sents test : 53\n",
      "# Sents dataset : 440\n",
      "----------------------\n",
      "# Words : 162275\n",
      "# Vocab : 3641\n",
      "# Words missing embedding : 162\n",
      "Embedding shape : (3641, 50)\n",
      "Max length sentence : 103\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "in (InputLayer)              (None, 103)               0         \n",
      "_________________________________________________________________\n",
      "em (Embedding)               (None, 103, 50)           182050    \n",
      "_________________________________________________________________\n",
      "dr (Dropout)                 (None, 103, 50)           0         \n",
      "_________________________________________________________________\n",
      "bi (Bidirectional)           (None, 103, 50)           15200     \n",
      "_________________________________________________________________\n",
      "at (AttentionLayer)          (None, 50)                2600      \n",
      "_________________________________________________________________\n",
      "ds (Dense)                   (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 199,952\n",
      "Trainable params: 199,952\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Training set length : 4833\n",
      "Test set length : 606\n",
      "Training set length : 4833\n",
      "Test set length : 606\n",
      "Train on 4833 samples, validate on 606 samples\n",
      "Epoch 1/4\n",
      "4833/4833 [==============================] - 40s 8ms/step - loss: 0.6642 - acc: 0.6098 - val_loss: 0.6888 - val_acc: 0.5594\n",
      "Training P : 0.5673758865248227, R : 0.20931449502878074, F1 : 0.3058103975535168\n",
      "Testing P : 0.49404761904761907, R : 0.3132075471698113, F1 : 0.38337182448036955\n",
      "--------------------Targets-------------------------\n",
      "[1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0 0 1 1 0 1 0 0 1 1 0 0\n",
      " 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 1\n",
      " 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0\n",
      " 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1\n",
      " 1 0 0 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0\n",
      " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0\n",
      " 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 1 1\n",
      " 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0\n",
      " 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 0 0 0 0 1 1 0 0 0 1 0 1 0\n",
      " 0 0 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0\n",
      " 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 1 1 0 1\n",
      " 0 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 0 0 0 0 0\n",
      " 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 0\n",
      " 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 1 1 1 1 0 0 0\n",
      " 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 1\n",
      " 0 1 1 0 0 0 0 1 1 1 0 0 1 1]\n",
      "--------------------Predictions-------------------------\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1\n",
      " 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Epoch 2/4\n",
      "4833/4833 [==============================] - 21s 4ms/step - loss: 0.6514 - acc: 0.6147 - val_loss: 0.6862 - val_acc: 0.5644\n",
      "Training P : 0.631578947368421, R : 0.16326530612244897, F1 : 0.2594594594594595\n",
      "Testing P : 0.5094339622641509, R : 0.1018867924528302, F1 : 0.169811320754717\n",
      "--------------------Targets-------------------------\n",
      "[1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0 0 1 1 0 1 0 0 1 1 0 0\n",
      " 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 1\n",
      " 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0\n",
      " 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1\n",
      " 1 0 0 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0\n",
      " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0\n",
      " 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 1 1\n",
      " 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0\n",
      " 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 0 0 0 0 1 1 0 0 0 1 0 1 0\n",
      " 0 0 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0\n",
      " 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 1 1 0 1\n",
      " 0 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 0 0 0 0 0\n",
      " 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 0\n",
      " 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 1 1 1 1 0 0 0\n",
      " 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 1\n",
      " 0 1 1 0 0 0 0 1 1 1 0 0 1 1]\n",
      "--------------------Predictions-------------------------\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Epoch 3/4\n",
      "4833/4833 [==============================] - 21s 4ms/step - loss: 0.6347 - acc: 0.6416 - val_loss: 0.6617 - val_acc: 0.6205\n",
      "Training P : 0.6539735099337748, R : 0.413396127681842, F1 : 0.5065726194293043\n",
      "Testing P : 0.5770925110132159, R : 0.49433962264150944, F1 : 0.5325203252032521\n",
      "--------------------Targets-------------------------\n",
      "[1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0 0 1 1 0 1 0 0 1 1 0 0\n",
      " 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 1\n",
      " 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0\n",
      " 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1\n",
      " 1 0 0 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0\n",
      " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0\n",
      " 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 1 1\n",
      " 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0\n",
      " 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 0 0 0 0 1 1 0 0 0 1 0 1 0\n",
      " 0 0 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0\n",
      " 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 1 1 0 1\n",
      " 0 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 0 0 0 0 0\n",
      " 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 0\n",
      " 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 1 1 1 1 0 0 0\n",
      " 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 1\n",
      " 0 1 1 0 0 0 0 1 1 1 0 0 1 1]\n",
      "--------------------Predictions-------------------------\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 0 1 1 0 0 0 1 1 1 1 1 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 0 1 1 0 0 0 0 0 1\n",
      " 0 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0\n",
      " 0 0 1 0 1 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 1\n",
      " 1 1 0 0 0 1 1 1 0 1 0 0 1 1]\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4833/4833 [==============================] - 21s 4ms/step - loss: 0.5665 - acc: 0.7008 - val_loss: 0.5598 - val_acc: 0.7129\n",
      "Training P : 0.8115038115038115, R : 0.6127681841967556, F1 : 0.6982707215265356\n",
      "Testing P : 0.7022222222222222, R : 0.5962264150943396, F1 : 0.6448979591836734\n",
      "--------------------Targets-------------------------\n",
      "[1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0 0 1 1 0 1 0 0 1 1 0 0\n",
      " 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 1\n",
      " 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0\n",
      " 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1\n",
      " 1 0 0 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0\n",
      " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0\n",
      " 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 1 1\n",
      " 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0\n",
      " 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 0 0 0 0 1 1 0 0 0 1 0 1 0\n",
      " 0 0 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0\n",
      " 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 1 1 0 1\n",
      " 0 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 0 0 0 0 0\n",
      " 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 0\n",
      " 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 1 1 1 1 0 0 0\n",
      " 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 1\n",
      " 0 1 1 0 0 0 0 1 1 1 0 0 1 1]\n",
      "--------------------Predictions-------------------------\n",
      "[0 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 0 0\n",
      " 0 1 0 1 1 0 0 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 0 0\n",
      " 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1\n",
      " 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0\n",
      " 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 1 0 0\n",
      " 1 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 1 0 1 0\n",
      " 0 0 1 0 1 0 0 0 0 1 1 0 0 1 1 1 0 1 1 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 1\n",
      " 0 1 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 0 0 1 1 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0\n",
      " 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 0 1 0 1 1 1 0 0\n",
      " 0 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1\n",
      " 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 0 0 1 1\n",
      " 0 0 1 1 0 1 0 1 0 1 0 0 1 0]\n",
      "Hyperparameter Tuning took 611.125\n"
     ]
    }
   ],
   "source": [
    "bilstm_params_datasets = hyperparameter_opt_v2_datasets(datasets, model_v2_attention_BiLSTM, \n",
    "                                    a_bilstm_models, a_bilstm_batch_sizes, a_bilstm_epochs, a_bilstm_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.4 CA-BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_bilstm_models = models\n",
    "ca_bilstm_batch_sizes = [1]\n",
    "ca_bilstm_epochs = [4]\n",
    "ca_bilstm_units_char = [25]\n",
    "ca_bilstm_units_word = [50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Testing : {'epoch': 4, 'bs': 1, 'embed': 'glove.6B.50d.txt', 'units_char': 25, 'units_word': 50}\n",
      "----------------------\n",
      "# Rows train : 4833\n",
      "# Rows test : 606\n",
      "# Rows dataset : 5439\n",
      "----------------------\n",
      "# Sents train : 387\n",
      "# Sents test : 53\n",
      "# Sents dataset : 440\n",
      "----------------------\n",
      "# Words : 10722\n",
      "# Vocab : 3896\n",
      "# Words missing embedding : 1298\n",
      "Embedding shape : (3896, 50)\n",
      "# Chars : 52647\n",
      "# Vocab (chars) : 118\n",
      "# Chars missing embedding : 72\n",
      "Embedding shape : (118, 50)\n",
      "Max length sentence : 101\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inc (InputLayer)                (None, 101, 11)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inw (InputLayer)                (None, 101)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tsce (TimeDistributed)          (None, 101, 11, 50)  5900        inc[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "emw (Embedding)                 (None, 101, 50)      194800      inw[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "tscls (TimeDistributed)         (None, 101, 50)      15200       tsce[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "co (Concatenate)                (None, 101, 100)     0           emw[0][0]                        \n",
      "                                                                 tscls[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "sp1 (SpatialDropout1D)          (None, 101, 100)     0           co[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "bi (Bidirectional)              (None, 101, 100)     60400       sp1[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "sp2 (SpatialDropout1D)          (None, 101, 100)     0           bi[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "ts (TimeDistributed)            (None, 101, 2)       202         sp2[0][0]                        \n",
      "==================================================================================================\n",
      "Total params: 276,502\n",
      "Trainable params: 276,502\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Training set length : 387\n",
      "Test set length : 53\n",
      "Training set length : 387\n",
      "Test set length : 53\n",
      "Training set length : 387\n",
      "Test set length : 53\n",
      "387\n",
      "53\n",
      "Train on 387 samples, validate on 53 samples\n",
      "Epoch 1/4\n",
      "387/387 [==============================] - 99s 257ms/step - loss: 0.3833 - acc: 0.8278 - val_loss: 0.2601 - val_acc: 0.8880\n",
      "(53, 101, 2)\n",
      "Training P : 0.13479033776011712, R : 0.6773515501839201, F1 : 0.224838653410082\n",
      "Testing P : 0.11072862880091795, R : 0.7121771217712177, F1 : 0.1916583912611718\n",
      "--------------------Targets-------------------------\n",
      "[0 0 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0\n",
      " 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
      " 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1\n",
      " 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0\n",
      " 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0\n",
      " 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0\n",
      " 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "--------------------Predictions-------------------------\n",
      "[0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0\n",
      " 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1\n",
      " 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1\n",
      " 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1\n",
      " 0 0 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 0\n",
      " 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 1\n",
      " 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0\n",
      " 1 0 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 0 1 0 1\n",
      " 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0\n",
      " 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387/387 [==============================] - 74s 192ms/step - loss: 0.2779 - acc: 0.8775 - val_loss: 0.2493 - val_acc: 0.8898\n",
      "(53, 101, 2)\n",
      "Training P : 0.16018518518518518, R : 0.7272727272727273, F1 : 0.2625438679692687\n",
      "Testing P : 0.1259305210918114, R : 0.7490774907749077, F1 : 0.21561338289962823\n",
      "--------------------Targets-------------------------\n",
      "[0 0 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0\n",
      " 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
      " 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1\n",
      " 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0\n",
      " 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0\n",
      " 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0\n",
      " 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "--------------------Predictions-------------------------\n",
      "[0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0\n",
      " 1 0 1 0 0 1 0 0 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1\n",
      " 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1\n",
      " 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1\n",
      " 0 0 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 0\n",
      " 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 1\n",
      " 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0\n",
      " 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0\n",
      " 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387/387 [==============================] - 75s 193ms/step - loss: 0.2422 - acc: 0.8918 - val_loss: 0.2434 - val_acc: 0.8887\n",
      "(53, 101, 2)\n",
      "Training P : 0.1596813725490196, R : 0.6847083552285864, F1 : 0.2589684984597038\n",
      "Testing P : 0.13083451202263083, R : 0.6826568265682657, F1 : 0.21958456973293766\n",
      "--------------------Targets-------------------------\n",
      "[0 0 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0\n",
      " 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
      " 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1\n",
      " 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0\n",
      " 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0\n",
      " 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0\n",
      " 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "--------------------Predictions-------------------------\n",
      "[0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 0\n",
      " 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1\n",
      " 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1\n",
      " 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0\n",
      " 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 1\n",
      " 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0\n",
      " 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0\n",
      " 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387/387 [==============================] - 76s 195ms/step - loss: 0.2153 - acc: 0.9064 - val_loss: 0.2481 - val_acc: 0.8893\n",
      "(53, 101, 2)\n",
      "Training P : 0.1320620842572062, R : 0.7824487651077247, F1 : 0.22598269843678856\n",
      "Testing P : 0.10623306233062331, R : 0.7232472324723247, F1 : 0.18525519848771266\n",
      "--------------------Targets-------------------------\n",
      "[0 0 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0\n",
      " 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
      " 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1\n",
      " 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0\n",
      " 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0\n",
      " 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0\n",
      " 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "--------------------Predictions-------------------------\n",
      "[0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0\n",
      " 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 0 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1\n",
      " 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1\n",
      " 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0\n",
      " 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1\n",
      " 0 0 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0\n",
      " 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 0\n",
      " 0 0 1 0 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 1\n",
      " 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0\n",
      " 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length : 387\n",
      "Test set length : 53\n",
      "Training set length : 387\n",
      "Test set length : 53\n",
      "Hyperparameter Tuning took 1431.171875\n"
     ]
    }
   ],
   "source": [
    "ca_bilstm_params_datasets = hyperparameter_opt_char_v1_datasets(datasets, ca_bilstm_models, \\\n",
    "                ca_bilstm_batch_sizes, ca_bilstm_epochs, \\\n",
    "                ca_bilstm_units_char, ca_bilstm_units_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.5 ELMo-R-BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_r_bilstm_epochs = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Testing : {'epoch': 1}\n",
      "----------------------\n",
      "# Rows train : 75\n",
      "# Rows test : 30\n",
      "# Rows dataset : 105\n",
      "----------------------\n",
      "# Sents train : 5\n",
      "# Sents test : 3\n",
      "# Sents dataset : 8\n",
      "----------------------\n",
      "# Words : 213\n",
      "# Vocab : 143\n",
      "# Words missing embedding : 4\n",
      "Embedding shape : (143, 50)\n",
      "Max length sentence : 53\n",
      "Training set length : 5\n",
      "Test set length : 3\n",
      "Training set length : 5\n",
      "Test set length : 3\n",
      "training extension 1\n",
      "shape train words (5, 53)\n",
      "shape train words extension (6, 53)\n",
      "shape train binaries extension (1, 53, 1)\n",
      "shape train binaries (5, 53, 1)\n",
      "shape train binaries (6, 53, 1)\n",
      "test cutoff 2\n",
      "shape test words cut (2, 53)\n",
      "shape test words (3, 53)\n",
      "(6, 53, 1)\n",
      "(3, 53, 1)\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-101-3be893ab16ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0melmo_r_bilstm_params_datasets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhyperparameter_opt_elmo_v1_datasets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset_check\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m                             \u001b[0mmodel_v1_ElMo_residual_BiLSTM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melmo_r_bilstm_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-60-7f5fbf0b6cda>\u001b[0m in \u001b[0;36mhyperparameter_opt_elmo_v1_datasets\u001b[1;34m(datasets, model, embed_model, epochs)\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         configs, best = hyperparameter_opt_elmo_v1(dataset, model, embed_model, \n\u001b[1;32m--> 110\u001b[1;33m                               epochs)\n\u001b[0m\u001b[0;32m    111\u001b[0m         \u001b[0mparams_per_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'configs'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'best'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mbest\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mparams_per_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-60-7f5fbf0b6cda>\u001b[0m in \u001b[0;36mhyperparameter_opt_elmo_v1\u001b[1;34m(dataset, model, embed_model, epochs)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'epoch'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Optimization Testing : {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mf1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_f1s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_f1s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreparation_elmo_v1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m             \u001b[0membed_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConfiguration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m                                 \u001b[0mtrain_f1s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_f1s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[0mconfigurations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-98-10535b17e461>\u001b[0m in \u001b[0;36mpreparation_elmo_v1\u001b[1;34m(dataset, embedding, model_func, epochs)\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_binaries_padded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m     \u001b[1;31m# Fit the model with a custom metric to obtain the F1-scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent_max_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"adam\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sparse_categorical_crossentropy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-dac6cd1e443a>\u001b[0m in \u001b[0;36mmodel_v1_ElMo_residual_BiLSTM\u001b[1;34m(sent_max_length, batch_size)\u001b[0m\n\u001b[0;32m     58\u001b[0m                            recurrent_dropout=0.2, dropout=0.2, name='ls1'), name='bi1')(embed)\n\u001b[0;32m     59\u001b[0m     lstm_2 = Bidirectional(LSTM(units=512, return_sequences=True,\n\u001b[1;32m---> 60\u001b[1;33m                                recurrent_dropout=0.2, dropout=0.2, name='ls2'), name='bi2')(lstm_1)\n\u001b[0m\u001b[0;32m     61\u001b[0m     \u001b[0mlstm_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlstm_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlstm_2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTimeDistributed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"softmax\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ds'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ts'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\layers\\wrappers.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m         \u001b[1;31m# Applies the same workaround as in `RNN.__call__`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    429\u001b[0m                                          \u001b[1;34m'You can build it manually via: '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[1;32m--> 431\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    432\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\layers\\wrappers.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    546\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 548\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    549\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    459\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstep_input_shape\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mconstants_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 461\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_input_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    462\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m         \u001b[1;31m# set or validate state_spec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m   1803\u001b[0m             \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecurrent_initializer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1804\u001b[0m             \u001b[0mregularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecurrent_regularizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1805\u001b[1;33m             constraint=self.recurrent_constraint)\n\u001b[0m\u001b[0;32m   1806\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1807\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint)\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m         weight = K.variable(initializer(shape),\n\u001b[0m\u001b[0;32m    250\u001b[0m                             \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m                             \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\initializers.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, shape, dtype)\u001b[0m\n\u001b[0;32m    251\u001b[0m             \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflat_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m         \u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m         \u001b[1;31m# Pick the one with the correct shape.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[0mq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mu\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mflat_shape\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36msvd\u001b[1;34m(a, full_matrices, compute_uv)\u001b[0m\n\u001b[0;32m   1442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1443\u001b[0m         \u001b[0msignature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'D->DdD'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'd->ddd'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1444\u001b[1;33m         \u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgufunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1445\u001b[0m         \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1446\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_realType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "elmo_r_bilstm_params_datasets = hyperparameter_opt_elmo_v1_datasets([dataset_check], \\\n",
    "                            model_v1_ElMo_residual_BiLSTM, models[0], elmo_r_bilstm_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.6 ELMo-R-BiLSTM-h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_r_bilstm_h_epochs = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_r_bilstm_h_params_datasets = hyperparameter_opt_elmo_v1_datasets(datasets[0:1], \\\n",
    "                            model_v1_ElMo_residual_BiLSTM_h, models[0], elmo_r_bilstm_h_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.7 ELMo-R-BiLSTM-e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_r_bilstm_e_epochs = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Testing : {'epoch': 1}\n",
      "----------------------\n",
      "# Rows train : 75\n",
      "# Rows test : 30\n",
      "# Rows dataset : 105\n",
      "----------------------\n",
      "# Sents train : 5\n",
      "# Sents test : 3\n",
      "# Sents dataset : 8\n",
      "----------------------\n",
      "# Words : 213\n",
      "# Vocab : 143\n",
      "# Words missing embedding : 4\n",
      "Embedding shape : (143, 50)\n",
      "Max length sentence : 53\n",
      "Training set length : 5\n",
      "Test set length : 3\n",
      "Training set length : 5\n",
      "Test set length : 3\n",
      "Training set length : 5\n",
      "Test set length : 3\n",
      "training extension 1\n",
      "shape train words (5, 53)\n",
      "shape train words extension (6, 53)\n",
      "shape train binaries extension (1, 53, 1)\n",
      "shape train binaries (5, 53, 1)\n",
      "shape train binaries (6, 53, 1)\n",
      "test cutoff 2\n",
      "shape test words cut (2, 53)\n",
      "shape test words (3, 53)\n",
      "(6, 53, 1)\n",
      "(2, 53, 1)\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "training extension 1\n",
      "shape train words (5, 53)\n",
      "shape train words extension (6, 53)\n",
      "shape train binaries extension (1, 53, 1)\n",
      "shape train binaries (5, 53, 1)\n",
      "shape train binaries (6, 53, 1)\n",
      "test cutoff 2\n",
      "shape test words cut (2, 53)\n",
      "shape test words (3, 53)\n",
      "(6, 53, 1)\n",
      "(2, 53, 1)\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inw (InputLayer)                (None, 53)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "em (Embedding)                  (None, 53, 50)       7150        inw[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "ine (InputLayer)                (None, 53)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dr (Dropout)                    (None, 53, 50)       0           em[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "la (Lambda)                     (None, 53, 1024)     0           ine[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "co (Concatenate)                (None, 53, 1074)     0           dr[0][0]                         \n",
      "                                                                 la[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "bi1 (Bidirectional)             (None, 53, 1024)     6500352     co[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "bi2 (Bidirectional)             (None, 53, 1024)     6295552     bi1[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, 53, 1024)     0           bi1[0][0]                        \n",
      "                                                                 bi2[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "ts (TimeDistributed)            (None, 53, 2)        2050        add_33[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 12,805,104\n",
      "Trainable params: 12,805,104\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "[['epidexipteryx', 'also', 'preserved', 'a', 'covering', 'of', 'simpler', 'body', 'feathers', 'composed', 'of', 'parallel', 'barbs', 'as', 'in', 'more', 'primitive', 'feathered', 'dinosaurs', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_'], ['her', 'novel', 'the', 'good', 'earth', 'was', 'the', 'best-selling', 'fiction', 'book', 'in', 'the', 'u.s.', 'in', '1931', 'and', '1932', 'and', 'won', 'the', 'pulitzer', 'prize', 'in', '1932', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_'], ['however', 'it', 'was', 'not', 'until', 'the', '1970s', 'when', 'indigenous', 'australians', 'both', 'australian', 'aborigines', 'and', 'torres', 'strait', 'islanders', 'became', 'more', 'politically', 'active', 'that', 'there', 'emerged', 'powerful', 'movement', 'for', 'the', 'recognition', 'of', 'aboriginal', 'land', 'rights', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_'], ['however', 'unlike', 'in', 'modern-style', 'rectrices', 'tail', 'feathers', 'the', 'vanes', 'were', 'not', 'branched', 'into', 'individual', 'filaments', 'but', 'made', 'up', 'of', 'a', 'single', 'ribbon-like', 'sheet', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_'], ['in', '1921', 'buck', 'mother', 'died', 'of', 'a', 'tropical', 'disease', 'sprue', 'and', 'shortly', 'afterward', 'her', 'father', 'moved', 'in', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_'], ['in', '1921', 'buck', 'mother', 'died', 'of', 'a', 'tropical', 'disease', 'sprue', 'and', 'shortly', 'afterward', 'her', 'father', 'moved', 'in', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_', '_pad_']]\n",
      "[[11, 12, 27, 6, 28, 2, 29, 30, 13, 31, 2, 32, 33, 7, 3, 14, 34, 35, 36, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [15, 37, 1, 38, 39, 8, 1, 40, 41, 42, 3, 1, 43, 3, 44, 4, 16, 4, 45, 1, 46, 47, 3, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [17, 48, 8, 18, 49, 1, 50, 51, 52, 53, 54, 19, 55, 4, 56, 57, 58, 59, 14, 60, 61, 9, 62, 63, 64, 65, 66, 1, 67, 2, 5, 10, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [17, 68, 3, 69, 70, 21, 13, 1, 71, 72, 18, 73, 74, 75, 76, 77, 78, 79, 2, 6, 80, 81, 82, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3, 83, 84, 85, 86, 2, 6, 87, 88, 89, 4, 90, 91, 15, 92, 93, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3, 83, 84, 85, 86, 2, 6, 87, 88, 89, 4, 90, 91, 15, 92, 93, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "Train on 6 samples, validate on 2 samples\n",
      "Epoch 1/1\n",
      "6/6 [==============================] - 79s 13s/step - loss: 0.7501 - acc: 0.7673 - val_loss: 0.9293 - val_acc: 0.4906\n",
      "ELMo Metric Test Prediction\n",
      "(2, 53, 1)\n",
      "(106,)\n",
      "(5,)\n",
      "input shape to elmo predictor (2, 53) (2, 53)\n",
      "ELMo Prediction\n",
      "Data shape elmo (2, 53)\n",
      "Data shape integer (2, 53)\n",
      "Data ext. elmo shape (2, 53)\n",
      "Data ext. integer shape (2, 53)\n",
      "0 2\n",
      "Test predictions\n",
      "(2, 53)\n",
      "shape test predict (106,)\n",
      "shape test label 106\n",
      "training : (6, 53, 1)\n",
      "ELMo Metric Training Prediction\n",
      "input shape to elmo predictor (6, 53) (6, 53)\n",
      "ELMo Prediction\n",
      "Data shape elmo (6, 53)\n",
      "Data shape integer (6, 53)\n",
      "Data ext. elmo shape (6, 53)\n",
      "Data ext. integer shape (6, 53)\n",
      "0 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 4\n",
      "4 6\n",
      "Training P : 0.23943661971830985, R : 0.6296296296296297, F1 : 0.346938775510204\n",
      "Testing P : 0.21212121212121213, R : 0.875, F1 : 0.3414634146341463\n",
      "--------------------Targets-------------------------\n",
      "[1 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0]\n",
      "--------------------Predictions-------------------------\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0]\n",
      "Training set length : 5\n",
      "Test set length : 3\n",
      "Training set length : 5\n",
      "Test set length : 3\n",
      "Training set length : 5\n",
      "Test set length : 3\n",
      "input shape to elmo predictor (3, 53) (3, 53)\n",
      "ELMo Prediction\n",
      "Data shape elmo (3, 53)\n",
      "Data shape integer (3, 53)\n",
      "Data ext. elmo shape (4, 53)\n",
      "Data ext. integer shape (4, 53)\n",
      "0 2\n",
      "2 4\n",
      "Training set length : 5\n",
      "Test set length : 3\n",
      "Training set length : 5\n",
      "Test set length : 3\n",
      "Hyperparameter Tuning took 621.09375\n"
     ]
    }
   ],
   "source": [
    "elmo_r_bilstm_e_params_datasets = hyperparameter_opt_elmo_wembeds_v1_datasets([dataset_check], \n",
    "                                            models[0], elmo_r_bilstm_e_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.8 CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_bilstm_models = models\n",
    "cnn_bilstm_batch_sizes = [32]\n",
    "cnn_bilstm_epochs = [4]\n",
    "cnn_bilstm_units = [25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Testing : {'epoch': 4, 'bs': 32, 'embed': 'glove.6B.50d.txt', 'unit': 25}\n",
      "----------------------\n",
      "# Rows train : 4833\n",
      "# Rows test : 606\n",
      "# Rows dataset : 5439\n",
      "----------------------\n",
      "# Sents train : 387\n",
      "# Sents test : 53\n",
      "# Sents dataset : 440\n",
      "----------------------\n",
      "# Words : 162275\n",
      "# Vocab : 3641\n",
      "# Words missing embedding : 162\n",
      "Embedding shape : (3641, 50)\n",
      "Max length sentence : 103\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "in (InputLayer)                 (None, 103)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "em (Embedding)                  (None, 103, 50)      182050      in[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "cv1 (Conv1D)                    (None, 99, 100)      25100       em[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "cv2 (Conv1D)                    (None, 100, 100)     20100       em[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "cv3 (Conv1D)                    (None, 101, 100)     15100       em[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "po1 (GlobalMaxPooling1D)        (None, 100)          0           cv1[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "po2 (GlobalMaxPooling1D)        (None, 100)          0           cv2[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "po3 (GlobalMaxPooling1D)        (None, 100)          0           cv3[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "co (Concatenate)                (None, 300)          0           po1[0][0]                        \n",
      "                                                                 po2[0][0]                        \n",
      "                                                                 po3[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dr (Dropout)                    (None, 300)          0           co[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "ds (Dense)                      (None, 2)            602         dr[0][0]                         \n",
      "==================================================================================================\n",
      "Total params: 242,952\n",
      "Trainable params: 242,952\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Training set length : 4833\n",
      "Test set length : 606\n",
      "Training set length : 4833\n",
      "Test set length : 606\n",
      "Train on 4833 samples, validate on 606 samples\n",
      "Epoch 1/4\n",
      "4833/4833 [==============================] - 29s 6ms/step - loss: 0.9310 - acc: 0.5781 - val_loss: 0.6597 - val_acc: 0.6221\n",
      "Training P : 0.857976653696498, R : 0.23076923076923078, F1 : 0.36371134020618556\n",
      "Testing P : 0.6698113207547169, R : 0.2679245283018868, F1 : 0.38274932614555257\n",
      "--------------------Targets-------------------------\n",
      "[1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0 0 1 1 0 1 0 0 1 1 0 0\n",
      " 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 1\n",
      " 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0\n",
      " 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1\n",
      " 1 0 0 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0\n",
      " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0\n",
      " 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 1 1\n",
      " 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0\n",
      " 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 0 0 0 0 1 1 0 0 0 1 0 1 0\n",
      " 0 0 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0\n",
      " 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 1 1 0 1\n",
      " 0 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 0 0 0 0 0\n",
      " 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 0\n",
      " 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 1 1 1 1 0 0 0\n",
      " 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 1\n",
      " 0 1 1 0 0 0 0 1 1 1 0 0 1 1]\n",
      "--------------------Predictions-------------------------\n",
      "[1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 0 0\n",
      " 0 1 0 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0 1 0 0 0\n",
      " 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 0 1 0 0 0 1 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 1 0]\n",
      "Epoch 2/4\n",
      "4833/4833 [==============================] - 13s 3ms/step - loss: 0.6397 - acc: 0.6729 - val_loss: 0.6521 - val_acc: 0.6353\n",
      "Training P : 0.6519647153167603, R : 0.8508634222919937, F1 : 0.7382519863791146\n",
      "Testing P : 0.553921568627451, R : 0.8528301886792453, F1 : 0.6716196136701338\n",
      "--------------------Targets-------------------------\n",
      "[1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0 0 1 1 0 1 0 0 1 1 0 0\n",
      " 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 1\n",
      " 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0\n",
      " 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1\n",
      " 1 0 0 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0\n",
      " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0\n",
      " 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 1 1\n",
      " 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0\n",
      " 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 0 0 0 0 1 1 0 0 0 1 0 1 0\n",
      " 0 0 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0\n",
      " 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 1 1 0 1\n",
      " 0 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 0 0 0 0 0\n",
      " 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 0\n",
      " 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 1 1 1 1 0 0 0\n",
      " 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 1\n",
      " 0 1 1 0 0 0 0 1 1 1 0 0 1 1]\n",
      "--------------------Predictions-------------------------\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1\n",
      " 0 0 0 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1\n",
      " 1 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1\n",
      " 1 0 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1\n",
      " 1 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 1 1 1 1 1\n",
      " 1 0 0 1 0 0 1 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 0 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 0\n",
      " 1 1 0 0 1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 0 0 0 0 1 1 0\n",
      " 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0 1 1 1 1 0 0 1 0 1 1 1 1 0 1 0\n",
      " 0 1 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0\n",
      " 1 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 0 1 1 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 1 1 0 1\n",
      " 1 0 1 1 0 1 0 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1\n",
      " 0 1 1 1 1 1 0 1 0 1 0 1 1 0]\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4833/4833 [==============================] - 13s 3ms/step - loss: 0.5554 - acc: 0.7178 - val_loss: 0.5811 - val_acc: 0.6716\n",
      "Training P : 0.871692060946271, R : 0.5688121402407117, F1 : 0.6884103863204559\n",
      "Testing P : 0.6813186813186813, R : 0.4679245283018868, F1 : 0.5548098434004474\n",
      "--------------------Targets-------------------------\n",
      "[1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0 0 1 1 0 1 0 0 1 1 0 0\n",
      " 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 1\n",
      " 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0\n",
      " 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1\n",
      " 1 0 0 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0\n",
      " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0\n",
      " 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 1 1\n",
      " 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0\n",
      " 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 0 0 0 0 1 1 0 0 0 1 0 1 0\n",
      " 0 0 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0\n",
      " 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 1 1 0 1\n",
      " 0 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 0 0 0 0 0\n",
      " 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 0\n",
      " 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 1 1 1 1 0 0 0\n",
      " 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 1\n",
      " 0 1 1 0 0 0 0 1 1 1 0 0 1 1]\n",
      "--------------------Predictions-------------------------\n",
      "[1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1\n",
      " 1 1 0 1 1 0 0 0 1 1 0 0 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1\n",
      " 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 1 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0\n",
      " 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0\n",
      " 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 0 0 0 1 1 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 1 1 0 0\n",
      " 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 0 1\n",
      " 0 0 1 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 0\n",
      " 0 0 0 1 0 1 0 1 0 0 0 0 1 0]\n",
      "Epoch 4/4\n",
      "4833/4833 [==============================] - 13s 3ms/step - loss: 0.4811 - acc: 0.7685 - val_loss: 0.5580 - val_acc: 0.7145\n",
      "Training P : 0.8527713625866051, R : 0.7728937728937729, F1 : 0.8108701619544332\n",
      "Testing P : 0.676923076923077, R : 0.6641509433962264, F1 : 0.6704761904761903\n",
      "--------------------Targets-------------------------\n",
      "[1 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0 0 1 1 0 1 0 0 1 1 0 0\n",
      " 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 1\n",
      " 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 0\n",
      " 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1\n",
      " 1 0 0 1 0 0 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 0 0 0 1 1 0 0 0 0 1 0 1 0 1 0 0\n",
      " 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 0 0 0\n",
      " 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 1 1\n",
      " 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0\n",
      " 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 0 0 0 0 1 1 0 0 0 1 0 1 0\n",
      " 0 0 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0\n",
      " 0 1 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 1 1 0 1 1 1 0 1\n",
      " 0 1 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 0 0 0 0 0\n",
      " 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 0 1 0 1 1 0 0 0\n",
      " 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 0 1 1 1 1 0 0 0\n",
      " 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 1\n",
      " 0 1 1 0 0 0 0 1 1 1 0 0 1 1]\n",
      "--------------------Predictions-------------------------\n",
      "[0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1\n",
      " 1 1 0 1 1 0 0 0 1 1 0 1 1 0 0 1 1 0 0 1 0 1 1 1 0 0 1 1 1 0 0 1 1 0 0 0 1\n",
      " 0 0 0 0 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 1 0 0 1 0 1\n",
      " 1 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1\n",
      " 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1\n",
      " 0 0 1 1 1 0 1 0 1 0 1 0 0 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0\n",
      " 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 0 0 0\n",
      " 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 1 0 0 0 0 0 0\n",
      " 1 0 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0\n",
      " 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 1 0 1 0\n",
      " 0 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 1\n",
      " 0 1 0 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0\n",
      " 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 1 1 0 1\n",
      " 0 1 1 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 0 0 0 1\n",
      " 0 0 1 1 0 0 0 0 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 0\n",
      " 0 0 0 1 1 1 0 1 0 1 0 0 1 0]\n",
      "Hyperparameter Tuning took 377.3125\n"
     ]
    }
   ],
   "source": [
    "cnn_params_datasets = hyperparameter_opt_v2_datasets(datasets[0:1], model_v2_adv_CNN, \n",
    "                            cnn_bilstm_models, cnn_bilstm_batch_sizes, cnn_bilstm_epochs, cnn_bilstm_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X. BiLSTM-CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_crf_models = models\n",
    "bilstm_crf_batch_sizes = [1]\n",
    "bilstm_crf_epochs = [1,2]\n",
    "bilstm_crf_units = [25, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Testing : {'epoch': 1, 'bs': 1, 'embed': 'glove.6B.50d.txt', 'unit': 25}\n",
      "----------------------\n",
      "# Rows train : 4833\n",
      "# Rows test : 606\n",
      "# Rows dataset : 5439\n",
      "----------------------\n",
      "# Sents train : 387\n",
      "# Sents test : 53\n",
      "# Sents dataset : 440\n",
      "----------------------\n",
      "# Words : 10722\n",
      "# Vocab : 3639\n",
      "# Words missing embedding : 160\n",
      "Embedding shape : (3639, 50)\n",
      "Max length sentence : 101\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "in (InputLayer)              (None, 101)               0         \n",
      "_________________________________________________________________\n",
      "em (Embedding)               (None, 101, 50)           181950    \n",
      "_________________________________________________________________\n",
      "dr (Dropout)                 (None, 101, 50)           0         \n",
      "_________________________________________________________________\n",
      "bi (Bidirectional)           (None, 101, 50)           15200     \n",
      "_________________________________________________________________\n",
      "ds (Dense)                   (None, 101, 30)           1530      \n",
      "_________________________________________________________________\n",
      "crf (CRF)                    (None, 101, 2)            70        \n",
      "=================================================================\n",
      "Total params: 198,750\n",
      "Trainable params: 198,750\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Training set length : 387\n",
      "Test set length : 53\n",
      "Training set length : 387\n",
      "Test set length : 53\n",
      "387\n",
      "53\n",
      "Train on 387 samples, validate on 53 samples\n",
      "Epoch 1/1\n",
      "386/387 [============================>.] - ETA: 0s - loss: 0.1107 - acc: 0.9534"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-112-6f9204e63ecb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m bilstm_crf_params_datasets = hyperparameter_opt_v1_datasets(datasets, model_v1_BiLSTM_CRF, \n\u001b[1;32m----> 2\u001b[1;33m                             bilstm_crf_models, bilstm_crf_batch_sizes, bilstm_crf_epochs, bilstm_crf_units)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-110-ad7d7ba9feb5>\u001b[0m in \u001b[0;36mhyperparameter_opt_v1_datasets\u001b[1;34m(datasets, model, embedding_models, batch_sizes, epochs, units)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         configs, best = hyperparameter_opt_v1(dataset, model, embedding_models, \n\u001b[1;32m---> 29\u001b[1;33m                               batch_sizes, epochs, units)\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0mparams_per_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'configs'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'best'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mbest\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mparams_per_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-110-ad7d7ba9feb5>\u001b[0m in \u001b[0;36mhyperparameter_opt_v1\u001b[1;34m(dataset, model, embedding_models, batch_sizes, epochs, units)\u001b[0m\n\u001b[0;32m     15\u001b[0m                         \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'epoch'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'bs'\u001b[0m \u001b[1;33m:\u001b[0m                                 \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'embed'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0membed_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'unit'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0munit\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Optimization Testing : {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m                         \u001b[0mf1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_f1s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_f1s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreparation_v1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m                             \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membed_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0munit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m                         \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConfiguration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m                                              \u001b[0mtrain_f1s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_f1s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                         \u001b[0mconfigurations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-98-b318f1e5186f>\u001b[0m in \u001b[0;36mpreparation_v1\u001b[1;34m(dataset, model_func, embedding, epochs, batch_size, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m     history = model.fit(train_words_padded, np.array(train_binaries_padded_cat),\n\u001b[0;32m     40\u001b[0m                     batch_size=batch_size, epochs=epochs, validation_data = (test_words_padded, \n\u001b[1;32m---> 41\u001b[1;33m                     np.array(test_binaries_padded_cat)), verbose=1, callbacks=[metrics_v1])\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[1;31m# Use trained model to predict the test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_words_padded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    210\u001b[0m                         val_outs = test_loop(model, val_f, val_ins,\n\u001b[0;32m    211\u001b[0m                                              \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m                                              verbose=0)\n\u001b[0m\u001b[0;32m    213\u001b[0m                         \u001b[0mval_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m                         \u001b[1;31m# Same labels assumed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m    390\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 392\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    393\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2664\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2666\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2633\u001b[0m                                 \u001b[0mfeed_symbols\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2635\u001b[1;33m                                 session)\n\u001b[0m\u001b[0;32m   2636\u001b[0m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2637\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_make_callable\u001b[1;34m(self, feed_arrays, feed_symbols, symbol_vals, session)\u001b[0m\n\u001b[0;32m   2585\u001b[0m         \u001b[0mcallable_opts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2586\u001b[0m         \u001b[1;31m# Create callable.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2587\u001b[1;33m         \u001b[0mcallable_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_callable_from_options\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallable_opts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2588\u001b[0m         \u001b[1;31m# Cache parameters corresponding to the generated callable, so that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2589\u001b[0m         \u001b[1;31m# we can detect future mismatches and refresh the callable.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_make_callable_from_options\u001b[1;34m(self, callable_options)\u001b[0m\n\u001b[0;32m   1412\u001b[0m     \"\"\"\n\u001b[0;32m   1413\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1414\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mBaseSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallable_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, session, callable_options)\u001b[0m\n\u001b[0;32m   1366\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m           self._handle = tf_session.TF_SessionMakeCallable(\n\u001b[1;32m-> 1368\u001b[1;33m               session._session, options_ptr, status)\n\u001b[0m\u001b[0;32m   1369\u001b[0m       \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1370\u001b[0m         \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_DeleteBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptions_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bilstm_crf_params_datasets = hyperparameter_opt_v1_datasets(datasets, model_v1_BiLSTM_CRF, \n",
    "                            bilstm_crf_models, bilstm_crf_batch_sizes, bilstm_crf_epochs, bilstm_crf_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "# Rows train : 5551\n",
      "# Rows test : 694\n",
      "# Rows dataset : 6245\n",
      "----------------------\n",
      "# Sents train : 387\n",
      "# Sents test : 53\n",
      "# Sents dataset : 440\n",
      "----------------------\n",
      "# Words : 186472\n",
      "# Vocab : 3641\n",
      "# Words missing embedding : 162\n",
      "Embedding shape : (3641, 200)\n",
      "Max length sentence : 103\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           (None, 103)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_15 (Embedding)        (None, 103, 200)     728200      input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 99, 100)      100100      embedding_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 100, 100)     80100       embedding_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 101, 100)     60100       embedding_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 100)          0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 100)          0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 100)          0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 300)          0           global_max_pooling1d_5[0][0]     \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 300)          0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 2)            602         dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 969,102\n",
      "Trainable params: 969,102\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Training set length : 5551\n",
      "Test set length : 694\n",
      "Training set length : 5551\n",
      "Test set length : 694\n",
      "Train on 5551 samples, validate on 694 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-8e41701fc8e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mf1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_f1s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_f1s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreparation_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m                     \u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_v2_adv_CNN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-d3f24f7f0ccd>\u001b[0m in \u001b[0;36mpreparation_v2\u001b[1;34m(dataset, embedding, model_func)\u001b[0m\n\u001b[0;32m    203\u001b[0m     history = model.fit(train_words_padded, np.array(train_binaries_categorical),\n\u001b[0;32m    204\u001b[0m                     batch_size=1, epochs=10, validation_data = (test_words_padded, \n\u001b[1;32m--> 205\u001b[1;33m                     np.array(test_binaries_categorical)), verbose=1, callbacks=[metrics_v2])\n\u001b[0m\u001b[0;32m    206\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_words_padded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_words_padded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2664\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2666\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2633\u001b[0m                                 \u001b[0mfeed_symbols\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2635\u001b[1;33m                                 session)\n\u001b[0m\u001b[0;32m   2636\u001b[0m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2637\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_make_callable\u001b[1;34m(self, feed_arrays, feed_symbols, symbol_vals, session)\u001b[0m\n\u001b[0;32m   2585\u001b[0m         \u001b[0mcallable_opts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2586\u001b[0m         \u001b[1;31m# Create callable.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2587\u001b[1;33m         \u001b[0mcallable_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_callable_from_options\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallable_opts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2588\u001b[0m         \u001b[1;31m# Cache parameters corresponding to the generated callable, so that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2589\u001b[0m         \u001b[1;31m# we can detect future mismatches and refresh the callable.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_make_callable_from_options\u001b[1;34m(self, callable_options)\u001b[0m\n\u001b[0;32m   1412\u001b[0m     \"\"\"\n\u001b[0;32m   1413\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1414\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mBaseSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallable_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, session, callable_options)\u001b[0m\n\u001b[0;32m   1366\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m           self._handle = tf_session.TF_SessionMakeCallable(\n\u001b[1;32m-> 1368\u001b[1;33m               session._session, options_ptr, status)\n\u001b[0m\u001b[0;32m   1369\u001b[0m       \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1370\u001b[0m         \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_DeleteBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptions_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "f1, train_f1s, test_f1s, prediction = preparation_v2(datasets[0], \\\n",
    "                    models[2].model, model_v2_adv_CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "# Rows train : 5551\n",
      "# Rows test : 694\n",
      "# Rows dataset : 6245\n",
      "----------------------\n",
      "# Sents train : 387\n",
      "# Sents test : 53\n",
      "# Sents dataset : 440\n",
      "----------------------\n",
      "# Words : 10722\n",
      "# Vocab : 3639\n",
      "# Words missing embedding : 160\n",
      "Embedding shape : (3639, 50)\n",
      "Max length sentence : 101\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "in (InputLayer)              (None, 101)               0         \n",
      "_________________________________________________________________\n",
      "em (Embedding)               (None, 101, 50)           181950    \n",
      "_________________________________________________________________\n",
      "dr (Dropout)                 (None, 101, 50)           0         \n",
      "_________________________________________________________________\n",
      "bi (Bidirectional)           (None, 101, 40)           11360     \n",
      "_________________________________________________________________\n",
      "ds (Dense)                   (None, 101, 30)           1230      \n",
      "_________________________________________________________________\n",
      "crf (CRF)                    (None, 101, 2)            70        \n",
      "=================================================================\n",
      "Total params: 194,610\n",
      "Trainable params: 194,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = preparation_v1(datasets[0], \\\n",
    "                    models[0].model, model_v1_BiLSTM_CRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, to_file='model_elmo_r_bilstm_h.pdf', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "# Rows train : 5551\n",
      "# Rows test : 694\n",
      "# Rows dataset : 6245\n",
      "----------------------\n",
      "# Sents train : 387\n",
      "# Sents test : 53\n",
      "# Sents dataset : 440\n",
      "----------------------\n",
      "# Words : 10722\n",
      "# Vocab : 3896\n",
      "# Words missing embedding : 1298\n",
      "Embedding shape : (3896, 50)\n",
      "# Chars : 52647\n",
      "# Vocab (chars) : 118\n",
      "# Chars missing embedding : 72\n",
      "Embedding shape : (118, 50)\n",
      "Max length sentence : 101\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inc (InputLayer)                (None, 101, 11)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inw (InputLayer)                (None, 101)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tsce (TimeDistributed)          (None, 101, 11, 50)  5900        inc[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "emw (Embedding)                 (None, 101, 50)      194800      inw[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "tscls (TimeDistributed)         (None, 101, 50)      15200       tsce[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "co (Concatenate)                (None, 101, 100)     0           emw[0][0]                        \n",
      "                                                                 tscls[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "sp1 (SpatialDropout1D)          (None, 101, 100)     0           co[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "bi (Bidirectional)              (None, 101, 80)      45120       sp1[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "sp2 (SpatialDropout1D)          (None, 101, 80)      0           bi[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "ts (TimeDistributed)            (None, 101, 2)       162         sp2[0][0]                        \n",
      "==================================================================================================\n",
      "Total params: 261,182\n",
      "Trainable params: 261,182\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = preparation_char_v1(datasets[0], \\\n",
    "                    models[0].model, compute_character_embeddings(models[0].model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import model_to_dot\n",
    "from IPython.display import SVG\n",
    "def plot_keras_model(model, show_shapes=True, show_layer_names=True):\n",
    "    return SVG(model_to_dot(model, show_shapes=show_shapes, show_layer_names=show_layer_names).create(prog='dot',format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, to_file='model_elmo_r_bilstm.pdf', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"636pt\" viewBox=\"0.00 0.00 918.00 636.00\" width=\"918pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 632)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-632 914,-632 914,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 164181472928 -->\n",
       "<g class=\"node\" id=\"node1\"><title>164181472928</title>\n",
       "<polygon fill=\"none\" points=\"141.5,-581.5 141.5,-627.5 427.5,-627.5 427.5,-581.5 141.5,-581.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"204.5\" y=\"-600.8\">input_8: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"267.5,-581.5 267.5,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"295.5\" y=\"-612.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"267.5,-604.5 323.5,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"295.5\" y=\"-589.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"323.5,-581.5 323.5,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"375.5\" y=\"-612.3\">(None, 101, 11)</text>\n",
       "<polyline fill=\"none\" points=\"323.5,-604.5 427.5,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"375.5\" y=\"-589.3\">(None, 101, 11)</text>\n",
       "</g>\n",
       "<!-- 164339008176 -->\n",
       "<g class=\"node\" id=\"node3\"><title>164339008176</title>\n",
       "<polygon fill=\"none\" points=\"6.5,-498.5 6.5,-544.5 562.5,-544.5 562.5,-498.5 6.5,-498.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"194\" y=\"-517.8\">time_distributed_4(embedding_8): TimeDistributed(Embedding)</text>\n",
       "<polyline fill=\"none\" points=\"381.5,-498.5 381.5,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"409.5\" y=\"-529.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"381.5,-521.5 437.5,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"409.5\" y=\"-506.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"437.5,-498.5 437.5,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"500\" y=\"-529.3\">(None, 101, 11)</text>\n",
       "<polyline fill=\"none\" points=\"437.5,-521.5 562.5,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"500\" y=\"-506.3\">(None, 101, 11, 50)</text>\n",
       "</g>\n",
       "<!-- 164181472928&#45;&gt;164339008176 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>164181472928-&gt;164339008176</title>\n",
       "<path d=\"M284.5,-581.366C284.5,-573.152 284.5,-563.658 284.5,-554.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"288,-554.607 284.5,-544.607 281,-554.607 288,-554.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 164339247760 -->\n",
       "<g class=\"node\" id=\"node2\"><title>164339247760</title>\n",
       "<polygon fill=\"none\" points=\"616,-498.5 616,-544.5 881,-544.5 881,-498.5 616,-498.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"679\" y=\"-517.8\">input_7: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"742,-498.5 742,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"770\" y=\"-529.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"742,-521.5 798,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"770\" y=\"-506.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"798,-498.5 798,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"839.5\" y=\"-529.3\">(None, 101)</text>\n",
       "<polyline fill=\"none\" points=\"798,-521.5 881,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"839.5\" y=\"-506.3\">(None, 101)</text>\n",
       "</g>\n",
       "<!-- 164339248880 -->\n",
       "<g class=\"node\" id=\"node4\"><title>164339248880</title>\n",
       "<polygon fill=\"none\" points=\"587,-415.5 587,-461.5 910,-461.5 910,-415.5 587,-415.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"668.5\" y=\"-434.8\">embedding_7: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"750,-415.5 750,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"778\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"750,-438.5 806,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"778\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"806,-415.5 806,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"858\" y=\"-446.3\">(None, 101)</text>\n",
       "<polyline fill=\"none\" points=\"806,-438.5 910,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"858\" y=\"-423.3\">(None, 101, 50)</text>\n",
       "</g>\n",
       "<!-- 164339247760&#45;&gt;164339248880 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>164339247760-&gt;164339248880</title>\n",
       "<path d=\"M748.5,-498.366C748.5,-490.152 748.5,-480.658 748.5,-471.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"752,-471.607 748.5,-461.607 745,-471.607 752,-471.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 164339071128 -->\n",
       "<g class=\"node\" id=\"node5\"><title>164339071128</title>\n",
       "<polygon fill=\"none\" points=\"0,-415.5 0,-461.5 569,-461.5 569,-415.5 0,-415.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"194\" y=\"-434.8\">time_distributed_5(bidirectional_7): TimeDistributed(Bidirectional)</text>\n",
       "<polyline fill=\"none\" points=\"388,-415.5 388,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"416\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"388,-438.5 444,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"416\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"444,-415.5 444,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"506.5\" y=\"-446.3\">(None, 101, 11, 50)</text>\n",
       "<polyline fill=\"none\" points=\"444,-438.5 569,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"506.5\" y=\"-423.3\">(None, 101, 50)</text>\n",
       "</g>\n",
       "<!-- 164339008176&#45;&gt;164339071128 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>164339008176-&gt;164339071128</title>\n",
       "<path d=\"M284.5,-498.366C284.5,-490.152 284.5,-480.658 284.5,-471.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"288,-471.607 284.5,-461.607 281,-471.607 288,-471.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 164339070232 -->\n",
       "<g class=\"node\" id=\"node6\"><title>164339070232</title>\n",
       "<polygon fill=\"none\" points=\"334.5,-332.5 334.5,-378.5 698.5,-378.5 698.5,-332.5 334.5,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"384\" y=\"-351.8\">c: Concatenate</text>\n",
       "<polyline fill=\"none\" points=\"433.5,-332.5 433.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"461.5\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"433.5,-355.5 489.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"461.5\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"489.5,-332.5 489.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"594\" y=\"-363.3\">[(None, 101, 50), (None, 101, 50)]</text>\n",
       "<polyline fill=\"none\" points=\"489.5,-355.5 698.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"594\" y=\"-340.3\">(None, 101, 100)</text>\n",
       "</g>\n",
       "<!-- 164339248880&#45;&gt;164339070232 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>164339248880-&gt;164339070232</title>\n",
       "<path d=\"M685.411,-415.473C655.748,-405.117 620.209,-392.709 589.4,-381.952\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"590.171,-378.514 579.576,-378.522 587.863,-385.123 590.171,-378.514\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 164339071128&#45;&gt;164339070232 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>164339071128-&gt;164339070232</title>\n",
       "<path d=\"M347.589,-415.473C377.252,-405.117 412.791,-392.709 443.6,-381.952\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"445.137,-385.123 453.424,-378.522 442.829,-378.514 445.137,-385.123\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 164510329600 -->\n",
       "<g class=\"node\" id=\"node7\"><title>164510329600</title>\n",
       "<polygon fill=\"none\" points=\"312.5,-249.5 312.5,-295.5 720.5,-295.5 720.5,-249.5 312.5,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"433\" y=\"-268.8\">spatial_dropout1d_3: SpatialDropout1D</text>\n",
       "<polyline fill=\"none\" points=\"553.5,-249.5 553.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"581.5\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"553.5,-272.5 609.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"581.5\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"609.5,-249.5 609.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"665\" y=\"-280.3\">(None, 101, 100)</text>\n",
       "<polyline fill=\"none\" points=\"609.5,-272.5 720.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"665\" y=\"-257.3\">(None, 101, 100)</text>\n",
       "</g>\n",
       "<!-- 164339070232&#45;&gt;164510329600 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>164339070232-&gt;164510329600</title>\n",
       "<path d=\"M516.5,-332.366C516.5,-324.152 516.5,-314.658 516.5,-305.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"520,-305.607 516.5,-295.607 513,-305.607 520,-305.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 164511022552 -->\n",
       "<g class=\"node\" id=\"node8\"><title>164511022552</title>\n",
       "<polygon fill=\"none\" points=\"298.5,-166.5 298.5,-212.5 734.5,-212.5 734.5,-166.5 298.5,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"433\" y=\"-185.8\">bidirectional_8(lstm_8): Bidirectional(LSTM)</text>\n",
       "<polyline fill=\"none\" points=\"567.5,-166.5 567.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"595.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"567.5,-189.5 623.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"595.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"623.5,-166.5 623.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"679\" y=\"-197.3\">(None, 101, 100)</text>\n",
       "<polyline fill=\"none\" points=\"623.5,-189.5 734.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"679\" y=\"-174.3\">(None, 101, 80)</text>\n",
       "</g>\n",
       "<!-- 164510329600&#45;&gt;164511022552 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>164510329600-&gt;164511022552</title>\n",
       "<path d=\"M516.5,-249.366C516.5,-241.152 516.5,-231.658 516.5,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"520,-222.607 516.5,-212.607 513,-222.607 520,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 164511144424 -->\n",
       "<g class=\"node\" id=\"node9\"><title>164511144424</title>\n",
       "<polygon fill=\"none\" points=\"316,-83.5 316,-129.5 717,-129.5 717,-83.5 316,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"436.5\" y=\"-102.8\">spatial_dropout1d_4: SpatialDropout1D</text>\n",
       "<polyline fill=\"none\" points=\"557,-83.5 557,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"585\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"557,-106.5 613,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"585\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"613,-83.5 613,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"665\" y=\"-114.3\">(None, 101, 80)</text>\n",
       "<polyline fill=\"none\" points=\"613,-106.5 717,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"665\" y=\"-91.3\">(None, 101, 80)</text>\n",
       "</g>\n",
       "<!-- 164511022552&#45;&gt;164511144424 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>164511022552-&gt;164511144424</title>\n",
       "<path d=\"M516.5,-166.366C516.5,-158.152 516.5,-148.658 516.5,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"520,-139.607 516.5,-129.607 513,-139.607 520,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 164517164256 -->\n",
       "<g class=\"node\" id=\"node10\"><title>164517164256</title>\n",
       "<polygon fill=\"none\" points=\"278,-0.5 278,-46.5 755,-46.5 755,-0.5 278,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"436.5\" y=\"-19.8\">time_distributed_6(dense_6): TimeDistributed(Dense)</text>\n",
       "<polyline fill=\"none\" points=\"595,-0.5 595,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"623\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"595,-23.5 651,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"623\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"651,-0.5 651,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"703\" y=\"-31.3\">(None, 101, 80)</text>\n",
       "<polyline fill=\"none\" points=\"651,-23.5 755,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"703\" y=\"-8.3\">(None, 101, 2)</text>\n",
       "</g>\n",
       "<!-- 164511144424&#45;&gt;164517164256 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>164511144424-&gt;164517164256</title>\n",
       "<path d=\"M516.5,-83.3664C516.5,-75.1516 516.5,-65.6579 516.5,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"520,-56.6068 516.5,-46.6068 513,-56.6069 520,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_keras_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "# Rows train : 5551\n",
      "# Rows test : 694\n",
      "# Rows dataset : 6245\n",
      "----------------------\n",
      "# Sents train : 387\n",
      "# Sents test : 53\n",
      "# Sents dataset : 440\n",
      "----------------------\n",
      "# Words : 10722\n",
      "# Vocab : 3639\n",
      "# Words missing embedding : 160\n",
      "Embedding shape : (3639, 50)\n",
      "Max length sentence : 101\n",
      "Training set length : 387\n",
      "Test set length : 53\n",
      "Training set length : 387\n",
      "Test set length : 53\n",
      "(386, 101, 1)\n",
      "(52, 101, 1)\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "in (InputLayer)                 (None, 101)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "la (Lambda)                     (None, 101, 1024)    0           in[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "bi1 (Bidirectional)             (None, 101, 1024)    6295552     la[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "bi2 (Bidirectional)             (None, 101, 1024)    6295552     bi1[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 101, 1024)    0           bi1[0][0]                        \n",
      "                                                                 bi2[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "ts (TimeDistributed)            (None, 101, 2)       2050        add_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 12,593,154\n",
      "Trainable params: 12,593,154\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = preparation_elmo_v1(datasets[0], models[0].model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "# Rows train : 5551\n",
      "# Rows test : 694\n",
      "# Rows dataset : 6245\n",
      "----------------------\n",
      "# Sents train : 387\n",
      "# Sents test : 53\n",
      "# Sents dataset : 440\n",
      "----------------------\n",
      "# Words : 10722\n",
      "# Vocab : 3639\n",
      "# Words missing embedding : 160\n",
      "Embedding shape : (3639, 50)\n",
      "Max length sentence : 101\n",
      "Training set length : 387\n",
      "Test set length : 53\n",
      "Training set length : 387\n",
      "Test set length : 53\n",
      "(386, 101, 1)\n",
      "(52, 101, 1)\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 101)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 101, 1024)    0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 101, 1024)    6295552     lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 101, 1024)    6295552     bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 101, 1024)    0           bidirectional_5[0][0]            \n",
      "                                                                 bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, 101, 2)       2050        add_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 12,593,154\n",
      "Trainable params: 12,593,154\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 386 samples, validate on 52 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-3b77aecb97a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mf1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_f1s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_f1s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreparation_elmo_v1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-d3f24f7f0ccd>\u001b[0m in \u001b[0;36mpreparation_elmo_v1\u001b[1;34m(dataset, embedding)\u001b[0m\n\u001b[0;32m    151\u001b[0m     history = model.fit(np.array(train_words_padded), np.array(train_binaries_padded),\n\u001b[0;32m    152\u001b[0m                          batch_size=batch_size, epochs=2, validation_data = (np.array(test_words_padded), \n\u001b[1;32m--> 153\u001b[1;33m                         np.array(test_binaries_padded)), verbose=1, callbacks=[metrics_elmo_v1])\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;31m# Use trained model to predict the test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[0mtrain_words_padded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_words_padded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_train_test_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords_padded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_num_sents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2657\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2658\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_make_callable_from_options'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2659\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[1;34m()\u001b[0m\n\u001b[0;32m    195\u001b[0m                 \u001b[1;31m# not already marked as initialized.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m                 is_initialized = session.run(\n\u001b[1;32m--> 197\u001b[1;33m                     [tf.is_variable_initialized(v) for v in candidate_vars])\n\u001b[0m\u001b[0;32m    198\u001b[0m                 \u001b[0muninitialized_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1100\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1272\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1273\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1276\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1259\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1260\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1261\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m   1263\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1293\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1294\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1295\u001b[1;33m       \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1297\u001b[0m   \u001b[1;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "f1, train_f1s, test_f1s = preparation_elmo_v1(datasets[0], models[0].model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.738095238095238"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "# Rows train : 5551\n",
      "# Rows test : 694\n",
      "# Rows dataset : 6245\n",
      "----------------------\n",
      "# Sents train : 387\n",
      "# Sents test : 53\n",
      "# Sents dataset : 440\n",
      "----------------------\n",
      "# Words : 10722\n",
      "# Vocab : 3639\n",
      "# Words missing embedding : 160\n",
      "Embedding shape : (3639, 50)\n",
      "Max length sentence : 101\n",
      "Training set length : 387\n",
      "Test set length : 53\n",
      "Training set length : 387\n",
      "Test set length : 53\n",
      "(386, 101, 1)\n",
      "(52, 101, 1)\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_32 (InputLayer)           (None, 101)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 101, 1024)    0           input_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_33 (Bidirectional (None, 101, 1024)    6295552     lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_34 (Bidirectional (None, 101, 1024)    6295552     bidirectional_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 101, 1024)    0           bidirectional_33[0][0]           \n",
      "                                                                 bidirectional_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_42 (TimeDistri (None, 101, 2)       2050        add_5[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 12,593,154\n",
      "Trainable params: 12,593,154\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 386 samples, validate on 52 samples\n",
      "Epoch 1/2\n",
      "386/386 [==============================] - 3787s 10s/step - loss: 0.1055 - acc: 0.9548 - val_loss: 0.0757 - val_acc: 0.9633\n",
      "(52, 101, 1)\n",
      "(54, 101)\n",
      "0 2\n",
      "2 4\n",
      "4 6\n",
      "6 8\n",
      "8 10\n",
      "10 12\n",
      "12 14\n",
      "14 16\n",
      "16 18\n",
      "18 20\n",
      "20 22\n",
      "22 24\n",
      "24 26\n",
      "26 28\n",
      "28 30\n",
      "30 32\n",
      "32 34\n",
      "34 36\n",
      "36 38\n",
      "38 40\n",
      "40 42\n",
      "42 44\n",
      "44 46\n",
      "46 48\n",
      "48 50\n",
      "50 52\n",
      "52 54\n",
      "(52, 101)\n",
      "(388, 101)\n",
      "0 2\n",
      "2 4\n",
      "4 6\n",
      "6 8\n",
      "8 10\n",
      "10 12\n",
      "12 14\n",
      "14 16\n",
      "16 18\n",
      "18 20\n",
      "20 22\n",
      "22 24\n",
      "24 26\n",
      "26 28\n",
      "28 30\n",
      "30 32\n",
      "32 34\n",
      "34 36\n",
      "36 38\n",
      "38 40\n",
      "40 42\n",
      "42 44\n",
      "44 46\n",
      "46 48\n",
      "48 50\n",
      "50 52\n",
      "52 54\n",
      "54 56\n",
      "56 58\n",
      "58 60\n",
      "60 62\n",
      "62 64\n",
      "64 66\n",
      "66 68\n",
      "68 70\n",
      "70 72\n",
      "72 74\n",
      "74 76\n",
      "76 78\n",
      "78 80\n",
      "80 82\n",
      "82 84\n",
      "84 86\n",
      "86 88\n",
      "88 90\n",
      "90 92\n",
      "92 94\n",
      "94 96\n",
      "96 98\n",
      "98 100\n",
      "100 102\n",
      "102 104\n",
      "104 106\n",
      "106 108\n",
      "108 110\n",
      "110 112\n",
      "112 114\n",
      "114 116\n",
      "116 118\n",
      "118 120\n",
      "120 122\n",
      "122 124\n",
      "124 126\n",
      "126 128\n",
      "128 130\n",
      "130 132\n",
      "132 134\n",
      "134 136\n",
      "136 138\n",
      "138 140\n",
      "140 142\n",
      "142 144\n",
      "144 146\n",
      "146 148\n",
      "148 150\n",
      "150 152\n",
      "152 154\n",
      "154 156\n",
      "156 158\n",
      "158 160\n",
      "160 162\n",
      "162 164\n",
      "164 166\n",
      "166 168\n",
      "168 170\n",
      "170 172\n",
      "172 174\n",
      "174 176\n",
      "176 178\n",
      "178 180\n",
      "180 182\n",
      "182 184\n",
      "184 186\n",
      "186 188\n",
      "188 190\n",
      "190 192\n",
      "192 194\n",
      "194 196\n",
      "196 198\n",
      "198 200\n",
      "200 202\n",
      "202 204\n",
      "204 206\n",
      "206 208\n",
      "208 210\n",
      "210 212\n",
      "212 214\n",
      "214 216\n",
      "216 218\n",
      "218 220\n",
      "220 222\n",
      "222 224\n",
      "224 226\n",
      "226 228\n",
      "228 230\n",
      "230 232\n",
      "232 234\n",
      "234 236\n",
      "236 238\n",
      "238 240\n",
      "240 242\n",
      "242 244\n",
      "244 246\n",
      "246 248\n",
      "248 250\n",
      "250 252\n",
      "252 254\n",
      "254 256\n",
      "256 258\n",
      "258 260\n",
      "260 262\n",
      "262 264\n",
      "264 266\n",
      "266 268\n",
      "268 270\n",
      "270 272\n",
      "272 274\n",
      "274 276\n",
      "276 278\n",
      "278 280\n",
      "280 282\n",
      "282 284\n",
      "284 286\n",
      "286 288\n",
      "288 290\n",
      "290 292\n",
      "292 294\n",
      "294 296\n",
      "296 298\n",
      "298 300\n",
      "300 302\n",
      "302 304\n",
      "304 306\n",
      "306 308\n",
      "308 310\n",
      "310 312\n",
      "312 314\n",
      "314 316\n",
      "316 318\n",
      "318 320\n",
      "320 322\n",
      "322 324\n",
      "324 326\n",
      "326 328\n",
      "328 330\n",
      "330 332\n",
      "332 334\n",
      "334 336\n",
      "336 338\n",
      "338 340\n",
      "340 342\n",
      "342 344\n",
      "344 346\n",
      "346 348\n",
      "348 350\n",
      "350 352\n",
      "352 354\n",
      "354 356\n",
      "356 358\n",
      "358 360\n",
      "360 362\n",
      "362 364\n",
      "364 366\n",
      "366 368\n",
      "368 370\n",
      "370 372\n",
      "372 374\n",
      "374 376\n",
      "376 378\n",
      "378 380\n",
      "380 382\n",
      "382 384\n",
      "384 386\n",
      "386 388\n",
      "Training F1-score : 0.5056751467710372\n",
      "Testing F1-score : 0.46537396121883656\n",
      "--------------------Targets-------------------------\n",
      "[0 0 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0\n",
      " 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
      " 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1\n",
      " 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0\n",
      " 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0\n",
      " 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0\n",
      " 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "--------------------Predictions-------------------------\n",
      "[0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0\n",
      " 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "386/386 [==============================] - 3801s 10s/step - loss: 0.0641 - acc: 0.9700 - val_loss: 0.0586 - val_acc: 0.9754\n",
      "(52, 101, 1)\n",
      "(54, 101)\n",
      "0 2\n",
      "2 4\n",
      "4 6\n",
      "6 8\n",
      "8 10\n",
      "10 12\n",
      "12 14\n",
      "14 16\n",
      "16 18\n",
      "18 20\n",
      "20 22\n",
      "22 24\n",
      "24 26\n",
      "26 28\n",
      "28 30\n",
      "30 32\n",
      "32 34\n",
      "34 36\n",
      "36 38\n",
      "38 40\n",
      "40 42\n",
      "42 44\n",
      "44 46\n",
      "46 48\n",
      "48 50\n",
      "50 52\n",
      "52 54\n",
      "(52, 101)\n",
      "(388, 101)\n",
      "0 2\n",
      "2 4\n",
      "4 6\n",
      "6 8\n",
      "8 10\n",
      "10 12\n",
      "12 14\n",
      "14 16\n",
      "16 18\n",
      "18 20\n",
      "20 22\n",
      "22 24\n",
      "24 26\n",
      "26 28\n",
      "28 30\n",
      "30 32\n",
      "32 34\n",
      "34 36\n",
      "36 38\n",
      "38 40\n",
      "40 42\n",
      "42 44\n",
      "44 46\n",
      "46 48\n",
      "48 50\n",
      "50 52\n",
      "52 54\n",
      "54 56\n",
      "56 58\n",
      "58 60\n",
      "60 62\n",
      "62 64\n",
      "64 66\n",
      "66 68\n",
      "68 70\n",
      "70 72\n",
      "72 74\n",
      "74 76\n",
      "76 78\n",
      "78 80\n",
      "80 82\n",
      "82 84\n",
      "84 86\n",
      "86 88\n",
      "88 90\n",
      "90 92\n",
      "92 94\n",
      "94 96\n",
      "96 98\n",
      "98 100\n",
      "100 102\n",
      "102 104\n",
      "104 106\n",
      "106 108\n",
      "108 110\n",
      "110 112\n",
      "112 114\n",
      "114 116\n",
      "116 118\n",
      "118 120\n",
      "120 122\n",
      "122 124\n",
      "124 126\n",
      "126 128\n",
      "128 130\n",
      "130 132\n",
      "132 134\n",
      "134 136\n",
      "136 138\n",
      "138 140\n",
      "140 142\n",
      "142 144\n",
      "144 146\n",
      "146 148\n",
      "148 150\n",
      "150 152\n",
      "152 154\n",
      "154 156\n",
      "156 158\n",
      "158 160\n",
      "160 162\n",
      "162 164\n",
      "164 166\n",
      "166 168\n",
      "168 170\n",
      "170 172\n",
      "172 174\n",
      "174 176\n",
      "176 178\n",
      "178 180\n",
      "180 182\n",
      "182 184\n",
      "184 186\n",
      "186 188\n",
      "188 190\n",
      "190 192\n",
      "192 194\n",
      "194 196\n",
      "196 198\n",
      "198 200\n",
      "200 202\n",
      "202 204\n",
      "204 206\n",
      "206 208\n",
      "208 210\n",
      "210 212\n",
      "212 214\n",
      "214 216\n",
      "216 218\n",
      "218 220\n",
      "220 222\n",
      "222 224\n",
      "224 226\n",
      "226 228\n",
      "228 230\n",
      "230 232\n",
      "232 234\n",
      "234 236\n",
      "236 238\n",
      "238 240\n",
      "240 242\n",
      "242 244\n",
      "244 246\n",
      "246 248\n",
      "248 250\n",
      "250 252\n",
      "252 254\n",
      "254 256\n",
      "256 258\n",
      "258 260\n",
      "260 262\n",
      "262 264\n",
      "264 266\n",
      "266 268\n",
      "268 270\n",
      "270 272\n",
      "272 274\n",
      "274 276\n",
      "276 278\n",
      "278 280\n",
      "280 282\n",
      "282 284\n",
      "284 286\n",
      "286 288\n",
      "288 290\n",
      "290 292\n",
      "292 294\n",
      "294 296\n",
      "296 298\n",
      "298 300\n",
      "300 302\n",
      "302 304\n",
      "304 306\n",
      "306 308\n",
      "308 310\n",
      "310 312\n",
      "312 314\n",
      "314 316\n",
      "316 318\n",
      "318 320\n",
      "320 322\n",
      "322 324\n",
      "324 326\n",
      "326 328\n",
      "328 330\n",
      "330 332\n",
      "332 334\n",
      "334 336\n",
      "336 338\n",
      "338 340\n",
      "340 342\n",
      "342 344\n",
      "344 346\n",
      "346 348\n",
      "348 350\n",
      "350 352\n",
      "352 354\n",
      "354 356\n",
      "356 358\n",
      "358 360\n",
      "360 362\n",
      "362 364\n",
      "364 366\n",
      "366 368\n",
      "368 370\n",
      "370 372\n",
      "372 374\n",
      "374 376\n",
      "376 378\n",
      "378 380\n",
      "380 382\n",
      "382 384\n",
      "384 386\n",
      "386 388\n",
      "Training F1-score : 0.7671852899575672\n",
      "Testing F1-score : 0.7445544554455444\n",
      "--------------------Targets-------------------------\n",
      "[0 0 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0\n",
      " 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
      " 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1\n",
      " 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0\n",
      " 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0\n",
      " 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0\n",
      " 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "--------------------Predictions-------------------------\n",
      "[0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0\n",
      " 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0\n",
      " 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1\n",
      " 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0\n",
      " 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Training set length : 387\n",
      "Test set length : 53\n",
      "Training set length : 387\n",
      "Test set length : 53\n",
      "(54, 101)\n",
      "0 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 4\n",
      "4 6\n",
      "6 8\n",
      "8 10\n",
      "10 12\n",
      "12 14\n",
      "14 16\n",
      "16 18\n",
      "18 20\n",
      "20 22\n",
      "22 24\n",
      "24 26\n",
      "26 28\n",
      "28 30\n",
      "30 32\n",
      "32 34\n",
      "34 36\n",
      "36 38\n",
      "38 40\n",
      "40 42\n",
      "42 44\n",
      "44 46\n",
      "46 48\n",
      "48 50\n",
      "50 52\n",
      "52 54\n",
      "Training set length : 387\n",
      "Test set length : 53\n",
      "Training set length : 387\n",
      "Test set length : 53\n"
     ]
    }
   ],
   "source": [
    "f1, train_f1s, test_f1s = preparation_elmo_v1(datasets[0], models[0].model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7644444444444445"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5056751467710372, 0.7671852899575672]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.46537396121883656, 0.7445544554455444]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = []\n",
    "datatype = []\n",
    "f1_scores = []\n",
    "epochs = [epoch for epoch, f1 in enumerate(train_f1s, 1)]\n",
    "epochs.extend(epochs)\n",
    "datatype = ['train' for elem in train_f1s]\n",
    "datatype.extend(['test' for elem in test_f1s])\n",
    "f1_scores.extend(train_f1s.copy())\n",
    "f1_scores.extend(test_f1s.copy())\n",
    "evaluation = [{'F1-score' : result[0], 'epoch' : result[1],\n",
    "                    'data' : result[2]} for result in zip(f1_scores, epochs, datatype)]\n",
    "epochs_f1_scores = pd.DataFrame.from_records(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEPCAYAAABcA4N7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "ax = sns.lineplot(x=\"epoch\", y=\"F1-score\",\n",
    "                   hue=\"data\", style=\"data\",\n",
    "                   markers=True, dashes=False, data=epochs_f1_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_lens = [len(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence lengths\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEBCAYAAAB13qL/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set()\n",
    "ax = sns.distplot(sentence_lens)\n",
    "print('Sentence lengths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "figure = ax.get_figure()\n",
    "figure.savefig('../plots/dl/sentence_length_dist.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(metrics.f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEBCAYAAAB13qL/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "epoch_f1s = plt.plot(metrics.f1_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "# Rows train : 5551\n",
      "# Rows test : 694\n",
      "# Rows dataset : 6245\n",
      "----------------------\n",
      "# Sents train : 387\n",
      "# Sents test : 53\n",
      "# Sents dataset : 440\n",
      "----------------------\n",
      "# Words : 10722\n",
      "# Vocab : 3639\n",
      "# Words missing embedding : 160\n",
      "Embedding shape : (3639, 50)\n",
      "Max length sentence : 101\n",
      "Training set length : 387\n",
      "Test set length : 53\n",
      "Training set length : 387\n",
      "Test set length : 53\n",
      "(386, 101, 1)\n",
      "(52, 101, 1)\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "in (InputLayer)                 (None, 101)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "la (Lambda)                     (None, 101, 1024)    0           in[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "bi1 (Bidirectional)             (None, 101, 1024)    6295552     la[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "bi2 (Bidirectional)             (None, 101, 1024)    6295552     bi1[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 101, 1024)    0           bi1[0][0]                        \n",
      "                                                                 bi2[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "co (Concatenate)                (None, 101, 2048)    0           add_6[0][0]                      \n",
      "                                                                 la[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "ts (TimeDistributed)            (None, 101, 2)       4098        co[0][0]                         \n",
      "==================================================================================================\n",
      "Total params: 12,595,202\n",
      "Trainable params: 12,595,202\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = preparation_elmo_v1(datasets[0], \\\n",
    "                    models[0].model, model_v1_ElMo_residual_BiLSTM_h)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
