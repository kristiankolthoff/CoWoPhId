{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1.1) Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from collections import defaultdict\n",
    "\n",
    "Dataset = namedtuple('Dataset', 'name, train, test')\n",
    "Model = namedtuple('Model', 'type, name, dimension, corpus, model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "MAIN_PATH_DATASET = \"../cwishareddataset/traindevset/english/\"\n",
    "genres = ['Wikipedia', 'WikiNews', 'News']\n",
    "datasets = ['Train', 'Dev']\n",
    "columns = ['id', 'sentence', \"start\", \"end\", \"target\", \n",
    "           \"nat\", \"non_nat\", \"nat_marked\", \"non_nat_marked\", \"binary\", \"prob\"]\n",
    "\n",
    "\n",
    "datasets = [Dataset('Wikipedia', 'Train', 'Dev'),\n",
    "            Dataset('WikiNews', 'Train', 'Dev'),\n",
    "            Dataset('News', 'Train', 'Dev')]\n",
    "\n",
    "feature_categories = []\n",
    "\n",
    "def load_df(path):\n",
    "    df = pd.read_csv(path, header=None, sep = \"\\t\")\n",
    "    df.columns = columns\n",
    "    return df\n",
    "\n",
    "datasets = [Dataset(d.name, load_df(MAIN_PATH_DATASET + d.name + '_' + d.train + '.tsv'),\n",
    "                            load_df(MAIN_PATH_DATASET + d.name + '_' + d.test + '.tsv'))\n",
    "                            for d in datasets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1.2) Load Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model : glove.6B.50d.txt\n",
      "load model : glove.twitter.27B.50d.txt\n",
      "[Model(type='glove', name='glove.6B.50d.txt', dimension=50, corpus='wikipedia+gigaword5', model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x0000007EE1F77358>), Model(type='glove', name='glove.twitter.27B.50d.txt', dimension=50, corpus='twitter', model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x0000007EEF61D7B8>)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "MAIN_PATH = 'D:/workspace_python/CoWoReId/python/resources/word-embeddings/'\n",
    "\n",
    "glove_defs = [#Model('glove', 'glove.42B.300d.txt', 300, 'cc42B', None),  \n",
    "              #Model('glove', 'glove.840B.300d.txt', 300, 'cc840B', None), \n",
    "              Model('glove', 'glove.6B.50d.txt', 50, 'wikipedia+gigaword5', None), \n",
    "              #Model('glove', 'glove.6B.100d.txt',100, 'wikipedia+gigaword5', None),\n",
    "              #Model('glove', 'glove.6B.200d.txt', 200, 'wikipedia+gigaword5', None), \n",
    "              #Model('glove', 'glove.6B.300d.txt', 300, 'wikipedia+gigaword5', None),\n",
    "              #Model('glove', 'glove.twitter.27B.25d.txt', 25, 'twitter', None)]\n",
    "              Model('glove', 'glove.twitter.27B.50d.txt', 50, 'twitter', None)] \n",
    "              #Model('glove', 'glove.twitter.27B.100d.txt', 100, 'twitter', None), \n",
    "              #Model('glove', 'glove.twitter.27B.200d.txt', 200, 'twitter', None)]\n",
    "\n",
    "glove_models = []\n",
    "for model in glove_defs:\n",
    "    glove_file = datapath(MAIN_PATH + model.name)\n",
    "    tmp_file = get_tmpfile(model.name + '-temp')\n",
    "    glove2word2vec(glove_file, tmp_file)\n",
    "    vecs = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "    glove_models.append(Model(model.type, model.name, model.dimension, model.corpus, vecs))\n",
    "    print('load model : {}'.format(model.name))\n",
    "    \n",
    "print(glove_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "models.extend(glove_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2.1) Preprocessing\n",
    "Here we present all the code to preprocess the data stored in a dataframe into a proper representation that can be used in sequence tagging models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from functools import lru_cache\n",
    "from utils import penn_to_wn\n",
    "import re\n",
    "import unicodedata\n",
    "import sys\n",
    "from collections import Counter\n",
    "from ngram_representation import missing_strat_random\n",
    "\n",
    "def overlaps(start1, end1, start2, end2):\n",
    "    return bool(range(max(start1, start2), min(end1, end2)+1))\n",
    "\n",
    "tbl = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "                      if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(tbl)\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def all_tokens_with_index(context):\n",
    "    '''\n",
    "    Receives a sentence denoted by context and applies tokenization\n",
    "    on the input. Each token is annotated with its word index starting\n",
    "    from 1 and the corresponding start and end character positions of \n",
    "    the word. Also applies some strategies to handle unproper formated\n",
    "    input sentence string such as removing additional whitespaces and \n",
    "    quotation marks that otherwise change the actual character start\n",
    "    and end positions. All results are cached in case it has to be computed\n",
    "    multiple times for the same sentence.\n",
    "    '''\n",
    "    curr_pos = 0\n",
    "    targets = []\n",
    "    j = 0\n",
    "    w = 0\n",
    "    curr_split = ''\n",
    "    ctx_split = context.split()\n",
    "    whitespaces = re.findall('\\s+', context)\n",
    "    num_whitespaces = [len(token) for token in whitespaces]\n",
    "    num_whitespaces.append(1)\n",
    "    tokens = word_tokenize(context)\n",
    "    tokens = ['\"' if token not in context else token for token in tokens]\n",
    "    for index, token in enumerate(tokens, 1):\n",
    "        targets.append((token, index, curr_pos, (curr_pos + len(token))))\n",
    "        curr_pos += len(token)\n",
    "        curr_split += token\n",
    "        if ctx_split[j] == curr_split:\n",
    "            curr_pos += num_whitespaces[w]\n",
    "            j += 1\n",
    "            w += 1\n",
    "            curr_split = ''\n",
    "    return [val for val in targets if val[0] != '\"']\n",
    "\n",
    "def build_vocabulary(sentences, embedding, dimension, \n",
    "                     missing='unique', provided = ['s_target', 'e_target']):\n",
    "    '''\n",
    "    Based on a list of sentences which are themselve represented\n",
    "    as a list of words, constructs a vocabulary of the words contained\n",
    "    and assigns unique indicies to the words. In particular, it returns \n",
    "    a map of indices to their words, a map of words to their indices\n",
    "    and based on the provided embedding model an embedding matrix\n",
    "    for the constructed vocabulary. For missing vocabulary, it \n",
    "    constructs a random embedding and a proper index is missing parameter\n",
    "    is set to 'unique', otherwise if it is set to 'equal' it creates\n",
    "    a random embedding for one special UNK embedding and neglects missing\n",
    "    vocabulary in the built index. All tokens in the 'provided' list,\n",
    "    receive under 'equal' mode still individual random embeddings.\n",
    "    '''\n",
    "    if missing not in ['unique', 'equal']:\n",
    "        raise ValueError(\"Parameter missing must be either 'equal' or 'unique'\")\n",
    "    all_words = [word for sentence in sentences for word in sentence]\n",
    "    print('# Words : {}'.format(len(all_words)))\n",
    "    counter = Counter(all_words)\n",
    "    index = 1\n",
    "    word2index = {}\n",
    "    for (word, count) in counter.most_common():\n",
    "        if (missing=='unique' or word in embedding.vocab):\n",
    "            word2index[word] = index\n",
    "            index += 1\n",
    "    word2index['_pad_'] = 0\n",
    "    if missing == 'equal':\n",
    "        word2index['_unk_'] = len(word2index)\n",
    "        for token in provided:\n",
    "            word2index[token] = len(word2index)\n",
    "    index2word = {index : word for word, index in word2index.items()}\n",
    "    vocab_size = len(word2index)\n",
    "    print('# Vocab : {}'.format(vocab_size))\n",
    "    embedding_matrix = np.zeros(((vocab_size), dimension))\n",
    "    embedding_matrix[0] = missing_strat_random('_pad_', dimension)\n",
    "    missing_embed_words = []\n",
    "    for word, index in word2index.items():\n",
    "        if word in embedding.vocab:\n",
    "            embedding_matrix[index] = embedding[word]\n",
    "        else:\n",
    "            embedding_matrix[index] = missing_strat_random(word, dimension)\n",
    "            missing_embed_words.append(word)\n",
    "    missing_embed_count = len(missing_embed_words)\n",
    "    print('# Words missing embedding : {}'.format(missing_embed_count))\n",
    "    print('Embedding shape : {}'.format(embedding_matrix.shape))\n",
    "    return word2index, index2word, embedding_matrix\n",
    "\n",
    "\n",
    "def build_char_vocabulary(sentences, embedding, dimension, \n",
    "                          missing='unique', provided = ['_']):\n",
    "    '''\n",
    "    Based on a list of sentences which are themselve represented\n",
    "    as a list of words, constructs a character vocabulary and provides\n",
    "    a mapping of unique indices to the found characters, a mapping of\n",
    "    the characters to their indicies and a character embedding matrix\n",
    "    where the i-th row represents the character embedding of the character\n",
    "    with index i. This is based on a provided character embedding, represented\n",
    "    as a dictionary. Provided tokens will be added as a single char to\n",
    "    the vocabulary.\n",
    "    '''\n",
    "    if missing not in ['unique', 'equal']:\n",
    "        raise ValueError(\"Parameter missing must be either 'equal' or 'unique'\")\n",
    "    all_chars = [char for sentence in sentences \n",
    "                 for word in sentence for char in word]\n",
    "    all_chars.extend(provided)\n",
    "    print('# Chars : {}'.format(len(all_chars)))\n",
    "    counter = Counter(all_chars)\n",
    "    index = 1\n",
    "    char2index = {}\n",
    "    for (char, count) in counter.most_common():\n",
    "        if (missing=='unique' or char in embedding.keys()):\n",
    "            char2index[char] = index\n",
    "            index += 1\n",
    "    char2index['_pad_'] = 0\n",
    "    if missing == 'equal':\n",
    "        char2index['_unk_'] = len(char2index)\n",
    "    index2char = {index : char for char, index in char2index.items()}\n",
    "    vocab_size = len(char2index)\n",
    "    print('# Vocab (chars) : {}'.format(vocab_size))\n",
    "    embedding_matrix = np.zeros(((vocab_size), dimension))\n",
    "    embedding_matrix[0] = missing_strat_random('_pad_', dimension)\n",
    "    missing_embed_chars = []\n",
    "    for char, index in char2index.items():\n",
    "        if char in embedding.keys():\n",
    "            embedding_matrix[index] = embedding[char]\n",
    "        else:\n",
    "            embedding_matrix[index] = missing_strat_random(char, dimension)\n",
    "            missing_embed_chars.append(char)\n",
    "    missing_embed_count = len(missing_embed_chars)\n",
    "    print('# Chars missing embedding : {}'.format(missing_embed_count))\n",
    "    print('Embedding shape : {}'.format(embedding_matrix.shape))\n",
    "    return char2index, index2char, embedding_matrix\n",
    "\n",
    "\n",
    "def compute_character_embeddings(embedding):\n",
    "    '''\n",
    "    Computes a character embedding as a dictionary of word to its\n",
    "    embedding based on a gensim word embedding. For each character,\n",
    "    averages the word embeddings containing the character as an\n",
    "    approximation to character-level embeddings.\n",
    "    '''\n",
    "    chars = {}\n",
    "    for word, vocab in embedding.vocab.items():\n",
    "        vector = embedding[word]\n",
    "        for char in word:\n",
    "            if ord(char)<128:\n",
    "                if char in chars:\n",
    "                    chars[char] = (chars[char][0]+vector, \n",
    "                                   chars[char][1]+1)\n",
    "                else:\n",
    "                    chars[char] = (vector, 1)\n",
    "    for char, (vector, num) in chars.items():\n",
    "        chars[char] = np.round(vector/num, 6).tolist()\n",
    "    return chars\n",
    "\n",
    "def forward_transformation(dataframe, lowercase = True, filter_punc = True, filtering = \"a132\"):\n",
    "    grouped = dataframe.groupby('sentence').apply(lambda row : \n",
    "                        {'sent_id' : list(set(row['sent_id']))[0],\n",
    "                         'sentence' : list(set(row['sentence']))[0], \n",
    "                         'tags': [tag for tag in zip(row['target'], \n",
    "                            row['start'], row['end'], row['binary'], row['prob'])]})\n",
    "    sentences = []\n",
    "    for vals in grouped:\n",
    "        sent_id = vals['sent_id']\n",
    "        sentence = vals['sentence']\n",
    "        tags = vals['tags']\n",
    "        tags_without_labels = [(word, start, end) for word, start, end, binary, prob in tags]\n",
    "        all_tokens = all_tokens_with_index(sentence)\n",
    "        sent_repr = [(word, start, end, tags[tags_without_labels.index((word, start, end))][3],\n",
    "                     tags[tags_without_labels.index((word, start, end))][4])\n",
    "           if (word, start, end) in tags_without_labels \n",
    "          else (word, start, end, 0, 0.0) for word, index, start, end in all_tokens]\n",
    "        if lowercase:\n",
    "            sent_repr = [(word.lower(), start, end, binary, prob) \n",
    "                         for word, start, end, binary, prob in sent_repr]\n",
    "        if filter_punc:\n",
    "            sent_repr = list(filter(lambda vals : remove_punctuation(vals[0]), sent_repr))\n",
    "        if filtering:\n",
    "            sent_repr = list(filter(lambda vals : vals[0] != \"'s\", sent_repr))\n",
    "            sent_repr = list(filter(lambda vals : vals[0] != \"``\", sent_repr))\n",
    "        sentences.append({'sent_id' : sent_id, 'sentence' : sentence, 'seq' : sent_repr})\n",
    "    return sentences\n",
    "\n",
    "def split_sentence_seqs(sentences):\n",
    "    words, start_end, binary, prob = [], [], [] ,[]\n",
    "    for sent in sentences:\n",
    "        sequence = sent['seq']\n",
    "        curr_w, curr_se, curr_b, curr_p = map(list, zip(*[(vals[0], \n",
    "            (vals[1], vals[2]), vals[3], vals[4]) for vals in sequence]))\n",
    "        words.append(curr_w)\n",
    "        start_end.append(curr_se)\n",
    "        binary.append(curr_b)\n",
    "        prob.append(curr_p)\n",
    "    return words, start_end, binary, prob\n",
    "\n",
    "def forward_transformation_be_tags(dataframe, start_tag = 's_target', end_tag = 'e_target',\n",
    "                                   lowercase = True, filter_punc = True, filtering = \"a132\"):\n",
    "    sentences = []\n",
    "    binaries = []\n",
    "    probabilities = []\n",
    "    for index, row in dataframe.iterrows():\n",
    "        sentence = row['sentence']\n",
    "        start = row['start']\n",
    "        end = row['end']\n",
    "        sentence = sentence[:start] + start_tag + ' ' + sentence[start:end] + \\\n",
    "                             ' ' + end_tag + sentence[end:]\n",
    "        if lowercase:\n",
    "            sentence = sentence.lower()\n",
    "        sent_repr = all_tokens_with_index(sentence)\n",
    "        if filter_punc:\n",
    "            sent_repr = list(filter(lambda vals : remove_punctuation(vals[0]), sent_repr))\n",
    "        if filtering:\n",
    "            sent_repr = list(filter(lambda vals : vals[0] != \"'s\", sent_repr))\n",
    "            sent_repr = list(filter(lambda vals : vals[0] != \"``\", sent_repr))\n",
    "        sentences.append([word for word, index, start, end in sent_repr])\n",
    "        binaries.append(row['binary'])\n",
    "        probabilities.append(row['prob'])\n",
    "    return (sentences, binaries, probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_embedding = models[0].model\n",
    "char_embeddings = compute_character_embeddings(word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Chars : 963941\n",
      "# Vocab (chars) : 48\n",
      "# Chars missing embedding : 2\n",
      "Embedding shape : (48, 50)\n"
     ]
    }
   ],
   "source": [
    "dimension = word_embedding.vector_size\n",
    "char2index, index2char, char_embedding = build_char_vocabulary(sentences, char_embeddings, dimension, missing='equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['e', 't', 'a', 'r', 'i', 'n', 's', 'o', 'h', 'l', 'd', 'c', 'g', 'u', 'm', 'f', 'p', 'b', 'w', '_', 'y', 'v', 'k', '0', '1', '-', '9', '2', 'x', 'j', '4', 'q', '6', '8', 'z', '3', '5', '7', '.', ',', '`', \"'\", '\\\\', '/', '=', '~', '_pad_', '_unk_'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2index.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Words : 186472\n",
      "# Vocab : 3641\n",
      "# Words missing embedding : 162\n",
      "Embedding shape : (3641, 50)\n"
     ]
    }
   ],
   "source": [
    "embedding = models[0].model\n",
    "dimension = embedding.vector_size\n",
    "word2index, index2word, word_embedding = build_vocabulary(sentences, embedding, dimension, missing='unique')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasets.append(Dataset('train_all_test_wiki', \n",
    "        datasets[0].train.append(datasets[1].train).append(datasets[2].train), datasets[0].test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Rows train : 5551\n",
      "# Rows test : 694\n",
      "# Rows dataset : 6245\n",
      "# Sents train : 387\n",
      "# Sents test : 53\n",
      "# Sents dataset : 440\n"
     ]
    }
   ],
   "source": [
    "# Append train and test set\n",
    "dataset_sel = datasets[0]\n",
    "train_num_rows = dataset_sel.train.shape[0]\n",
    "train_num_sents = len(list(set(dataset_sel.train.sentence.values.tolist())))\n",
    "\n",
    "test_num_rows = dataset_sel.test.shape[0]\n",
    "test_num_sents = len(list(set(dataset_sel.test.sentence.values.tolist())))\n",
    "\n",
    "dataset = dataset_sel.train.append(dataset_sel.test)\n",
    "dataset['sent_id'] = dataset.groupby('sentence').ngroup()\n",
    "dataset_num_rows = dataset.shape[0]\n",
    "dataset_num_sents = len(list(set(dataset.sentence.values.tolist())))\n",
    "\n",
    "print('# Rows train : {}'.format(train_num_rows))\n",
    "print('# Rows test : {}'.format(test_num_rows))\n",
    "print('# Rows dataset : {}'.format(dataset_num_rows))\n",
    "\n",
    "print('# Sents train : {}'.format(train_num_sents))\n",
    "print('# Sents test : {}'.format(test_num_sents))\n",
    "print('# Sents dataset : {}'.format(dataset_num_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences, binary, prob = forward_transformation_be_tags(dataset)\n",
    "train_sentences = sentences[:train_num_rows]\n",
    "test_sentences = sentences[train_num_rows:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_lens = [len(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence lengths\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEBCAYAAAB13qL/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set()\n",
    "ax = sns.distplot(sentence_lens)\n",
    "print('Sentence lengths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "figure = ax.get_figure()\n",
    "figure.savefig('../plots/dl/sentence_length_dist.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Words : 186472\n",
      "# Vocab : 3641\n",
      "# Words missing embedding : 161\n",
      "Embedding shape : (3641, 50)\n"
     ]
    }
   ],
   "source": [
    "embedding_model = models[0].model\n",
    "dimension = embedding_model.vector_size\n",
    "word2index, index2word, embedding = build_vocabulary(sentences, embedding_model, dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_with_indices = [[word2index[word] for word in sent] for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length sentence : 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "sent_lens = [len(sentence) for sentence in sentences]\n",
    "sent_max_length = np.max(sent_lens)\n",
    "print('Max length sentence : {}'.format(sent_max_length))\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "words_padded = pad_sequences(maxlen=sent_max_length, sequences=words_with_indices, padding=\"post\", value=0)\n",
    "\n",
    "binary_padded_categorical = [to_categorical(clazz, num_classes=2) for clazz in binary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length : 5551\n",
      "Test set length : 694\n"
     ]
    }
   ],
   "source": [
    "# Split the previously combined train and test set\n",
    "\n",
    "# (1) Training set\n",
    "train_words_padded = words_padded[:train_num_rows]\n",
    "train_binary_padded_categorical = binary_padded_categorical[:train_num_rows]\n",
    "\n",
    "# (2) Test set\n",
    "test_words_padded = words_padded[train_num_rows:]\n",
    "test_binary_padded_categorical = binary_padded_categorical[train_num_rows:]\n",
    "\n",
    "print('Training set length : {}'.format(len(train_words_padded)))\n",
    "print('Test set length : {}'.format(len(test_words_padded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import keras.callbacks\n",
    "\n",
    "class Metrics(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.f1_scores = []\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        predict = np.asarray(self.model.predict(self.validation_data[0]))\n",
    "        targ = self.validation_data[1]\n",
    "        targ = np.array(targ)\n",
    "        targ = np.argmax(targ, axis = 1)\n",
    "        predict = np.argmax(predict, axis = 1)\n",
    "        self.f1s=f1_score(targ, predict)\n",
    "        print(f1_score(targ, np.ones(targ.shape[0])))\n",
    "        print(targ.shape)\n",
    "        print(predict.shape)\n",
    "        print('--------------------Targets-------------------------')\n",
    "        print(self.f1s)\n",
    "        self.f1_scores.append(self.f1s)\n",
    "        print(targ)\n",
    "        print('--------------------Predictions-------------------------')\n",
    "        print(predict)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (None, 103)               0         \n",
      "_________________________________________________________________\n",
      "embedding_12 (Embedding)     (None, 103, 50)           182050    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 103, 50)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_12 (Bidirectio (None, 300)               241200    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 423,852\n",
      "Trainable params: 423,852\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5551 samples, validate on 694 samples\n",
      "Epoch 1/2\n",
      "5551/5551 [==============================] - 912s 164ms/step - loss: 0.6876 - acc: 0.5622 - val_loss: 0.6925 - val_acc: 0.5375\n",
      "0.6563407550822846\n",
      "(694,)\n",
      "(694,)\n",
      "--------------------Targets-------------------------\n",
      "0.6090133982947625\n",
      "[1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 1 0 1\n",
      " 0 0 1 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 0 0\n",
      " 1 1 1 0 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0\n",
      " 0 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1\n",
      " 1 1 0 1 0 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0\n",
      " 0 1 1 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 1 0 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 0 0 0 0 1\n",
      " 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 1\n",
      " 1 1 1 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 1\n",
      " 1 1 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 1 1 0 0 0 1 0 1 1\n",
      " 1 0 0 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1\n",
      " 0 1 0 0 0 1 1 1 0 0 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 1\n",
      " 0 0 0 0 1 1 0 0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 0 1 1 1 0 1 1 0 0 0\n",
      " 0 1 0 0 0 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0\n",
      " 1 1 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0\n",
      " 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0\n",
      " 1 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1\n",
      " 1 1 0 0 0 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 1 1 0 0 0 1 1 1 1 0\n",
      " 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1]\n",
      "--------------------Predictions-------------------------\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1 0 1 1 1 1 1 1 1 1 1 1]\n",
      "Epoch 2/2\n",
      "5551/5551 [==============================] - 887s 160ms/step - loss: 0.6263 - acc: 0.6570 - val_loss: 0.5552 - val_acc: 0.7262\n",
      "0.6563407550822846\n",
      "(694,)\n",
      "(694,)\n",
      "--------------------Targets-------------------------\n",
      "0.7551546391752577\n",
      "[1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 1 0 1\n",
      " 0 0 1 1 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 0 0\n",
      " 1 1 1 0 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 0 1 0\n",
      " 0 1 0 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1\n",
      " 1 1 0 1 0 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0\n",
      " 0 1 1 0 1 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 1 0 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 0 0 0 0 1\n",
      " 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 1 1 0 0 0 0 0 0 1\n",
      " 1 1 1 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 1\n",
      " 1 1 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1 0 1 1 1 1 0 0 1 1 1 1 0 0 0 1 0 1 1\n",
      " 1 0 0 1 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 0 1 0 1 1 1 0 0 0 1 1 1 1\n",
      " 0 1 0 0 0 1 1 1 0 0 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 0 1\n",
      " 0 0 0 0 1 1 0 0 1 1 0 1 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 0 1 1 1 0 1 1 0 0 0\n",
      " 0 1 0 0 0 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0\n",
      " 1 1 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 1 0\n",
      " 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 0 0\n",
      " 1 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 1 1 1 0 1 1 1\n",
      " 1 1 0 0 0 0 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 1 1 0 0 0 1 1 1 1 0\n",
      " 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1]\n",
      "--------------------Predictions-------------------------\n",
      "[1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1\n",
      " 1 1 1 0 1 1 1 0 0 0 1 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 0 1 0 0 1 0 1 1 0 1 1\n",
      " 1 0 1 1 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 1 1 1\n",
      " 1 1 1 1 0 1 0 1 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1\n",
      " 0 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 1 1 0 1\n",
      " 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 1 1 1 0 0 1 1\n",
      " 0 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0 1\n",
      " 1 1 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1 1 1 0 0 1 1\n",
      " 1 0 0 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1\n",
      " 1 1 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 1 1 0 0 0 1 1 0 1\n",
      " 0 1 0 0 0 1 1 1 0 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 0 1 1 0 0 0 0 1 1 1 0 1\n",
      " 0 0 0 0 1 1 0 0 1 0 0 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 1 0 1 1 1 0 1 1 1 0 0\n",
      " 1 1 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 1 1 1 0 1 1 0 0 0 1 1 1 0 1 1 1 1\n",
      " 1 1 0 0 1 1 1 0 0 0 1 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 0 0 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 0 1\n",
      " 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "\n",
    "vocab_size = embedding.shape[0]\n",
    "dimension = embedding.shape[1]\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "in_seq = Input(shape=(sent_max_length,))\n",
    "embed = Embedding(input_dim=vocab_size, output_dim=dimension, \\\n",
    "                  weights=[embedding], input_length=sent_max_length)(in_seq)\n",
    "drop = Dropout(0.1)(embed)\n",
    "lstm = Bidirectional(LSTM(units=150, return_sequences=False, recurrent_dropout=0.1))(drop)\n",
    "out = Dense(2, activation=\"softmax\")(lstm)\n",
    "\n",
    "model = Model(in_seq, out)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "metrics = Metrics()\n",
    "history = model.fit(train_words_padded, np.array(train_binary_padded_categorical), batch_size=1, \n",
    "                    epochs=2, validation_data = (test_words_padded, np.array(test_binary_padded_categorical)), \n",
    "                    verbose=1, callbacks=[metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(metrics.f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEBCAYAAAB13qL/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "epoch_f1s = plt.plot(metrics.f1_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(694, 2)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(test_words_padded)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0,\n",
       "       1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1,\n",
       "       0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "       0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "       0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_predictions = np.argmax(predictions, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "694"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targets = np.argmax(test_binary_padded_categorical, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "694"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7551546391752577"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(targets, final_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-442b3afdad24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0msent_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_words_padded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_sentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentence'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0msent_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_sentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sent_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'pad'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "def overlaps(start1, end1, start2, end2):\n",
    "    return bool(range(max(start1, start2), min(end1, end2)+1))\n",
    "\n",
    "def ngram_prediction_agg_majority_vote(predictions):\n",
    "    positive_sum = np.sum(predictions)\n",
    "    ratio = positive_sum / len(predictions)\n",
    "    return int(ratio + 0.5)\n",
    "\n",
    "def ngram_prediction_agg_max(predictions):\n",
    "    return np.max(predictions)\n",
    "\n",
    "def ngram_prediction_agg_begin(predictions):\n",
    "    return predictions[0]\n",
    "\n",
    "def ngram_prediction_agg_end(predictions):\n",
    "    return predictions[-1]\n",
    "\n",
    "ngram_prediction_agg = ngram_prediction_agg_max\n",
    "results = []\n",
    "dataframe = dataset\n",
    "num_missing_match = []\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "count=0\n",
    "sent_count = 0\n",
    "for ind, sent in enumerate(test_words_padded):\n",
    "    sentence = test_sentences[ind]['sentence']\n",
    "    sent_id = test_sentences[ind]['sent_id']\n",
    "    words = [index2word.get(index, 'pad')  for index in sent]\n",
    "    print(sentence)\n",
    "    print(sent_id)\n",
    "    print(words)\n",
    "    ses = test_start_end[ind]\n",
    "    preds = final_predictions[ind]\n",
    "    print(ses)\n",
    "    print(preds)\n",
    "    selected = dataframe.loc[dataframe.sent_id==sent_id,]\n",
    "    targets = selected.target.values.tolist()\n",
    "    start_ends = list(zip(selected.start.values.tolist(), selected.end.values.tolist()))\n",
    "    binary_y = selected.binary.values.tolist()\n",
    "    print(binary_y)\n",
    "    print(targets)\n",
    "    print(start_ends)\n",
    "    for label_index,(start, end) in enumerate(start_ends):\n",
    "        print('-----',start, end)\n",
    "        matching_indices = [i for i, (s, e) in enumerate(ses) if overlaps(start, end, s, e)]\n",
    "        if not matching_indices:\n",
    "            num_missing_match.append((sentence, sent_id, (start,end), ses, words))\n",
    "            prediction = 0\n",
    "        matching_predictions = [preds[i] for i in matching_indices]\n",
    "        if len(matching_predictions)>1:\n",
    "            prediction = ngram_prediction_agg(matching_predictions)\n",
    "        else:\n",
    "            if matching_indices:\n",
    "                prediction = matching_predictions[0]\n",
    "        matching_labels = binary_y[label_index]\n",
    "        print(matching_indices)\n",
    "        print(matching_labels)\n",
    "        print(matching_predictions)\n",
    "        print(prediction)\n",
    "        all_labels.append(matching_labels)\n",
    "        all_predictions.append(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7076271186440678"
      ]
     },
     "execution_count": 811,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(all_labels, all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "707"
      ]
     },
     "execution_count": 691,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>target</th>\n",
       "      <th>nat</th>\n",
       "      <th>non_nat</th>\n",
       "      <th>nat_marked</th>\n",
       "      <th>non_nat_marked</th>\n",
       "      <th>binary</th>\n",
       "      <th>prob</th>\n",
       "      <th>sent_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>3KVQ0UJWPXMUWB3UPVC6XSKX54H5WE</td>\n",
       "      <td>The zero point of any thermodynamic temperature scale , such as Kelvin or Rankine , is set at absolute zero .</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>zero point</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>3KVQ0UJWPXMUWB3UPVC6XSKX54H5WE</td>\n",
       "      <td>The zero point of any thermodynamic temperature scale , such as Kelvin or Rankine , is set at absolute zero .</td>\n",
       "      <td>22</td>\n",
       "      <td>35</td>\n",
       "      <td>thermodynamic</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.20</td>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>3KVQ0UJWPXMUWB3UPVC6XSKX54H5WE</td>\n",
       "      <td>The zero point of any thermodynamic temperature scale , such as Kelvin or Rankine , is set at absolute zero .</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>point</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>3KVQ0UJWPXMUWB3UPVC6XSKX54H5WE</td>\n",
       "      <td>The zero point of any thermodynamic temperature scale , such as Kelvin or Rankine , is set at absolute zero .</td>\n",
       "      <td>36</td>\n",
       "      <td>47</td>\n",
       "      <td>temperature</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>3KVQ0UJWPXMUWB3UPVC6XSKX54H5WE</td>\n",
       "      <td>The zero point of any thermodynamic temperature scale , such as Kelvin or Rankine , is set at absolute zero .</td>\n",
       "      <td>36</td>\n",
       "      <td>53</td>\n",
       "      <td>temperature scale</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>3KVQ0UJWPXMUWB3UPVC6XSKX54H5WE</td>\n",
       "      <td>The zero point of any thermodynamic temperature scale , such as Kelvin or Rankine , is set at absolute zero .</td>\n",
       "      <td>48</td>\n",
       "      <td>53</td>\n",
       "      <td>scale</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>3KVQ0UJWPXMUWB3UPVC6XSKX54H5WE</td>\n",
       "      <td>The zero point of any thermodynamic temperature scale , such as Kelvin or Rankine , is set at absolute zero .</td>\n",
       "      <td>64</td>\n",
       "      <td>70</td>\n",
       "      <td>Kelvin</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>3KVQ0UJWPXMUWB3UPVC6XSKX54H5WE</td>\n",
       "      <td>The zero point of any thermodynamic temperature scale , such as Kelvin or Rankine , is set at absolute zero .</td>\n",
       "      <td>74</td>\n",
       "      <td>81</td>\n",
       "      <td>Rankine</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>3KVQ0UJWPXMUWB3UPVC6XSKX54H5WE</td>\n",
       "      <td>The zero point of any thermodynamic temperature scale , such as Kelvin or Rankine , is set at absolute zero .</td>\n",
       "      <td>94</td>\n",
       "      <td>102</td>\n",
       "      <td>absolute</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>3KVQ0UJWPXMUWB3UPVC6XSKX54H5WE</td>\n",
       "      <td>The zero point of any thermodynamic temperature scale , such as Kelvin or Rankine , is set at absolute zero .</td>\n",
       "      <td>87</td>\n",
       "      <td>90</td>\n",
       "      <td>set</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>3KVQ0UJWPXMUWB3UPVC6XSKX54H5WE</td>\n",
       "      <td>The zero point of any thermodynamic temperature scale , such as Kelvin or Rankine , is set at absolute zero .</td>\n",
       "      <td>94</td>\n",
       "      <td>107</td>\n",
       "      <td>absolute zero</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  \\\n",
       "226  3KVQ0UJWPXMUWB3UPVC6XSKX54H5WE   \n",
       "227  3KVQ0UJWPXMUWB3UPVC6XSKX54H5WE   \n",
       "228  3KVQ0UJWPXMUWB3UPVC6XSKX54H5WE   \n",
       "229  3KVQ0UJWPXMUWB3UPVC6XSKX54H5WE   \n",
       "230  3KVQ0UJWPXMUWB3UPVC6XSKX54H5WE   \n",
       "231  3KVQ0UJWPXMUWB3UPVC6XSKX54H5WE   \n",
       "232  3KVQ0UJWPXMUWB3UPVC6XSKX54H5WE   \n",
       "233  3KVQ0UJWPXMUWB3UPVC6XSKX54H5WE   \n",
       "234  3KVQ0UJWPXMUWB3UPVC6XSKX54H5WE   \n",
       "235  3KVQ0UJWPXMUWB3UPVC6XSKX54H5WE   \n",
       "236  3KVQ0UJWPXMUWB3UPVC6XSKX54H5WE   \n",
       "\n",
       "                                                                                                          sentence  \\\n",
       "226  The zero point of any thermodynamic temperature scale , such as Kelvin or Rankine , is set at absolute zero .   \n",
       "227  The zero point of any thermodynamic temperature scale , such as Kelvin or Rankine , is set at absolute zero .   \n",
       "228  The zero point of any thermodynamic temperature scale , such as Kelvin or Rankine , is set at absolute zero .   \n",
       "229  The zero point of any thermodynamic temperature scale , such as Kelvin or Rankine , is set at absolute zero .   \n",
       "230  The zero point of any thermodynamic temperature scale , such as Kelvin or Rankine , is set at absolute zero .   \n",
       "231  The zero point of any thermodynamic temperature scale , such as Kelvin or Rankine , is set at absolute zero .   \n",
       "232  The zero point of any thermodynamic temperature scale , such as Kelvin or Rankine , is set at absolute zero .   \n",
       "233  The zero point of any thermodynamic temperature scale , such as Kelvin or Rankine , is set at absolute zero .   \n",
       "234  The zero point of any thermodynamic temperature scale , such as Kelvin or Rankine , is set at absolute zero .   \n",
       "235  The zero point of any thermodynamic temperature scale , such as Kelvin or Rankine , is set at absolute zero .   \n",
       "236  The zero point of any thermodynamic temperature scale , such as Kelvin or Rankine , is set at absolute zero .   \n",
       "\n",
       "     start  end             target  nat  non_nat  nat_marked  non_nat_marked  \\\n",
       "226      4   14         zero point   10       10           1               0   \n",
       "227     22   35      thermodynamic   10       10           2               2   \n",
       "228      9   14              point   10       10           0               0   \n",
       "229     36   47        temperature   10       10           2               0   \n",
       "230     36   53  temperature scale   10       10           1               0   \n",
       "231     48   53              scale   10       10           1               1   \n",
       "232     64   70             Kelvin   10       10           2               0   \n",
       "233     74   81            Rankine   10       10           2               0   \n",
       "234     94  102           absolute   10       10           0               0   \n",
       "235     87   90                set   10       10           0               0   \n",
       "236     94  107      absolute zero   10       10           1               0   \n",
       "\n",
       "     binary  prob  sent_id  \n",
       "226       1  0.05      387  \n",
       "227       1  0.20      387  \n",
       "228       0  0.00      387  \n",
       "229       1  0.10      387  \n",
       "230       1  0.05      387  \n",
       "231       1  0.10      387  \n",
       "232       1  0.10      387  \n",
       "233       1  0.10      387  \n",
       "234       0  0.00      387  \n",
       "235       0  0.00      387  \n",
       "236       1  0.05      387  "
      ]
     },
     "execution_count": 637,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.loc[dataset.sent_id==387,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
