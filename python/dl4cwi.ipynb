{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1.1) Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from collections import defaultdict\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "Dataset = namedtuple('Dataset', 'name, train, test')\n",
    "Model = namedtuple('Model', 'type, name, dimension, corpus, model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "MAIN_PATH_DATASET = \"../cwishareddataset/traindevset/english/\"\n",
    "genres = ['Wikipedia', 'WikiNews', 'News']\n",
    "datasets = ['Train', 'Dev']\n",
    "columns = ['id', 'sentence', \"start\", \"end\", \"target\", \n",
    "           \"nat\", \"non_nat\", \"nat_marked\", \"non_nat_marked\", \"binary\", \"prob\"]\n",
    "\n",
    "\n",
    "datasets = [Dataset('Wikipedia', 'Train', 'Dev'),\n",
    "            Dataset('WikiNews', 'Train', 'Dev'),\n",
    "            Dataset('News', 'Train', 'Dev')]\n",
    "\n",
    "feature_categories = []\n",
    "\n",
    "def load_df(path):\n",
    "    df = pd.read_csv(path, header=None, sep = \"\\t\")\n",
    "    df.columns = columns\n",
    "    return df\n",
    "\n",
    "datasets = [Dataset(d.name, load_df(MAIN_PATH_DATASET + d.name + '_' + d.train + '.tsv'),\n",
    "                            load_df(MAIN_PATH_DATASET + d.name + '_' + d.test + '.tsv'))\n",
    "                            for d in datasets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1.2) Load Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\gensim-3.5.0-py3.6-win-amd64.egg\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model : glove.6B.50d.txt\n",
      "load model : glove.6B.100d.txt\n",
      "load model : glove.6B.200d.txt\n",
      "load model : glove.twitter.27B.50d.txt\n",
      "load model : glove.twitter.27B.100d.txt\n",
      "load model : glove.twitter.27B.200d.txt\n",
      "[Model(type='glove', name='glove.6B.50d.txt', dimension=50, corpus='wikipedia+gigaword5', model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x000000251C3072B0>), Model(type='glove', name='glove.6B.100d.txt', dimension=100, corpus='wikipedia+gigaword5', model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x0000002529CF19B0>), Model(type='glove', name='glove.6B.200d.txt', dimension=200, corpus='wikipedia+gigaword5', model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x000000253BD97EB8>), Model(type='glove', name='glove.twitter.27B.50d.txt', dimension=50, corpus='twitter', model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x0000002557711D68>), Model(type='glove', name='glove.twitter.27B.100d.txt', dimension=100, corpus='twitter', model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x0000002581E3BB70>), Model(type='glove', name='glove.twitter.27B.200d.txt', dimension=200, corpus='twitter', model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x00000025B7BA8D30>)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "MAIN_PATH = 'D:/workspace_python/CoWoReId/python/resources/word-embeddings/'\n",
    "\n",
    "glove_defs = [#Model('glove', 'glove.42B.300d.txt', 300, 'cc42B', None),  \n",
    "              #Model('glove', 'glove.840B.300d.txt', 300, 'cc840B', None), \n",
    "              Model('glove', 'glove.6B.50d.txt', 50, 'wikipedia+gigaword5', None), \n",
    "              Model('glove', 'glove.6B.100d.txt',100, 'wikipedia+gigaword5', None),\n",
    "              Model('glove', 'glove.6B.200d.txt', 200, 'wikipedia+gigaword5', None), \n",
    "              #Model('glove', 'glove.6B.300d.txt', 300, 'wikipedia+gigaword5', None),\n",
    "              #Model('glove', 'glove.twitter.27B.25d.txt', 25, 'twitter', None)]\n",
    "              Model('glove', 'glove.twitter.27B.50d.txt', 50, 'twitter', None),\n",
    "              Model('glove', 'glove.twitter.27B.100d.txt', 100, 'twitter', None),\n",
    "              Model('glove', 'glove.twitter.27B.200d.txt', 200, 'twitter', None)]\n",
    "\n",
    "glove_models = []\n",
    "for model in glove_defs:\n",
    "    glove_file = datapath(MAIN_PATH + model.name)\n",
    "    tmp_file = get_tmpfile(model.name + '-temp')\n",
    "    glove2word2vec(glove_file, tmp_file)\n",
    "    vecs = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "    glove_models.append(Model(model.type, model.name, model.dimension, model.corpus, vecs))\n",
    "    print('load model : {}'.format(model.name))\n",
    "    \n",
    "print(glove_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "models.extend(glove_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2.1) Preprocessing\n",
    "Here we present all the code to preprocess the data stored in a dataframe into a proper representation that can be used in sequence tagging models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from functools import lru_cache\n",
    "from utils import penn_to_wn\n",
    "import re\n",
    "import unicodedata\n",
    "import sys\n",
    "from collections import Counter\n",
    "from ngram_representation import missing_strat_random\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "tbl = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "                      if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "def overlaps(start1, end1, start2, end2):\n",
    "    return bool(range(max(start1, start2), min(end1, end2)+1))\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(tbl)\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def all_tokens_with_index(context):\n",
    "    '''\n",
    "    Receives a sentence denoted by context and applies tokenization\n",
    "    on the input. Each token is annotated with its word index starting\n",
    "    from 1 and the corresponding start and end character positions of \n",
    "    the word. Also applies some strategies to handle unproper formated\n",
    "    input sentence string such as removing additional whitespaces and \n",
    "    quotation marks that otherwise change the actual character start\n",
    "    and end positions. All results are cached in case it has to be computed\n",
    "    multiple times for the same sentence.\n",
    "    '''\n",
    "    curr_pos = 0\n",
    "    targets = []\n",
    "    j = 0\n",
    "    w = 0\n",
    "    curr_split = ''\n",
    "    ctx_split = context.split()\n",
    "    whitespaces = re.findall('\\s+', context)\n",
    "    num_whitespaces = [len(token) for token in whitespaces]\n",
    "    num_whitespaces.append(1)\n",
    "    tokens = word_tokenize(context)\n",
    "    tokens = ['\"' if token not in context else token for token in tokens]\n",
    "    for index, token in enumerate(tokens, 1):\n",
    "        targets.append((token, index, curr_pos, (curr_pos + len(token))))\n",
    "        curr_pos += len(token)\n",
    "        curr_split += token\n",
    "        if ctx_split[j] == curr_split:\n",
    "            curr_pos += num_whitespaces[w]\n",
    "            j += 1\n",
    "            w += 1\n",
    "            curr_split = ''\n",
    "    return [val for val in targets if val[0] != '\"']\n",
    "\n",
    "def build_vocabulary(sentences, embedding, dimension, \n",
    "                     missing='unique', provided = ['s_target', 'e_target']):\n",
    "    '''\n",
    "    Based on a list of sentences which are themselve represented\n",
    "    as a list of words, constructs a vocabulary of the words contained\n",
    "    and assigns unique indicies to the words. In particular, it returns \n",
    "    a map of indices to their words, a map of words to their indices\n",
    "    and based on the provided embedding model an embedding matrix\n",
    "    for the constructed vocabulary. For missing vocabulary, it \n",
    "    constructs a random embedding and a proper index is missing parameter\n",
    "    is set to 'unique', otherwise if it is set to 'equal' it creates\n",
    "    a random embedding for one special UNK embedding and neglects missing\n",
    "    vocabulary in the built index. All tokens in the 'provided' list,\n",
    "    receive under 'equal' mode still individual random embeddings.\n",
    "    '''\n",
    "    if missing not in ['unique', 'equal']:\n",
    "        raise ValueError(\"Parameter missing must be either 'equal' or 'unique'\")\n",
    "    all_words = [word for sentence in sentences for word in sentence]\n",
    "    print('# Words : {}'.format(len(all_words)))\n",
    "    counter = Counter(all_words)\n",
    "    index = 1\n",
    "    word2index = {}\n",
    "    for (word, count) in counter.most_common():\n",
    "        if (missing=='unique' or word in embedding.vocab):\n",
    "            word2index[word] = index\n",
    "            index += 1\n",
    "    word2index['_pad_'] = 0\n",
    "    if missing == 'equal':\n",
    "        word2index['_unk_'] = len(word2index)\n",
    "        for token in provided:\n",
    "            word2index[token] = len(word2index)\n",
    "    index2word = {index : word for word, index in word2index.items()}\n",
    "    vocab_size = len(word2index)\n",
    "    print('# Vocab : {}'.format(vocab_size))\n",
    "    embedding_matrix = np.zeros(((vocab_size), dimension))\n",
    "    embedding_matrix[0] = missing_strat_random('_pad_', dimension)\n",
    "    missing_embed_words = []\n",
    "    for word, index in word2index.items():\n",
    "        if word in embedding.vocab:\n",
    "            embedding_matrix[index] = embedding[word]\n",
    "        else:\n",
    "            embedding_matrix[index] = missing_strat_random(word, dimension)\n",
    "            missing_embed_words.append(word)\n",
    "    missing_embed_count = len(missing_embed_words)\n",
    "    print('# Words missing embedding : {}'.format(missing_embed_count))\n",
    "    print('Embedding shape : {}'.format(embedding_matrix.shape))\n",
    "    return word2index, index2word, embedding_matrix\n",
    "\n",
    "\n",
    "def build_char_vocabulary(sentences, embedding, dimension, \n",
    "                          missing='unique', provided = ['_']):\n",
    "    '''\n",
    "    Based on a list of sentences which are themselve represented\n",
    "    as a list of words, constructs a character vocabulary and provides\n",
    "    a mapping of unique indices to the found characters, a mapping of\n",
    "    the characters to their indicies and a character embedding matrix\n",
    "    where the i-th row represents the character embedding of the character\n",
    "    with index i. This is based on a provided character embedding, represented\n",
    "    as a dictionary. Provided tokens will be added as a single char to\n",
    "    the vocabulary.\n",
    "    '''\n",
    "    if missing not in ['unique', 'equal']:\n",
    "        raise ValueError(\"Parameter missing must be either 'equal' or 'unique'\")\n",
    "    all_chars = [char for sentence in sentences \n",
    "                 for word in sentence for char in word]\n",
    "    all_chars.extend(provided)\n",
    "    print('# Chars : {}'.format(len(all_chars)))\n",
    "    counter = Counter(all_chars)\n",
    "    index = 1\n",
    "    char2index = {}\n",
    "    for (char, count) in counter.most_common():\n",
    "        if (missing=='unique' or char in embedding.keys()):\n",
    "            char2index[char] = index\n",
    "            index += 1\n",
    "    char2index['_pad_'] = 0\n",
    "    if missing == 'equal':\n",
    "        char2index['_unk_'] = len(char2index)\n",
    "    index2char = {index : char for char, index in char2index.items()}\n",
    "    vocab_size = len(char2index)\n",
    "    print('# Vocab (chars) : {}'.format(vocab_size))\n",
    "    embedding_matrix = np.zeros(((vocab_size), dimension))\n",
    "    embedding_matrix[0] = missing_strat_random('_pad_', dimension)\n",
    "    missing_embed_chars = []\n",
    "    for char, index in char2index.items():\n",
    "        if char in embedding.keys():\n",
    "            embedding_matrix[index] = embedding[char]\n",
    "        else:\n",
    "            embedding_matrix[index] = missing_strat_random(char, dimension)\n",
    "            missing_embed_chars.append(char)\n",
    "    missing_embed_count = len(missing_embed_chars)\n",
    "    print('# Chars missing embedding : {}'.format(missing_embed_count))\n",
    "    print('Embedding shape : {}'.format(embedding_matrix.shape))\n",
    "    return char2index, index2char, embedding_matrix\n",
    "\n",
    "\n",
    "def compute_character_embeddings(embedding):\n",
    "    '''\n",
    "    Computes a character embedding as a dictionary of word to its\n",
    "    embedding based on a gensim word embedding. For each character,\n",
    "    averages the word embeddings containing the character as an\n",
    "    approximation to character-level embeddings.\n",
    "    '''\n",
    "    chars = {}\n",
    "    for word, vocab in embedding.vocab.items():\n",
    "        vector = embedding[word]\n",
    "        for char in word:\n",
    "            if ord(char)<128:\n",
    "                if char in chars:\n",
    "                    chars[char] = (chars[char][0]+vector, \n",
    "                                   chars[char][1]+1)\n",
    "                else:\n",
    "                    chars[char] = (vector, 1)\n",
    "    for char, (vector, num) in chars.items():\n",
    "        chars[char] = np.round(vector/num, 6).tolist()\n",
    "    return chars\n",
    "\n",
    "def forward_transformation_v1(dataframe, lowercase = True, filter_punc = True, filtering = \"a132\"):\n",
    "    '''\n",
    "    Transforms the provided dataframe rows into a representation\n",
    "    to be used in a sequence classifier. For each sentence in the\n",
    "    dataset, returns the sentence id, the sentence as a list of \n",
    "    words based on tokenization, the binary label for each word, \n",
    "    the probability label for each word and a list of (start, end)\n",
    "    tuples representing the start and end positions of the words \n",
    "    in the sentence word list. This can be used to map back the\n",
    "    predictions later. This function should be used if classification\n",
    "    is done on word level.\n",
    "    '''\n",
    "    grouped = dataframe.groupby('sentence').apply(lambda row : \n",
    "                        {'sent_id' : list(set(row['sent_id']))[0],\n",
    "                         'sentence' : list(set(row['sentence']))[0], \n",
    "                         'tags': [tag for tag in zip(row['target'], \n",
    "                            row['start'], row['end'], row['binary'], row['prob'])]})\n",
    "    sentence_ids = []\n",
    "    sentences = []\n",
    "    binaries = []\n",
    "    probabilities = []\n",
    "    start_ends = []\n",
    "    for vals in grouped:\n",
    "        sent_id = vals['sent_id']\n",
    "        sentence = vals['sentence']\n",
    "        tags = vals['tags']\n",
    "        tags_without_labels = [(word, start, end) for word, start, end, binary, prob in tags]\n",
    "        all_tokens = all_tokens_with_index(sentence)\n",
    "        sent_repr = [(word, start, end, tags[tags_without_labels.index((word, start, end))][3],\n",
    "                     tags[tags_without_labels.index((word, start, end))][4])\n",
    "           if (word, start, end) in tags_without_labels \n",
    "          else (word, start, end, 0, 0.0) for word, index, start, end in all_tokens]\n",
    "        if lowercase:\n",
    "            sent_repr = [(word.lower(), start, end, binary, prob) \n",
    "                         for word, start, end, binary, prob in sent_repr]\n",
    "        if filter_punc:\n",
    "            sent_repr = list(filter(lambda vals : remove_punctuation(vals[0]), sent_repr))\n",
    "        if filtering:\n",
    "            sent_repr = list(filter(lambda vals : vals[0] != \"'s\", sent_repr))\n",
    "            sent_repr = list(filter(lambda vals : vals[0] != \"``\", sent_repr))\n",
    "        sentence_ids.append(sent_id)\n",
    "        sentences.append([vals[0] for vals in sent_repr])\n",
    "        binaries.append([vals[3] for vals in sent_repr])\n",
    "        probabilities.append([vals[4] for vals in sent_repr])\n",
    "        start_ends.append([(vals[1], vals[2]) for vals in sent_repr])\n",
    "    return sentence_ids, sentences, start_ends, binaries, probabilities\n",
    "\n",
    "\n",
    "def forward_transformation_v2(dataframe, start_tag = 's_target', end_tag = 'e_target',\n",
    "                                   lowercase = True, filter_punc = True, filtering = \"a132\"):\n",
    "    '''\n",
    "    Transforms the provided dataframe rows into a representation\n",
    "    to be used in a sequence classifier. Here, each row in the dataframe\n",
    "    is transformed into one instance to classify and each target in a \n",
    "    sentence is tagged by surrounding it with the provided start and\n",
    "    end tag. Hence, the whole sequence is classified at once. Returns\n",
    "    the transformed sentences (one for each row in the dataframe), the\n",
    "    corresponding binary and probabilitiy label.\n",
    "    '''\n",
    "    sentences = []\n",
    "    binaries = []\n",
    "    probabilities = []\n",
    "    for index, row in dataframe.iterrows():\n",
    "        sentence = row['sentence']\n",
    "        start = row['start']\n",
    "        end = row['end']\n",
    "        sentence = sentence[:start] + start_tag + ' ' + sentence[start:end] + \\\n",
    "                             ' ' + end_tag + sentence[end:]\n",
    "        if lowercase:\n",
    "            sentence = sentence.lower()\n",
    "        sent_repr = all_tokens_with_index(sentence)\n",
    "        if filter_punc:\n",
    "            sent_repr = list(filter(lambda vals : remove_punctuation(vals[0]), sent_repr))\n",
    "        if filtering:\n",
    "            sent_repr = list(filter(lambda vals : vals[0] != \"'s\", sent_repr))\n",
    "            sent_repr = list(filter(lambda vals : vals[0] != \"``\", sent_repr))\n",
    "        sentences.append([word for word, index, start, end in sent_repr])\n",
    "        binaries.append(row['binary'])\n",
    "        probabilities.append(row['prob'])\n",
    "    return sentences, binaries, probabilities\n",
    "\n",
    "\n",
    "def merge_train_test_dataset(dataset):\n",
    "    '''\n",
    "    Computes the training and test set size as (1) the\n",
    "    number of rows and (2) the number of unique sentences\n",
    "    contained. Merges the training and test set and\n",
    "    computes the same values for the merged dataframe.\n",
    "    This can be used before creating the vocabulary,\n",
    "    to also not miss vocabulary contained solely in the\n",
    "    test set. Before training, the dataset has to be split\n",
    "    up again based on the provided numbers.\n",
    "    '''\n",
    "    # Compute num rows and sents for train and test\n",
    "    train_num_rows = dataset.train.shape[0]\n",
    "    train_num_sents = len(list(set(dataset.train.sentence.values.tolist())))\n",
    "    test_num_rows = dataset.test.shape[0]\n",
    "    test_num_sents = len(list(set(dataset.test.sentence.values.tolist())))\n",
    "    # Merge dataframes and compute same values\n",
    "    dataset_merged = dataset.train.append(dataset.test)\n",
    "    dataset_merged['sent_id'] = dataset_merged.groupby('sentence').ngroup()\n",
    "    dataset_num_rows = dataset_merged.shape[0]\n",
    "    dataset_num_sents = len(list(set(dataset_merged.sentence.values.tolist())))\n",
    "    print('----------------------')\n",
    "    print('# Rows train : {}'.format(train_num_rows))\n",
    "    print('# Rows test : {}'.format(test_num_rows))\n",
    "    print('# Rows dataset : {}'.format(dataset_num_rows))\n",
    "    print('----------------------')\n",
    "    print('# Sents train : {}'.format(train_num_sents))\n",
    "    print('# Sents test : {}'.format(test_num_sents))\n",
    "    print('# Sents dataset : {}'.format(dataset_num_sents))\n",
    "    print('----------------------')\n",
    "    return dataset_merged, train_num_rows, test_num_rows, \\\n",
    "                dataset_num_rows, train_num_sents, test_num_sents, \\\n",
    "                dataset_num_sents\n",
    "        \n",
    "def split_train_test_dataset(dataset, index):\n",
    "    '''\n",
    "    Splits the given dataset (here the list of sentences represented\n",
    "    as list of words, binary label lists etc.) into train and test\n",
    "    based on the given index. Depending on the used sequence representation\n",
    "    (v1 or v2), the index should be either the number of training sentences \n",
    "    (unique) or the number of training rows.\n",
    "    '''\n",
    "    train = dataset[:index]\n",
    "    test = dataset[index:]\n",
    "    print('Training set length : {}'.format(len(train)))\n",
    "    print('Test set length : {}'.format(len(test)))\n",
    "    return train, test\n",
    "        \n",
    "def sentence_max_length(sentences):\n",
    "    sent_lens = [len(sentence) for sentence in sentences]\n",
    "    sent_max_length = np.max(sent_lens)\n",
    "    print('Max length sentence : {}'.format(sent_max_length))\n",
    "    return sent_max_length\n",
    "\n",
    "def pad_encode_sequences(sentences, word2index, missing, max_length):\n",
    "    '''\n",
    "    Encodes a list of sentences (a sentence represented as a list of words)\n",
    "    into their corresponding integer index based on the provided dictionary.\n",
    "    For missing tokens in the dictionary, applies the provided 'missing' value.\n",
    "    Afterwards, applies padding to the sentences and uses 'max_length' as the\n",
    "    maximum overall sentence length.\n",
    "    '''\n",
    "    words_with_indices = [[word2index.get(word, missing)\n",
    "                        for word in sent] for sent in sentences]\n",
    "    pad_val = word2index['_pad_']\n",
    "    return pad_sequences(maxlen=max_length, \\\n",
    "            sequences=words_with_indices, padding=\"post\", value=pad_val)\n",
    "\n",
    "def pad_binaries_probs(binaries, probabilities, max_length):\n",
    "    '''\n",
    "    Applies padding to binary labels and the probabilities.\n",
    "    Parameter 'max_length' is used as the maximum length\n",
    "    to pad the two provided lists to.\n",
    "    '''\n",
    "    binary_padded = pad_sequences(maxlen=max_length, sequences=binaries, padding=\"post\", value=0)\n",
    "    prob_padded = pad_sequences(maxlen=max_length, sequences=probabilities, padding=\"post\", value=0, dtype=\"float\")\n",
    "    return binary_padded, prob_padded\n",
    "\n",
    "def pad_chars(sentences, char2index, sent_max_length, char_max_length):\n",
    "    words_as_chars_indices = []\n",
    "    for sent in sentences:\n",
    "        sent_repr = []\n",
    "        for sent_index in range(sent_max_length):\n",
    "            words_repr = []\n",
    "            for word_index in range(char_max_length):\n",
    "                try:\n",
    "                    words_repr.append(char2index[sent[sent_index][word_index]])\n",
    "                except:\n",
    "                    words_repr.append(char2index.get('_pad_'))\n",
    "            sent_repr.append(words_repr)\n",
    "        words_as_chars_indices.append(sent_repr)\n",
    "    return words_as_chars_indices\n",
    "\n",
    "def ngram_prediction_agg_majority_vote(predictions):\n",
    "    positive_sum = np.sum(predictions)\n",
    "    ratio = positive_sum / len(predictions)\n",
    "    return int(ratio + 0.5)\n",
    "\n",
    "def ngram_prediction_agg_max(predictions):\n",
    "    return np.max(predictions)\n",
    "\n",
    "def ngram_prediction_agg_begin(predictions):\n",
    "    return predictions[0]\n",
    "\n",
    "def ngram_prediction_agg_end(predictions):\n",
    "    return predictions[-1]\n",
    "\n",
    "def backward_transformation_v1(dataset, sent_ids, test_words_padded, \n",
    "                    index2word, test_start_end, final_predictions):\n",
    "    '''\n",
    "    Based on the dataset, the list of sentence_ids, the padded words\n",
    "    from the test set, the start and end tuple list of the test set and\n",
    "    the produced v1 predictions, maps the predictions back to original\n",
    "    representation (from number of sentences to number of rows) and\n",
    "    computes the F1-score between predictions and labels of the orignal\n",
    "    representation. For n-gram targets, applies an aggregation strategy.\n",
    "    '''\n",
    "    ngram_prediction_agg = ngram_prediction_agg_max\n",
    "    results = []\n",
    "    num_missing_match = []\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    count=0\n",
    "    sent_count = 0\n",
    "    for ind, sent in enumerate(test_words_padded):\n",
    "        sent_id = sent_ids[ind]\n",
    "        ses = test_start_end[ind]\n",
    "        preds = final_predictions[ind]\n",
    "        selected = dataset.loc[dataset.sent_id==sent_id,]\n",
    "        targets = selected.target.values.tolist()\n",
    "        start_ends = list(zip(selected.start.values.tolist(), selected.end.values.tolist()))\n",
    "        binary_y = selected.binary.values.tolist()\n",
    "        for label_index, (start, end) in enumerate(start_ends):\n",
    "            matching_indices = [i for i, (s, e) in enumerate(ses) if overlaps(start, end, s, e)]\n",
    "            if not matching_indices:\n",
    "                num_missing_match.append((sent_id, (start,end), ses, sent))\n",
    "                prediction = 1\n",
    "            matching_predictions = [preds[i] for i in matching_indices]\n",
    "            if len(matching_predictions)>1:\n",
    "                prediction = ngram_prediction_agg(matching_predictions)\n",
    "            else:\n",
    "                if matching_indices:\n",
    "                    prediction = matching_predictions[0]\n",
    "            matching_labels = binary_y[label_index]\n",
    "            all_labels.append(matching_labels)\n",
    "            all_predictions.append(prediction)\n",
    "    f1 = f1_score(all_labels, all_predictions)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2.2) DL Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2.2.1) Utility Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import keras.callbacks\n",
    "\n",
    "class MetricsV2(keras.callbacks.Callback):\n",
    "    def __init__(self, training_data, show_test_data=True):\n",
    "        self.train_f1_scores = []\n",
    "        self.test_f1_scores = []\n",
    "        self.training_data = training_data\n",
    "        self.show_test_data = show_test_data\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        # Compute the F1-score for the test data\n",
    "        test_label = self.validation_data[1]\n",
    "        test_label = np.array(test_label)\n",
    "        test_label = np.argmax(test_label, axis = 1)\n",
    "        test_predict = np.asarray(self.model.predict(self.validation_data[0]))\n",
    "        test_predict = np.argmax(test_predict, axis = 1)\n",
    "        test_f1 = f1_score(test_label, test_predict)\n",
    "        self.test_f1_scores.append(test_f1)\n",
    "        # Compute the F1-score for the training data\n",
    "        train_label = self.training_data[1]\n",
    "        train_label = np.array(train_label)\n",
    "        train_label = np.argmax(train_label, axis = 1)\n",
    "        train_predict = np.asarray(self.model.predict(self.training_data[0]))\n",
    "        train_predict = np.argmax(train_predict, axis = 1)\n",
    "        train_f1 = f1_score(train_label, train_predict)\n",
    "        self.train_f1_scores.append(train_f1)\n",
    "        print('Training F1-score : {}'.format(train_f1))\n",
    "        print('Testing F1-score : {}'.format(test_f1))\n",
    "        if self.show_test_data:\n",
    "            print('--------------------Targets-------------------------')\n",
    "            print(test_label)\n",
    "            print('--------------------Predictions-------------------------')\n",
    "            print(test_predict)\n",
    "    \n",
    "class MetricsV1(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, training_data, show_test_data=True):\n",
    "        self.train_f1_scores = []\n",
    "        self.test_f1_scores = []\n",
    "        self.training_data = training_data\n",
    "        self.show_test_data = show_test_data\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        # Compute the F1-score for the test data\n",
    "        test_label = self.validation_data[1]\n",
    "        test_label = np.array(test_label)\n",
    "        test_shape = test_label.shape\n",
    "        test_label = test_label.reshape((test_shape[0]*test_shape[1], test_shape[2]))\n",
    "        test_label = np.argmax(test_label, axis = 1)\n",
    "        test_predict = np.asarray(self.model.predict(self.validation_data[0]))\n",
    "        test_predict = test_predict.reshape((test_shape[0]*test_shape[1]), test_shape[2])\n",
    "        test_predict = np.argmax(test_predict, axis = 1)\n",
    "        test_f1 = f1_score(test_label, test_predict)\n",
    "        self.test_f1_scores.append(test_f1)\n",
    "        # Compute the F1-score for the training data\n",
    "        train_label = self.training_data[1]\n",
    "        train_label = np.array(train_label)\n",
    "        train_shape = train_label.shape\n",
    "        train_label = train_label.reshape((train_shape[0]*train_shape[1], train_shape[2]))\n",
    "        train_label = np.argmax(train_label, axis = 1)\n",
    "        train_predict = np.asarray(self.model.predict(self.training_data[0]))\n",
    "        train_predict = train_predict.reshape((train_shape[0]*train_shape[1]), train_shape[2])\n",
    "        train_predict = np.argmax(train_predict, axis = 1)\n",
    "        train_f1 = f1_score(train_label, train_predict)\n",
    "        self.train_f1_scores.append(train_f1)\n",
    "        print('Training F1-score : {}'.format(train_f1))\n",
    "        print('Testing F1-score : {}'.format(test_f1))\n",
    "        if self.show_test_data:\n",
    "            print('--------------------Targets-------------------------')\n",
    "            print(test_label)\n",
    "            print('--------------------Predictions-------------------------')\n",
    "            print(test_predict)\n",
    "            \n",
    "class MetricsV1Char(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, training_data, show_test_data=True):\n",
    "        self.train_f1_scores = []\n",
    "        self.test_f1_scores = []\n",
    "        self.training_data = training_data\n",
    "        self.show_test_data = show_test_data\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        # Compute the F1-score for the test data\n",
    "        test_label = self.validation_data[2]\n",
    "        test_label = np.array(test_label)\n",
    "        test_shape = test_label.shape\n",
    "        test_label = test_label.reshape((test_shape[0]*test_shape[1], test_shape[2]))\n",
    "        test_label = np.argmax(test_label, axis = 1)\n",
    "        test_predict = np.asarray(self.model.predict([self.validation_data[0], self.validation_data[1]]))\n",
    "        print(test_predict.shape)\n",
    "        test_predict = test_predict.reshape((test_shape[0]*test_shape[1]), test_shape[2])\n",
    "        test_predict = np.argmax(test_predict, axis = 1)\n",
    "        test_f1 = f1_score(test_label, test_predict)\n",
    "        self.test_f1_scores.append(test_f1)\n",
    "        # Compute the F1-score for the training data\n",
    "        train_label = self.training_data[1]\n",
    "        train_label = np.array(train_label)\n",
    "        train_shape = train_label.shape\n",
    "        train_label = train_label.reshape((train_shape[0]*train_shape[1], train_shape[2]))\n",
    "        train_label = np.argmax(train_label, axis = 1)\n",
    "        train_predict = np.asarray(self.model.predict(self.training_data[0]))\n",
    "        train_predict = train_predict.reshape((train_shape[0]*train_shape[1]), train_shape[2])\n",
    "        train_predict = np.argmax(train_predict, axis = 1)\n",
    "        train_f1 = f1_score(train_label, train_predict)\n",
    "        self.train_f1_scores.append(train_f1)\n",
    "        print('Training F1-score : {}'.format(train_f1))\n",
    "        print('Testing F1-score : {}'.format(test_f1))\n",
    "        if self.show_test_data:\n",
    "            print('--------------------Targets-------------------------')\n",
    "            print(test_label)\n",
    "            print('--------------------Predictions-------------------------')\n",
    "            print(test_predict)\n",
    "            \n",
    "\n",
    "class MetricsElmoV1(keras.callbacks.Callback):\n",
    "    '''\n",
    "    Make sure to use 'sparse_categorical_crossentropy' as the loss function.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, training_data, batch_size, show_test_data=True):\n",
    "        self.train_f1_scores = []\n",
    "        self.test_f1_scores = []\n",
    "        self.training_data = training_data\n",
    "        self.show_test_data = show_test_data\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        # Compute the F1-score for the test data\n",
    "        test_label = self.validation_data[1]\n",
    "        test_label = np.array(test_label)\n",
    "        test_shape = test_label.shape\n",
    "        print(test_label.shape)\n",
    "        test_label = test_label.reshape((test_shape[0]*test_shape[1]))\n",
    "        test_predict = prediction_elmo_model(self.validation_data[0], self.model, self.batch_size)\n",
    "        print(test_predict.shape)\n",
    "        test_predict = test_predict.reshape((test_shape[0]*test_shape[1]))\n",
    "        test_f1 = f1_score(test_label, test_predict)\n",
    "        self.test_f1_scores.append(test_f1)\n",
    "        # Compute the F1-score for the training data\n",
    "        train_label = self.training_data[1]\n",
    "        train_label = np.array(train_label)\n",
    "        train_shape = train_label.shape\n",
    "        train_label = train_label.reshape((train_shape[0]*train_shape[1]))\n",
    "        train_predict = prediction_elmo_model(self.training_data[0], self.model, self.batch_size)\n",
    "        train_predict = train_predict.reshape((train_shape[0]*train_shape[1]))\n",
    "        train_f1 = f1_score(train_label, train_predict)\n",
    "        self.train_f1_scores.append(train_f1)\n",
    "        print('Training F1-score : {}'.format(train_f1))\n",
    "        print('Testing F1-score : {}'.format(test_f1))\n",
    "        if self.show_test_data:\n",
    "            print('--------------------Targets-------------------------')\n",
    "            print(test_label)\n",
    "            print('--------------------Predictions-------------------------')\n",
    "            print(test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_elmo_model(data, model, batch_size):\n",
    "    '''\n",
    "    Returns the predictions of an ELMo Keras model for the\n",
    "    provided data and batch_size. If required, extends the\n",
    "    data to make it divisible by batch_size in order to\n",
    "    work with the ELMo model and cuts of the extension.\n",
    "    Creates predictions iteratively for batch_size large\n",
    "    fractions of the provided data.\n",
    "    '''\n",
    "    data = np.array(data.copy())\n",
    "    # Ensure that the data has number of samples\n",
    "    # which is divisible by the batch size\n",
    "    add_index = (batch_size - (data.shape[0] % batch_size))\n",
    "    data_ext = np.concatenate((data, data[:add_index]))\n",
    "    # Get the predictions of the elmo model on the \n",
    "    # extended data\n",
    "    predictions = None\n",
    "    print(data_ext.shape)\n",
    "    num_samples = data_ext.shape[0]\n",
    "    for index in range(0, int((num_samples/batch_size))):\n",
    "        print((index*batch_size),((index+1)*batch_size))\n",
    "        test_words_batch = data_ext[(index*batch_size):((index+1)*batch_size)]\n",
    "        prediction = model.predict(test_words_batch)\n",
    "        final_prediction = np.argmax(prediction, axis = -1)\n",
    "        if predictions is None:\n",
    "            predictions = final_prediction\n",
    "        else:\n",
    "            predictions = np.concatenate((predictions, final_prediction))\n",
    "    return predictions[:data.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2.2.2) Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using C:\\Users\\Studio\\AppData\\Local\\Temp\\tfhub_modules to cache modules.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from keras import backend as K\n",
    "\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "\n",
    "elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.tables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model dataset[0] elmo batch=2 epoch=1 f1=0.7632978723404255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7632978723404255"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Input\n",
    "from keras.layers.merge import add, concatenate\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, \\\n",
    "        Bidirectional, Lambda, average, SpatialDropout1D, Reshape, Flatten, Conv2D, MaxPooling2D, Conv1D, GlobalMaxPooling1D\n",
    "from attention_keras import AttentionLayer\n",
    "from keras_contrib.layers import CRF\n",
    "\n",
    "def model_v1_LSTM(sent_max_length, vocab_size, dimension, embedding):\n",
    "    in_seq = Input(shape=(sent_max_length,))\n",
    "    embed = Embedding(input_dim=vocab_size, output_dim=dimension, \\\n",
    "                  weights=[embedding], input_length=sent_max_length)(in_seq)\n",
    "    drop = Dropout(0.1)(embed)\n",
    "    lstm = LSTM(units=150, return_sequences=True, recurrent_dropout=0.1)(drop)\n",
    "    output = TimeDistributed(Dense(2, activation=\"softmax\"))(lstm)\n",
    "    return Model(in_seq, output)\n",
    "\n",
    "def model_v1_BiLSTM(sent_max_length, vocab_size, dimension, embedding):\n",
    "    in_seq = Input(shape=(sent_max_length,))\n",
    "    embed = Embedding(input_dim=vocab_size, output_dim=dimension, \\\n",
    "                  weights=[embedding], input_length=sent_max_length)(in_seq)\n",
    "    drop = Dropout(0.1)(embed)\n",
    "    lstm = Bidirectional(LSTM(units=20, return_sequences=True, recurrent_dropout=0.1))(drop)\n",
    "    output = TimeDistributed(Dense(2, activation=\"softmax\"))(lstm)\n",
    "    return Model(in_seq, output)\n",
    "\n",
    "def model_v1_char_BiLSTM(sent_max_length, char_max_length, word_embedding, char_embedding):\n",
    "    # (1.1) Word input parameters\n",
    "    word_vocab_size = word_embedding.shape[0]\n",
    "    word_dimension = word_embedding.shape[1]\n",
    "    # (1.2) Word embedding layers\n",
    "    word_in = Input(shape=(sent_max_length,))\n",
    "    word_embed = Embedding(input_dim=word_vocab_size, output_dim=word_dimension, \\\n",
    "                      weights=[word_embedding], input_length=sent_max_length)(word_in)\n",
    "    # (2.1) Char input parameters\n",
    "    char_vocab_size = char_embedding.shape[0]\n",
    "    char_dimension = char_embedding.shape[1]\n",
    "    # (2.2) Char embedding layers\n",
    "    char_in = Input(shape=(sent_max_length, char_max_length))\n",
    "    char_embed = TimeDistributed(Embedding(input_dim=char_vocab_size, output_dim=char_dimension, \\\n",
    "                        weights=[char_embedding], input_length=char_max_length, mask_zero=True))(char_in)\n",
    "    char_lstm = TimeDistributed(Bidirectional(LSTM(units=25, return_sequences=False, recurrent_dropout=0.1)))(char_embed)\n",
    "    concatenation = concatenate([word_embed, char_lstm], name='c')\n",
    "    drop1 = SpatialDropout1D(0.3)(concatenation)\n",
    "    lstm = Bidirectional(LSTM(units=40, return_sequences=True, recurrent_dropout=0.1))(drop1)\n",
    "    drop2 = SpatialDropout1D(0.3)(lstm)\n",
    "    out = TimeDistributed(Dense(2, activation=\"softmax\"))(drop2) \n",
    "    return Model([word_in, char_in], out)\n",
    "\n",
    "\n",
    "def model_v1_ElMo_residual_BiLSTM(sent_max_length, batch_size):\n",
    "    def ElmoEmbedding(x):\n",
    "        return elmo_model(inputs={\n",
    "                                \"tokens\": tf.squeeze(tf.cast(x, tf.string)),\n",
    "                                \"sequence_len\": tf.constant(batch_size*[sent_max_length])\n",
    "                          },\n",
    "                          signature=\"tokens\",\n",
    "                          as_dict=True)[\"elmo\"]\n",
    "    in_seq = Input(shape=(sent_max_length,), dtype=tf.string)\n",
    "    embed = Lambda(ElmoEmbedding, output_shape=(sent_max_length, 1024))(in_seq)\n",
    "    lstm_1 = Bidirectional(LSTM(units=512, return_sequences=True,\n",
    "                           recurrent_dropout=0.2, dropout=0.2))(embed)\n",
    "    lstm_2 = Bidirectional(LSTM(units=512, return_sequences=True,\n",
    "                               recurrent_dropout=0.2, dropout=0.2))(lstm_1)\n",
    "    lstm_1 = add([lstm_1, lstm_2])\n",
    "    out = TimeDistributed(Dense(2, activation=\"softmax\"))(lstm_1)\n",
    "    return Model(in_seq, out)\n",
    "\n",
    "def model_v1_BiLSTM_CRF(sent_max_length, vocab_size, dimension, embedding):\n",
    "    in_seq = Input(shape=(sent_max_length,))\n",
    "    embed = Embedding(input_dim=vocab_size, output_dim=dimension, \\\n",
    "                  weights=[embedding], input_length=sent_max_length)(in_seq)\n",
    "    drop = Dropout(0.1)(embed)\n",
    "    lstm = Bidirectional(LSTM(units=20, return_sequences=True, recurrent_dropout=0.1))(drop)\n",
    "    dense = Dense(30, activation=\"relu\")(lstm)\n",
    "    crf = CRF(2, learn_mode='marginal')\n",
    "    output = crf(dense)\n",
    "    return Model(in_seq, output)\n",
    "\n",
    "def model_v2_LSTM(sent_max_length, vocab_size, dimension, embedding):\n",
    "    in_seq = Input(shape=(sent_max_length,))\n",
    "    embed = Embedding(input_dim=vocab_size, output_dim=dimension, \\\n",
    "                  weights=[embedding], input_length=sent_max_length)(in_seq)\n",
    "    drop = Dropout(0.1)(embed)\n",
    "    lstm = LSTM(units=150, return_sequences=False, recurrent_dropout=0.1)(drop)\n",
    "    output = Dense(2, activation=\"softmax\")(lstm)\n",
    "    return Model(in_seq, output)\n",
    "\n",
    "def model_v2_BiLSTM(sent_max_length, vocab_size, dimension, embedding):\n",
    "    in_seq = Input(shape=(sent_max_length,))\n",
    "    embed = Embedding(input_dim=vocab_size, output_dim=dimension, \\\n",
    "                  weights=[embedding], input_length=sent_max_length)(in_seq)\n",
    "    drop = Dropout(0.1)(embed)\n",
    "    lstm = Bidirectional(LSTM(units=20, return_sequences=False, recurrent_dropout=0.1))(drop)\n",
    "    output = Dense(2, activation=\"softmax\")(lstm)\n",
    "    return Model(in_seq, output)\n",
    "\n",
    "def model_v2_attention_BiLSTM(sent_max_length, vocab_size, dimension, embedding):\n",
    "    in_seq = Input(shape=(sent_max_length,))\n",
    "    embed = Embedding(input_dim=vocab_size, output_dim=dimension, \\\n",
    "                  weights=[embedding], input_length=sent_max_length)(in_seq)\n",
    "    drop = Dropout(0.1)(embed)\n",
    "    lstm = Bidirectional(LSTM(units=20, return_sequences=True, recurrent_dropout=0.1))(drop)\n",
    "    attended = AttentionLayer(name='attention')(lstm)\n",
    "    output = Dense(2, activation=\"softmax\")(attended)\n",
    "    return Model(in_seq, output)\n",
    "\n",
    "def model_v2_simple_CNN(sent_max_length, vocab_size, dimension, embedding):\n",
    "    in_seq = Input(shape=(sent_max_length,))\n",
    "    embed = Embedding(input_dim=vocab_size, output_dim=dimension, \\\n",
    "                  weights=[embedding], input_length=sent_max_length)(in_seq)\n",
    "    embed_reshaped = Reshape((sent_max_length, dimension, 1))(embed)\n",
    "    conv2d = Conv2D(100, (5, dimension), activation='relu')(embed_reshaped)\n",
    "    max2d = MaxPooling2D((sent_max_length - 5 + 1, 1))(conv2d)\n",
    "    flatten = Flatten()(max2d)\n",
    "    drop = Dropout(0.5)(flatten)\n",
    "    out = Dense(2, activation='softmax')(drop)\n",
    "    return Model(in_seq, out)\n",
    "\n",
    "def model_v2_adv_CNN(sent_max_length, vocab_size, dimension, embedding):\n",
    "    in_seq = Input(shape=(sent_max_length,))\n",
    "    embed = Embedding(input_dim=vocab_size, output_dim=dimension, \\\n",
    "                      weights=[embedding], input_length=sent_max_length)(in_seq)\n",
    "    embed_reshaped = Reshape((sent_max_length, dimension, 1))(embed)\n",
    "    # First convolution and pooling layer\n",
    "    conv_1 = Conv1D(100, 5, activation='relu')(embed)\n",
    "    pool_1 = GlobalMaxPooling1D()(conv_1)\n",
    "    #pool_1 = MaxPooling2D((sent_max_length - 5 + 1, 1))(conv_1)\n",
    "    # Second convolution and pooling layer\n",
    "    conv_2 = Conv1D(100, 4, activation='relu')(embed)\n",
    "    pool_2 = GlobalMaxPooling1D()(conv_2)\n",
    "    #pool_2 = MaxPooling2D((sent_max_length - 4 + 1, 1))(conv_2)\n",
    "    # Third convolution and pooling layer\n",
    "    conv_3 = Conv1D(100, 3, activation='relu')(embed)\n",
    "    pool_3 = GlobalMaxPooling1D()(conv_3)\n",
    "    #pool_3 = MaxPooling2D((sent_max_length - 3 + 1, 1))(conv_3)\n",
    "    # Concatenate all three layers\n",
    "    concat = concatenate([pool_1, pool_2, pool_3])\n",
    "    # Flatten concatenation\n",
    "    #flatten = Flatten()(concat)\n",
    "    drop = Dropout(0.5)(concat)\n",
    "    out = Dense(2, activation='softmax')(drop)\n",
    "    return Model(in_seq, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2.2.3) Models preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import model_to_dot\n",
    "from IPython.display import SVG\n",
    "def plot_keras_model(model, show_shapes=True, show_layer_names=True):\n",
    "    return SVG(model_to_dot(model, show_shapes=show_shapes, show_layer_names=show_layer_names).create(prog='dot',format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "def preparation_v1(dataset, embedding, model_func):\n",
    "    '''\n",
    "    Provides wrapping of preprocessing and postprocessing\n",
    "    for the first variant of sequence classification v1 \n",
    "    (where complexity of each word is predicted individually)\n",
    "    based on the provided dataset, embedding and model_func.\n",
    "    The model_func is a function that should accept values\n",
    "    specifying the embedding layer (max_sentence_length, \n",
    "    vocab_dimension, embedding_weights) and return the \n",
    "    constructed Keras model instance. Returns the final test \n",
    "    F1-score, F1-scores of training and F1-scores\n",
    "    of test sets for each epoch.\n",
    "    '''\n",
    "    # Merge the train and test set in the dataset\n",
    "    dataframe_merged, train_num_rows, test_num_rows, \\\n",
    "                dataset_num_rows, train_num_sents, test_num_sents, \\\n",
    "                dataset_num_sents = merge_train_test_dataset(dataset)\n",
    "    # Forward transform the data represented in a dataframe into\n",
    "    # a sequence representation for classifying each word individually\n",
    "    sentence_ids, sentences, start_ends, \\\n",
    "        binaries, probabilties = forward_transformation_v1(dataframe_merged)\n",
    "    # Build the vocbulary and embedding\n",
    "    word2index, index2word, word_embedding = build_vocabulary(sentences, \\\n",
    "                                embedding, embedding.vector_size, missing='unique')\n",
    "    # Pad the sentences and labels\n",
    "    sent_max_length = sentence_max_length(sentences)\n",
    "    words_padded = pad_encode_sequences(sentences, word2index, '', sent_max_length)\n",
    "    binaries_padded, probabilities_padded = pad_binaries_probs(binaries, probabilties, sent_max_length)\n",
    "    binaries_padded_categorical = [to_categorical(clazz, num_classes=2) for clazz in binaries_padded]\n",
    "    # Instantiate the model\n",
    "    model = model_func(sent_max_length, len(word2index), embedding.vector_size, word_embedding)\n",
    "    model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "    return model\n",
    "    # Split merged data into train and test again\n",
    "    train_words_padded, test_words_padded = split_train_test_dataset(words_padded, train_num_sents)\n",
    "    train_binaries_padded_cat, test_binaries_padded_cat = split_train_test_dataset(\\\n",
    "                    binaries_padded_categorical, train_num_sents)\n",
    "    print(len(train_words_padded))\n",
    "    print(len(test_words_padded))\n",
    "    # Fit the model with a custom metric to obtain the F1-scores\n",
    "    metrics_v1 = MetricsV1((train_words_padded, np.array(train_binaries_padded_cat)))\n",
    "    history = model.fit(train_words_padded, np.array(train_binaries_padded_cat),\n",
    "                    batch_size=1, epochs=10, validation_data = (test_words_padded, \n",
    "                    np.array(test_binaries_padded_cat)), verbose=1, callbacks=[metrics_v1])\n",
    "    # Use trained model to predict the test set\n",
    "    predictions = model.predict(test_words_padded)\n",
    "    final_predictions = np.argmax(predictions, axis = 2)\n",
    "    train_ses, test_ses = split_train_test_dataset(start_ends, train_num_sents)\n",
    "    train_sent_ids, test_sent_ids = split_train_test_dataset(sentence_ids, train_num_sents)\n",
    "    # Transform the sequence prediction back into the format which we\n",
    "    # had in the dataframe and compute the F1-score on that representation\n",
    "    f1 = backward_transformation_v1(dataframe_merged, test_sent_ids, \\\n",
    "                    test_words_padded, index2word, test_ses, final_predictions)\n",
    "    return f1, metrics_v1.train_f1_scores, metrics_v1.test_f1_scores\n",
    "\n",
    "\n",
    "def preparation_char_v1(dataset, w_embedding, c_embedding):\n",
    "    '''\n",
    "    Provides wrapping of preprocessing and postprocessing\n",
    "    for the first variant of sequence classification v1 \n",
    "    (where complexity of each word is predicted individually)\n",
    "    based on the provided dataset, embedding and model_func.\n",
    "    The model_func is a function that should accept values\n",
    "    specifying the embedding layer (max_sentence_length, \n",
    "    vocab_dimension, embedding_weights) and return the \n",
    "    constructed Keras model instance. Returns the final test \n",
    "    F1-score, F1-scores of training and F1-scores\n",
    "    of test sets for each epoch.\n",
    "    '''\n",
    "    # Merge the train and test set in the dataset\n",
    "    dataframe_merged, train_num_rows, test_num_rows, \\\n",
    "                dataset_num_rows, train_num_sents, test_num_sents, \\\n",
    "                dataset_num_sents = merge_train_test_dataset(dataset)\n",
    "    # Forward transform the data represented in a dataframe into\n",
    "    # a sequence representation for classifying each word individually\n",
    "    sentence_ids, sentences, start_ends, \\\n",
    "        binaries, probabilties = forward_transformation_v1(dataframe_merged, lowercase=False)\n",
    "    # Build the vocbulary and embedding\n",
    "    word2index, index2word, word_embedding = build_vocabulary(sentences, \\\n",
    "                                w_embedding, w_embedding.vector_size, missing='unique')\n",
    "    char_dimension = len(list(c_embedding.values())[0])\n",
    "    char2index, index2char, char_embedding = build_char_vocabulary(sentences, \\\n",
    "                                c_embedding, char_dimension, missing='unique')\n",
    "    # Pad the sentences and labels\n",
    "    sent_max_length = sentence_max_length(sentences)\n",
    "    words_padded = pad_encode_sequences(sentences, word2index, '', sent_max_length)\n",
    "    binaries_padded, probabilities_padded = pad_binaries_probs(binaries, probabilties, sent_max_length)\n",
    "    binaries_padded_categorical = [to_categorical(clazz, num_classes=2) for clazz in binaries_padded]\n",
    "    # Pad char sequences\n",
    "    char_max_length = 11\n",
    "    chars_padded = pad_chars(sentences, char2index, sent_max_length, char_max_length)\n",
    "    # Instantiate the model\n",
    "    model = model_v1_char_BiLSTM(sent_max_length, char_max_length, word_embedding, char_embedding)\n",
    "    adam = optimizers.Adam(clipnorm=1.)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "    return model\n",
    "    # Split merged data into train and test again\n",
    "    train_words_padded, test_words_padded = split_train_test_dataset(words_padded, train_num_sents)\n",
    "    train_chars_padded, test_chars_padded = split_train_test_dataset(chars_padded, train_num_sents)\n",
    "    train_chars_reshaped = np.array(train_chars_padded).reshape(len(train_chars_padded), \\\n",
    "                                    sent_max_length, char_max_length)\n",
    "    test_chars_reshaped = np.array(test_chars_padded).reshape(len(test_chars_padded), \\\n",
    "                                    sent_max_length, char_max_length)\n",
    "    train_binaries_padded_cat, test_binaries_padded_cat = split_train_test_dataset(\\\n",
    "                    binaries_padded_categorical, train_num_sents)\n",
    "    print(len(train_words_padded))\n",
    "    print(len(test_words_padded))\n",
    "    # Fit the model with a custom metric to obtain the F1-scores\n",
    "    metrics_v1 = MetricsV1Char(([train_words_padded, train_chars_reshaped], np.array(train_binaries_padded_cat)))\n",
    "    history = model.fit([train_words_padded, train_chars_reshaped], np.array(train_binaries_padded_cat),\n",
    "                    batch_size=1, epochs=3, validation_data = ([test_words_padded, test_chars_reshaped], \n",
    "                    np.array(test_binaries_padded_cat)), verbose=1, callbacks=[metrics_v1])\n",
    "    # Use trained model to predict the test set\n",
    "    predictions = model.predict([test_words_padded, test_chars_reshaped])\n",
    "    final_predictions = np.argmax(predictions, axis = -1)\n",
    "    train_ses, test_ses = split_train_test_dataset(start_ends, train_num_sents)\n",
    "    train_sent_ids, test_sent_ids = split_train_test_dataset(sentence_ids, train_num_sents)\n",
    "    # Transform the sequence prediction back into the format which we\n",
    "    # had in the dataframe and compute the F1-score on that representation\n",
    "    f1 = backward_transformation_v1(dataframe_merged, test_sent_ids, \\\n",
    "                     test_words_padded, index2word, test_ses, final_predictions)\n",
    "    return f1, metrics_v1.train_f1_scores, metrics_v1.test_f1_scores, final_predictions\n",
    "\n",
    "\n",
    "def preparation_elmo_v1(dataset, embedding):\n",
    "    # Merge the train and test set in the dataset\n",
    "    dataframe_merged, train_num_rows, test_num_rows, \\\n",
    "                dataset_num_rows, train_num_sents, test_num_sents, \\\n",
    "                dataset_num_sents = merge_train_test_dataset(dataset)\n",
    "    # Forward transform the data represented in a dataframe into\n",
    "    # a sequence representation for classifying each word individually\n",
    "    sentence_ids, sentences, start_ends, \\\n",
    "            binaries, probabilties = forward_transformation_v1(dataframe_merged)\n",
    "    # Build the vocbulary and embedding\n",
    "    word2index, index2word, word_embedding = build_vocabulary(sentences, \\\n",
    "                                    embedding, embedding.vector_size, missing='unique')\n",
    "    # Pad the sentences (for elmo we need the actual words, so we\n",
    "    # first map it to integers, pad the integer sequence and map \n",
    "    #it back to string) and labels\n",
    "    sent_max_length = sentence_max_length(sentences)\n",
    "    words_padded = pad_encode_sequences(sentences, word2index, '', sent_max_length)\n",
    "    words_padded = [[index2word[index] for index in sentence] for sentence in words_padded]\n",
    "    binaries_padded, probabilities_padded = pad_binaries_probs(binaries, probabilties, sent_max_length)\n",
    "    # Split merged data into train and test again\n",
    "    train_words_padded, test_words_padded = split_train_test_dataset(words_padded, train_num_sents)\n",
    "    # We use sparse_categorical_crossentropy so we not require the\n",
    "    # binaries mapped to categorical representation, but we reshape them\n",
    "    train_binaries_padded, test_binaries_padded = split_train_test_dataset(binaries_padded, train_num_sents)\n",
    "    train_binaries_padded = train_binaries_padded.reshape((train_binaries_padded.shape[0], \\\n",
    "                                                              train_binaries_padded.shape[1], 1))\n",
    "    test_binaries_padded = test_binaries_padded.reshape((test_binaries_padded.shape[0], \\\n",
    "                                                              test_binaries_padded.shape[1], 1))\n",
    "    # Make sure the training sample size is divisible by the batch size\n",
    "    batch_size = 2\n",
    "    training_cutoff = 386\n",
    "    train_words_padded = train_words_padded[:training_cutoff]\n",
    "    train_binaries_padded = train_binaries_padded[:training_cutoff]\n",
    "    # Make sure the test sample size is divisible by the batch size\n",
    "    test_cutoff = 52\n",
    "    test_words_padded = test_words_padded[:test_cutoff]\n",
    "    test_binaries_padded = test_binaries_padded[:test_cutoff]\n",
    "    print(np.array(train_binaries_padded).shape)\n",
    "    print(np.array(test_binaries_padded).shape)\n",
    "    # Fit the model with a custom metric to obtain the F1-scores\n",
    "    model = model_v1_ElMo_residual_BiLSTM(sent_max_length, batch_size)\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "    # Use custom ELMo metric to evaluate F1-score after each epoch\n",
    "    metrics_elmo_v1 = MetricsElmoV1((np.array(train_words_padded), \\\n",
    "                                     np.array(train_binaries_padded)), batch_size)\n",
    "    history = model.fit(np.array(train_words_padded), np.array(train_binaries_padded),\n",
    "                         batch_size=batch_size, epochs=2, validation_data = (np.array(test_words_padded), \n",
    "                        np.array(test_binaries_padded)), verbose=1, callbacks=[metrics_elmo_v1])\n",
    "    # Use trained model to predict the test set\n",
    "    train_words_padded, test_words_padded = split_train_test_dataset(words_padded, train_num_sents)\n",
    "    train_binaries_padded, test_binaries_padded = split_train_test_dataset(binaries_padded, train_num_sents)\n",
    "    prediction = prediction_elmo_model(test_words_padded, model, batch_size)\n",
    "    test_words_padded = test_words_padded[:53]\n",
    "    preds = prediction[:53]\n",
    "    # Transform the sequence prediction back into the format which we\n",
    "    # had in the dataframe and compute the F1-score on that representation\n",
    "    train_ses, test_ses = split_train_test_dataset(start_ends, train_num_sents)\n",
    "    train_sent_ids, test_sent_ids = split_train_test_dataset(sentence_ids, train_num_sents)\n",
    "    f1 = backward_transformation_v1(dataframe_merged, test_sent_ids, \\\n",
    "                    test_words_padded, index2word, test_ses, preds)\n",
    "    return f1, metrics_elmo_v1.train_f1_scores, metrics_elmo_v1.test_f1_scores\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "def preparation_v2(dataset, embedding, model_func):\n",
    "    '''\n",
    "    Provides wrapping of preprocessing and postprocessing\n",
    "    for the second variant of sequence classification v1 \n",
    "    (where a target is marked in the sentence and the overall\n",
    "    sentence is classified) based on the provided dataset,\n",
    "    embedding and model_func.The model_func is a function \n",
    "    that should accept values specifying the embedding layer\n",
    "    (max_sentenc_length, vocab_dimension, embedding_weights) \n",
    "    and return the constructed Keras model instance. Returns\n",
    "    the final test F1-score, F1-scores of training and F1-scores\n",
    "    of test sets for each epoch.\n",
    "    '''\n",
    "    # Merge the train and test set in the dataset\n",
    "    dataframe_merged, train_num_rows, test_num_rows, \\\n",
    "                dataset_num_rows, train_num_sents, test_num_sents, \\\n",
    "                dataset_num_sents = merge_train_test_dataset(dataset)\n",
    "    # Forward transform the data represented in a dataframe into\n",
    "    # a sequence representation where each target is marked in its sentence\n",
    "    sentences, binaries, probabilties = forward_transformation_v2(dataframe_merged)\n",
    "    # Build the vocbulary and embedding\n",
    "    word2index, index2word, word_embedding = build_vocabulary(sentences, \\\n",
    "                            embedding, embedding.vector_size, missing='unique')\n",
    "    # Pad the sentences and labels\n",
    "    sent_max_length = sentence_max_length(sentences)\n",
    "    words_padded = pad_encode_sequences(sentences, word2index, '', sent_max_length)\n",
    "    binaries_categorical = [to_categorical(clazz, num_classes=2) for clazz in binaries]\n",
    "    # Instantiate the model\n",
    "    model = model_func(sent_max_length, len(word2index), embedding.vector_size, word_embedding)\n",
    "    model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "    # Split merged data into train and test again\n",
    "    train_words_padded, test_words_padded = split_train_test_dataset(words_padded, train_num_rows)\n",
    "    train_binaries_categorical, test_binaries_categorical = split_train_test_dataset(binaries_categorical, train_num_rows)\n",
    "    # Fit the model with a custom metric to obtain the F1-scores\n",
    "    metrics_v2 = MetricsV2((train_words_padded, np.array(train_binaries_categorical)))\n",
    "    history = model.fit(train_words_padded, np.array(train_binaries_categorical),\n",
    "                    batch_size=1, epochs=10, validation_data = (test_words_padded, \n",
    "                    np.array(test_binaries_categorical)), verbose=1, callbacks=[metrics_v2])\n",
    "    print(len(train_words_padded))\n",
    "    print(len(test_words_padded))\n",
    "    # Use trained model to predict the test set\n",
    "    predictions = model.predict(test_words_padded)\n",
    "    final_predictions = np.argmax(predictions, axis = 1)\n",
    "    targets = np.argmax(test_binaries_categorical, axis = 1)\n",
    "    f1 = f1_score(targets, final_predictions)\n",
    "    return f1, metrics_v2.train_f1_scores, metrics_v2.test_f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "# Rows train : 5551\n",
      "# Rows test : 694\n",
      "# Rows dataset : 6245\n",
      "----------------------\n",
      "# Sents train : 387\n",
      "# Sents test : 53\n",
      "# Sents dataset : 440\n",
      "----------------------\n",
      "# Words : 186472\n",
      "# Vocab : 3641\n",
      "# Words missing embedding : 162\n",
      "Embedding shape : (3641, 200)\n",
      "Max length sentence : 103\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           (None, 103)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_15 (Embedding)        (None, 103, 200)     728200      input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 99, 100)      100100      embedding_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 100, 100)     80100       embedding_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 101, 100)     60100       embedding_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 100)          0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 100)          0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 100)          0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 300)          0           global_max_pooling1d_5[0][0]     \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 300)          0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 2)            602         dropout_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 969,102\n",
      "Trainable params: 969,102\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Training set length : 5551\n",
      "Test set length : 694\n",
      "Training set length : 5551\n",
      "Test set length : 694\n",
      "Train on 5551 samples, validate on 694 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-8e41701fc8e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mf1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_f1s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_f1s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreparation_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m                     \u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_v2_adv_CNN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-d3f24f7f0ccd>\u001b[0m in \u001b[0;36mpreparation_v2\u001b[1;34m(dataset, embedding, model_func)\u001b[0m\n\u001b[0;32m    203\u001b[0m     history = model.fit(train_words_padded, np.array(train_binaries_categorical),\n\u001b[0;32m    204\u001b[0m                     batch_size=1, epochs=10, validation_data = (test_words_padded, \n\u001b[1;32m--> 205\u001b[1;33m                     np.array(test_binaries_categorical)), verbose=1, callbacks=[metrics_v2])\n\u001b[0m\u001b[0;32m    206\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_words_padded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_words_padded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2664\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2666\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2633\u001b[0m                                 \u001b[0mfeed_symbols\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2635\u001b[1;33m                                 session)\n\u001b[0m\u001b[0;32m   2636\u001b[0m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2637\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_make_callable\u001b[1;34m(self, feed_arrays, feed_symbols, symbol_vals, session)\u001b[0m\n\u001b[0;32m   2585\u001b[0m         \u001b[0mcallable_opts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2586\u001b[0m         \u001b[1;31m# Create callable.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2587\u001b[1;33m         \u001b[0mcallable_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_callable_from_options\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallable_opts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2588\u001b[0m         \u001b[1;31m# Cache parameters corresponding to the generated callable, so that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2589\u001b[0m         \u001b[1;31m# we can detect future mismatches and refresh the callable.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_make_callable_from_options\u001b[1;34m(self, callable_options)\u001b[0m\n\u001b[0;32m   1412\u001b[0m     \"\"\"\n\u001b[0;32m   1413\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1414\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mBaseSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallable_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, session, callable_options)\u001b[0m\n\u001b[0;32m   1366\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m           self._handle = tf_session.TF_SessionMakeCallable(\n\u001b[1;32m-> 1368\u001b[1;33m               session._session, options_ptr, status)\n\u001b[0m\u001b[0;32m   1369\u001b[0m       \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1370\u001b[0m         \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_DeleteBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptions_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "f1, train_f1s, test_f1s, prediction = preparation_v2(datasets[0], \\\n",
    "                    models[2].model, model_v2_adv_CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "# Rows train : 5551\n",
      "# Rows test : 694\n",
      "# Rows dataset : 6245\n",
      "----------------------\n",
      "# Sents train : 387\n",
      "# Sents test : 53\n",
      "# Sents dataset : 440\n",
      "----------------------\n",
      "# Words : 10722\n",
      "# Vocab : 3639\n",
      "# Words missing embedding : 160\n",
      "Embedding shape : (3639, 200)\n",
      "Max length sentence : 101\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-50829ecadb6f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreparation_v1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m                     \u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_v1_char_BiLSTM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-30-abfbb15ac1ef>\u001b[0m in \u001b[0;36mpreparation_v1\u001b[1;34m(dataset, embedding, model_func)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mbinaries_padded_categorical\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclazz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mclazz\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbinaries_padded\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m# Instantiate the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent_max_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword2index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_embedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"adam\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-71a542368584>\u001b[0m in \u001b[0;36mmodel_v1_char_BiLSTM\u001b[1;34m(sent_max_length, char_max_length, word_embedding, char_embedding)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmodel_v1_char_BiLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent_max_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar_max_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_embedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar_embedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# (1.1) Word input parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mword_vocab_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_embedding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0mword_dimension\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_embedding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m# (1.2) Word embedding layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "model = preparation_v1(datasets[0], \\\n",
    "                    models[2].model, model_v1_char_BiLSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"470pt\" viewBox=\"0.00 0.00 444.00 470.00\" width=\"444pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 466)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-466 440,-466 440,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 164335700792 -->\n",
       "<g class=\"node\" id=\"node1\"><title>164335700792</title>\n",
       "<polygon fill=\"none\" points=\"85.5,-415.5 85.5,-461.5 350.5,-461.5 350.5,-415.5 85.5,-415.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"148.5\" y=\"-434.8\">input_4: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"211.5,-415.5 211.5,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"239.5\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"211.5,-438.5 267.5,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"239.5\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"267.5,-415.5 267.5,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"309\" y=\"-446.3\">(None, 101)</text>\n",
       "<polyline fill=\"none\" points=\"267.5,-438.5 350.5,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"309\" y=\"-423.3\">(None, 101)</text>\n",
       "</g>\n",
       "<!-- 164339727776 -->\n",
       "<g class=\"node\" id=\"node2\"><title>164339727776</title>\n",
       "<polygon fill=\"none\" points=\"53,-332.5 53,-378.5 383,-378.5 383,-332.5 53,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"134.5\" y=\"-351.8\">embedding_4: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"216,-332.5 216,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"244\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"216,-355.5 272,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"244\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"272,-332.5 272,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"327.5\" y=\"-363.3\">(None, 101)</text>\n",
       "<polyline fill=\"none\" points=\"272,-355.5 383,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"327.5\" y=\"-340.3\">(None, 101, 200)</text>\n",
       "</g>\n",
       "<!-- 164335700792&#45;&gt;164339727776 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>164335700792-&gt;164339727776</title>\n",
       "<path d=\"M218,-415.366C218,-407.152 218,-397.658 218,-388.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"221.5,-388.607 218,-378.607 214.5,-388.607 221.5,-388.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 164417983376 -->\n",
       "<g class=\"node\" id=\"node3\"><title>164417983376</title>\n",
       "<polygon fill=\"none\" points=\"69.5,-249.5 69.5,-295.5 366.5,-295.5 366.5,-249.5 69.5,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"134.5\" y=\"-268.8\">dropout_4: Dropout</text>\n",
       "<polyline fill=\"none\" points=\"199.5,-249.5 199.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"227.5\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"199.5,-272.5 255.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"227.5\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"255.5,-249.5 255.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"311\" y=\"-280.3\">(None, 101, 200)</text>\n",
       "<polyline fill=\"none\" points=\"255.5,-272.5 366.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"311\" y=\"-257.3\">(None, 101, 200)</text>\n",
       "</g>\n",
       "<!-- 164339727776&#45;&gt;164417983376 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>164339727776-&gt;164417983376</title>\n",
       "<path d=\"M218,-332.366C218,-324.152 218,-314.658 218,-305.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"221.5,-305.607 218,-295.607 214.5,-305.607 221.5,-305.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 164417985560 -->\n",
       "<g class=\"node\" id=\"node4\"><title>164417985560</title>\n",
       "<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 436,-212.5 436,-166.5 0,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"134.5\" y=\"-185.8\">bidirectional_4(lstm_4): Bidirectional(LSTM)</text>\n",
       "<polyline fill=\"none\" points=\"269,-166.5 269,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"297\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"269,-189.5 325,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"297\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"325,-166.5 325,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"380.5\" y=\"-197.3\">(None, 101, 200)</text>\n",
       "<polyline fill=\"none\" points=\"325,-189.5 436,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"380.5\" y=\"-174.3\">(None, 101, 40)</text>\n",
       "</g>\n",
       "<!-- 164417983376&#45;&gt;164417985560 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>164417983376-&gt;164417985560</title>\n",
       "<path d=\"M218,-249.366C218,-241.152 218,-231.658 218,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"221.5,-222.607 218,-212.607 214.5,-222.607 221.5,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 164340471232 -->\n",
       "<g class=\"node\" id=\"node5\"><title>164340471232</title>\n",
       "<polygon fill=\"none\" points=\"86,-83.5 86,-129.5 350,-129.5 350,-83.5 86,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"138\" y=\"-102.8\">dense_4: Dense</text>\n",
       "<polyline fill=\"none\" points=\"190,-83.5 190,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"190,-106.5 246,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"218\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"246,-83.5 246,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"298\" y=\"-114.3\">(None, 101, 40)</text>\n",
       "<polyline fill=\"none\" points=\"246,-106.5 350,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"298\" y=\"-91.3\">(None, 101, 30)</text>\n",
       "</g>\n",
       "<!-- 164417985560&#45;&gt;164340471232 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>164417985560-&gt;164340471232</title>\n",
       "<path d=\"M218,-166.366C218,-158.152 218,-148.658 218,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"221.5,-139.607 218,-129.607 214.5,-139.607 221.5,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 164432122432 -->\n",
       "<g class=\"node\" id=\"node6\"><title>164432122432</title>\n",
       "<polygon fill=\"none\" points=\"97.5,-0.5 97.5,-46.5 338.5,-46.5 338.5,-0.5 97.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"138\" y=\"-19.8\">crf_4: CRF</text>\n",
       "<polyline fill=\"none\" points=\"178.5,-0.5 178.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"206.5\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"178.5,-23.5 234.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"206.5\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"234.5,-0.5 234.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"286.5\" y=\"-31.3\">(None, 101, 30)</text>\n",
       "<polyline fill=\"none\" points=\"234.5,-23.5 338.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"286.5\" y=\"-8.3\">(None, 101, 2)</text>\n",
       "</g>\n",
       "<!-- 164340471232&#45;&gt;164432122432 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>164340471232-&gt;164432122432</title>\n",
       "<path d=\"M218,-83.3664C218,-75.1516 218,-65.6579 218,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"221.5,-56.6068 218,-46.6068 214.5,-56.6069 221.5,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_keras_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "# Rows train : 5551\n",
      "# Rows test : 694\n",
      "# Rows dataset : 6245\n",
      "----------------------\n",
      "# Sents train : 387\n",
      "# Sents test : 53\n",
      "# Sents dataset : 440\n",
      "----------------------\n",
      "# Words : 10722\n",
      "# Vocab : 3896\n",
      "# Words missing embedding : 1298\n",
      "Embedding shape : (3896, 50)\n",
      "# Chars : 52647\n",
      "# Vocab (chars) : 118\n",
      "# Chars missing embedding : 72\n",
      "Embedding shape : (118, 50)\n",
      "Max length sentence : 101\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 101, 11)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 101)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 101, 11, 50)  5900        input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 101, 50)      194800      input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, 101, 50)      15200       time_distributed_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "c (Concatenate)                 (None, 101, 100)     0           embedding_7[0][0]                \n",
      "                                                                 time_distributed_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_3 (SpatialDro (None, 101, 100)     0           c[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 101, 80)      45120       spatial_dropout1d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_4 (SpatialDro (None, 101, 80)      0           bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, 101, 2)       162         spatial_dropout1d_4[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 261,182\n",
      "Trainable params: 261,182\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = preparation_char_v1(datasets[0], \\\n",
    "                    models[0].model, compute_character_embeddings(models[0].model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, to_file='model_plot.pdf', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"636pt\" viewBox=\"0.00 0.00 918.00 636.00\" width=\"918pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 632)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-632 914,-632 914,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 164181472928 -->\n",
       "<g class=\"node\" id=\"node1\"><title>164181472928</title>\n",
       "<polygon fill=\"none\" points=\"141.5,-581.5 141.5,-627.5 427.5,-627.5 427.5,-581.5 141.5,-581.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"204.5\" y=\"-600.8\">input_8: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"267.5,-581.5 267.5,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"295.5\" y=\"-612.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"267.5,-604.5 323.5,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"295.5\" y=\"-589.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"323.5,-581.5 323.5,-627.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"375.5\" y=\"-612.3\">(None, 101, 11)</text>\n",
       "<polyline fill=\"none\" points=\"323.5,-604.5 427.5,-604.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"375.5\" y=\"-589.3\">(None, 101, 11)</text>\n",
       "</g>\n",
       "<!-- 164339008176 -->\n",
       "<g class=\"node\" id=\"node3\"><title>164339008176</title>\n",
       "<polygon fill=\"none\" points=\"6.5,-498.5 6.5,-544.5 562.5,-544.5 562.5,-498.5 6.5,-498.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"194\" y=\"-517.8\">time_distributed_4(embedding_8): TimeDistributed(Embedding)</text>\n",
       "<polyline fill=\"none\" points=\"381.5,-498.5 381.5,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"409.5\" y=\"-529.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"381.5,-521.5 437.5,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"409.5\" y=\"-506.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"437.5,-498.5 437.5,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"500\" y=\"-529.3\">(None, 101, 11)</text>\n",
       "<polyline fill=\"none\" points=\"437.5,-521.5 562.5,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"500\" y=\"-506.3\">(None, 101, 11, 50)</text>\n",
       "</g>\n",
       "<!-- 164181472928&#45;&gt;164339008176 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>164181472928-&gt;164339008176</title>\n",
       "<path d=\"M284.5,-581.366C284.5,-573.152 284.5,-563.658 284.5,-554.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"288,-554.607 284.5,-544.607 281,-554.607 288,-554.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 164339247760 -->\n",
       "<g class=\"node\" id=\"node2\"><title>164339247760</title>\n",
       "<polygon fill=\"none\" points=\"616,-498.5 616,-544.5 881,-544.5 881,-498.5 616,-498.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"679\" y=\"-517.8\">input_7: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"742,-498.5 742,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"770\" y=\"-529.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"742,-521.5 798,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"770\" y=\"-506.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"798,-498.5 798,-544.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"839.5\" y=\"-529.3\">(None, 101)</text>\n",
       "<polyline fill=\"none\" points=\"798,-521.5 881,-521.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"839.5\" y=\"-506.3\">(None, 101)</text>\n",
       "</g>\n",
       "<!-- 164339248880 -->\n",
       "<g class=\"node\" id=\"node4\"><title>164339248880</title>\n",
       "<polygon fill=\"none\" points=\"587,-415.5 587,-461.5 910,-461.5 910,-415.5 587,-415.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"668.5\" y=\"-434.8\">embedding_7: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"750,-415.5 750,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"778\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"750,-438.5 806,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"778\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"806,-415.5 806,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"858\" y=\"-446.3\">(None, 101)</text>\n",
       "<polyline fill=\"none\" points=\"806,-438.5 910,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"858\" y=\"-423.3\">(None, 101, 50)</text>\n",
       "</g>\n",
       "<!-- 164339247760&#45;&gt;164339248880 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>164339247760-&gt;164339248880</title>\n",
       "<path d=\"M748.5,-498.366C748.5,-490.152 748.5,-480.658 748.5,-471.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"752,-471.607 748.5,-461.607 745,-471.607 752,-471.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 164339071128 -->\n",
       "<g class=\"node\" id=\"node5\"><title>164339071128</title>\n",
       "<polygon fill=\"none\" points=\"0,-415.5 0,-461.5 569,-461.5 569,-415.5 0,-415.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"194\" y=\"-434.8\">time_distributed_5(bidirectional_7): TimeDistributed(Bidirectional)</text>\n",
       "<polyline fill=\"none\" points=\"388,-415.5 388,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"416\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"388,-438.5 444,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"416\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"444,-415.5 444,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"506.5\" y=\"-446.3\">(None, 101, 11, 50)</text>\n",
       "<polyline fill=\"none\" points=\"444,-438.5 569,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"506.5\" y=\"-423.3\">(None, 101, 50)</text>\n",
       "</g>\n",
       "<!-- 164339008176&#45;&gt;164339071128 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>164339008176-&gt;164339071128</title>\n",
       "<path d=\"M284.5,-498.366C284.5,-490.152 284.5,-480.658 284.5,-471.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"288,-471.607 284.5,-461.607 281,-471.607 288,-471.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 164339070232 -->\n",
       "<g class=\"node\" id=\"node6\"><title>164339070232</title>\n",
       "<polygon fill=\"none\" points=\"334.5,-332.5 334.5,-378.5 698.5,-378.5 698.5,-332.5 334.5,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"384\" y=\"-351.8\">c: Concatenate</text>\n",
       "<polyline fill=\"none\" points=\"433.5,-332.5 433.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"461.5\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"433.5,-355.5 489.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"461.5\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"489.5,-332.5 489.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"594\" y=\"-363.3\">[(None, 101, 50), (None, 101, 50)]</text>\n",
       "<polyline fill=\"none\" points=\"489.5,-355.5 698.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"594\" y=\"-340.3\">(None, 101, 100)</text>\n",
       "</g>\n",
       "<!-- 164339248880&#45;&gt;164339070232 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>164339248880-&gt;164339070232</title>\n",
       "<path d=\"M685.411,-415.473C655.748,-405.117 620.209,-392.709 589.4,-381.952\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"590.171,-378.514 579.576,-378.522 587.863,-385.123 590.171,-378.514\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 164339071128&#45;&gt;164339070232 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>164339071128-&gt;164339070232</title>\n",
       "<path d=\"M347.589,-415.473C377.252,-405.117 412.791,-392.709 443.6,-381.952\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"445.137,-385.123 453.424,-378.522 442.829,-378.514 445.137,-385.123\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 164510329600 -->\n",
       "<g class=\"node\" id=\"node7\"><title>164510329600</title>\n",
       "<polygon fill=\"none\" points=\"312.5,-249.5 312.5,-295.5 720.5,-295.5 720.5,-249.5 312.5,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"433\" y=\"-268.8\">spatial_dropout1d_3: SpatialDropout1D</text>\n",
       "<polyline fill=\"none\" points=\"553.5,-249.5 553.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"581.5\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"553.5,-272.5 609.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"581.5\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"609.5,-249.5 609.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"665\" y=\"-280.3\">(None, 101, 100)</text>\n",
       "<polyline fill=\"none\" points=\"609.5,-272.5 720.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"665\" y=\"-257.3\">(None, 101, 100)</text>\n",
       "</g>\n",
       "<!-- 164339070232&#45;&gt;164510329600 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>164339070232-&gt;164510329600</title>\n",
       "<path d=\"M516.5,-332.366C516.5,-324.152 516.5,-314.658 516.5,-305.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"520,-305.607 516.5,-295.607 513,-305.607 520,-305.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 164511022552 -->\n",
       "<g class=\"node\" id=\"node8\"><title>164511022552</title>\n",
       "<polygon fill=\"none\" points=\"298.5,-166.5 298.5,-212.5 734.5,-212.5 734.5,-166.5 298.5,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"433\" y=\"-185.8\">bidirectional_8(lstm_8): Bidirectional(LSTM)</text>\n",
       "<polyline fill=\"none\" points=\"567.5,-166.5 567.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"595.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"567.5,-189.5 623.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"595.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"623.5,-166.5 623.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"679\" y=\"-197.3\">(None, 101, 100)</text>\n",
       "<polyline fill=\"none\" points=\"623.5,-189.5 734.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"679\" y=\"-174.3\">(None, 101, 80)</text>\n",
       "</g>\n",
       "<!-- 164510329600&#45;&gt;164511022552 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>164510329600-&gt;164511022552</title>\n",
       "<path d=\"M516.5,-249.366C516.5,-241.152 516.5,-231.658 516.5,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"520,-222.607 516.5,-212.607 513,-222.607 520,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 164511144424 -->\n",
       "<g class=\"node\" id=\"node9\"><title>164511144424</title>\n",
       "<polygon fill=\"none\" points=\"316,-83.5 316,-129.5 717,-129.5 717,-83.5 316,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"436.5\" y=\"-102.8\">spatial_dropout1d_4: SpatialDropout1D</text>\n",
       "<polyline fill=\"none\" points=\"557,-83.5 557,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"585\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"557,-106.5 613,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"585\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"613,-83.5 613,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"665\" y=\"-114.3\">(None, 101, 80)</text>\n",
       "<polyline fill=\"none\" points=\"613,-106.5 717,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"665\" y=\"-91.3\">(None, 101, 80)</text>\n",
       "</g>\n",
       "<!-- 164511022552&#45;&gt;164511144424 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>164511022552-&gt;164511144424</title>\n",
       "<path d=\"M516.5,-166.366C516.5,-158.152 516.5,-148.658 516.5,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"520,-139.607 516.5,-129.607 513,-139.607 520,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 164517164256 -->\n",
       "<g class=\"node\" id=\"node10\"><title>164517164256</title>\n",
       "<polygon fill=\"none\" points=\"278,-0.5 278,-46.5 755,-46.5 755,-0.5 278,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"436.5\" y=\"-19.8\">time_distributed_6(dense_6): TimeDistributed(Dense)</text>\n",
       "<polyline fill=\"none\" points=\"595,-0.5 595,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"623\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"595,-23.5 651,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"623\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"651,-0.5 651,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"703\" y=\"-31.3\">(None, 101, 80)</text>\n",
       "<polyline fill=\"none\" points=\"651,-23.5 755,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"703\" y=\"-8.3\">(None, 101, 2)</text>\n",
       "</g>\n",
       "<!-- 164511144424&#45;&gt;164517164256 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>164511144424-&gt;164517164256</title>\n",
       "<path d=\"M516.5,-83.3664C516.5,-75.1516 516.5,-65.6579 516.5,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"520,-56.6068 516.5,-46.6068 513,-56.6069 520,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_keras_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "# Rows train : 5551\n",
      "# Rows test : 694\n",
      "# Rows dataset : 6245\n",
      "----------------------\n",
      "# Sents train : 387\n",
      "# Sents test : 53\n",
      "# Sents dataset : 440\n",
      "----------------------\n",
      "# Words : 10722\n",
      "# Vocab : 3639\n",
      "# Words missing embedding : 160\n",
      "Embedding shape : (3639, 50)\n",
      "Max length sentence : 101\n",
      "Training set length : 387\n",
      "Test set length : 53\n",
      "Training set length : 387\n",
      "Test set length : 53\n",
      "(386, 101, 1)\n",
      "(52, 101, 1)\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 101)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 101, 1024)    0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 101, 1024)    6295552     lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 101, 1024)    6295552     bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 101, 1024)    0           bidirectional_5[0][0]            \n",
      "                                                                 bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, 101, 2)       2050        add_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 12,593,154\n",
      "Trainable params: 12,593,154\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 386 samples, validate on 52 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-3b77aecb97a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mf1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_f1s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_f1s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreparation_elmo_v1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-d3f24f7f0ccd>\u001b[0m in \u001b[0;36mpreparation_elmo_v1\u001b[1;34m(dataset, embedding)\u001b[0m\n\u001b[0;32m    151\u001b[0m     history = model.fit(np.array(train_words_padded), np.array(train_binaries_padded),\n\u001b[0;32m    152\u001b[0m                          batch_size=batch_size, epochs=2, validation_data = (np.array(test_words_padded), \n\u001b[1;32m--> 153\u001b[1;33m                         np.array(test_binaries_padded)), verbose=1, callbacks=[metrics_elmo_v1])\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;31m# Use trained model to predict the test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[0mtrain_words_padded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_words_padded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_train_test_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords_padded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_num_sents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2657\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2658\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_make_callable_from_options'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2659\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[1;34m()\u001b[0m\n\u001b[0;32m    195\u001b[0m                 \u001b[1;31m# not already marked as initialized.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m                 is_initialized = session.run(\n\u001b[1;32m--> 197\u001b[1;33m                     [tf.is_variable_initialized(v) for v in candidate_vars])\n\u001b[0m\u001b[0;32m    198\u001b[0m                 \u001b[0muninitialized_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1100\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1272\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1273\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1276\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1259\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1260\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1261\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m   1263\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1293\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1294\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1295\u001b[1;33m       \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1297\u001b[0m   \u001b[1;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "f1, train_f1s, test_f1s = preparation_elmo_v1(datasets[0], models[0].model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.738095238095238"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "# Rows train : 5551\n",
      "# Rows test : 694\n",
      "# Rows dataset : 6245\n",
      "----------------------\n",
      "# Sents train : 387\n",
      "# Sents test : 53\n",
      "# Sents dataset : 440\n",
      "----------------------\n",
      "# Words : 10722\n",
      "# Vocab : 3639\n",
      "# Words missing embedding : 160\n",
      "Embedding shape : (3639, 50)\n",
      "Max length sentence : 101\n",
      "Training set length : 387\n",
      "Test set length : 53\n",
      "Training set length : 387\n",
      "Test set length : 53\n",
      "(386, 101, 1)\n",
      "(52, 101, 1)\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_32 (InputLayer)           (None, 101)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 101, 1024)    0           input_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_33 (Bidirectional (None, 101, 1024)    6295552     lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_34 (Bidirectional (None, 101, 1024)    6295552     bidirectional_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 101, 1024)    0           bidirectional_33[0][0]           \n",
      "                                                                 bidirectional_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_42 (TimeDistri (None, 101, 2)       2050        add_5[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 12,593,154\n",
      "Trainable params: 12,593,154\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 386 samples, validate on 52 samples\n",
      "Epoch 1/2\n",
      "386/386 [==============================] - 3787s 10s/step - loss: 0.1055 - acc: 0.9548 - val_loss: 0.0757 - val_acc: 0.9633\n",
      "(52, 101, 1)\n",
      "(54, 101)\n",
      "0 2\n",
      "2 4\n",
      "4 6\n",
      "6 8\n",
      "8 10\n",
      "10 12\n",
      "12 14\n",
      "14 16\n",
      "16 18\n",
      "18 20\n",
      "20 22\n",
      "22 24\n",
      "24 26\n",
      "26 28\n",
      "28 30\n",
      "30 32\n",
      "32 34\n",
      "34 36\n",
      "36 38\n",
      "38 40\n",
      "40 42\n",
      "42 44\n",
      "44 46\n",
      "46 48\n",
      "48 50\n",
      "50 52\n",
      "52 54\n",
      "(52, 101)\n",
      "(388, 101)\n",
      "0 2\n",
      "2 4\n",
      "4 6\n",
      "6 8\n",
      "8 10\n",
      "10 12\n",
      "12 14\n",
      "14 16\n",
      "16 18\n",
      "18 20\n",
      "20 22\n",
      "22 24\n",
      "24 26\n",
      "26 28\n",
      "28 30\n",
      "30 32\n",
      "32 34\n",
      "34 36\n",
      "36 38\n",
      "38 40\n",
      "40 42\n",
      "42 44\n",
      "44 46\n",
      "46 48\n",
      "48 50\n",
      "50 52\n",
      "52 54\n",
      "54 56\n",
      "56 58\n",
      "58 60\n",
      "60 62\n",
      "62 64\n",
      "64 66\n",
      "66 68\n",
      "68 70\n",
      "70 72\n",
      "72 74\n",
      "74 76\n",
      "76 78\n",
      "78 80\n",
      "80 82\n",
      "82 84\n",
      "84 86\n",
      "86 88\n",
      "88 90\n",
      "90 92\n",
      "92 94\n",
      "94 96\n",
      "96 98\n",
      "98 100\n",
      "100 102\n",
      "102 104\n",
      "104 106\n",
      "106 108\n",
      "108 110\n",
      "110 112\n",
      "112 114\n",
      "114 116\n",
      "116 118\n",
      "118 120\n",
      "120 122\n",
      "122 124\n",
      "124 126\n",
      "126 128\n",
      "128 130\n",
      "130 132\n",
      "132 134\n",
      "134 136\n",
      "136 138\n",
      "138 140\n",
      "140 142\n",
      "142 144\n",
      "144 146\n",
      "146 148\n",
      "148 150\n",
      "150 152\n",
      "152 154\n",
      "154 156\n",
      "156 158\n",
      "158 160\n",
      "160 162\n",
      "162 164\n",
      "164 166\n",
      "166 168\n",
      "168 170\n",
      "170 172\n",
      "172 174\n",
      "174 176\n",
      "176 178\n",
      "178 180\n",
      "180 182\n",
      "182 184\n",
      "184 186\n",
      "186 188\n",
      "188 190\n",
      "190 192\n",
      "192 194\n",
      "194 196\n",
      "196 198\n",
      "198 200\n",
      "200 202\n",
      "202 204\n",
      "204 206\n",
      "206 208\n",
      "208 210\n",
      "210 212\n",
      "212 214\n",
      "214 216\n",
      "216 218\n",
      "218 220\n",
      "220 222\n",
      "222 224\n",
      "224 226\n",
      "226 228\n",
      "228 230\n",
      "230 232\n",
      "232 234\n",
      "234 236\n",
      "236 238\n",
      "238 240\n",
      "240 242\n",
      "242 244\n",
      "244 246\n",
      "246 248\n",
      "248 250\n",
      "250 252\n",
      "252 254\n",
      "254 256\n",
      "256 258\n",
      "258 260\n",
      "260 262\n",
      "262 264\n",
      "264 266\n",
      "266 268\n",
      "268 270\n",
      "270 272\n",
      "272 274\n",
      "274 276\n",
      "276 278\n",
      "278 280\n",
      "280 282\n",
      "282 284\n",
      "284 286\n",
      "286 288\n",
      "288 290\n",
      "290 292\n",
      "292 294\n",
      "294 296\n",
      "296 298\n",
      "298 300\n",
      "300 302\n",
      "302 304\n",
      "304 306\n",
      "306 308\n",
      "308 310\n",
      "310 312\n",
      "312 314\n",
      "314 316\n",
      "316 318\n",
      "318 320\n",
      "320 322\n",
      "322 324\n",
      "324 326\n",
      "326 328\n",
      "328 330\n",
      "330 332\n",
      "332 334\n",
      "334 336\n",
      "336 338\n",
      "338 340\n",
      "340 342\n",
      "342 344\n",
      "344 346\n",
      "346 348\n",
      "348 350\n",
      "350 352\n",
      "352 354\n",
      "354 356\n",
      "356 358\n",
      "358 360\n",
      "360 362\n",
      "362 364\n",
      "364 366\n",
      "366 368\n",
      "368 370\n",
      "370 372\n",
      "372 374\n",
      "374 376\n",
      "376 378\n",
      "378 380\n",
      "380 382\n",
      "382 384\n",
      "384 386\n",
      "386 388\n",
      "Training F1-score : 0.5056751467710372\n",
      "Testing F1-score : 0.46537396121883656\n",
      "--------------------Targets-------------------------\n",
      "[0 0 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0\n",
      " 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
      " 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1\n",
      " 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0\n",
      " 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0\n",
      " 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0\n",
      " 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "--------------------Predictions-------------------------\n",
      "[0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0\n",
      " 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "386/386 [==============================] - 3801s 10s/step - loss: 0.0641 - acc: 0.9700 - val_loss: 0.0586 - val_acc: 0.9754\n",
      "(52, 101, 1)\n",
      "(54, 101)\n",
      "0 2\n",
      "2 4\n",
      "4 6\n",
      "6 8\n",
      "8 10\n",
      "10 12\n",
      "12 14\n",
      "14 16\n",
      "16 18\n",
      "18 20\n",
      "20 22\n",
      "22 24\n",
      "24 26\n",
      "26 28\n",
      "28 30\n",
      "30 32\n",
      "32 34\n",
      "34 36\n",
      "36 38\n",
      "38 40\n",
      "40 42\n",
      "42 44\n",
      "44 46\n",
      "46 48\n",
      "48 50\n",
      "50 52\n",
      "52 54\n",
      "(52, 101)\n",
      "(388, 101)\n",
      "0 2\n",
      "2 4\n",
      "4 6\n",
      "6 8\n",
      "8 10\n",
      "10 12\n",
      "12 14\n",
      "14 16\n",
      "16 18\n",
      "18 20\n",
      "20 22\n",
      "22 24\n",
      "24 26\n",
      "26 28\n",
      "28 30\n",
      "30 32\n",
      "32 34\n",
      "34 36\n",
      "36 38\n",
      "38 40\n",
      "40 42\n",
      "42 44\n",
      "44 46\n",
      "46 48\n",
      "48 50\n",
      "50 52\n",
      "52 54\n",
      "54 56\n",
      "56 58\n",
      "58 60\n",
      "60 62\n",
      "62 64\n",
      "64 66\n",
      "66 68\n",
      "68 70\n",
      "70 72\n",
      "72 74\n",
      "74 76\n",
      "76 78\n",
      "78 80\n",
      "80 82\n",
      "82 84\n",
      "84 86\n",
      "86 88\n",
      "88 90\n",
      "90 92\n",
      "92 94\n",
      "94 96\n",
      "96 98\n",
      "98 100\n",
      "100 102\n",
      "102 104\n",
      "104 106\n",
      "106 108\n",
      "108 110\n",
      "110 112\n",
      "112 114\n",
      "114 116\n",
      "116 118\n",
      "118 120\n",
      "120 122\n",
      "122 124\n",
      "124 126\n",
      "126 128\n",
      "128 130\n",
      "130 132\n",
      "132 134\n",
      "134 136\n",
      "136 138\n",
      "138 140\n",
      "140 142\n",
      "142 144\n",
      "144 146\n",
      "146 148\n",
      "148 150\n",
      "150 152\n",
      "152 154\n",
      "154 156\n",
      "156 158\n",
      "158 160\n",
      "160 162\n",
      "162 164\n",
      "164 166\n",
      "166 168\n",
      "168 170\n",
      "170 172\n",
      "172 174\n",
      "174 176\n",
      "176 178\n",
      "178 180\n",
      "180 182\n",
      "182 184\n",
      "184 186\n",
      "186 188\n",
      "188 190\n",
      "190 192\n",
      "192 194\n",
      "194 196\n",
      "196 198\n",
      "198 200\n",
      "200 202\n",
      "202 204\n",
      "204 206\n",
      "206 208\n",
      "208 210\n",
      "210 212\n",
      "212 214\n",
      "214 216\n",
      "216 218\n",
      "218 220\n",
      "220 222\n",
      "222 224\n",
      "224 226\n",
      "226 228\n",
      "228 230\n",
      "230 232\n",
      "232 234\n",
      "234 236\n",
      "236 238\n",
      "238 240\n",
      "240 242\n",
      "242 244\n",
      "244 246\n",
      "246 248\n",
      "248 250\n",
      "250 252\n",
      "252 254\n",
      "254 256\n",
      "256 258\n",
      "258 260\n",
      "260 262\n",
      "262 264\n",
      "264 266\n",
      "266 268\n",
      "268 270\n",
      "270 272\n",
      "272 274\n",
      "274 276\n",
      "276 278\n",
      "278 280\n",
      "280 282\n",
      "282 284\n",
      "284 286\n",
      "286 288\n",
      "288 290\n",
      "290 292\n",
      "292 294\n",
      "294 296\n",
      "296 298\n",
      "298 300\n",
      "300 302\n",
      "302 304\n",
      "304 306\n",
      "306 308\n",
      "308 310\n",
      "310 312\n",
      "312 314\n",
      "314 316\n",
      "316 318\n",
      "318 320\n",
      "320 322\n",
      "322 324\n",
      "324 326\n",
      "326 328\n",
      "328 330\n",
      "330 332\n",
      "332 334\n",
      "334 336\n",
      "336 338\n",
      "338 340\n",
      "340 342\n",
      "342 344\n",
      "344 346\n",
      "346 348\n",
      "348 350\n",
      "350 352\n",
      "352 354\n",
      "354 356\n",
      "356 358\n",
      "358 360\n",
      "360 362\n",
      "362 364\n",
      "364 366\n",
      "366 368\n",
      "368 370\n",
      "370 372\n",
      "372 374\n",
      "374 376\n",
      "376 378\n",
      "378 380\n",
      "380 382\n",
      "382 384\n",
      "384 386\n",
      "386 388\n",
      "Training F1-score : 0.7671852899575672\n",
      "Testing F1-score : 0.7445544554455444\n",
      "--------------------Targets-------------------------\n",
      "[0 0 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0\n",
      " 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
      " 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1\n",
      " 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0\n",
      " 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0\n",
      " 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0\n",
      " 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "--------------------Predictions-------------------------\n",
      "[0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0\n",
      " 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0\n",
      " 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1\n",
      " 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0\n",
      " 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Training set length : 387\n",
      "Test set length : 53\n",
      "Training set length : 387\n",
      "Test set length : 53\n",
      "(54, 101)\n",
      "0 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 4\n",
      "4 6\n",
      "6 8\n",
      "8 10\n",
      "10 12\n",
      "12 14\n",
      "14 16\n",
      "16 18\n",
      "18 20\n",
      "20 22\n",
      "22 24\n",
      "24 26\n",
      "26 28\n",
      "28 30\n",
      "30 32\n",
      "32 34\n",
      "34 36\n",
      "36 38\n",
      "38 40\n",
      "40 42\n",
      "42 44\n",
      "44 46\n",
      "46 48\n",
      "48 50\n",
      "50 52\n",
      "52 54\n",
      "Training set length : 387\n",
      "Test set length : 53\n",
      "Training set length : 387\n",
      "Test set length : 53\n"
     ]
    }
   ],
   "source": [
    "f1, train_f1s, test_f1s = preparation_elmo_v1(datasets[0], models[0].model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7644444444444445"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5056751467710372, 0.7671852899575672]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.46537396121883656, 0.7445544554455444]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = []\n",
    "datatype = []\n",
    "f1_scores = []\n",
    "epochs = [epoch for epoch, f1 in enumerate(train_f1s, 1)]\n",
    "epochs.extend(epochs)\n",
    "datatype = ['train' for elem in train_f1s]\n",
    "datatype.extend(['test' for elem in test_f1s])\n",
    "f1_scores.extend(train_f1s.copy())\n",
    "f1_scores.extend(test_f1s.copy())\n",
    "evaluation = [{'F1-score' : result[0], 'epoch' : result[1],\n",
    "                    'data' : result[2]} for result in zip(f1_scores, epochs, datatype)]\n",
    "epochs_f1_scores = pd.DataFrame.from_records(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEPCAYAAABcA4N7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "ax = sns.lineplot(x=\"epoch\", y=\"F1-score\",\n",
    "                   hue=\"data\", style=\"data\",\n",
    "                   markers=True, dashes=False, data=epochs_f1_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_lens = [len(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence lengths\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEBCAYAAAB13qL/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xl4VPW9P/D37MlksmcmCQkQJGGL\nCVFZwtKIVQkCKTbirVVvvO2V1l9viz96L7coPLS29Gr90cZ7W+vtY31sb4tXKCpprAZciltQJAoB\nEwJhSSCEySSTbTJLZjm/P0JGQmaYLLOf9+t5eGTOMvP5MuO853vO+X6PRBAEAUREJFrSUBdARESh\nxSAgIhI5BgERkcgxCIiIRI5BQEQkcgwCIiKRYxAQEYkcg4CISOQYBEREIscgICISOQYBEZHIMQiI\niESOQUBEJHLyUBdwPd3dA3C5on9y1NRUDbq6TKEuIyjY1ugjlnYC4d9WqVSC5OS4ce8X1kHgcgmi\nCAIAomknwLZGI7G0E4jOtvLQEBGRyDEIiIhEjkFARCRyDAIiIpFjEBARiRyDgIhI5BgEREQiF9bj\nCCgwHC7AZneMWKZSyCHnzwIiUWIQiJDN7sCnjfoRyxbOTYdcxY8DkRjxNyARkcgxCIiIRI5BQEQk\ncgwCIiKRYxAQEYkcg4CISOQYBEREIscgICISOQYBEZHIMQiIiESOQUBEJHIMAiIikRtTEFRXV2P1\n6tVYuXIldu3aNWp9Y2MjysvLUVpaiq1bt8LhGJrZ8rXXXsPy5cuxbt06rFu3DpWVlf6tnoiIJs3n\ndJN6vR6VlZV49dVXoVQqcd9992Hx4sXIzc11b7N582bs2LEDRUVFePzxx7Fnzx7cf//9OHHiBLZs\n2YK1a9cGtBFERDRxPnsEtbW1KC4uRlJSEtRqNUpLS1FTU+Ne39bWBqvViqKiIgBAeXm5e/3x48fx\n2muvoaysDP/2b/+G3t7eADWDiIgmymcQdHR0QKvVuh/rdDro9Xqv67VarXu9VqvF9773Pfz1r39F\nZmYmfvrTn/qzdiIi8gOfh4ZcLhckEon7sSAIIx5fb/2zzz7rXv7www/jzjvvHFdxqamacW0fqfrN\ngxBkslHLY2PkiFcr/f56gtGMeE3MiGVqtQraFLXfX8sTrTY+KK8TDsTSVrG0E4jOtvoMgoyMDBw5\ncsT92GAwQKfTjVhvMBjcjzs7O6HT6dDf349XXnkF//RP/wRgKCBkHr7srqerywSXSxjXPpFIkMnw\nXl3rqOUL56bDOmDz++uZbQ70m6wjl5ltMDidfn+ta2m18TAY+gP+OuFALG0VSzuB8G+rVCqZ0A9o\nn4eGli5dikOHDsFoNMJiseDAgQMoKSlxr8/KyoJKpUJdXR0AoKqqCiUlJVCr1fj973+PY8eOAQD+\n/Oc/j7tHQEREgeezR5Ceno5NmzahoqICdrsd69evR2FhITZs2ICNGzeioKAAO3fuxLZt22AymZCf\nn4+KigrIZDI888wz+MlPfgKr1YqcnBw8/fTTwWiTKPGG9EQ0URJBEML22AsPDaUjbow3lB+web4h\nvaf9x7Otv4V719qfxNJWsbQTCP+2BuzQEBERRTcGARGRyDEIiIhEjkFARCRyDAIiIpFjEBARiRyD\ngIhI5AJ/4TiN4Gngl0wRomKIiMAgCDqbffRgrgX5mSGqhoiIh4aIiESPQUBEJHIMAiIikWMQEBGJ\nHIOAiEjkGARERCLHICAiEjkGARGRyHFAWZjpGxjE6Ys9UMhlcLoEzJ+ZirTE2FCXRURRjEEQJlwu\nAQ3njTjW3AWXIEAQgKOnO/GyVILlhZlYs2Q6A4GIAoJBECY+Ot6Oc+39mJauwaK56VAppJiemYCP\nT1zG+8cuofbEZVSUzsayAk5HQUT+xXMEYeB8ex/OtffjxhkpWHFTFtQxcshkUmSmxuHBlbPx1HeX\nYOaUBLzwt0b8seYk7A5XqEsmoijCIAgxQRBQ9f4ZxChlKJiZ6nGblIQY/Ot9RVhdPB3vHb2E5/ad\ngNPFMCAi/2AQhNhFwwDOXOzF/NxUKOTe3w6ZVIr1K2biwZWzcLS5E3944yRcghDESokoWvEcQQgJ\ngoDPThmgS45FXnbSmPb56s3ZMJnt2PfhOSRolLh3RW6AqySiaMceQQgZeqzoNQ1iZfF0SKWSMe9X\ntiwHK4qm4M2PW3HibFcAKyQiMWAQhNCFDhMkEqBgZtqY93G4APOgE2VfmYHMVDV+/7dGdPRY4OJR\nIiKaIAZBCF3sMCEjRY1Y1diP0A3f4ezY6U7cMlsLk3kQv33tOOxOZwArJaJoxiAIkb6BQfQODGKq\nTjPh50hJiEFRXhpa9SYcPd3px+qISEwYBAHkcAEDNseIP8OHcC50mAAA2ZMIAgCYNyMFqQkqVL1/\nFoN29gqIaPzGFATV1dVYvXo1Vq5ciV27do1a39jYiPLycpSWlmLr1q1wOBwj1jc0NODGG2/0T8UR\nZPgwztV/HFeu/7/QYUJyvAqaWMWkXkMqkaA4PwMmix2fs1dARBPgMwj0ej0qKyvx0ksvYd++fdi9\nezeam5tHbLN582Zs374d+/fvhyAI2LNnj3udxWLBz372M9jtdv9XH6Gsgw4Yui2TOix0tdTEGCwv\nzERTaw86eyx+eU4iEg+fQVBbW4vi4mIkJSVBrVajtLQUNTU17vVtbW2wWq0oKioCAJSXl49Y/9RT\nT+Ghhx4KQOmRq80wAAHwWxAAwKri6YhVyfBxgx4uXkJEROPg83KVjo4OaLVa92OdTof6+nqv67Va\nLfR6PQDgnXfegdVqxapVqyZUXGqq/74oQ0EwmhGviRmxTKGQo9s0CJVShulTEiGRDI0fuHY7AFCr\nVdCmqH0+5/D+JTdlY//HLTivN2F+ntbj/t6ew9u2gaDVxgfldcKBWNoqlnYC0dlWn0HgcrncX1bA\n0GjYqx97W28wGPDcc8/hD3/4w4SL6+oyRfSvW7PNgX6TdcQyu92BSwYTUhNiYBqwuZdfux0AmM02\nGK65LNTTcw4/ry5Rhay0OHx8oh3pyTEwT00ctb+35/D0WoGg1cbDYOgP+OuEA7G0VSztBMK/rVKp\nZEI/oH0eGsrIyIDBYHA/NhgM0Ol0Xtd3dnZCp9Ph4MGD6OnpwQMPPIB169YBANatWweTyTTuIqOJ\nbdCJXtMg0hJH/6qfLIlEgkXzdBAE4NPGDgici4iIxsBnECxduhSHDh2C0WiExWLBgQMHUFJS4l6f\nlZUFlUqFuro6AEBVVRVKSkpw77334u2330ZVVRWqqqrc6zSayD7cM1kXDSYIANKS/B8EABCvVmJ+\nbipa9SZ8UN8ekNcgoujiMwjS09OxadMmVFRU4O6778batWtRWFiIDRs24Pjx4wCAnTt34sknn8Sq\nVatgNptRUVER8MIjVevloW5lIHoEw/JnpCBLG4dXDp5B88XegL0OEUWHMc1tUFZWhrKyshHLnn/+\nefff58yZg7179173OZqamiZQXvRp1fdDE6tAjDJwE79KJEO3t3znyEU8u+84tj+0EMnxqoC9HhFF\nNo4sDrLWy6aA9gaGqRQybPhaPqw2J57+38/R3W/zvRMRiRKDIIjMVgd6TLaAnR+41pS0OGz6h/no\nMdkYBkTkFYMgiLr6hi7ZTEuMDdprzpqahB9eCYNf7j4Ki83heyciEhUGQRB19lgglUqQkhDc4/V5\n2UnYWF6Ay11m/P71Bt7ikohGYBAEUWevFVNS1ZDLgv/PPjcnBd+4PRefn+5EzcetQX99IgpfDIIg\nMvbZJj3t9GTccUs2lt2YgTc/bkFnLyenI6IhDIIgsQ46YLM7kZ4anPl8PJFIJLj/zlmIVcnwxVlj\nyOogovDCIAiSHtMgACA9OXRBAACxKjmWF05Bi96EvoHBkNZCROGBQRAkvaahSzfTgzTD5/WsuCkL\nUokEDefZKyAiBkHQ9JoGoZBJkahRhroUJMQpMTMrAc1tfbyclIgYBMHSMzCIRI1yxJTdoTQvJwUu\nl4BTF3pCXQoRhRiDIEh6TYNIjAt9b2BYokaJ9ORYtOrFPS04ETEIgmLQ7oTF5giLw0JXy9Jp0N1v\nw4CF95MmEjMGQRD0XrliKFETXjOAZmvjAAzdQ5mIxItBEAQ9Vy7TTAqzHkFinBKaWAUuGnh4iEjM\nGARB0GuyQSqVIC5WEepSRpBIJMjSxqG9y4xBR+DvV0xE4YlBEAS9A0MniqVhcsXQ1bK1GjhdAk5f\n4J3MiMSKQRAEvabBsDtRPCwjJRZymQRfnOPgMiKxYhAEmN3hgsliR1IYXTp6NZlMiozUOHxxrgsC\np6cmEiUGQYANz+cTblcMXW1KqhrGPhu6eq2hLoWIQoBBEGD95qEgSIgLrxPFV0tPGbpj2umLPE9A\nJEYMggDrvzJYSxMbnoeGgKHeSqxKhtMXOd0EkRgxCALMZLZDpZBBIQ/ff2qpRIIZmQnsERCJVPh+\nO0UJk8WOeHX4HhYadkNWIto6B2DidBNEosMgCDCTxQ5NmA0k82TmlAQAQHMbewVEYsMgCCCXSxgK\nggjoEUzLiIdMKuF5AiIRYhAEUI/JBkFARPQIlHIZcjLjOcKYSIQYBAE0fF1+JAQBAORlJ+Fcex8G\n7Zx3iEhMGAQB1HklCCLhZDEA5GUnwukScP5yf6hLIaIgGlMQVFdXY/Xq1Vi5ciV27do1an1jYyPK\ny8tRWlqKrVu3wuEYug/ukSNHUF5ejrKyMjzyyCPo7RXXYYeuPiskAOJiIiMIcrMSAYDnCYhExmcQ\n6PV6VFZW4qWXXsK+ffuwe/duNDc3j9hm8+bN2L59O/bv3w9BELBnzx4AwGOPPYann34a1dXVyM3N\nxQsvvBCYVoSprl4r4mIVkErDb9ZRT+LVSmSmqjmegEhkfAZBbW0tiouLkZSUBLVajdLSUtTU1LjX\nt7W1wWq1oqioCABQXl7uXv/GG28gNzcXdrsder0eCQkJAWpGeOrqtUbM+YFhs6Ym4fTFXrg4AR2R\naMh9bdDR0QGtVut+rNPpUF9f73W9VquFXq8HACgUCjQ1NeFb3/oW5HI5fvjDH46ruNRUzbi2DzfG\nPiumaDWI18S4lykU8hGPh3laplaroE1Rj1gmGM0et/X0vJ729/Ycw9vePDcD7x29BIsTyMmMv34D\nJ0irDczzhiOxtFUs7QSis60+g8DlckFy1Q1VBEEY8djX+tmzZ6O2thYvv/wyNm3ahJdffnnMxXV1\nmeByReYvU5vdid6BQcyYIkW/6ctZPe12x4jHwzwtM5ttMDhHXsFjtnne39Pzetrf23MMb5uRODRL\n6uH6NsTJ/X9IS6uNh8EgjpPRYmmrWNoJhH9bpVLJhH5A+zw0lJGRAYPB4H5sMBig0+m8ru/s7IRO\np4PNZsPbb7/tXv61r30NTU1N4y4wUnWGwaWjEqkEAzbHqD+esnV429gYORLjlGho6YbDFfyaiSj4\nfAbB0qVLcejQIRiNRlgsFhw4cAAlJSXu9VlZWVCpVKirqwMAVFVVoaSkBHK5HE888QROnDgBAHjz\nzTdx8803B6gZ4aezxwIgtJeO2uxOfNqoH/XH4Rr9DT+87ZGTHUiKV6GxpRs2uyMEVRNRsPk8NJSe\nno5NmzahoqICdrsd69evR2FhITZs2ICNGzeioKAAO3fuxLZt22AymZCfn4+KigrIZDJUVlZi+/bt\ncDqdSE9Px89//vNgtCksGK4EQaSdLAYAXXIsWi73w9hnRZw2ss/TEJFvPoMAAMrKylBWVjZi2fPP\nP+/++5w5c7B3795R+y1YsACvvvrqJEuMTJ29VijlUsQoZaEuZdx0yUM3qjl7qQ9TGQREUY8jiwOk\ns9eKlISYESfOI0WyRgWFTIoznImUSBQYBAFi7LMiOT5871N8PVKpBNrkGI4wJhIJBkGAGPttSIrQ\nIACAKWlx0Bst6Oy1hLoUIgowBkEAOJwu9A0MIkkT2UEAAMfPGkNcCREFGoMgALr7bQAQsYeGACAx\nTomUBBVOnO0KdSlEFGAMggAYDoKkeGWIK5k4iUSCeTkpQwPLnBxZRhTNGAQBYOwbGlWc7GFOoEgy\nLycFtkEnTl8YedLY4cKo0cochUwUucY0joDGx3hVj+BCR4iLmYRZU5Mgk0pw/JwRc3NS3Mttdgc+\nbdSP2Hbh3HTIVfw4EUUi9ggCoLvPhliVHDHKyX0xeporKJhz8KmUMsyamoTjPE9AFNX4Ey4AjP1W\npCRM/kSxze7EsVOGEcvmz9J62TowCm5IxZ6/N6Ozx4K0pNigvjYRBQd7BAFg7LdF9BVDV1swWwsJ\ngPfrL4W6FCIKEAZBAHT3WZESH9knioelJcWicGYq3j96iVcPEUUpBoGf2R0u9Jntfjk0FC6+eks2\n+sx2HGmK4DPfROQVg8DPuk2RP5jsWvkzUqBLjsW7n7V53cbbTXB4WSlR+OPJYj/rvjKGICUhOg4N\nAYBUIsFtN2Vh97vNaNX3I9XDSWNPJ7YBXlZKFAnYI/Cz4TEEKVHUIwCAZQWZUMqlqK49H+pSiMjP\nGAR+5h5VHGVBoIlVoGxZDuqaDPjcwy9/IopcDAI/M/bboPbDYLJwtGrxNORkxGPPu82w2Hg/Y6Jo\nwSDws+4+W1RdMXQ1mVSKf14zF9ZBBz5p0EMQgjjMmYgChkHgZ0OjiqPnRPG1srQarF6Sg1a9CU0X\neAczomjAIPCz7igaVezN7QuykaWNw5HGDhh6eAczokjHIPAju8OJfrM96oNAKpFgeUEm1DEKvHf0\nEqyDPF9AFMkYBH4UDXcmGyuVUoZbi6bAYnPg6GnOTkoUyRgEftTtHkMQvecIrpaaGIO87CScvtiD\nTh4iIopYDAI/Gp5eIkkEPYJh83NTIZNK8ObHLaEuhYgmiEHgRz39gwCAZI14giBWJcfc6ck4eqoT\nXb3WUJdDRBPAIPCj7n4bVAoZYlWyUJcSVPkzUqCOkeNYc2eoSyGiCWAQ+FG3yYakeBUkEkmoSwkq\npUKG4vwMtHUO8AoiogjEIPCj7n4rkjXKUJcREjfP1kIQgPOX+0NdChGN05iCoLq6GqtXr8bKlSux\na9euUesbGxtRXl6O0tJSbN26FQ7H0K/Curo6rF+/HuvWrcNDDz2Etjbv89lHgx4RDCbzJjMtDkka\nJc5dYhAQRRqfQaDX61FZWYmXXnoJ+/btw+7du9Hc3Dxim82bN2P79u3Yv38/BEHAnj173Mt37NiB\nqqoqlJWVYceOHYFpRRhwCQJ6TINIFsmlo57MyEyAoccCk9ke6lKIaBx8BkFtbS2Ki4uRlJQEtVqN\n0tJS1NTUuNe3tbXBarWiqKgIAFBeXo6amhoMDg7i0UcfxZw5cwAAs2fPRnt7e4CaEXr9ZjucLkG0\nPQJgKAgA4Fx7X4grIaLx8DlXckdHB7RarfuxTqdDfX291/VarRZ6vR5KpRLr1q0DALhcLvzmN7/B\nHXfcMa7iUlM149o+lHptQxOwTc9KhFYbDwAQjGbEa0b2EBQK+ahlADwu87Stt/0Dsa1arYI2RT1q\nf2/tytTFIzNVjRa9CUvnZ3l9juF/HzEQS1vF0k4gOtvqMwhcLteIq2AEQRjx2Nf6wcFBbNmyBQ6H\nA9/97nfHVVxXlwkuV2RMdXyutRsAIHUJMBiGjpObbQ70m0ZeW2+3j14GwOMyT9t62z8Q25rNNhic\nzlH7X69dU3UaHG7swIXLvUjSqEY9h1Yb7/73iXZiaatY2gmEf1ulUsmEfkD7PDSUkZEBg+HLO1IZ\nDAbodDqv6zs7O93rBwYG8PDDD8PhcOC5556DQqEYd4GRwOEC9N1mAIBKJXPfuD1CMsyvpuqGPoRt\nhoEQV0JEY+UzCJYuXYpDhw7BaDTCYrHgwIEDKCkpca/PysqCSqVCXV0dAKCqqsq9fvPmzZg+fTqe\neeYZKJXRe1mlze5Aw3kjJACaWrrxaaMenzbq4XC5Ql1a0MXFKpCkUeJSJ4OAKFL4PDSUnp6OTZs2\noaKiAna7HevXr0dhYSE2bNiAjRs3oqCgADt37sS2bdtgMpmQn5+PiooKNDQ04J133kFubi6+/vWv\nAxg6v/D8888HvFGhYLY5EKOSQyoV12AyT6akxeFkSw/sDvEFIVEkGtONdcvKylBWVjZi2dVf6HPm\nzMHevXtHrJ83bx6ampr8UGJkMFsdiIuJvvsUT8SUtDg0nO+G3mgOdSlENAYcWewnZpsDagYBACA9\nJRZymQRtPDxEFBEYBH5itjoQq2IQAEM3uc9IUaPNMMAb3BNFAAaBH9jsTtgdLvYIrjJFGweTxQ5D\nD6emJgp3DAI/6L1yQxo1ewRuWWlxAIDG88YQV0JEvjAI/GD4FpXsEXwpXq1EvFqBBgYBUdhjEPhB\nr2nozmRqVXQOmJuoLG0cTl/sxaB99OhkIgofDAI/6DGxR+BJVloc7A4XTl3oCXUpRHQdDAI/6O63\nQamQQiHnP+fV0lPUUMikOH6Wh4eIwhm/ufygp9+GuBgeFrqWXCZFbnYiTpzrCnUpRHQdDAI/6DbZ\nOKrYi7k5yWjvMqOzxxLqUojICwaBH3T32aBmj8CjeTkpAIDj53h4iChcMQgmyTbohNnmQFwsewSe\n6JJjkZYYg+NneHiIKFwxCCbJ2D80cpbnCDyTSCQouCEVjS3dsA3yMlKicMQgmKSuvuEgYI/Am0Vz\ndbDZnfi82eB1G4cL7hv6XP2HM1kTBR6/vSbJ2Dc0hoA9Au/ypiYhJUGFj7/Qo+zWPI/b2OwOfNqo\nH7V84dx0yDl1B1FAsUcwScY+KyTgYLLrkUokKJ6XgRNnjei5Mh0HEYUPBsEkGftsSIhT8s5kPhTn\np8MlCPjwWFuoSyGiazAIJsnYb0VyvCrUZYS9bK0GU3UaHKy7GOpSiOgaDIJJ6uqzIYlBMCZL8jPQ\n1NqNy7yFJVFYYRBMgiAI6O5jj2CsFs9Lh1wmxd8OnQ91KUR0FQbBJJgsdgw6XAyCMUqOV6HsKzeg\n9vhltOr7Q10OEV3BIJiE4UtHk+NjQlxJ5PiH2/OgjpFjz9+beT9jojDBIJiE4VHF7BGMnUatRNmy\nGWg4340TnH+IKCwwCCbhyx4Bg2A8vnpzFnRJsfifmib3TX2IKHQYBJNg7LNCLpNAo+ao4vGQy6T4\n7rp8mCx2/Odf6mGxOUJdEpGoMQgmwdhvQ3K8ClIJB5N5I5FKRswd1GE0w+ECZmQm4P/cnY8LHSY8\nt+8EHE5OKkQUKpwXYRK6+qxI4Yni67LZnTh26svJ5uI1MZg7PQk2u4CZ2Um4745cvPTWabzwegPm\n56ZCJuVvE6JgYxBMQlevFXOnJ4e6jIhzdTjIZVIsmqfD4YYOdPfbsKJoCmQyhgFRMPH/uAmy2Z3o\n7rchPTk21KVEvDnTkrH+q7loMwzgg/p2XlZKFGRjCoLq6mqsXr0aK1euxK5du0atb2xsRHl5OUpL\nS7F161Y4HCNP/j3zzDP49a9/7Z+Kw4She+gevLpkdYgriQ5LbszAgtlatOpNOHq6c0LP4emeBryf\nAZFvPoNAr9ejsrISL730Evbt24fdu3ejubl5xDabN2/G9u3bsX//fgiCgD179gAA+vv78fjjj+PF\nF18MTPUhpO8emi8nPYU9An+Zm5OMvOxEHD9rxJm23nHvP3xPg6v/2Oy8IonIF59BUFtbi+LiYiQl\nJUGtVqO0tBQ1NTXu9W1tbbBarSgqKgIAlJeXu9e/8847yMnJwbe+9a0AlR86+is9gnT2CPxGIpFg\n8bx0ZKSoceiEHl291lCXRCQKPk8Wd3R0QKvVuh/rdDrU19d7Xa/VaqHXD91p6u677waACR8WSk3V\nTGi/YOg125GkUWFadjI6jGbEa0ZfPaRQyEct97QMwKT3D8S2arUK2pTRQSd4aO9kX+vqZauXzcCe\nt0/hg/p2rFo6A1pt/Kjn9cRTXd7aECxjrT3SiaWdQHS21WcQuFwuSK66Tl4QhBGPfa2fjK4uE1yu\n8Dxx2NLeh7SkGBgM/TDbHOg3jf71arePXu5pGYBJ7x+Ibc1mGwzO0Tec99Tesb5WvCZmTK+/vDAT\n+w+34vdVJ7DxnoIxfaY81eWtDcGg1cbDYIj+yfXE0k4g/NsqlUom9APa56GhjIwMGAxfXgduMBig\n0+m8ru/s7ByxPlrpu81Rf8XQtYPBhv8EI5t1ybG4eZYWx5o78cbHLYF/QSIR89kjWLp0KX7961/D\naDQiNjYWBw4cwM9+9jP3+qysLKhUKtTV1eGWW25BVVUVSkpKAlp0qFkHHeg1DUb9+YFrB4MNmz9L\n62Fr/5uXkwwBwCvvnUVGihq3zA7+DwyHCx5POKsUcsh58TVFCZ9BkJ6ejk2bNqGiogJ2ux3r169H\nYWEhNmzYgI0bN6KgoAA7d+7Etm3bYDKZkJ+fj4qKimDUHjIdwyeKQ3jsWQwkEgkeXDkbvf02PF/d\ngJSEGMzITAhqDcNXIl1r4dx0yFUcj0nRYUyf5LKyMpSVlY1Y9vzzz7v/PmfOHOzdu9fr/j/4wQ8m\nWF54cgdBlB8aCgcKuRTfv6cQO/54BL98+Sh+cE8BZk/jaG4if2LndgKGxxDoGARBkRinxI8euAmJ\nGiV+ufsoDnv4hU5EE8cgmAC90YLEOCVilDw0ECxpibF47MFbcENmAv676gu88t6ZsL2ijCjSMAgm\nQAxXDIUjTawC/3pfEW4tmoK/HWrBL3cfRZ95MNRlEUU8BsEE6Lst0PFEcUgo5DI8tGoOvr16Lprb\nevH//vdzmCz2UJdFFNEYBONksTnQNzDIHkGILS/MxKPrC6E3WvCr3Ud5lzOiSWAQjFMH5xgKG/Ny\nUvC9r9+ICx0m/PqVep4zIJogBsE4teqHhpdnaeNCXAkBQFFuGipWzcbJ1h68f+yS355XEATUn+nE\nF+eM7G1Q1ONlL+N0rr0PsSo5B5OFkeUFmahrMqD6w3NYuzQHGrViUs/ndLnw5wOn8N7RL4MlPTkW\nty/Ihpx3T6MoxE/1OJ1r78eMzHjesD6Err0BjXnQifW3zYREIsGhLy5P6g5n1kEH/nNvPd47egmr\ni6dj472FmJ+bCn23BR9/oefd0ygqsUcwDoN2Jy4aTFi1eFqoSxE1b9M+rF2Wg1cOnsH59n7MmDL+\nqSgcLuDFN06i4ZwR992Rh2VyMu3jAAAOkUlEQVQFmXAJQE+/DYIA1J/pgjYpFrOnJfmjGURhgz2C\ncWjtMMHpEoI+3w2NTXFBBlISVPjslAFOp/d7VHq6peWAzYFPGi7j05MduPGGVCjlUnzaqIfDNfQ8\n83NTkZUWh08b9TD28YY5FF0YBONwrr0PABgEYUoqkeCW2VoMWB1obO3xup2nW1p+cKwNu985jeR4\nFQpmpo7aRyKRYFlhJuRyKeqaRs/IShTJGATjcK69D0kaJZLjVaEuhbzITI1DljYOx890wTo49qt9\nPmnogMXmwPLCTMikns//xChlKJyZivYuMxrPG/1VMkUwT71Lh/fOaNhiEIzD0Ili9gbC3S2ztXA4\nXDjW3DWm7c+196Hlcj9KF0/zGfKzpyVBE6vAvg/OcdwCeexderp/RbhjEIzRgNUOvdHMIIgASRoV\nZk1LQlNrD85e6r3uthabA5806JGWGIMVt2T7fG6ZVIqbZqXhUucAak9c9lfJRCHFIBij8+1DA8kY\nBJHh5llaxMXI8dJbp2B3eL5nsSAIOHTiMpxOAcsKMrweErpWTkY8pmfEY+97Z2C2Rt6vP6JrMQjG\naPhEcU5mfIgrobFQyKVYcmMG9EYL/vrR+VHrBUHA0dOduGgYwE15aUjUjP28j0QiwT/clov+gUG8\n9sFZP1ZNFBoMgjFqbOlGZqoacTGTG7VKwTMlLQ7F+el44+MW/O3QebiuGgx2rLkLx88akZediLk5\n47/j2bSMeKy4OQvvfnYRLZf7/Vg1UfBxQNkY9JpsONnajTVLckJdCo3T+hW5cDoFvPLeWTRd6EFO\nRgJOXejBqQs9yM1ORHF+OiQTHCV+T8kNqDvZgf/ZfxKPPXgLp5+giMVP7hgcPtkBQQAWz0sPdSk0\nTiqlDI+sy8c/ls7GyZYevHGoBSaLHYUzU7FkEiEAAOoYBe6/cxbOtffjd3/9Ak6X5+sGo+USQ4pe\n7BGMweFGPbK1GmSlccbRSCSRSHDbTVlYPFcHmUwKh0vwOEXFRCyam44e0yBefuc0nq9uwHfK8iG9\n5qSzpykxFs5Nh1zF//0oPPCT6ENnjwVn2vpwz603hLoUmiT1lfM7Dj9PK71y4VQ4nS785eAZtHeZ\n8bVlM3DTrDS/vgZRIDEIfPjkyi+5RXN5WIi8u6t4OlISYrDvg7N49rXjyExV445F01EwPQmxsbzA\ngMIbg+A6BEHAJw0dmDklAdok3poyFCRSCQau+QUfrgN6F89Lx4I5Whxu6MDBo23405uNkAC4ZY4O\nU3VxiFcrQ10iBZhLEOC4zoSH4YpBcB2fnTLgosGEf1w5K9SliJbN7sSxUyMneZs/Sxuiaq7P4QJs\ndhcK89JQmJcG86ALb31yHu99fgmfNXUgf0YK5uemjTqHQJHNJQg4c7EXFw0D0BvN2P12M+bmJOOm\nvDQsvTETCnn4X5PDIPDCbHXgz2+dwlSdBl+ZPyXU5VAEuPakcLwmBlPS4vDYQ7fgzzVNOH7WCH23\nBV+ZnxnCKsmfOrot2P9JKww9VmhiFZiWEY+stDh8cc6I+jNdePezNnynbB6ytJpQl3pdDAIvXnn/\nDPpMg9h4TyGvD6dJSdSosLwwE1lpcTj0xWW8/lELtElqLJqjC3VpNAkfHW/Hn/Y3ARLgK4WZyMmM\nh0QiwcK56agoleFYcxdefLMRP/3jEdx3ex5WFE2Z1OXKgcQg8ODE2S4c/KwNty/IHjW30FD3PzKO\nWVN4mTElAamJMXjv6CX8974TOL94GspLbuAPjQjjdLmw590zeOvIBeRNTcT8manuK9KGSSQSFOWl\n4adTFuOF1xvwp/1NaLncjwdXzgrL95tBcI33j13Cn/Y3ITMtDqWLp3k8UVl3cuQ14eF6zJrCT0Kc\nEquLp6FFb0LNJ604fbEHj3ztRqQmxoS6NBoDvdGMnf97FE0XenDHgmysXTYDnzV1eN0+MU6J/3vv\nfLz2wVn87VAL2jpNeHjtPKQnq4NYtW9jCoLq6mo899xzcDgceOihh/DAAw+MWN/Y2IitW7diYGAA\nCxYswBNPPAG5XI5Lly5h8+bN6OrqwowZM7Bz507ExYXnoKzOXgter23B+8cu4cYZKai4aw6Onxk9\nnz2/9GmyZDIpvnF7Hm6ckYI/vHkSP3nxMB5cORsL5+h4IjlMOV0uvH+sHXsPNkMQgH9eMxfLCjJH\n/VD0RCqV4J5bZ2KqToM/1jThxy8cxt1fuQF3LsyGTBoevQOfQaDX61FZWYlXX30VSqUS9913HxYv\nXozc3Fz3Nps3b8aOHTtQVFSExx9/HHv27MH999+PJ554Avfffz/WrFmDZ599Fr/97W+xefPmgDZo\nPIx9VjS19uBocyfqmgyQSIYGB91720xY7ZF3CRhFlkVz0zE9Ix7/ve8L/O6vX+DV98/g9puzUTAz\nFRkp6rA9niwWgiCgo9uCwyc7cPDzNnT321CUp8UDd+RNqAe3aG468rKT8Kf9Tdjz92a8deQCvlKY\nieL8DKQnx4b0/fYZBLW1tSguLkZSUhIAoLS0FDU1Nfj+978PAGhra4PVakVRUREAoLy8HP/1X/+F\ne++9F59++imeffZZ9/IHH3xwXEEwkV9HLpcLh08a0G8ehMslQBCGLu9yugRYbA5YbU50m2zo7rPC\nfCXN1So57rl1JpYVZiDpynTEDhdGHfcDALlMOmq5p2Xj2VYukwTttcZXl/9fK1YlD3q7rv0c+eu1\nfD1vrEoOp0Nx3boyU+Pw428twBfnuvH+8Xa8+3kb3v28DeoYBdKTY5EQp4QmZug5ZDIJZDIJ5FLJ\n0JdGAL43JBN4UrW6C2bzoM/tBAiAMPz34YVfnmATrl135W/CVQ8F97YjT8yN3vfaB1dv46mOof84\nXC4MWB3oH7Chvcvs/o7Iz0nG0oIpWH5zNozGAfdzjvUzNyw1MQaP3luIk609+OhEOz5p0OPjBj1i\nlHJMSVMjIU6FuBg5Fs1NR2bq+A8fTbRH6TMIOjo6oNV+eThEp9Ohvr7e63qtVgu9Xo/u7m5oNBrI\n5fIRy8cjOXlih5HWaP1z85jszESPy2/IHj1tsadl49l2arrnmgPxWuPZNhpeyxN/vLeTfd6rrdAm\nYMWi6WN+HQqd1NSRl4J6e8+vZ3laPJbfPNVfJU2azwNULpdrRJdFEIQRj72tv3Y7AOzqEhGFIZ9B\nkJGRAYPhy5GdBoMBOp3O6/rOzk7odDqkpKSgv78fTqfT435ERBQefAbB0qVLcejQIRiNRlgsFhw4\ncAAlJSXu9VlZWVCpVKirqwMAVFVVoaSkBAqFAgsWLMAbb7wBANi3b9+I/YiIKDxIhGvPunhQXV2N\n3/3ud7Db7Vi/fj02bNiADRs2YOPGjSgoKMDJkyexbds2mEwm5Ofn48knn4RSqURbWxu2bNmCrq4u\nZGZm4le/+hUSE8d/PI2IiAJnTEFARETRKzxGMxARUcgwCIiIRI5BQEQkcgwCIiKRC8sgqK6uxurV\nq7Fy5Urs2rUr1OX41W9+8xusWbMGa9aswdNPPw1gaBqPsrIyrFy5EpWVlSGu0P9+8YtfYMuWLQCG\nJigsLy9HaWkptm7dCofDvzeSD5V3330X5eXluOuuu7Bjxw4A0fu+VlVVuT/Dv/jFLwBE1/tqMpmw\ndu1aXLx4EYD39zGa2gwhzFy+fFm47bbbhO7ubmFgYEAoKysTTp8+Heqy/OKjjz4SvvGNbwg2m00Y\nHBwUKioqhOrqauHWW28VWltbBbvdLnz7298WDh48GOpS/aa2tlZYvHix8KMf/UgQBEFYs2aN8Pnn\nnwuCIAiPPfaYsGvXrlCW5xetra3C8uXLhfb2dmFwcFD45je/KRw8eDAq31ez2SwsXLhQ6OrqEux2\nu7B+/Xrho48+ipr39ejRo8LatWuF/Px84cKFC4LFYvH6PkZLmwVBEMKuR3D1JHdqtdo9yV000Gq1\n2LJlC5RKJRQKBWbOnInz589j+vTpmDp1KuRyOcrKyqKmvT09PaisrMQjjzwCwPMEhdHQ1rfeegur\nV69GRkYGFAoFKisrERsbG5Xvq9PphMvlgsVigcPhgMPhgFwuj5r3dc+ePfjxj3/sngWhvr7e4/sY\nbZ/lsLsxja9J7iJZXl6e++/nz5/Hm2++iQcffHBUe8c7OV+42r59OzZt2oT29nYA3icojHQtLS1Q\nKBR45JFH0N7ejhUrViAvLy8q31eNRoNHH30Ud911F2JjY7Fw4UIoFIqoeV9//vOfj3js6ftIr9dH\n3Wc57HoEvia5iwanT5/Gt7/9bfz7v/87pk6dGpXt/ctf/oLMzEwsWbLEvSxa31un04lDhw7hP/7j\nP7B7927U19fjwoULUdnWkydP4pVXXsHf//53fPDBB5BKpfjoo4+isq2A989stH2Ww65HkJGRgSNH\njrgfR9tkdXV1ddi4cSMef/xxrFmzBocPH77upH6R6o033oDBYMC6devQ29sLs9kMiUTicYLCSJeW\nloYlS5YgJSUFAHDHHXegpqYGMpnMvU20vK8ffvghlixZgtTUVABDh0ReeOGFqHxfAe+TbnqbbDNS\nhV2PwNckd5Gsvb0d//Iv/4KdO3dizZo1AID58+fj3LlzaGlpgdPpxOuvvx4V7X3xxRfx+uuvo6qq\nChs3bsRXv/pVPPnkkx4nKIx0t912Gz788EP09fXB6XTigw8+wKpVq6LyfZ0zZw5qa2thNpshCALe\nffddLFq0KCrfV8D7/5/eJtuMVGHXI0hPT8emTZtQUVHhnuSusLAw1GX5xQsvvACbzYannnrKvey+\n++7DU089hR/84Aew2Wy49dZbsWrVqhBWGVg7d+4cMUFhRUVFqEuatPnz5+Phhx/G/fffD7vdjmXL\nluGb3/wmbrjhhqh7X5cvX46GhgaUl5dDoVCgoKAA3/nOd3DnnXdG3fsKACqVyuv/n9H0Weakc0RE\nIhd2h4aIiCi4GARERCLHICAiEjkGARGRyDEIiIhEjkFARCRyDAIiIpFjEBARidz/B3yN11HWgUAQ\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set()\n",
    "ax = sns.distplot(sentence_lens)\n",
    "print('Sentence lengths')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "figure = ax.get_figure()\n",
    "figure.savefig('../plots/dl/sentence_length_dist.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(metrics.f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEBCAYAAAB13qL/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "epoch_f1s = plt.plot(metrics.f1_scores)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
