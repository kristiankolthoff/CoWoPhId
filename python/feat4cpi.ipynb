{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1) Load Data\n",
    "First, we load all the data we need into pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.wsd import lesk\n",
    "import nltk\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from collections import defaultdict\n",
    "\n",
    "Dataset = namedtuple('Dataset', 'name, train, test')\n",
    "FeatureDataset = namedtuple('FeatureDataset', 'name, fc, agg, train, test')\n",
    "FeatureCategory = namedtuple('FeatureCategory', 'name, func')\n",
    "Aggregation = namedtuple('Aggregation', 'name, agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "columns = ['id', 'sentence', \"start\", \"end\", \"target\", \n",
    "           \"nat\", \"non_nat\", \"nat_marked\", \"non_nat_marked\", \"binary\", \"prob\"]\n",
    "\n",
    "def load_df(path, d_type, header):\n",
    "    df = pd.read_csv(path, header=header, sep = \"\\t\")\n",
    "    if len(df.columns) == len(columns):\n",
    "        df.columns = columns\n",
    "    if d_type == 'word':\n",
    "        df = df.loc[df.target.map(lambda target : len(word_tokenize(target)))<=1,]\n",
    "    elif d_type == 'phrase':\n",
    "        df = df.loc[df.target.map(lambda target : len(word_tokenize(target)))>1,]\n",
    "    return df\n",
    "\n",
    "def load_datasets(names, train_name, test_name, type_train = None, type_test = None, header=None):\n",
    "    MAIN_PATH_DATASET = \"../cwishareddataset/traindevset/english/\"\n",
    "    datasets = [Dataset(name, load_df(MAIN_PATH_DATASET + name + '_' + train_name + '.tsv', type_train, header),\n",
    "                              load_df(MAIN_PATH_DATASET + name + '_' + test_name + '.tsv', type_test, header))\n",
    "                              for name in names]\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2.1) Preprocessing\n",
    "Here we compute preprocessed variants of the target words. We provide a preprocessed target word with whitespace removel, lowercasing etc. In addition, we provide the lemma of the target and the preprocessed versions of the lemma. Finall, we also compute the POS tags and the PennTreebank POS tags, so later feature functions requiring POS tags can easily access the precomputed tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import *\n",
    "from nltk import word_tokenize\n",
    "from functools import lru_cache\n",
    "from utils import penn_to_wn\n",
    "import re\n",
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "wordNetLemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def overlaps(start1, end1, start2, end2):\n",
    "    return bool(range(max(start1, start2), min(end1, end2)+1))\n",
    "\n",
    "tbl = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "                      if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(tbl)\n",
    "\n",
    "def ratio_non_alpha(target):\n",
    "    return 1 - (np.sum([1 for letter in target if (ord(letter)>=65 and ord(letter)<=90) \n",
    "             or (ord(letter)>=97 and ord(letter)<=122)]) / len(target))\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def targets_with_index(start, end, context):\n",
    "    curr_pos = 0\n",
    "    targets = []\n",
    "    j = 0\n",
    "    w = 0\n",
    "    curr_split = ''\n",
    "    ctx_split = context.split()\n",
    "    whitespaces = re.findall('\\s+', context)\n",
    "    num_whitespaces = [len(token) for token in whitespaces]\n",
    "    num_whitespaces.append(1)\n",
    "    tokens = word_tokenize(context)\n",
    "    tokens = ['\"' if token not in context else token for token in tokens]\n",
    "    for index, token in enumerate(tokens, 1):\n",
    "        targets.append((token, index, curr_pos, (curr_pos + len(token))))\n",
    "        curr_pos += len(token)\n",
    "        curr_split += token\n",
    "        if ctx_split[j] == curr_split:\n",
    "            curr_pos += num_whitespaces[w]\n",
    "            j += 1\n",
    "            w += 1\n",
    "            curr_split = ''\n",
    "    vals = [(target[0], target[1]) for target in targets \\\n",
    "            if overlaps(start, end, target[2], target[3])]\n",
    "    return [val for val in vals if val[0] != '\"']\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def wordnet_pos_tagging(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    return nltk.pos_tag(tokens)\n",
    "\n",
    "def pos_tags(start, end, target, sentence):\n",
    "    wordPOSPairs = wordnet_pos_tagging(sentence)\n",
    "    targets_index = targets_with_index(start, end, sentence)\n",
    "    results = [wordPOSPairs[tpl[1]-1][1] for tpl in targets_index]\n",
    "    filtered_results = [result for result in results \n",
    "                        if remove_punctuation(result).strip() and result != 'POS']\n",
    "    if len(nltk.word_tokenize(target)) != len(filtered_results):\n",
    "            return ['n' for word in target.split()]\n",
    "    return filtered_results if len(filtered_results) > 0 else None\n",
    "\n",
    "def wordnet_lemma(target, pos):\n",
    "    #tokens = nltk.word_tokenize(target)\n",
    "    tokens = target.split()\n",
    "    if pos:\n",
    "        if len(pos) != len(tokens):\n",
    "            return target\n",
    "        pos = [penn_to_wn(poss) if penn_to_wn(poss) else 'n' for poss in pos]\n",
    "        lemmas = [wordNetLemmatizer.lemmatize(token, poss)\n",
    "                     for token, poss in zip(tokens, pos)]\n",
    "        return ' '.join(lemmas)\n",
    "    return target\n",
    "\n",
    "def preprocessing(dataframe):\n",
    "    df = dataframe.copy()\n",
    "    df['sentence'] = df.sentence.apply(lambda sent : sent.replace(\"''\", \"``\"))\n",
    "    df['p_target'] = df.target.apply(lambda target : target.strip().lower())\n",
    "    df['pos_tags'] = df[['start', 'end', 'target', 'sentence']].apply(lambda vals : pos_tags(*vals), axis = 1)\n",
    "    df['pos_tags_pt'] = df.pos_tags.apply(lambda pos : [penn_to_wn(poss) if penn_to_wn(poss) else 'n' for poss in pos] \n",
    "                                          if pos else [])\n",
    "    df['lemma'] = df[['target', 'pos_tags']].apply(lambda vals : wordnet_lemma(*vals), axis = 1)\n",
    "    df['p_lemma'] = df.lemma.apply(lambda lemma : lemma.strip().lower())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_datasets(datasets):\n",
    "    return [Dataset(ds.name, preprocessing(ds.train), \n",
    "                             preprocessing(ds.test)) \n",
    "                             for ds in datasets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2.1.2) Regularization\n",
    "Here we provide some functions to compute a regularized binary label based on thresholds of the probability, the number of native annotations, the number of non-native annotations and the sum of native and non-native annotations. Setting the threshold up in order to require more than a single mark for a word to be complex, may help in regularizing the model. Note that this regularized binary label of course should only used on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_regularied_label_prob(dataframe, prob_thresh = 0.05):\n",
    "    df = dataframe.copy()\n",
    "    df['binary'] = df.prob.apply(lambda prob : 1 if prob >= prob_thresh else 0)\n",
    "    return df\n",
    "\n",
    "def create_regularized_label_nat(dataframe, nat_thresh = 1):\n",
    "    df = dataframe.copy()\n",
    "    df['binary'] = df.nat_marked.apply(lambda nat : 1 if nat >= nat_thresh else 0)\n",
    "    return df\n",
    "\n",
    "def create_regularized_label_non_nat(dataframe, non_nat_thresh = 1):\n",
    "    df = dataframe.copy()\n",
    "    df['binary'] = df.non_nat_marked.apply(lambda nat : 1 if nat >= non_nat_thresh else 0)\n",
    "    return df\n",
    "\n",
    "def create_regularized_label_marks_sum(dataframe, sum_thresh = 1):\n",
    "    df = dataframe.copy()\n",
    "    df['binary'] = df[['nat_marked','non_nat_marked']].apply(lambda marks : 1 \\\n",
    "                                        if sum(marks) > sum_thresh else 0, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_regularization(datasets, regularizer, val):\n",
    "     return [Dataset(ds.name, regularizer(ds.train, val), \n",
    "                        ds.test) for ds in datasets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2.2) Aggregation (A2)\n",
    "Since many labels are multi-word expression, we first of all define some aggregation functions that aggregate feature values over multiple tokens. Implementing this seperately allows to easily exchange the used aggregation function and keeps the feature computation functions clean. These feature computation functions should only compute features for a single target word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_wiki = {}\n",
    "sum_counts = 0\n",
    "with open(\"resources/word-freq-dumps/enwiki-20150602-words-frequency.txt\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        word, freq = line.partition(\" \")[::2]\n",
    "        sum_counts+=int(freq)\n",
    "        word_freq_wiki[word.strip()] = int(freq)\n",
    "        \n",
    "def get_unigram_probability(word):\n",
    "    return word_freq_wiki.get(word,1) / (sum_counts + len(word_freq_wiki))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def agg_feat_num_average(target, func_feature, *args, **kwargs):\n",
    "    if 'pos' in kwargs:\n",
    "        pos = kwargs.pop('pos')\n",
    "        return np.mean([func_feature(token, *args, pos=poss) \n",
    "                for token, poss in zip(word_tokenize(target), pos)])\n",
    "    return np.mean([func_feature(token, *args) for token in word_tokenize(target)])\n",
    "\n",
    "def agg_feat_num_weighted_average(target, func_feature, alpha, *args, **kwargs):\n",
    "    if 'pos' in kwargs:\n",
    "        pos = kwargs.pop('pos')\n",
    "        prob_sum = np.sum([(alpha/(alpha+get_unigram_probability(token))) for token in word_tokenize(target)])\n",
    "        return np.mean([((alpha/(alpha+get_unigram_probability(token)))/prob_sum) * \n",
    "                func_feature(token, *args, pos=poss) for token, poss in zip(word_tokenize(target), pos)])\n",
    "    prob_sum = np.sum([(alpha/(alpha+get_unigram_probability(token))) for token in word_tokenize(target)])\n",
    "    return np.sum([((alpha/(alpha+get_unigram_probability(token)))/prob_sum) * \n",
    "                func_feature(token, *args) for token in word_tokenize(target)])\n",
    "\n",
    "agg_feat_num_weighted_average_medium = lambda target, func_feature, *args, **kwargs: \\\n",
    "                        agg_feat_num_weighted_average(target, func_feature, 0.0001, *args, **kwargs)\n",
    "\n",
    "def agg_feat_num_median(target, func_feature, *args, **kwargs):\n",
    "    if 'pos' in kwargs:\n",
    "        pos = kwargs.pop('pos')\n",
    "        return np.median([func_feature(token, *args, pos=poss) \n",
    "                for token, poss in zip(word_tokenize(target), pos)])\n",
    "    return np.median([func_feature(token, *args) for token in word_tokenize(target)])\n",
    "\n",
    "def agg_feat_num_max(target, func_feature, *args, **kwargs):\n",
    "    if 'pos' in kwargs:\n",
    "        pos = kwargs.pop('pos')\n",
    "        return np.max([func_feature(token, *args, pos=poss) for token, poss in zip(word_tokenize(target), pos)])\n",
    "    return np.max([func_feature(token, *args) for token in word_tokenize(target)])\n",
    "\n",
    "def agg_feat_num_min(target, func_feature, *args, **kwargs):\n",
    "    if 'pos' in kwargs:\n",
    "        pos = kwargs.pop('pos')\n",
    "        return np.min([func_feature(token, *args, pos=poss) for token, poss in zip(word_tokenize(target), pos)])\n",
    "    return np.min([func_feature(token, *args) for token in word_tokenize(target)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.400640363656745"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simple(target):\n",
    "    return len(target)\n",
    "\n",
    "agg_feat_num_weighted_average_medium('and web science group', simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_default = [Aggregation('mean', agg_feat_num_average)]\n",
    "aggs_small = [Aggregation('mean', agg_feat_num_average), Aggregation('max', agg_feat_num_max)]\n",
    "aggs_all = [Aggregation('mean', agg_feat_num_average),\n",
    "            Aggregation('max', agg_feat_num_max), Aggregation('min', agg_feat_num_min),\n",
    "           Aggregation('weighted_mean', agg_feat_num_weighted_average_medium)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggs = agg_default "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_feature_datasets(*args, name=None):\n",
    "    zipped = zip(*args)\n",
    "    concat_features = []\n",
    "    for dataset in zipped:\n",
    "        df_train = None\n",
    "        df_test = None\n",
    "        fcs = []\n",
    "        aggs = []\n",
    "        for tpl in dataset:\n",
    "            if not fcs:\n",
    "                df_train = tpl.train.copy()\n",
    "                df_test = tpl.test.copy()\n",
    "            else:\n",
    "                df_train = pd.concat([df_train, tpl.train.copy()], axis = 1)\n",
    "                df_test = pd.concat([df_test, tpl.test.copy()], axis = 1)\n",
    "            fcs.append(tpl.fc)\n",
    "            aggs.append(tpl.agg)\n",
    "        if name:\n",
    "            data_name = (name,)\n",
    "        else:\n",
    "            data_name = fcs\n",
    "        concat_features.append(FeatureDataset(tpl.name, data_name, aggs,\n",
    "                    df_train.loc[:,~df_train.columns.duplicated()], \n",
    "                    df_test.loc[:,~df_test.columns.duplicated()]))\n",
    "    return concat_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3) Features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3.0.1) Baseline I\n",
    "The baseline I feature set covers only the two most relevant features as previous work has been shown. In many research work, only these two features, namely the word length and the word frequency are employed as features to compute complexity. Hence, we set this as our first feature baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_wiki = {}\n",
    "freq_sum_wiki = 0\n",
    "with open(\"resources/word-freq-dumps/enwiki-20150602-words-frequency.txt\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        word, freq = line.partition(\" \")[::2]\n",
    "        word_freq_wiki[word.strip()] = int(freq)\n",
    "        freq_sum_wiki+=int(freq)\n",
    "        \n",
    "def get_dict_count(target, freqs):\n",
    "    return freqs.get(target.strip().lower(), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_baseline_1(dataframe, agg, drop_features):\n",
    "    df = dataframe.copy()\n",
    "    df['length (bl1)'] = df.target.apply(lambda target : agg(target, len))\n",
    "    df['freq_wiki (bl1)'] = df.p_target.apply(lambda target : agg(target, get_dict_count, word_freq_wiki))\n",
    "    df['log_freq_wiki (bl1)'] = df['freq_wiki (bl1)'].apply(lambda freq : np.log(freq))\n",
    "    df = df.drop(drop_features, axis = 1)\n",
    "    return df\n",
    "\n",
    "fc_baseline_1 = FeatureCategory('baseline_1', features_baseline_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features_baseline_1(datasets, aggs = agg_default, drop_features = []):\n",
    "    return [FeatureDataset(ds.name, fc_baseline_1, agg,\n",
    "                        fc_baseline_1.func(ds.train, agg.agg, drop_features), \n",
    "                        fc_baseline_1.func(ds.test, agg.agg, drop_features)) \n",
    "                        for ds in datasets\n",
    "                        for agg in aggs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3.0.2) Basline II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordmodel import Word\n",
    "\n",
    "words_mrc_database = {}\n",
    "with open(\"resources/mrc-database/mrc2.dct\", encoding=\"utf8\") as file:\n",
    "    for index, line in enumerate(file):\n",
    "        line = line.strip()\n",
    "        word, phon, dphon, stress = line[51:].split('|')\n",
    "        w = Word(\n",
    "                wid = index,\n",
    "                nlet = int(line[0:2]),\n",
    "                nphon = int(line[2:4]),\n",
    "                nsyl = int(line[4]),\n",
    "                kf_freq = int(line[5:10]),\n",
    "                kf_ncats = int(line[10:12]),\n",
    "                kf_nsamp = int(line[12:15]),\n",
    "                tl_freq = int(line[15:21]),\n",
    "                brown_freq = int(line[21:25]),\n",
    "                fam = int(line[25:28]),\n",
    "                conc = int(line[28:31]),\n",
    "                imag = int(line[31:34]),\n",
    "                meanc = int(line[34:37]),\n",
    "                meanp = int(line[37:40]),\n",
    "                aoa = int(line[40:43]),\n",
    "                tq2 = line[43],\n",
    "                wtype = line[44],\n",
    "                pdwtype = line[45],\n",
    "                alphasyl = line[46],\n",
    "                status = line[47],\n",
    "                var = line[48],\n",
    "                cap = line[49],\n",
    "                irreg = line[50],\n",
    "                word=word,\n",
    "                phon=phon,\n",
    "                dphon=dphon,\n",
    "                stress=stress)\n",
    "        words_mrc_database[w.word.strip().lower()] = w\n",
    "\n",
    "def mrc_database(target, func, missing_val):\n",
    "    word = words_mrc_database.get(target.strip().lower())\n",
    "    val = func(word) if word else missing_val\n",
    "    return val if val != 0 else missing_val\n",
    "\n",
    "word_concreteness = {}\n",
    "with open(\"resources/word-freq-dumps/concreteness_brysbaert_et_al.txt\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        word, bigram, conc_m, conc_sd, \\\n",
    "        unknown, total, percent_known, \\\n",
    "        subtlex, dom_pos = line.split('\\t')\n",
    "        word_concreteness[word.strip()] = float(conc_m)\n",
    "\n",
    "word_freq_wiki = {}\n",
    "freq_sum_wiki = 0\n",
    "with open(\"resources/word-freq-dumps/enwiki-20150602-words-frequency.txt\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        word, freq = line.partition(\" \")[::2]\n",
    "        word_freq_wiki[word.strip()] = int(freq)\n",
    "        freq_sum_wiki+=int(freq)\n",
    "        \n",
    "def get_dict_count(target, freqs):\n",
    "    return freqs.get(target.strip().lower(), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_baseline_2(dataframe, agg, drop_features):\n",
    "    df = dataframe.copy()\n",
    "    df['length (bl2)'] = df.target.apply(lambda target : agg(target, len))\n",
    "    df['freq_wiki (bl2)'] = df.p_target.apply(lambda target : agg(target, get_dict_count, word_freq_wiki))\n",
    "    df['log_freq_wiki (bl2)'] = df[['freq_wiki (bl2)']].apply(lambda freq : np.log(freq))\n",
    "    df['mrc_fam (bl2)'] = df.p_target.apply(lambda target : agg(target, mrc_database, lambda word : word.fam, 400))\n",
    "    df['mrc_conc (bl2)'] = df.p_target.apply(lambda target : agg(target, mrc_database, lambda word : word.conc, 400))\n",
    "    df['mrc_imag (bl2)'] = df.p_target.apply(lambda target : agg(target, mrc_database, lambda word : word.imag, 400))\n",
    "    df['mrc_meanc (bl2)'] = df.p_target.apply(lambda target : agg(target, mrc_database, lambda word : word.meanc, 400))\n",
    "    df['concreteness (bl2)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                                lambda target : word_concreteness.get(target, 2.5)))\n",
    "    df = df.drop(drop_features, axis = 1)\n",
    "    return df\n",
    "\n",
    "fc_baseline_2 = FeatureCategory('baseline_2', features_baseline_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features_baseline_2(datasets, aggs = agg_default, drop_features = []):\n",
    "     return [FeatureDataset(ds.name, fc_baseline_2, agg,\n",
    "                        fc_baseline_2.func(ds.train, agg.agg, drop_features), \n",
    "                        fc_baseline_2.func(ds.test, agg.agg, drop_features)) \n",
    "                        for ds in datasets\n",
    "                        for agg in aggs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3.1) Linguistic Features\n",
    "Here we compute linguistic word features like the number of vowels the word has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import cmudict\n",
    "import numpy as np\n",
    "import pronouncing as pnc\n",
    "from wordmodel import Word\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.wordnet import *\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "from functools import lru_cache\n",
    "from collections import Counter\n",
    "from mezmorize import Cache\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import pronouncing as pnc\n",
    "from utils import penn_to_wn\n",
    "from nltk.parse.corenlp import *\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "\n",
    "java_path = \"C:/Program Files (x86)/Java/jdk1.8.0_144/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "path_to_jar = 'resources/stanford-dependency-parser/stanford-parser.jar'\n",
    "path_to_models_jar = 'resources/stanford-dependency-parser/stanford-parser-3.9.1-models.jar'\n",
    "\n",
    "porterStemmer = PorterStemmer()\n",
    "wordNetLemmatizer = WordNetLemmatizer()\n",
    "nerTagger = StanfordNERTagger('resources/stanford-ner-tagger/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "               'resources/stanford-ner-tagger/stanford-ner.jar',\n",
    "               encoding='utf-8')\n",
    "\n",
    "words_mrc_database = {}\n",
    "with open(\"resources/mrc-database/mrc2.dct\", encoding=\"utf8\") as file:\n",
    "    for index, line in enumerate(file):\n",
    "        line = line.strip()\n",
    "        word, phon, dphon, stress = line[51:].split('|')\n",
    "        w = Word(\n",
    "                wid = index,\n",
    "                nlet = int(line[0:2]),\n",
    "                nphon = int(line[2:4]),\n",
    "                nsyl = int(line[4]),\n",
    "                kf_freq = int(line[5:10]),\n",
    "                kf_ncats = int(line[10:12]),\n",
    "                kf_nsamp = int(line[12:15]),\n",
    "                tl_freq = int(line[15:21]),\n",
    "                brown_freq = int(line[21:25]),\n",
    "                fam = int(line[25:28]),\n",
    "                conc = int(line[28:31]),\n",
    "                imag = int(line[31:34]),\n",
    "                meanc = int(line[34:37]),\n",
    "                meanp = int(line[37:40]),\n",
    "                aoa = int(line[40:43]),\n",
    "                tq2 = line[43],\n",
    "                wtype = line[44],\n",
    "                pdwtype = line[45],\n",
    "                alphasyl = line[46],\n",
    "                status = line[47],\n",
    "                var = line[48],\n",
    "                cap = line[49],\n",
    "                irreg = line[50],\n",
    "                word=word,\n",
    "                phon=phon,\n",
    "                dphon=dphon,\n",
    "                stress=stress)\n",
    "        words_mrc_database[w.word.strip().lower()] = w\n",
    "\n",
    "def mrc_database(target, func, missing_val):\n",
    "    word = words_mrc_database.get(target.strip().lower())\n",
    "    val = func(word) if word else missing_val\n",
    "    return val if val != 0 else missing_val\n",
    "\n",
    "d = cmudict.dict()\n",
    "\n",
    "def num_syllables_rule_based(target):\n",
    "    vowels = \"aeiouy\"\n",
    "    numVowels = 0\n",
    "    lastWasVowel = False\n",
    "    for wc in target:\n",
    "        foundVowel = False\n",
    "        for v in vowels:\n",
    "            if v == wc:\n",
    "                if not lastWasVowel: numVowels+=1  \n",
    "                foundVowel = lastWasVowel = True\n",
    "                break\n",
    "        if not foundVowel:  \n",
    "            lastWasVowel = False\n",
    "    if len(target) > 2 and target[-2:] == \"es\":\n",
    "        numVowels-=1\n",
    "    elif len(target) > 1 and target[-1:] == \"e\":\n",
    "        numVowels-=1\n",
    "    return numVowels\n",
    "\n",
    "def num_syllables(target):\n",
    "    if target in d:\n",
    "        return np.mean([len(list(y for y in x if y[-1].isdigit())) for x in d[target.lower()]])\n",
    "    else:\n",
    "        return num_syllables_rule_based(target)\n",
    "    \n",
    "def num_vowels(target):\n",
    "    return np.sum([target.lower().count(vowel) for vowel in 'aeiouy'])\n",
    "\n",
    "def cognate_across_languages_sim(target, sim_func, agg_func, translations):\n",
    "    targ = target.strip().lower()\n",
    "    translated = translations.get(targ)\n",
    "    if not translated:\n",
    "        return 0\n",
    "    trans_texts = set([trans_word.text for trans_word in translated])\n",
    "    similarities = [sim_func(targ,trans_text) \n",
    "                    for trans_text in trans_texts]\n",
    "    return agg_func(similarities)\n",
    "\n",
    "def porter_stem_len(target):\n",
    "    return len(str(porterStemmer.stem(target)))\n",
    "\n",
    "def porter_stemmer_num_steps(target):\n",
    "    stem = target.lower()\n",
    "    applied_steps = 0\n",
    "    if porterStemmer.mode == porterStemmer.NLTK_EXTENSIONS and target in porterStemmer.pool:\n",
    "            return applied_steps\n",
    "    if porterStemmer.mode != porterStemmer.ORIGINAL_ALGORITHM and len(target) <= 2:\n",
    "            return applied_steps\n",
    "    step_funcs = [porterStemmer._step1a, porterStemmer._step1b, porterStemmer._step1c,\n",
    "                  porterStemmer._step2, porterStemmer._step3, porterStemmer._step3,\n",
    "                  porterStemmer._step4, porterStemmer._step5a, porterStemmer._step5b]\n",
    "    for step_func in step_funcs:\n",
    "        stem_step = step_func(stem)\n",
    "        if stem_step != stem:\n",
    "            stem = stem_step\n",
    "            applied_steps += 1\n",
    "    return applied_steps\n",
    "\n",
    "def is_named_entity(sentence, target):\n",
    "    tokenized_sent = word_tokenize(sentence)\n",
    "    tagged_sent = nerTagger.tag(tokenized_sent)\n",
    "    for token, tag in tagged_sent:\n",
    "        if token == target and tag != 'O':\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def named_entity_type(sentence, target):\n",
    "    tokenized_sent = word_tokenize(sentence)\n",
    "    tagged_sent = nerTagger.tag(tokenized_sent)\n",
    "    return [tag for token, tag in tagged_sent if token == target][0]\n",
    "\n",
    "def ratio_cap_letters(target):\n",
    "    return np.sum([1 for letter in target if letter.isupper()]) / len(target)\n",
    "\n",
    "def ratio_num_letters(target):\n",
    "    return np.sum([1 for letter in target if letter.isdigit()]) / len(target)\n",
    "\n",
    "def ratio_non_ascii_letters(target):\n",
    "    ascii = set(string.printable)   \n",
    "    return 1 - (np.sum([1 for letter in target if letter in ascii]) / len(target))\n",
    "\n",
    "def ratio_non_alpha(target):\n",
    "    return 1 - (np.sum([1 for letter in target if (ord(letter)>=65 and ord(letter)<=90) \n",
    "             or (ord(letter)>=97 and ord(letter)<=122)]) / len(target))\n",
    "\n",
    "def grapheme_to_phoneme_ratio(target):\n",
    "    phoneme_lengths = [len(prons.split()) \n",
    "            for prons in pnc.phones_for_word(target)]\n",
    "    if phoneme_lengths:\n",
    "        return len(target) / np.mean(phoneme_lengths)\n",
    "    return 1\n",
    "\n",
    "def num_pronounciations(target):\n",
    "    length = len(pnc.phones_for_word(target))\n",
    "    return length if length != 0 else 1\n",
    "\n",
    "# First make sure that the StanfordCoreNLP Server is running under port 9011\n",
    "# cd to stanfordCoreNLP directory\n",
    "# java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9011 -timeout 15000\n",
    "parser = CoreNLPDependencyParser(url='http://localhost:9011/')\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def dependency_parse_with_root(sentence):\n",
    "    try:\n",
    "        dependency_parser = parser.raw_parse(sentence)\n",
    "        dependencies = []\n",
    "        parsetree = list(dependency_parser)[0]\n",
    "        for index, node in parsetree.nodes.items():\n",
    "            for relation, dependant in parsetree.nodes[index]['deps'].items():\n",
    "                for dep in dependant:\n",
    "                    triple = ((node['word'], index), relation, \\\n",
    "                              (parsetree.nodes[dep]['word'], dep))\n",
    "                    dependencies.append(triple)\n",
    "        return dependencies\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def dependency_parse(sentence):\n",
    "    dependencies = dependency_parse_with_root(sentence)\n",
    "    filtered_dependencies = [triple for triple in dependencies if triple[1] != 'ROOT']\n",
    "    return filtered_dependencies\n",
    "\n",
    "\n",
    "def dep_dist_to_head(target, start, end, context):\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    return np.nan_to_num(np.mean([np.abs(triple[0][1] - triple[2][1])-1 \n",
    "                                for triple in triples if triple[2] in targets]))\n",
    "\n",
    "def dep_dist_to_root(target, start, end, context):\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse_with_root(context)\n",
    "    root_nodes = list(filter(lambda triple : triple[1] == 'ROOT' , triples))\n",
    "    if root_nodes: \n",
    "        root_node = root_nodes[0]\n",
    "    else:\n",
    "        return 0\n",
    "    dist = np.nan_to_num(np.mean([np.abs(root_node[2][1] - triple[2][1])-1 \n",
    "                                for triple in triples if triple[2] in targets]))\n",
    "    return dist if dist != -1 else 0\n",
    "\n",
    "def dep_relation_to_head(target, start, end, context):\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse_with_root(context)\n",
    "    relations = [triple[1] for triple in triples if triple[2] in targets]\n",
    "    return relations[0] if len(relations) == 1 else 'misc'\n",
    "    \n",
    "def dep_head_word_len(target, start, end, context):\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    return np.nan_to_num(np.mean([len(triple[0][0]) \n",
    "        for triple in triples if triple[2] in targets]))\n",
    "\n",
    "def dep_num_dependents(target, start, end, context):\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse_with_root(context)\n",
    "    return len([triple[1] for triple in triples if triple[0] in targets])\n",
    "\n",
    "def dep_max_num_dependents(context):\n",
    "    triples = dependency_parse_with_root(context)\n",
    "    most = Counter([triple[0][0] for triple in triples]).most_common(1)\n",
    "    return most[0][1] if most else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len ta_train : 8251\n",
      "Len ta_test : 2097\n",
      "Len targets : 10348\n"
     ]
    }
   ],
   "source": [
    "datasets = load_datasets(['Wikipedia', 'WikiNews', 'News'], 'TrainDev', 'Test')\n",
    "targets_train = list(set([ngram for ds in datasets for mwe in ds.train['target'].tolist() for ngram in mwe.split()]))\n",
    "targets_test = list(set([ngram for ds in datasets for mwe in ds.test['target'].tolist() for ngram in mwe.split()]))\n",
    "targets = targets_train.copy()\n",
    "targets.extend(targets_test)\n",
    "print('Len ta_train : {}'.format(len(targets_train)))\n",
    "print('Len ta_test : {}'.format(len(targets_test)))\n",
    "print('Len targets : {}'.format(len(targets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from googletrans import Translator\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "translator = Translator()\n",
    "targets = [target.strip().lower() for target in targets]\n",
    "\n",
    "trans_word = translator.translate(word, dest='de')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from googletrans import Translator\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "translator = Translator()\n",
    "targets = [target.strip().lower() for target in targets]\n",
    "languages = ['fr', 'de', 'es']\n",
    "translations = defaultdict(list)\n",
    "for index, word in enumerate(targets):\n",
    "    print(word)\n",
    "    translator = Translator()\n",
    "    for lang in languages:\n",
    "        trans_word = translator.translate(word, dest=lang)\n",
    "        translations[word].append(trans_word)\n",
    "        print(str(index) + \" \" + word + \" \" + trans_word.text)\n",
    "with open('resources/translations/translations.json', 'wb') as fp:\n",
    "    pickle.dump(translations, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "with open('resources/translations/data.json', 'rb') as fp:\n",
    "    data = pickle.load(fp)\n",
    "\n",
    "if not data:\n",
    "    translator = Translator()\n",
    "    targets = [target.strip().lower() for target in targets]\n",
    "    languages = ['fr', 'de', 'es']\n",
    "    translations = defaultdict(list)\n",
    "    for index, word in enumerate(targets):\n",
    "        translator = Translator()\n",
    "        for lang in languages:\n",
    "            trans_word = translator.translate(word, dest=lang)\n",
    "            translations[word].append(trans_word)\n",
    "            print(str(index) + \" \" + word + \" \" + trans_word.text)\n",
    "    with open('resources/translations/data.json', 'wb') as fp:\n",
    "        pickle.dump(translations, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    translations = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from similarity.ngram import NGram\n",
    "bigram_dist = NGram(2)\n",
    "trigram_dist = NGram(3)\n",
    "\n",
    "def features_linguistic(dataframe, agg, drop_features):\n",
    "    df = dataframe.copy()\n",
    "    df['length (lin)'] = df.target.apply(lambda target : agg(target, len))\n",
    "    df['phrase_length (lin)'] = df.target.apply(lambda target : len(target))\n",
    "    df['target_num_words (lin)'] = df.target.apply(lambda target : len(word_tokenize(target)))\n",
    "    # Relative positions of the target word based on character counting\n",
    "    df['relative_position_left (lin)'] = df[['sentence', 'start']].apply(lambda vals : vals[1] / len(vals[0]), axis = 1)\n",
    "    df['relative_position_centered (lin)'] = df[['sentence', 'start', 'end']].apply(lambda vals : \n",
    "                ((vals[1] + vals[2]) / 2) / len(vals[0]), axis = 1)\n",
    "    df['relative_position_right (lin)'] = df[['sentence', 'end']].apply(lambda vals : vals[1] / len(vals[0]), axis = 1)\n",
    "    df['ratio_cap_letters (lin)'] = df.target.apply(lambda target : agg(target, ratio_cap_letters))\n",
    "    df['all_caps (lin)'] = df[['ratio_cap_letters (lin)']] == 1\n",
    "    df['ratio_num_letters (lin)'] = df.target.apply(lambda target : agg(target, ratio_num_letters))\n",
    "    df['ratio_non_ascii_letters (lin)'] = df.target.apply(lambda target : agg(target, ratio_non_ascii_letters))\n",
    "    df['ratio_non_alpha (lin)'] = df.target.apply(lambda target : agg(target, ratio_non_alpha))\n",
    "    df['grapheme_to_phoneme_ratio (lin)'] = df.target.apply(lambda target : agg(target, grapheme_to_phoneme_ratio))\n",
    "    df['num_pronounciations (lin)'] = df.target.apply(lambda target : agg(target, num_pronounciations))\n",
    "    df['hyphenated (lin)'] = df.target.apply(lambda target : int('-' in target))\n",
    "    df['is_title (lin)'] = df.target.apply(lambda target : target.istitle())\n",
    "    df['mrc_nphon (lin)'] = df.p_target.apply(lambda target : agg(target, mrc_database, lambda word : word.nphon, 0))\n",
    "    df['cal_ngram_2_sim_min (lin)'] = df.p_target.apply(lambda target : agg(target, cognate_across_languages_sim, \\\n",
    "                                lambda source, dest : 1 - bigram_dist.distance(source, dest), np.min, translations))\n",
    "    df['cal_ngram_2_sim_max (lin)'] = df.p_target.apply(lambda target : agg(target, cognate_across_languages_sim, \\\n",
    "                                lambda source, dest : 1 - bigram_dist.distance(source, dest), np.max, translations))\n",
    "    df['cal_ngram_2_sim_mean (lin)'] = df.p_target.apply(lambda target : agg(target, cognate_across_languages_sim, \\\n",
    "                                lambda source, dest : 1 - bigram_dist.distance(source, dest), np.mean, translations))\n",
    "    df['cal_ngram_3_sim_min (lin)'] = df.p_target.apply(lambda target : agg(target, cognate_across_languages_sim, \\\n",
    "                                lambda source, dest : 1 - trigram_dist.distance(source, dest), np.min, translations))\n",
    "    df['cal_ngram_3_sim_max (lin)'] = df.p_target.apply(lambda target : agg(target, cognate_across_languages_sim, \\\n",
    "                                lambda source, dest : 1 - trigram_dist.distance(source, dest), np.max, translations))\n",
    "    df['cal_ngram_3_sim_mean (lin)'] = df.p_target.apply(lambda target : agg(target, cognate_across_languages_sim, \\\n",
    "                                lambda source, dest : 1 - trigram_dist.distance(source, dest), np.mean, translations))\n",
    "    df['num_syllables (lin)'] = df.p_target.apply(lambda target : agg(target, num_syllables))\n",
    "    df['num_vowels (lin)'] = df.p_target.apply(lambda target : agg(target, num_vowels))\n",
    "    df['vowel_consonant_ratio (lin)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                            lambda target : num_vowels(target) / (len(target) - num_vowels(target))))\n",
    "    # Porter stemmer stem length, number of applied steps,\n",
    "    # difference of stem length to target and reduction ratio\n",
    "    df['porter_stem_len (lin)'] = df.p_target.apply(lambda target : agg(target, porter_stem_len))\n",
    "    df['porter_stemmer_num_steps (lin)'] = df.p_target.apply(lambda target : agg(target, porter_stemmer_num_steps))\n",
    "    df['diff_len_stem_len (lin)'] = df['length (lin)'] - df['porter_stem_len (lin)']\n",
    "    df['reduction_stem_len (lin)'] = 1 - df['porter_stem_len (lin)'] / df['length (lin)']\n",
    "    df['norm_num_syllables (lin)'] = df['num_syllables (lin)'] / df['length (lin)']\n",
    "    df['norm_num_vowels (lin)'] = df['num_vowels (lin)'] / df['length (lin)']\n",
    "    df['lemma_len (lin)'] = df.lemma.apply(lambda lemma : agg(lemma, len))\n",
    "    df['reduction_lemma_len (lin)'] = 1 - df['lemma_len (lin)'] / df['length (lin)']\n",
    "    df['diff_len_lemma_len (lin)'] = df['length (lin)'] - df['lemma_len (lin)']\n",
    "    df['dep_dist_to_head (lin)'] = df[['target', 'start', 'end', 'sentence']].apply(lambda vals : \n",
    "                                               dep_dist_to_head(*vals), axis=1)\n",
    "    df['dep_dist_to_root (lin)'] = df[['target', 'start', 'end', 'sentence']].apply(lambda vals : \n",
    "                                                dep_dist_to_root(*vals), axis=1)\n",
    "    df['dep_dist_to_root_norm (lin)'] = df[['dep_dist_to_root (lin)', 'sentence']].apply(lambda vals : \\\n",
    "                                                float(vals[0]) / (len(word_tokenize(vals[1]))-1), axis=1)\n",
    "    df['dep_relation_to_head (lin)'] = df[['target', 'start', 'end', 'sentence']].apply(lambda vals : \\\n",
    "                                                dep_relation_to_head(*vals), axis = 1)\n",
    "    df['dep_num_dependents (lin)'] = df[['target', 'start', 'end', 'sentence']].apply(lambda vals : \\\n",
    "                                                dep_num_dependents(*vals), axis = 1)\n",
    "    df['dep_max_num_dependents (lin)'] = df.sentence.apply(lambda sentence : dep_max_num_dependents(sentence))\n",
    "    df['dep_num_dependents_norm (lin)'] = df['dep_num_dependents (lin)'] / df['dep_max_num_dependents (lin)']\n",
    "    df['dep_head_word_len (lin)'] = df[['target', 'start', 'end', 'sentence']].apply(lambda vals : \\\n",
    "                                                dep_head_word_len(*vals), axis = 1)\n",
    "    df = df.drop(drop_features, axis = 1)\n",
    "    return df\n",
    "    \n",
    "fc_linguistic = FeatureCategory('linguistic', features_linguistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features_linguistic(datasets, aggs = agg_default, drop_features = []):\n",
    "    return [FeatureDataset(ds.name, fc_linguistic, agg,\n",
    "                        fc_linguistic.func(ds.train, agg.agg, drop_features), \n",
    "                        fc_linguistic.func(ds.test, agg.agg, drop_features)) \n",
    "                        for ds in datasets\n",
    "                        for agg in aggs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3.3) Corpus-Based Features\n",
    "Here we compute features which are based on larger corpora. In this category we distinguish e.g. between frequency counts and N-Gram Language Model probabilites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (11,25) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import *\n",
    "from collections import defaultdict\n",
    "from wordmodel import Word\n",
    "import pandas as pd\n",
    "\n",
    "words_mrc_database = {}\n",
    "with open(\"resources/mrc-database/mrc2.dct\", encoding=\"utf8\") as file:\n",
    "    for index, line in enumerate(file):\n",
    "        line = line.strip()\n",
    "        word, phon, dphon, stress = line[51:].split('|')\n",
    "        w = Word(\n",
    "                wid = index,\n",
    "                nlet = int(line[0:2]),\n",
    "                nphon = int(line[2:4]),\n",
    "                nsyl = int(line[4]),\n",
    "                kf_freq = int(line[5:10]),\n",
    "                kf_ncats = int(line[10:12]),\n",
    "                kf_nsamp = int(line[12:15]),\n",
    "                tl_freq = int(line[15:21]),\n",
    "                brown_freq = int(line[21:25]),\n",
    "                fam = int(line[25:28]),\n",
    "                conc = int(line[28:31]),\n",
    "                imag = int(line[31:34]),\n",
    "                meanc = int(line[34:37]),\n",
    "                meanp = int(line[37:40]),\n",
    "                aoa = int(line[40:43]),\n",
    "                tq2 = line[43],\n",
    "                wtype = line[44],\n",
    "                pdwtype = line[45],\n",
    "                alphasyl = line[46],\n",
    "                status = line[47],\n",
    "                var = line[48],\n",
    "                cap = line[49],\n",
    "                irreg = line[50],\n",
    "                word=word,\n",
    "                phon=phon,\n",
    "                dphon=dphon,\n",
    "                stress=stress)\n",
    "        words_mrc_database[w.word.strip().lower()] = w\n",
    "\n",
    "def mrc_database(target, func, missing_val):\n",
    "    word = words_mrc_database.get(target.strip().lower())\n",
    "    val = func(word) if word else missing_val\n",
    "    return val if val != 0 else missing_val\n",
    "\n",
    "word_freq_wiki = {}\n",
    "freq_sum_wiki = 0\n",
    "with open(\"resources/word-freq-dumps/enwiki-20150602-words-frequency.txt\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        word, freq = line.partition(\" \")[::2]\n",
    "        word_freq_wiki[word.strip()] = int(freq)\n",
    "        freq_sum_wiki+=int(freq)\n",
    "        \n",
    "word_freq_simple_wiki = {}\n",
    "freq_sum_simple_wiki = 0\n",
    "with open(\"resources/word-freq-dumps/simple_wiki_word_freqs.txt\", encoding=\"ISO-8859-1\") as file:\n",
    "    for line in file:\n",
    "        word, freq = line.split()\n",
    "        word_freq_simple_wiki[word.strip()] = int(freq)\n",
    "        freq_sum_simple_wiki+=int(freq)\n",
    "        \n",
    "word_freq_lang8 = {}\n",
    "freq_sum_lang8 = 0\n",
    "with open(\"resources/word-freq-dumps/word_freqs_lang8.txt\", encoding=\"ISO-8859-1\") as file:\n",
    "    for line in file:\n",
    "        word, freq = line.split()\n",
    "        word_freq_lang8[word.strip()] = int(freq)\n",
    "        freq_sum_lang8+=int(freq)\n",
    "\n",
    "word_freq_bnc = {}\n",
    "with open(\"resources/word-freq-dumps/bnc_freq_all.al\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        freq, word, pos, num_files = line.split()\n",
    "        word_freq_bnc[word.strip()] = (int(freq), pos, int(num_files))\n",
    "\n",
    "word_freq_bnc_lemma = {}\n",
    "with open(\"resources/word-freq-dumps/bnc_lemma.al\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        sort_order, frequency, word, word_class = line.split()\n",
    "        word_freq_bnc_lemma[word.strip()] = (int(sort_order), word_class, int(frequency))\n",
    "\n",
    "        \n",
    "word_pknown_nobs_prev_freqZipf = {}\n",
    "with open(\"resources/word-freq-dumps/word_prevelance.csv\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        word, p_known, nobs, prevelance, freqZipf = line.split(\";\")\n",
    "        word_pknown_nobs_prev_freqZipf[word.strip()] = (float(p_known.replace(',','.')), \n",
    "                                                        float(nobs.replace(',','.')), \n",
    "                                                        float(prevelance.replace(',','.')), \n",
    "                                                        float(freqZipf.replace(',','.')))\n",
    "        \n",
    "subtlex_us = {}\n",
    "with open(\"resources/dictionaries/SUBTLEXus.txt\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        word, freq, cd_count, freq_low, cd_low, subtl_wf, lg10_wf, Subtlcd, lg10_cd = line.split('\\t')\n",
    "        subtlex_us[word.strip().lower()] = (int(freq), int(cd_count))\n",
    "        \n",
    "subtlex_uk = pd.read_csv(\"resources/dictionaries/SUBTLEXuk.txt\", sep = \"\\t\")\n",
    "subtlex_uk_dict = dict(zip(subtlex_uk['Spelling'], subtlex_uk['CD_count']))\n",
    "\n",
    "def get_dict_count(target, freqs):\n",
    "    return freqs.get(target.strip().lower(), 0)\n",
    "\n",
    "def freqZipf_func(target):\n",
    "    stats = word_pknown_nobs_prev_freqZipf.get(target)\n",
    "    return stats[3] if stats else 3.5\n",
    "\n",
    "\n",
    "WEIGHT_WIKI_SIMPLE_WIKI = freq_sum_wiki / freq_sum_simple_wiki\n",
    "WEIGHT_WIKI_LANG_8 = freq_sum_wiki / freq_sum_lang8\n",
    "\n",
    "def weighted_freq_ratio(target, word_freq_n, word_freq_m, weight):\n",
    "    freq_n = word_freq_n.get(target.strip().lower(), 1)\n",
    "    freq_m = word_freq_m.get(target.strip().lower(), 1)\n",
    "    return -1 + (2 * (freq_n / ((freq_m * weight) + freq_n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len ta_train : 11555\n",
      "Len ta_test : 2546\n",
      "Len targets : 14101\n"
     ]
    }
   ],
   "source": [
    "mwe_targets_train = list(set([mwe for ds in datasets for mwe in ds.train['target'].tolist()]))\n",
    "mwe_targets_test = list(set([mwe for ds in datasets for mwe in ds.test['target'].tolist()]))\n",
    "mwe_targets = mwe_targets_train.copy()\n",
    "mwe_targets.extend(mwe_targets_test)\n",
    "print('Len ta_train : {}'.format(len(mwe_targets_train)))\n",
    "print('Len ta_test : {}'.format(len(mwe_targets_test)))\n",
    "print('Len targets : {}'.format(len(mwe_targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phrasefinder as pf\n",
    "\n",
    "google_books_n_grams = {}\n",
    "options = pf.SearchOptions()\n",
    "options.topk = 10\n",
    "n_grams = mwe_targets\n",
    "\n",
    "with open('resources/word-freq-dumps/ngram_google.json', 'rb') as fp:\n",
    "    google_books_n_grams = pickle.load(fp)\n",
    "\n",
    "if not google_books_n_grams:\n",
    "    for index, n_gram in enumerate(n_grams):\n",
    "        try:\n",
    "            print(index, n_gram)\n",
    "            result = pf.search(pf.Corpus.AMERICAN_ENGLISH, n_gram, options)\n",
    "            vals = [(phrase.match_count, phrase.volume_count, phrase.first_year, phrase.last_year)\n",
    "                        for phrase in result.phrases]\n",
    "            mean_vals = [np.sum(elem) / len(elem) for elem in zip(*vals)]\n",
    "            google_books_n_grams[n_gram] = mean_vals\n",
    "            if result.status != pf.Status.Ok:\n",
    "                print('Request was not successful: {}'.format(result.status))\n",
    "        except Exception as error:\n",
    "            pass\n",
    "    with open('resources/word-freq-dumps/ngram_google.json', 'wb') as fp:\n",
    "        pickle.dump(google_books_n_grams, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_frequency(dataframe, agg, drop_features):   \n",
    "    df = dataframe.copy()\n",
    "    df['mrc_kf_freq (cor)'] = df.p_target.apply(lambda target : agg(target, mrc_database, \\\n",
    "                                                                    lambda word : word.kf_freq, 0))\n",
    "    df['mrc_kf_ncats (cor)'] = df.p_target.apply(lambda target : agg(target, mrc_database, \\\n",
    "                                                                    lambda word : word.kf_ncats, 0))\n",
    "    df['mrc_tl_freq (cor)'] = df.p_target.apply(lambda target : agg(target, mrc_database, \\\n",
    "                                                                    lambda word : word.tl_freq, 0))\n",
    "    df['mrc_brown_freq (cor)'] = df.p_target.apply(lambda target : agg(target, mrc_database, \\\n",
    "                                                                    lambda word : word.brown_freq, 0))\n",
    "    df['freq_wiki (cor)'] = df.p_target.apply(lambda target : agg(target, get_dict_count, word_freq_wiki))\n",
    "    df['log_freq_wiki (cor)'] = df['freq_wiki (cor)'].apply(lambda freq : np.log(freq))\n",
    "    df['freq_simple_wiki (cor)'] = df.p_target.apply(lambda target : agg(target, get_dict_count, word_freq_simple_wiki))\n",
    "    df['log_freq_simple_wiki (cor)'] = df['freq_simple_wiki (cor)'].apply(lambda freq : np.log(freq))\n",
    "    df['freq_bnc (cor)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                    lambda target : word_freq_bnc.get(target)[0] if word_freq_bnc.get(target) else 0))\n",
    "    df['log_freq_bnc (cor)'] = df['freq_bnc (cor)'].apply(lambda freq : np.log(freq))\n",
    "    df['freq_bnc_lemma (cor)'] = df.p_lemma.apply(lambda target : agg(target, \\\n",
    "                    lambda target : word_freq_bnc_lemma.get(target)[2] \\\n",
    "                                         if word_freq_bnc_lemma.get(target) else 0))\n",
    "    df['log_freq_bnc_lemma (cor)'] = df['freq_bnc_lemma (cor)'].apply(lambda freq : np.log(freq))\n",
    "    df['freqZipf (cor)'] = df.p_target.apply(lambda target : agg(target, freqZipf_func))\n",
    "    df['google_books_n_gram_freq (cor)'] = df.p_target.apply(lambda target : google_books_n_grams.get(target)[0] \\\n",
    "                                                     if google_books_n_grams.get(target) else 0)\n",
    "    df['log_google_books_n_gram_freq (cor)'] = df['google_books_n_gram_freq (cor)'].apply(lambda freq : np.log(freq))\n",
    "    df['google_books_n_gram_doc_freq (cor)'] = df.p_target.apply(lambda target : google_books_n_grams.get(target)[1] \\\n",
    "                                                        if google_books_n_grams.get(target)  else 0)\n",
    "    df['log_google_books_n_gram_doc_freq (cor)'] = df['google_books_n_gram_doc_freq (cor)'].apply(lambda freq : np.log(freq))\n",
    "    df['google_books_n_gram_first_year (cor)'] = df.p_target.apply(lambda target : google_books_n_grams.get(target)[2] \\\n",
    "                                                          if google_books_n_grams.get(target) else 1900)\n",
    "    df['google_books_n_gram_last_year (cor)'] = df.p_target.apply(lambda target : google_books_n_grams.get(target)[3] \\\n",
    "                                                         if google_books_n_grams.get(target)  else 1900)\n",
    "    df['subtlex_cd_us (cor)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                    lambda target : subtlex_us[target.strip().lower()][1] if subtlex_us.get(target.strip().lower()) else 0))\n",
    "    df['subtlex_cd_uk (cor)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                    lambda target : subtlex_uk_dict.get(target, 0)))\n",
    "    df['weighted_wiki_simple_wiki_ratio (cor)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                    weighted_freq_ratio, word_freq_wiki, word_freq_simple_wiki, WEIGHT_WIKI_SIMPLE_WIKI))\n",
    "    df['weighted_wiki_lang8_ratio (cor)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                    weighted_freq_ratio, word_freq_wiki, word_freq_lang8, WEIGHT_WIKI_SIMPLE_WIKI))\n",
    "    df = df.drop(drop_features, axis = 1)\n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "fc_frequency = FeatureCategory('frequency', features_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features_frequency(datasets, aggs = agg_default, drop_features = []):\n",
    "    return [FeatureDataset(ds.name, fc_frequency, agg,\n",
    "                        fc_frequency.func(ds.train, agg.agg, drop_features), \n",
    "                        fc_frequency.func(ds.test, agg.agg, drop_features)) \n",
    "                        for ds in datasets\n",
    "                        for agg in aggs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Language Model\n",
    "Here we load the different Kneser-Ney n-gram models we trained previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('resources/language-models/ngram_char_1.json', 'rb') as fp:\n",
    "    ngram_char_1 = pickle.load(fp)\n",
    "    \n",
    "with open('resources/language-models/ngram_word_1.json', 'rb') as fp:\n",
    "    ngram_word_1 = pickle.load(fp)\n",
    "\n",
    "with open('resources/language-models/ngram_char_2.json', 'rb') as fp:\n",
    "    ngram_char_2 = pickle.load(fp)\n",
    "    \n",
    "with open('resources/language-models/ngram_word_2.json', 'rb') as fp:\n",
    "    ngram_word_2 = pickle.load(fp)\n",
    "    \n",
    "with open('resources/language-models/ngram_char_3.json', 'rb') as fp:\n",
    "    ngram_char_3 = pickle.load(fp)\n",
    "    \n",
    "with open('resources/language-models/ngram_char_2_complex.json', 'rb') as fp:\n",
    "    ngram_char_2_complex = pickle.load(fp)\n",
    "    \n",
    "with open('resources/language-models/ngram_char_2_non_complex.json', 'rb') as fp:\n",
    "    ngram_char_2_non_complex = pickle.load(fp)\n",
    "    \n",
    "with open('resources/language-models/ngram_char_2_complex_cleaned.json', 'rb') as fp:\n",
    "    ngram_char_2_complex_cleaned = pickle.load(fp)\n",
    "    \n",
    "with open('resources/language-models/ngram_char_2_non_complex_cleaned.json', 'rb') as fp:\n",
    "    ngram_char_2_non_complex_cleaned = pickle.load(fp)\n",
    "    \n",
    "def kneser_ney_word_uni_gram(target):\n",
    "    return ngram_word_1.cond_prob(target)\n",
    "\n",
    "def kneser_ney_word_bi_gram(target):\n",
    "    words = target.split()\n",
    "    if len(words) <= 1:\n",
    "        return ngram_word_2.cond_prob(target)\n",
    "    return np.mean([ngram_word_2.cond_prob(words[index+1], (word,)) \n",
    "                for index, word in enumerate(words) \n",
    "                if index <= len(words)-2])\n",
    "    \n",
    "def kneser_ney_char_uni_gram_avg(target):\n",
    "    return np.mean([ngram_char_1.cond_prob(character) \n",
    "            for character in target])\n",
    "\n",
    "def kneser_ney_char_bi_gram_avg(target):\n",
    "    return np.mean([ngram_char_2.cond_prob(target[index+1], (character,)) \n",
    "            for index, character in enumerate(target) if index <= len(target)-2])\n",
    "\n",
    "def kneser_ney_char_bi_gram_avg_model(target, kn_model):\n",
    "    return np.mean([kn_model.cond_prob(target[index+1], (character,)) \n",
    "            for index, character in enumerate(target) if index <= len(target)-2])\n",
    "\n",
    "def kneser_ney_char_tri_gram_avg(target):\n",
    "    return np.mean([ngram_char_3.cond_prob(target[index+2], (character, target[index+1])) \n",
    "            for index, character in enumerate(target) if index <= len(target)-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import *\n",
    "wordNetLemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def features_language_model(dataframe, agg, drop_features):\n",
    "    df = dataframe.copy()\n",
    "    df['kneser_ney_word_uni_gram (cor)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                                    kneser_ney_word_uni_gram))\n",
    "    df['kneser_ney_word_bi_gram (cor)'] = df.p_target.apply(lambda target :  kneser_ney_word_bi_gram(target))\n",
    "    df['kneser_ney_char_uni_gram_avg (cor)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                                    kneser_ney_char_uni_gram_avg))\n",
    "    df['kneser_ney_char_bi_gram_avg (cor)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                                    kneser_ney_char_bi_gram_avg))\n",
    "    df['kneser_ney_char_tri_gram_avg (cor)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                                    kneser_ney_char_tri_gram_avg))\n",
    "    df['kneser_ney_char_bi_complex (cor)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                                    kneser_ney_char_bi_gram_avg_model, ngram_char_2_complex))\n",
    "    df['kneser_ney_char_bi_non_complex (cor)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                                    kneser_ney_char_bi_gram_avg_model, ngram_char_2_non_complex))\n",
    "    df['kneser_ney_char_bi_c_nc_ratio (cor)'] = df['kneser_ney_char_bi_complex (cor)'] / df['kneser_ney_char_bi_non_complex (cor)']\n",
    "    df['kneser_ney_char_bi_complex_cl (cor)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                                    kneser_ney_char_bi_gram_avg_model, ngram_char_2_complex_cleaned))\n",
    "    df['kneser_ney_char_bi_non_complex_cl (cor)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                                    kneser_ney_char_bi_gram_avg_model, ngram_char_2_non_complex_cleaned))\n",
    "    df['kneser_ney_char_bi_c_nc_ratio_cl (cor)'] = df['kneser_ney_char_bi_complex_cl (cor)'] / df['kneser_ney_char_bi_non_complex_cl (cor)']\n",
    "    df = df.fillna(0)\n",
    "    df = df.drop(drop_features, axis = 1)\n",
    "    return df\n",
    "\n",
    "fc_language_model = FeatureCategory('language_model', features_language_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features_language_model(datasets, aggs = agg_default, drop_features = []):\n",
    "    return [FeatureDataset(ds.name, fc_language_model, agg,\n",
    "                        fc_language_model.func(ds.train, agg.agg, drop_features), \n",
    "                        fc_language_model.func(ds.test, agg.agg, drop_features)) \n",
    "                        for ds in datasets\n",
    "                        for agg in aggs]\n",
    "\n",
    "fc_corpus = FeatureCategory('corpus', [fc_frequency, fc_language_model])\n",
    "\n",
    "def compute_features_corpus(datasets):\n",
    "    return [FeatureDataset(ds.name, fc_corpus,  ds.agg,\n",
    "            ds.train, ds.test) for ds in concat_feature_datasets(*datasets)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3.4) Psycholinguistic Features based on MRC Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordmodel import Word\n",
    "\n",
    "words_mrc_database = {}\n",
    "with open(\"resources/mrc-database/mrc2.dct\", encoding=\"utf8\") as file:\n",
    "    for index, line in enumerate(file):\n",
    "        line = line.strip()\n",
    "        word, phon, dphon, stress = line[51:].split('|')\n",
    "        w = Word(\n",
    "                wid = index,\n",
    "                nlet = int(line[0:2]),\n",
    "                nphon = int(line[2:4]),\n",
    "                nsyl = int(line[4]),\n",
    "                kf_freq = int(line[5:10]),\n",
    "                kf_ncats = int(line[10:12]),\n",
    "                kf_nsamp = int(line[12:15]),\n",
    "                tl_freq = int(line[15:21]),\n",
    "                brown_freq = int(line[21:25]),\n",
    "                fam = int(line[25:28]),\n",
    "                conc = int(line[28:31]),\n",
    "                imag = int(line[31:34]),\n",
    "                meanc = int(line[34:37]),\n",
    "                meanp = int(line[37:40]),\n",
    "                aoa = int(line[40:43]),\n",
    "                tq2 = line[43],\n",
    "                wtype = line[44],\n",
    "                pdwtype = line[45],\n",
    "                alphasyl = line[46],\n",
    "                status = line[47],\n",
    "                var = line[48],\n",
    "                cap = line[49],\n",
    "                irreg = line[50],\n",
    "                word=word,\n",
    "                phon=phon,\n",
    "                dphon=dphon,\n",
    "                stress=stress)\n",
    "        words_mrc_database[w.word.strip().lower()] = w\n",
    "\n",
    "def mrc_database(target, func, missing_val):\n",
    "    word = words_mrc_database.get(target.strip().lower())\n",
    "    val = func(word) if word else missing_val\n",
    "    return val if val != 0 else missing_val\n",
    "\n",
    "word_concreteness = {}\n",
    "with open(\"resources/word-freq-dumps/concreteness_brysbaert_et_al.txt\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        word, bigram, conc_m, conc_sd, \\\n",
    "        unknown, total, percent_known, \\\n",
    "        subtlex, dom_pos = line.split('\\t')\n",
    "        word_concreteness[word.strip()] = float(conc_m)\n",
    "        \n",
    "word_age_of_aquisition = {}\n",
    "with open(\"resources/word-freq-dumps/AoA_ratings_Kuperman_et_al_BRM.csv\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        word, occur_total, occur_num, freq_pm, rating_Mean, rating_SD, dunno = line.split()\n",
    "        word_age_of_aquisition[word.strip()] = float(rating_Mean.replace(',', '.')) if rating_Mean != 'NA' else 0\n",
    "\n",
    "word_pknown_nobs_prev_freqZipf = {}\n",
    "with open(\"resources/word-freq-dumps/word_prevelance.csv\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        word, p_known, nobs, prevelance, freqZipf = line.split(\";\")\n",
    "        word_pknown_nobs_prev_freqZipf[word.strip()] = (float(p_known.replace(',','.')), \n",
    "                                                        float(nobs.replace(',','.')), \n",
    "                                                        float(prevelance.replace(',','.')), \n",
    "                                                        float(freqZipf.replace(',','.')))\n",
    "\n",
    "def perc_known_func(target, missing_value):\n",
    "    stats = word_pknown_nobs_prev_freqZipf.get(target)\n",
    "    return stats[0] if stats else missing_value\n",
    "\n",
    "def nobs_func(target):\n",
    "    stats = word_pknown_nobs_prev_freqZipf.get(target)\n",
    "    return stats[1] if stats else 0\n",
    "\n",
    "def prevelance_func(target):\n",
    "    stats = word_pknown_nobs_prev_freqZipf.get(target)\n",
    "    return stats[2] if stats else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_psycholingusitic(dataframe, agg, drop_features):\n",
    "    df = dataframe.copy()\n",
    "    df['mrc_fam (psy)'] = df.p_target.apply(lambda target : agg(target, mrc_database, lambda word : word.fam, 400))\n",
    "    df['mrc_conc (psy)'] = df.p_target.apply(lambda target : agg(target, mrc_database, lambda word : word.conc, 400))\n",
    "    df['mrc_imag (psy)'] = df.p_target.apply(lambda target : agg(target, mrc_database, lambda word : word.imag, 400))\n",
    "    df['mrc_meanc (psy)'] = df.p_target.apply(lambda target : agg(target, mrc_database, lambda word : word.meanc, 400))\n",
    "    df['mrc_meanp (psy)'] = df.p_target.apply(lambda target : agg(target, mrc_database, lambda word : word.meanp, 400))\n",
    "    df['mrc_aoa (psy)'] = df.p_target.apply(lambda target : agg(target, mrc_database, lambda word : word.aoa, 3.5))\n",
    "    df['perc_known (psy)'] = df.p_target.apply(lambda target : agg(target, perc_known_func, 0.5))\n",
    "    df['nobs (psy)'] = df.p_target.apply(lambda target : agg(target, nobs_func))\n",
    "    df['prevelance (psy)'] = df.p_target.apply(lambda target : agg(target, prevelance_func))\n",
    "    df['concreteness (psy)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                                lambda target : word_concreteness.get(target, 2.5)))\n",
    "    df['age_of_aquisition (psy)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                                    lambda target : word_age_of_aquisition.get(target, 8.5)))\n",
    "    df = df.drop(drop_features, axis = 1)\n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "fc_psycholinguistic = FeatureCategory('psycholinguistic', features_psycholingusitic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features_psycholinguistic(datasets, aggs = agg_default, drop_features = []):\n",
    "    return [FeatureDataset(ds.name, fc_psycholinguistic, agg,\n",
    "                        fc_psycholinguistic.func(ds.train, agg.agg, drop_features), \n",
    "                        fc_psycholinguistic.func(ds.test, agg.agg, drop_features)) \n",
    "                        for ds in datasets\n",
    "                        for agg in aggs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3.5) Semantic Features\n",
    "Here we implement all the relevant features based on WordNet and SentiWordNet. For example, the number of synsets the target word is contained in or the average length of the lemmas of all the synsets the target word is contained in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.5.1) WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.wsd import lesk\n",
    "from nltk.stem.wordnet import *\n",
    "from utils import penn_to_wn\n",
    "\n",
    "wordNetLemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def wn_synset_freq(target):\n",
    "    return len(wn.synsets(target))\n",
    "\n",
    "def wn_synset_avg_lemma_freq(target):\n",
    "    return np.nan_to_num(np.mean([len(synset.lemmas()) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_lemma_len(target):\n",
    "    return np.nan_to_num(np.nanmean([len(lemma.name()) \n",
    "            for synset in wn.synsets(target) \n",
    "            for lemma in synset.lemmas()]))\n",
    "\n",
    "def wn_synset_avg_hypernyms(target):\n",
    "    return np.nan_to_num(np.nanmean([len(synset.hypernyms()) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_hyponyms(target):\n",
    "    return np.nan_to_num(np.mean([len(synset.hyponyms()) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_sum_hypernyms(target):\n",
    "    return np.sum(([len(synset.hypernyms()) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_definition_len(target):\n",
    "    return np.nan_to_num(np.mean([len(str(synset.definition())) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_hyptree_depth(target):\n",
    "    return np.nan_to_num(np.mean([synset.max_depth() \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_num_distinct_pos(target):\n",
    "    return len(set([synset.pos() for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_num_relations(target):\n",
    "    return np.nan_to_num(np.mean([np.sum([len(synset.hypernyms()), len(synset.hyponyms()), \n",
    "             len(synset.instance_hypernyms()), len(synset.instance_hyponyms()),\n",
    "             len(synset.member_holonyms()), len(synset.substance_holonyms()),\n",
    "             len(synset.part_holonyms()), len(synset.member_meronyms()),\n",
    "             len(synset.substance_meronyms()), len(synset.part_meronyms())]) \n",
    "             for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_freq_pos(target, pos):\n",
    "    return len(wn.synsets(target, pos = pos))\n",
    "\n",
    "def wn_synset_pos_ratio_1(target, pos):\n",
    "    tokens = word_tokenize(target)\n",
    "    ratios = []\n",
    "    for token, poss in zip(tokens, pos):\n",
    "        synsets_freqs = len(wn.synsets(token))\n",
    "        ratios.append(len(wn.synsets(token, pos = poss)) / synsets_freqs \\\n",
    "                if synsets_freqs != 0 else 0.25)\n",
    "    return np.mean(ratios)\n",
    "\n",
    "def wn_synset_pos_ratio_2(target, pos):\n",
    "    tokens = word_tokenize(target)\n",
    "    ratios = []\n",
    "    for token, poss in zip(tokens, pos):\n",
    "        synsets_counts = np.sum([lemma.count() \n",
    "                for sn in wn.synsets(token) for lemma in sn.lemmas()])\n",
    "        ratios.append(np.sum([lemma.count() for sn in wn.synsets(token, pos = poss) \n",
    "                    for lemma in sn.lemmas()]) / synsets_counts if synsets_counts != 0 else 0.25)\n",
    "    return np.mean(ratios)\n",
    "\n",
    "def wn_synset_sense_entropy_uniform(target):\n",
    "    num_senses = len(wn.synsets(target))\n",
    "    return -np.sum([((1 / num_senses) * np.log2(1 / num_senses)) \n",
    "                     for index in range(0, num_senses)])\n",
    "\n",
    "def wn_synset_sense_entropy_pos_uniform(target):\n",
    "    num_senses = len(wn.synsets(target))\n",
    "    pos_distribution = [len(wn.synsets(target, pos = wn.NOUN)),\n",
    "                        len(wn.synsets(target, pos = wn.VERB)),\n",
    "                        len(wn.synsets(target, pos = wn.ADJ)),\n",
    "                        len(wn.synsets(target, pos = wn.ADV))]\n",
    "    return -np.sum([(np.nan_to_num((count / num_senses) * np.log2(count / num_senses))) \n",
    "            for count in pos_distribution]) if num_senses != 0 else 0\n",
    "\n",
    "def wn_synsets_sense_entropy_pos_central(target, pos):\n",
    "    num_senses_pos = len(wn.synsets(target, pos = pos))\n",
    "    return -np.sum([((1 / num_senses_pos) * np.log2(1 / num_senses_pos))\n",
    "                     for index in range(0, num_senses_pos)])\n",
    "\n",
    "def wn_synset_pos_probability_1(target, pos):\n",
    "    synsets = wn.synsets(target)\n",
    "    syn_freq_other_pos = np.sum([1 for synset in synsets if synset.pos() != pos])\n",
    "    return len(wn.synsets(target, pos = pos)) / syn_freq_other_pos\n",
    "\n",
    "def wn_synsets_avg_lemma_freq(target, freqs_func, freqs):\n",
    "    synsets = wn.synsets(target)\n",
    "    if not synsets:\n",
    "        return 0\n",
    "    return np.mean([np.nan_to_num(freqs_func(lemma.name(), freqs)) for synset in synsets\n",
    "                    for lemma in synset.lemmas()])\n",
    "\n",
    "def wn_synsets_freq_ratio_to_max_agg_min(target, freqs_func, freqs):\n",
    "    lemmas = [lemma.name().split('_') for synset in wn.synsets(target) \n",
    "                  for lemma in synset.lemmas()]\n",
    "    if not lemmas:\n",
    "        return 1\n",
    "    freqis = [np.min([freqs_func(lemma, freqs) for lemma in lemmata]) \n",
    "              for lemmata in lemmas]\n",
    "    target_freq = freqs_func(target, freqs)\n",
    "    if target_freq not in freqis: freqis.append(target_freq)\n",
    "    max_freq = np.max(freqis)\n",
    "    return target_freq / max_freq\n",
    "\n",
    "def wn_synsets_freq_ratio_to_max_agg_mean(target, freqs_func, freqs):\n",
    "    lemmas = [lemma.name().split('_') for synset in wn.synsets(target) \n",
    "                  for lemma in synset.lemmas()]\n",
    "    if not lemmas:\n",
    "        return 1\n",
    "    freqis = [np.mean([freqs_func(lemma, freqs) for lemma in lemmata]) \n",
    "              for lemmata in lemmas]\n",
    "    target_freq = freqs_func(target, freqs)\n",
    "    if target_freq not in freqis: freqis.append(target_freq)\n",
    "    max_freq = np.max(freqis)\n",
    "    return target_freq / max_freq\n",
    "\n",
    "def wn_synsets_freq_ratio_to_max_agg_median(target, freqs_func, freqs):\n",
    "    lemmas = [lemma.name().split('_') for synset in wn.synsets(target) \n",
    "                  for lemma in synset.lemmas()]\n",
    "    if not lemmas:\n",
    "        return 1\n",
    "    freqis = [np.median([freqs_func(lemma, freqs) for lemma in lemmata]) \n",
    "              for lemmata in lemmas]\n",
    "    target_freq = freqs_func(target, freqs)\n",
    "    if target_freq not in freqis: freqis.append(target_freq)\n",
    "    max_freq = np.max(freqis)\n",
    "    return target_freq / max_freq\n",
    "    \n",
    "def swn_avg_objective_score(target):\n",
    "    return np.nan_to_num(np.mean([senti_synset.obj_score() \n",
    "                for senti_synset in swn.senti_synsets(target)]))\n",
    "\n",
    "def wn_synset_lesk_wsd_ratio_hi_freq(target, sentence, freqs_func, freqs, pos):\n",
    "    wsd_synset = lesk(sentence.split(), target, pos)\n",
    "    if not wsd_synset:\n",
    "        return 0\n",
    "    lemmas = [lemma.name().split('_') for lemma in wsd_synset.lemmas()]\n",
    "    if not lemmas:\n",
    "        return 0\n",
    "    freqis = [np.min([freqs_func(lemma, freqs) for lemma in lemmata])\n",
    "              for lemmata in lemmas]\n",
    "    target_freq = freqs_func(target, freqs)\n",
    "    return np.sum([1 for freq in freqis if freq > target_freq]) / len(freqis)\n",
    "\n",
    "def wn_synset_lesk_wsd_ratio_hi_freq_sum(target, sentence, freqs_func, freqs, pos):\n",
    "    wsd_synset = lesk(sentence.split(), target, pos)\n",
    "    if not wsd_synset:\n",
    "        return 0\n",
    "    lemmas = [lemma.name().split('_') for lemma in wsd_synset.lemmas()]\n",
    "    if not lemmas:\n",
    "        return 0\n",
    "    freqis = [np.min([freqs_func(lemma, freqs) for lemma in lemmata])\n",
    "              for lemmata in lemmas]\n",
    "    target_freq = freqs_func(target, freqs)\n",
    "    return np.sum([freq for freq in freqis if freq > target_freq]) / np.sum(freqis)\n",
    "\n",
    "def wn_synset_lesk_wsd_ratio_hi_nopos_freq(target, sentence, freqs_func, freqs):\n",
    "    wsd_synset = lesk(sentence.split(), target)\n",
    "    if not wsd_synset:\n",
    "        return 0\n",
    "    lemmas = [lemma.name().split('_') for lemma in wsd_synset.lemmas()]\n",
    "    if not lemmas:\n",
    "        return 0\n",
    "    freqis = [np.min([freqs_func(lemma, freqs) for lemma in lemmata])\n",
    "              for lemmata in lemmas]\n",
    "    target_freq = freqs_func(target, freqs)\n",
    "    return np.sum([1 for freq in freqis if freq > target_freq]) / len(freqis)\n",
    "\n",
    "def wn_synset_lesk_wsd_ratio_low_freq(target, sentence, freqs_func, freqs, pos):\n",
    "    wsd_synset = lesk(sentence.split(), target, pos)\n",
    "    if not wsd_synset:\n",
    "        return 0\n",
    "    lemmas = [lemma.name().split('_') for lemma in wsd_synset.lemmas()]\n",
    "    if not lemmas:\n",
    "        return 0\n",
    "    freqis = [np.min([freqs_func(lemma, freqs) for lemma in lemmata])\n",
    "              for lemmata in lemmas]\n",
    "    target_freq = freqs_func(target, freqs)\n",
    "    return np.sum([1 for freq in freqis if freq < target_freq]) / len(freqis)\n",
    "\n",
    "def wn_synset_lesk_wsd_ratio_low_freq_sum(target, sentence, freqs_func, freqs, pos):\n",
    "    wsd_synset = lesk(sentence.split(), target, pos)\n",
    "    if not wsd_synset:\n",
    "        return 0\n",
    "    lemmas = [lemma.name().split('_') for lemma in wsd_synset.lemmas()]\n",
    "    if not lemmas:\n",
    "        return 0\n",
    "    freqis = [np.min([freqs_func(lemma, freqs) for lemma in lemmata])\n",
    "              for lemmata in lemmas]\n",
    "    target_freq = freqs_func(target, freqs)\n",
    "    return np.sum([freq for freq in freqis if freq < target_freq]) / np.sum(freqis)\n",
    "\n",
    "def wn_synset_lesk_wsd_ratio_low_nopos_freq(target, sentence, freqs_func, freqs):\n",
    "    wsd_synset = lesk(sentence.split(), target)\n",
    "    if not wsd_synset:\n",
    "        return 0\n",
    "    lemmas = [lemma.name().split('_') for lemma in wsd_synset.lemmas()]\n",
    "    if not lemmas:\n",
    "        return 0\n",
    "    freqis = [np.min([freqs_func(lemma, freqs) for lemma in lemmata])\n",
    "              for lemmata in lemmas]\n",
    "    target_freq = freqs_func(target, freqs)\n",
    "    return np.sum([1 for freq in freqis if freq < target_freq]) / len(freqis)\n",
    "\n",
    "def wn_synset_lesk_wsd_ratio_to_freq_sum(target, sentence, freqs_func, freqs, pos):\n",
    "    wsd_synset = lesk(sentence.split(), target, pos)\n",
    "    if not wsd_synset:\n",
    "        return 0\n",
    "    lemmas = [lemma.name().split('_') for lemma in wsd_synset.lemmas()]\n",
    "    if not lemmas:\n",
    "        return 0\n",
    "    freqis = [np.min([freqs_func(lemma, freqs) for lemma in lemmata])\n",
    "              for lemmata in lemmas]\n",
    "    target_freq = freqs_func(target, freqs)\n",
    "    return target_freq / np.sum(freqis)\n",
    "\n",
    "def wn_synset_lesk_wsd__norm_sense_rank(target, sentence, freqs_func, freqs, wsd_func, pos):\n",
    "    wsd_synset = wsd_func(sentence.split(), target, pos)\n",
    "    senses = wn.synsets(target)\n",
    "    if not wsd_synset:\n",
    "        return 0\n",
    "    wsd_synset = lesk(sentence.split(), target, pos)\n",
    "    sense_freqs = sorted([(sense, np.sum([lemma.count() for lemma in sense.lemmas()])) \n",
    "                   for sense in senses], key = lambda tpl : tpl[1], reverse=True)\n",
    "    sense_index = [sense for sense, cnt in sense_freqs].index(wsd_synset)\n",
    "    return sense_index / len(senses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warming up PyWSD (takes ~10 secs)... took 15.818924188613892 secs.\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "from pywsd.lesk import adapted_lesk\n",
    "\n",
    "def features_wordnet(dataframe, agg, drop_features):\n",
    "    df = dataframe.copy()\n",
    "    df['wn_synset_freq (sem)'] = df.p_target.apply(lambda target : agg(target, wn_synset_freq))\n",
    "    df['wn_synset_avg_lemma_freq (sem)'] = df.p_target.apply(lambda target : agg(target, wn_synset_avg_lemma_freq))\n",
    "    df['wn_synset_avg_lemma_len (sem)'] = df.p_target.apply(lambda target : agg(target, wn_synset_avg_lemma_len))\n",
    "    \n",
    "    df['length'] = df.target.apply(lambda target : agg(target, len))\n",
    "    df['wn_synset_diff_len_avg_lemma_len (sem)'] = df['wn_synset_avg_lemma_len (sem)'] - df.length\n",
    "    df['wn_synset_avg_hypernyms (sem)'] = df.p_target.apply(lambda target : agg(target, wn_synset_avg_hypernyms))\n",
    "    df['wn_synset_sum_hypernyms (sem)'] = df.p_target.apply(lambda target : agg(target, wn_synset_sum_hypernyms))\n",
    "    df['wn_synset_avg_hyponyms (sem)'] = df.p_target.apply(lambda target : agg(target, wn_synset_avg_hyponyms))\n",
    "\n",
    "    df['wn_synset_avg_definition_len (sem)'] = df.p_target.apply(lambda target : \n",
    "                                                         agg(target, wn_synset_avg_definition_len))\n",
    "    df['wn_synset_avg_hyptree_depth (sem)'] = df.p_target.apply(lambda target :\n",
    "                                                         agg(target, wn_synset_avg_hyptree_depth))\n",
    "    df['wn_synset_num_distinct_pos (sem)'] = df.p_target.apply(lambda target : \n",
    "                                                         agg(target, wn_synset_num_distinct_pos))\n",
    "    df['wn_synset_avg_num_relations (sem)'] = df.p_target.apply(lambda target : \n",
    "                                                         agg(target, wn_synset_avg_num_relations))\n",
    "\n",
    "    df['wn_synset_avg_freq_pos_noun (sem)'] = df.p_target.apply(lambda target : \n",
    "                                                        agg(target, wn_synset_avg_freq_pos, wn.NOUN))\n",
    "    df['wn_synset_avg_freq_pos_verb (sem)'] = df.p_target.apply(lambda target : \n",
    "                                                        agg(target, wn_synset_avg_freq_pos, wn.VERB))\n",
    "    df['wn_synset_avg_freq_pos_adj (sem)'] = df.p_target.apply(lambda target : \n",
    "                                                       agg(target, wn_synset_avg_freq_pos, wn.ADJ))\n",
    "    df['wn_synset_avg_freq_pos_adv (sem)'] = df.p_target.apply(lambda target : \n",
    "                                                       agg(target, wn_synset_avg_freq_pos, wn.ADV))\n",
    "\n",
    "    df['wn_synset_avg_freq_pos_noun_norm (sem)'] = np.nan_to_num(df['wn_synset_avg_freq_pos_noun (sem)'] / \\\n",
    "                                                                 df['wn_synset_freq (sem)'])\n",
    "    df['wn_synset_avg_freq_pos_verb_norm (sem)'] = np.nan_to_num(df['wn_synset_avg_freq_pos_verb (sem)'] / \\\n",
    "                                                                 df['wn_synset_freq (sem)'])\n",
    "    df['wn_synset_avg_freq_pos_adj_norm (sem)'] = np.nan_to_num(df['wn_synset_avg_freq_pos_adj (sem)'] / \\\n",
    "                                                                df['wn_synset_freq (sem)'])\n",
    "    df['wn_synset_avg_freq_pos_adv_norm (sem)'] = np.nan_to_num(df['wn_synset_avg_freq_pos_adv (sem)'] / \\\n",
    "                                                                df['wn_synset_freq (sem)'])\n",
    "\n",
    "    df['wn_synset_sense_entropy_uniform (sem)'] = df.p_target.apply(lambda target : \n",
    "                                            agg(target, wn_synset_sense_entropy_uniform))\n",
    "    df['wn_synset_sense_entropy_pos_uniform (sem)'] = df.p_target.apply(lambda target :\n",
    "                                            agg(target, wn_synset_sense_entropy_pos_uniform))\n",
    "    df['wn_synsets_sense_entropy_pos_central (sem)'] = df[['p_target', 'pos_tags_pt']].apply(\n",
    "        lambda vals : wn_synsets_sense_entropy_pos_central(vals[0], vals[1]), axis = 1)\n",
    "    \n",
    "    df['wn_synset_pos_ratio_1 (sem)'] = df[['p_target', 'pos_tags_pt']].apply(\n",
    "                    lambda vals : wn_synset_pos_ratio_1(vals[0], vals[1]), axis = 1)\n",
    "    \n",
    "    df['wn_synset_pos_ratio_2 (sem)'] = df[['p_target', 'pos_tags_pt']].apply(\n",
    "                    lambda vals : wn_synset_pos_ratio_2(vals[0], vals[1]), axis = 1)\n",
    "\n",
    "    df['swn_avg_objective_score (sem)'] = df.p_target.apply(lambda target : agg(target, swn_avg_objective_score))\n",
    "\n",
    "    df['wn_synsets_freq_ratio_to_max_agg_min (sem)'] = df.p_target.apply(lambda target : \\\n",
    "                                                    agg(target, wn_synsets_freq_ratio_to_max_agg_min, \\\n",
    "                                                                         get_dict_count, word_freq_wiki))\n",
    "    df['wn_synsets_freq_ratio_to_max_agg_mean (sem)'] = df.p_target.apply(lambda target : \\\n",
    "                                                    agg(target, wn_synsets_freq_ratio_to_max_agg_mean, \\\n",
    "                                                                         get_dict_count, word_freq_wiki))\n",
    "    df['wn_synsets_freq_ratio_to_max_agg_median (sem)'] = df.p_target.apply(lambda target : \\\n",
    "                                                    agg(target, wn_synsets_freq_ratio_to_max_agg_median, \\\n",
    "                                                                         get_dict_count, word_freq_wiki))\n",
    "    df['wn_synsets_avg_lemma_freq (sem)'] = df.p_target.apply(lambda target : \\\n",
    "                                                    agg(target, wn_synsets_avg_lemma_freq, \\\n",
    "                                                                         get_dict_count, word_freq_wiki))\n",
    "    df['freq_wiki'] = df.p_target.apply(lambda target : agg(target, get_dict_count, word_freq_wiki))\n",
    "    df['wn_synsets_freq_ratio_to_avg (sem)'] = df['wn_synsets_avg_lemma_freq (sem)'] / df.freq_wiki\n",
    "    df['wn_synset_lesk_wsd_ratio_hi_freq (sem)'] = df[['p_target','sentence', 'pos_tags_pt']].apply(lambda vals : \\\n",
    "                agg(vals[0], wn_synset_lesk_wsd_ratio_hi_freq, vals[1], \\\n",
    "                                     get_dict_count, word_freq_wiki, pos=vals[2]), axis = 1)\n",
    "    df['wn_synset_lesk_wsd_ratio_low_freq (sem)'] = df[['p_target','sentence', 'pos_tags_pt']].apply(lambda vals : \\\n",
    "                agg(vals[0], wn_synset_lesk_wsd_ratio_low_freq, vals[1], \\\n",
    "                                     get_dict_count, word_freq_wiki, pos=vals[2]), axis = 1)\n",
    "    df['wn_synset_lesk_wsd_ratio_hi_nopos_freq (sem)'] = df[['p_target','sentence']].apply(lambda vals : \\\n",
    "                agg(vals[0], wn_synset_lesk_wsd_ratio_hi_nopos_freq, vals[1], \\\n",
    "                                     get_dict_count, word_freq_wiki), axis = 1)\n",
    "    df['wn_synset_lesk_wsd_ratio_low_nopos_freq (sem)'] = df[['p_target','sentence']].apply(lambda vals : \\\n",
    "                agg(vals[0], wn_synset_lesk_wsd_ratio_low_nopos_freq, vals[1], \\\n",
    "                                     get_dict_count, word_freq_wiki), axis = 1)\n",
    "    df['wn_synset_lesk_wsd_ratio_hi_freq_sum (sem)'] = df[['p_target','sentence', 'pos_tags_pt']].apply(lambda vals : \\\n",
    "                agg(vals[0], wn_synset_lesk_wsd_ratio_hi_freq_sum, vals[1], \\\n",
    "                                     get_dict_count, word_freq_wiki, pos=vals[2]), axis = 1)\n",
    "    df['wn_synset_lesk_wsd_ratio_low_freq_sum (sem)'] = df[['p_target','sentence', 'pos_tags_pt']].apply(lambda vals : \\\n",
    "                agg(vals[0], wn_synset_lesk_wsd_ratio_low_freq_sum, vals[1], \\\n",
    "                                     get_dict_count, word_freq_wiki, pos=vals[2]), axis = 1)\n",
    "    df['wn_synset_lesk_wsd_ratio_to_freq_sum (sem)'] = df[['p_target','sentence', 'pos_tags_pt']].apply(lambda vals : \\\n",
    "                agg(vals[0], wn_synset_lesk_wsd_ratio_to_freq_sum, vals[1], \\\n",
    "                                     get_dict_count, word_freq_wiki, pos=vals[2]), axis = 1)\n",
    "    df['wn_synset_lesk_wsd__norm_sense_rank (sem)'] = df[['p_target','sentence', 'pos_tags_pt']].apply(lambda vals : \\\n",
    "                agg(vals[0], wn_synset_lesk_wsd__norm_sense_rank, vals[1], \\\n",
    "                                     get_dict_count, word_freq_wiki, lesk, pos=vals[2]), axis = 1)\n",
    "    df = df.drop(['length', 'freq_wiki'], axis = 1)\n",
    "    df = df.drop(drop_features, axis = 1)\n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "fc_wordnet = FeatureCategory('wordnet', features_wordnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features_wordnet(datasets, aggs = agg_default, drop_features = []):\n",
    "     return [FeatureDataset(ds.name, fc_wordnet, agg,\n",
    "                        fc_wordnet.func(ds.train, agg.agg, drop_features), \n",
    "                        fc_wordnet.func(ds.test, agg.agg, drop_features)) \n",
    "                        for ds in datasets\n",
    "                        for agg in aggs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.5.2) DBpedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('resources/dbpedia-cache/dbpedia_annotations_00.json', 'rb') as fp:\n",
    "    dbpedia_00 = pickle.load(fp)\n",
    "    \n",
    "with open('resources/dbpedia-cache/dbpedia_annotations_25.json', 'rb') as fp:\n",
    "    dbpedia_25 = pickle.load(fp)\n",
    "    \n",
    "with open('resources/dbpedia-cache/dbpedia_annotations_50.json', 'rb') as fp:\n",
    "    dbpedia_50 = pickle.load(fp)\n",
    "    \n",
    "with open('resources/dbpedia-cache/dbpedia_annotations_75.json', 'rb') as fp:\n",
    "    dbpedia_75 = pickle.load(fp)\n",
    "    \n",
    "with open('resources/dbpedia-cache/pagerank.json', 'rb') as fp:\n",
    "        page_rank = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def overlaps(start1, end1, start2, end2):\n",
    "    return bool(range(max(start1, start2), min(end1, end2)+1))\n",
    "\n",
    "def dbp_match_entities(sentence, target, start, end, annotations):\n",
    "    an_sents = annotations.get(sentence)\n",
    "    if an_sents:\n",
    "        ans = [(an['offset'], an['offset']+len(an['surfaceForm']), an) for an in an_sents]\n",
    "        return [an for s, e, an in ans if overlaps(start, end, s, e)]\n",
    "    return []\n",
    "\n",
    "def dbp_entity_ratio(sentence, target, start, end, annotations):\n",
    "    entities = dbp_match_entities(sentence, target, start, end, annotations)\n",
    "    if entities:\n",
    "        return np.min([np.sum([len(entity['surfaceForm']) \n",
    "                for entity in entities]) / len(target), 1])\n",
    "    return 0\n",
    "\n",
    "def dbp_support(sentence, target, start, end, annotations):\n",
    "    entities = dbp_match_entities(sentence, target, start, end, annotations)\n",
    "    if entities:\n",
    "        return np.mean([entity['support'] for entity in entities])\n",
    "    return 0\n",
    "\n",
    "def dbp_type_hierachy_depth(sentence, target, start, end, annotations):\n",
    "    entities = dbp_match_entities(sentence, target, start, end, annotations)\n",
    "    if entities:\n",
    "        return np.mean([np.sum([1 for cat in entity['types'].split(',') if 'DBpedia' in cat])\n",
    "                 for entity in entities])\n",
    "    return 0\n",
    "\n",
    "def dbp_freq_types(sentence, target, start, end, annotations):\n",
    "    entities = dbp_match_entities(sentence, target, start, end, annotations)\n",
    "    if entities:\n",
    "        return np.mean([len(entity['types'].split(',')) for entity in entities])\n",
    "    return 0\n",
    "\n",
    "def dbp_confidence(sentence, target, start, end):\n",
    "    entities = dbp_match_entities(sentence, target, start, end, dbpedia_75)\n",
    "    if entities:\n",
    "        return 0.75\n",
    "    entities = dbp_match_entities(sentence, target, start, end, dbpedia_50)\n",
    "    if entities:\n",
    "        return 0.5\n",
    "    entities = dbp_match_entities(sentence, target, start, end, dbpedia_25)\n",
    "    if entities:\n",
    "        return 0.25\n",
    "    return 0\n",
    "\n",
    "def dbp_pagerank(sentence, target, start, end, annotations):\n",
    "    entities = dbp_match_entities(sentence, target, start, end, annotations)\n",
    "    return np.nan_to_num(np.mean([page_rank.get(entity['URI'], 0) for entity in entities]))\n",
    "\n",
    "dbp_types = [('DBpedia:Place', 1, 'dbo:Place'), ('DBpedia:Person',2, 'dbo:Person'), \n",
    "             ('DBpedia:Organisation',3, 'dbo:Organisation'), ('DBpedia:Timeperiod', 4, 'dbo:Timeperiod')]\n",
    "\n",
    "def dbp_extract_type(entity):\n",
    "    types = [(cat, rank, name) for cat, rank, name in dbp_types if cat in entity['types']]\n",
    "    if not types and not entity['types']:\n",
    "        return ('dbo:notype', 0, 'dbo:notype')\n",
    "    if not types and entity['types']:\n",
    "        return ('dbo:misc', 5, 'dbo:misc')\n",
    "    else:\n",
    "        return types[0]\n",
    "\n",
    "def dbp_type(sentence, target, start, end, annotations):\n",
    "    entities = dbp_match_entities(sentence, target, start, end, annotations)\n",
    "    if not entities:\n",
    "        return 'dbo:missing'\n",
    "    types = [dbp_extract_type(entity) for entity in entities]\n",
    "    sorted(types, key=lambda tpl : tpl[1])\n",
    "    return types[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_dbpedia(dataframe, agg, drop_features):\n",
    "    df = dataframe.copy()\n",
    "    df['dbp_confidence (sem)'] = df[['sentence', 'target', 'start', 'end']].apply(lambda vals :\n",
    "                                                          dbp_confidence(*vals),axis = 1)\n",
    "    \n",
    "    df['dbp_entity_ratio_25 (sem)'] = df[['sentence', 'target', 'start', 'end']].apply(lambda vals :\n",
    "                                                          dbp_entity_ratio(*vals, dbpedia_25),axis = 1)\n",
    "    df['dbp_entity_support_25 (sem)'] = df[['sentence', 'target', 'start', 'end']].apply(lambda vals :\n",
    "                                                          dbp_support(*vals, dbpedia_25),axis = 1)\n",
    "    df['dbp_type_hierachy_depth_25 (sem)'] = df[['sentence', 'target', 'start', 'end']].apply(lambda vals :\n",
    "                                                          dbp_type_hierachy_depth(*vals, dbpedia_25),axis = 1)\n",
    "    df['dbp_freq_types_25 (sem)'] = df[['sentence', 'target', 'start', 'end']].apply(lambda vals :\n",
    "                                                          dbp_freq_types(*vals, dbpedia_25),axis = 1)\n",
    "    df['dbp_pagerank_25 (sem)'] = df[['sentence', 'target', 'start', 'end']].apply(lambda vals :\n",
    "                                                          dbp_pagerank(*vals, dbpedia_25),axis = 1)\n",
    "    max_page_rank = np.max(df['dbp_pagerank_25 (sem)'])\n",
    "    df['dbp_norm_pagerank_25 (sem)'] = df['dbp_pagerank_25 (sem)'] / max_page_rank\n",
    "    df['dbp_type_25 (sem)'] = df[['sentence', 'target', 'start', 'end']].apply(lambda vals :\n",
    "                                                          dbp_type(*vals, dbpedia_25),axis = 1)\n",
    "    \n",
    "    df['dbp_entity_ratio_00 (sem)'] = df[['sentence', 'target', 'start', 'end']].apply(lambda vals :\n",
    "                                                          dbp_entity_ratio(*vals, dbpedia_00),axis = 1)\n",
    "    df['dbp_entity_support_00 (sem)'] = df[['sentence', 'target', 'start', 'end']].apply(lambda vals :\n",
    "                                                          dbp_support(*vals, dbpedia_00),axis = 1)\n",
    "    df['dbp_type_hierachy_depth_00 (sem)'] = df[['sentence', 'target', 'start', 'end']].apply(lambda vals :\n",
    "                                                          dbp_type_hierachy_depth(*vals, dbpedia_00),axis = 1)\n",
    "    df['dbp_freq_types_00 (sem)'] = df[['sentence', 'target', 'start', 'end']].apply(lambda vals :\n",
    "                                                          dbp_freq_types(*vals, dbpedia_00),axis = 1)\n",
    "    df['dbp_pagerank_00 (sem)'] = df[['sentence', 'target', 'start', 'end']].apply(lambda vals :\n",
    "                                                          dbp_pagerank(*vals, dbpedia_00),axis = 1)\n",
    "    max_page_rank = np.max(df['dbp_pagerank_00 (sem)'])\n",
    "    df['dbp_norm_pagerank_00 (sem)'] = df['dbp_pagerank_00 (sem)'] / max_page_rank\n",
    "    df['dbp_type_00 (sem)'] = df[['sentence', 'target', 'start', 'end']].apply(lambda vals :\n",
    "                                                          dbp_type(*vals, dbpedia_00),axis = 1)\n",
    "    \n",
    "    df = df.drop(drop_features, axis = 1)\n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "fc_dbpedia = FeatureCategory('dbpedia', features_dbpedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features_dbpedia(datasets, aggs = agg_default, drop_features = []):\n",
    "    return [FeatureDataset(ds.name, fc_dbpedia, agg,\n",
    "                        fc_dbpedia.func(ds.train, agg.agg, drop_features), \n",
    "                        fc_dbpedia.func(ds.test, agg.agg, drop_features)) \n",
    "                        for ds in datasets\n",
    "                        for agg in aggs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3.5.3) Brown Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_cluster_word2cluster = {}\n",
    "brown_cluster_cluster2words = defaultdict(list)\n",
    "with open(\"resources/brown-clustering/paths/rcv1.clean-c6000-p1.paths\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        binary_cluster, word, _ = line.split()\n",
    "        brown_cluster_word2cluster[word] = binary_cluster\n",
    "        brown_cluster_cluster2words[binary_cluster].append(word)\n",
    "\n",
    "def brown_clustering_cluster_size(target):\n",
    "    cluster = brown_cluster_word2cluster.get(target)\n",
    "    return len(brown_cluster_cluster2words[cluster]) if cluster else 0\n",
    "\n",
    "def brown_clustering_cluster_depth_simple(target):\n",
    "    cluster = brown_cluster_word2cluster.get(target)\n",
    "    return int(cluster, 2) if cluster else 0\n",
    "\n",
    "def brown_clustering_cluster_depth_bit(target):\n",
    "    cluster = brown_cluster_word2cluster.get(target)\n",
    "    if not cluster:\n",
    "        return 8.75\n",
    "    return np.sum([1 for bit in cluster if bit == '1'])\n",
    "\n",
    "def brown_clustering_cluster_size_all(target):\n",
    "    cluster = brown_cluster_word2cluster.get(target)\n",
    "    if not cluster:\n",
    "        return 0\n",
    "    upper_clusters = [cluster[0:(len(cluster) - index)] + '0' * index \\\n",
    "         for index, bit in enumerate(reversed(cluster)) if bit == '1']\n",
    "    cluster_counts = [len(brown_cluster_cluster2words.get(clu, [])) \\\n",
    "                         for clu in upper_clusters]\n",
    "    return np.sum(cluster_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_brown_clustering(dataframe, agg, drop_features):\n",
    "    df = dataframe.copy()\n",
    "    df['brown_clustering_cluster_size (sem)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                                            brown_clustering_cluster_size))\n",
    "    df['brown_clustering_cluster_size_all (sem)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                                            brown_clustering_cluster_size_all))\n",
    "    df['brown_clustering_cluster_depth_simple (sem)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                                            brown_clustering_cluster_depth_simple))\n",
    "    df['brown_clustering_cluster_depth_bit (sem)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                                            brown_clustering_cluster_depth_bit))\n",
    "    df = df.drop(drop_features, axis = 1)\n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "fc_brown_clustering = FeatureCategory('brown_clustering', features_brown_clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features_brown_clustering(datasets, aggs = agg_default, drop_features = []):\n",
    "    return [FeatureDataset(ds.name, fc_brown_clustering, agg,\n",
    "                        fc_brown_clustering.func(ds.train, agg.agg, drop_features), \n",
    "                        fc_brown_clustering.func(ds.test, agg.agg, drop_features)) \n",
    "                        for ds in datasets\n",
    "                        for agg in aggs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_semantic = FeatureCategory('semantic', [fc_wordnet, fc_dbpedia, fc_brown_clustering])\n",
    "\n",
    "def compute_features_semantic(datasets):\n",
    "    return [FeatureDataset(ds.name, fc_semantic,  ds.agg,\n",
    "            ds.train, ds.test) for ds in concat_feature_datasets(*datasets)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3.6) Dictionary Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textatistic\n",
    "from collections import Counter\n",
    "\n",
    "academic_words = {}\n",
    "with open(\"resources/dictionaries/academic_word_list.txt\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        word, rank = line.split()\n",
    "        academic_words[word.strip()] = rank\n",
    "\n",
    "prefixes = {}\n",
    "with open(\"resources/dictionaries/prefixes.txt\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        prefix, definition, examples = line.split('\\t')\n",
    "        prefixes[prefix.replace('-', '').strip()] = definition\n",
    "\n",
    "suffixes = {}\n",
    "with open(\"resources/dictionaries/suffixes.txt\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        suffix, definition, examples = line.split('\\t')\n",
    "        suffixes[suffix.replace('-', '').strip()] = definition\n",
    "\n",
    "with open(\"resources/dictionaries/biology_glossary.csv\", encoding=\"utf8\") as file:\n",
    "    content = [line.strip().lower() for line in file.readlines()]\n",
    "    gloss_biology = set(content)\n",
    "\n",
    "with open(\"resources/dictionaries/geography_glossary.csv\", encoding=\"utf8\") as file:\n",
    "    content = [line.strip().lower() for line in file.readlines()]\n",
    "    gloss_geography = set(content)\n",
    "    \n",
    "with open(\"resources/dictionaries/physics_glossary.csv\", encoding=\"utf8\") as file:\n",
    "    content = [line.strip().lower() for line in file.readlines()]\n",
    "    gloss_physics = set(content)\n",
    "    \n",
    "with open(\"resources/dictionaries/stopwords_en.txt\", encoding=\"utf8\") as file:\n",
    "    content = [line.strip().lower() for line in file.readlines()]\n",
    "    stop_words = set(content)\n",
    "    \n",
    "with open(\"resources/dictionaries/most_freq_used_3000_words.txt\", encoding=\"utf8\") as file:\n",
    "    content = [line.strip().lower() for line in file.readlines()]\n",
    "    most_freq_used_3000_words = set(content)\n",
    "    \n",
    "with open(\"resources/dictionaries/most_freq_used_5000_words.txt\", encoding=\"utf8\") as file:\n",
    "    content = [line.split()[1].strip().lower() for line in file.readlines()]\n",
    "    most_freq_used_5000_words = set(content)\n",
    "    \n",
    "\n",
    "'''\n",
    "Extract all words that are exactly identified as either complex\n",
    "or non-complex and use this as the vocabulary. Words that occur\n",
    "as both complex and non-complex are neglected for the vocabulary\n",
    "'''\n",
    "def build_clean_vocabulary(train):\n",
    "    targets_complex = set([mwe.strip().lower() for mwe in \n",
    "                train.loc[train['binary'] == 1,]['target'].tolist()])\n",
    "    targets_non_complex = set([mwe.strip().lower() for mwe in\n",
    "                train.loc[train['binary'] == 0,]['target'].tolist()])\n",
    "    targets_complex_cleaned = list(targets_complex.difference(targets_non_complex))\n",
    "    targets_non_complex_cleaned = list(targets_non_complex.difference(targets_complex))\n",
    "    vocabulary = {}\n",
    "    for target in targets_complex_cleaned:\n",
    "        vocabulary[target] = 1\n",
    "    for target in targets_non_complex_cleaned:\n",
    "        vocabulary[target] = 0\n",
    "    return vocabulary\n",
    "\n",
    "'''\n",
    "Extract all words that are identified as either complex\n",
    "or non-complex and use this as the vocabulary. Words that occur\n",
    "as both complex and non-complex are weighted based on the number\n",
    "of occurrences. If the word has been tagged more times as non-complex\n",
    "we save it as non-complex otherwise it is complex\n",
    "'''\n",
    "def build_weighted_vocabulary(train):\n",
    "    targets_complex = [mwe.strip().lower() for mwe in \n",
    "                train.loc[train['binary'] == 1,]['target'].tolist()]\n",
    "    targets_non_complex = [mwe.strip().lower() for mwe in\n",
    "                train.loc[train['binary'] == 0,]['target'].tolist()]\n",
    "    counts_complex = Counter(targets_complex)\n",
    "    counts_non_complex = Counter(targets_non_complex)\n",
    "    vocabulary = {}\n",
    "    for word, count in counts_complex.items():\n",
    "        count_nc = counts_non_complex.get(word, None)\n",
    "        if count_nc and count_nc > count:\n",
    "            vocabulary[word] = 0\n",
    "        else:\n",
    "            vocabulary[word] = 1\n",
    "    for word, count in counts_non_complex.items():\n",
    "        if word not in vocabulary:\n",
    "            vocabulary[word] = 0\n",
    "    return vocabulary\n",
    "\n",
    "def build_confidence_vocabulary_1(train, confidence):\n",
    "    targets_complex = [mwe.strip().lower() for mwe in \n",
    "                train.loc[train['prob'] >= confidence,]['target'].tolist()]\n",
    "    targets_non_complex = [mwe.strip().lower() for mwe in \n",
    "                train.loc[train['prob'] < confidence,]['target'].tolist()]\n",
    "    counts_complex = Counter(targets_complex)\n",
    "    counts_non_complex = Counter(targets_non_complex)\n",
    "    vocabulary = {}\n",
    "    for word, count in counts_complex.items():\n",
    "        count_nc = counts_non_complex.get(word, None)\n",
    "        if count_nc and count_nc > count:\n",
    "            vocabulary[word] = 0\n",
    "        else:\n",
    "            vocabulary[word] = 1\n",
    "    for word, count in counts_non_complex.items():\n",
    "        if word not in vocabulary:\n",
    "            vocabulary[word] = 0\n",
    "    return vocabulary\n",
    "\n",
    "def build_confidence_vocabulary_2(train, confidence):\n",
    "    targets_complex = [mwe.strip().lower() for mwe in \n",
    "                train.loc[train['prob'] >= confidence,]['target'].tolist()]\n",
    "    targets_non_complex = [mwe.strip().lower() for mwe in \n",
    "                train.loc[train['binary'] == 0,]['target'].tolist()]\n",
    "    counts_complex = Counter(targets_complex)\n",
    "    counts_non_complex = Counter(targets_non_complex)\n",
    "    vocabulary = {}\n",
    "    for word, count in counts_complex.items():\n",
    "        count_nc = counts_non_complex.get(word, None)\n",
    "        if count_nc and count_nc > count:\n",
    "            vocabulary[word] = 0\n",
    "        else:\n",
    "            vocabulary[word] = 1\n",
    "    for word, count in counts_non_complex.items():\n",
    "        if word not in vocabulary:\n",
    "            vocabulary[word] = 0\n",
    "    return vocabulary\n",
    "\n",
    "def build_confidence_vocabulary_mean(train):\n",
    "    train['target'] = train.target.apply(lambda target : target.strip().lower())\n",
    "    agg = train[['target', 'prob']].groupby('target',\n",
    "                        as_index=False).mean().values\n",
    "    tuples = [tuple(val) for val in agg]\n",
    "    vocabulary = {}\n",
    "    for target, confidence in tuples:\n",
    "        vocabulary[target] = confidence\n",
    "    return vocabulary\n",
    "\n",
    "def build_confidence_vocabulary_max(train):\n",
    "    train['target'] = train.target.apply(lambda target : target.strip().lower())\n",
    "    agg = train[['target', 'prob']].groupby('target',\n",
    "                        as_index=False).max().values\n",
    "    tuples = [tuple(val) for val in agg]\n",
    "    vocabulary = {}\n",
    "    for target, confidence in tuples:\n",
    "        vocabulary[target] = confidence\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_dictionary(dataframe, agg, drop_features):\n",
    "    df = dataframe.copy()\n",
    "    df['dict_dale_chall (dic)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                            lambda target :  0 if textatistic.notdalechall_count(target) >= 1 else 1))\n",
    "    df['dict_570_academic_words (dic)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                                    lambda target : int(target in academic_words)))\n",
    "    df['common_prefix (dic)'] = df.p_target.apply(lambda target : int(np.sum([1 for prefix in prefixes if target.startswith(prefix)]) > 0))\n",
    "    df['common_suffix (dic)'] = df.p_target.apply(lambda target : int(np.sum([1 for suffix in suffixes if target.endswith(suffix)]) > 0))\n",
    "    df['gloss_biology (dic)'] = df.p_target.apply(lambda target : int(target in gloss_biology))\n",
    "    df['gloss_physics (dic)'] = df.p_target.apply(lambda target : int(target in gloss_physics))\n",
    "    df['gloss_geography (dic)'] = df.p_target.apply(lambda target : int(target in gloss_geography))\n",
    "    df['stop_word (dic)'] = df.p_target.apply(lambda target : int(target in stop_words))\n",
    "    df['most_freq_used_3000_words (dic)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                                    lambda target : int(target in most_freq_used_3000_words)))\n",
    "    df['most_freq_used_5000_words (dic)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                                    lambda target : int(target in most_freq_used_5000_words)))\n",
    "    df = df.drop(drop_features, axis = 1)\n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "fc_dictionary = FeatureCategory('dictionary', features_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features_dictionary(datasets, aggs = agg_default, drop_features = []):\n",
    "    return [FeatureDataset(ds.name, fc_dictionary, agg,\n",
    "                        fc_dictionary.func(ds.train, agg.agg, drop_features), \n",
    "                        fc_dictionary.func(ds.test, agg.agg, drop_features)) \n",
    "                        for ds in datasets\n",
    "                        for agg in aggs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Classification Models\n",
    "Here we compute individual feature importance based on different metrics. For example, we implement and compute the F-Score, providing an idea of the discrimination power the feature has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Result = namedtuple('Result', 'dataset, fc, agg, measure')\n",
    "Dataset = namedtuple('Dataset', 'name, train, test')\n",
    "FeatureDataset = namedtuple('FeatureDataset', 'name, fc, agg, train, test')\n",
    "FeatureCategory = namedtuple('FeatureCategory', 'name, func')\n",
    "Feature = namedtuple('Feature', 'name, fc_name, train, test')\n",
    "Metric = namedtuple('Metric', 'name, func')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4.1) Utility Functions\n",
    "Here we provide several utility functions for working with the datasets and classification algorithms. For example, we provide functions to clean the datasets from all non-features (such as id, sentence, the annotator information etc.) and functions to transform the feature datasets into a proper representation for the algorithms (such as one-hot-encoding of categorical attributes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_labels_for_binary_df(dataframe, drop=[]):\n",
    "    drop_list = ['id', 'sentence', 'target', 'nat', 'non_nat', \n",
    "                  'nat_marked', 'non_nat_marked', 'prob', 'start', \n",
    "                  'end', 'p_target', 'lemma', 'p_lemma', 'pos_tags', 'pos_tags_pt']\n",
    "    drop_list.extend(drop)\n",
    "    df = dataframe.copy()\n",
    "    df = df.drop(drop_list, axis = 1)\n",
    "    return df\n",
    "\n",
    "def remove_labels_phrase_for_binary_df(dataframe, drop=[]):\n",
    "    drop_list = ['id', 'sentence', 'target', 'nat', 'non_nat', \n",
    "                  'nat_marked', 'non_nat_marked', 'prob', 'start', \n",
    "                  'end', 'p_target', 'lemma', 'p_lemma', 'pos_tags', 'pos_tags_pt', 'phrase_index']\n",
    "    drop_list.extend(drop)\n",
    "    df = dataframe.copy()\n",
    "    df = df.drop(drop_list, axis = 1)\n",
    "    return df\n",
    "\n",
    "def remove_labels_for_regr_df(dataframe, drop=[]):\n",
    "    drop_list = ['id', 'sentence', 'target', 'nat', 'non_nat', \n",
    "                  'nat_marked', 'non_nat_marked', 'binary', 'start', \n",
    "                  'end', 'p_target', 'lemma', 'p_lemma', 'pos_tags', 'pos_tags_pt']\n",
    "    drop_list.extend(drop)\n",
    "    df = dataframe.copy()\n",
    "    df = df.drop(drop_list, axis = 1)\n",
    "    return df\n",
    "    \n",
    "def transform_feat_to_num(train, test):\n",
    "    train_copy = train.copy()\n",
    "    test_copy = test.copy()\n",
    "    train_copy = train_copy.replace([np.inf, -np.inf], np.nan)\n",
    "    train_copy = train_copy.fillna(0)\n",
    "    test_copy = test_copy.replace([np.inf, -np.inf], np.nan)\n",
    "    test_copy = test_copy.fillna(0)\n",
    "    shape_train = train.shape\n",
    "    shape_test = test.shape\n",
    "    df = train_copy.append(test_copy, ignore_index=True)\n",
    "    df = pd.get_dummies(df)\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df = df.fillna(0)\n",
    "    df = df.applymap(lambda x: 1 if x == True else x)\n",
    "    df = df.applymap(lambda x: 0 if x == False else x)\n",
    "    return (df.loc[0:(shape_train[0]-1),], \n",
    "            df.loc[shape_train[0]:df.shape[0],])\n",
    "\n",
    "def prep_data(train, test):\n",
    "    x_train = train.loc[:, train.columns != 'binary']\n",
    "    y_train = train['binary'].values\n",
    "    x_test = test.loc[:, test.columns != 'binary']\n",
    "    y_test = test.binary.values\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def create_eval_df_from_results(results, remove_agg=True):\n",
    "    if remove_agg:\n",
    "        evaluation = [{'dataset' : result.dataset.name,\n",
    "                        'zc' : result.fc[0], 'prec' : result.measure[0][1],\n",
    "                   'rec' : result.measure[1][1], 'f1' : result.measure[2][1]} \n",
    "                       for result in results]\n",
    "    else:\n",
    "        evaluation = [{'dataset' : result.dataset.name, 'agg' : result.agg[0],\n",
    "                        'zc' : result.fc[0], 'prec' : result.measure[0][1],\n",
    "                   'rec' : result.measure[1][1], 'f1' : result.measure[2][1]} \n",
    "                       for result in results]\n",
    "    return pd.DataFrame.from_records(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4.2) Feature Importance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import permutation_test_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn import model_selection\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "def metric_fi_f_score(train, test, feat_name):\n",
    "    df = train.copy()\n",
    "    mean_feat = np.mean(df.loc[:, [feat_name]].values)\n",
    "    means = df.loc[: , [feat_name, 'binary']].groupby('binary').mean().reset_index()\n",
    "    mean_negativ = means.loc[means['binary'] == 0, [feat_name]][feat_name][0]\n",
    "    mean_positiv = means.loc[means['binary'] == 1, [feat_name]][feat_name][1]\n",
    "    # Compute the sum of deviations of the class mean from the overall mean\n",
    "    class_mean_devs = (mean_positiv - mean_feat)**2 + (mean_negativ - mean_feat)**2\n",
    "    # Compute neagtive instance based values\n",
    "    neg_inst = df.loc[df['binary'] == 0, [feat_name]].values\n",
    "    std_dev_neg = (np.sum((neg_inst - mean_negativ)**2) / (len(neg_inst) - 1))\n",
    "    #Compute positive instance based values\n",
    "    pos_inst = df.loc[df['binary'] == 1, [feat_name]].values\n",
    "    std_dev_pos = (np.sum((pos_inst - mean_positiv)**2) / (len(pos_inst) - 1))\n",
    "    return class_mean_devs / (std_dev_neg + std_dev_pos)\n",
    "\n",
    "\n",
    "def metric_fi_permutation_test(train, test, feat_name):\n",
    "    svm = SVC(kernel='rbf', verbose=2)\n",
    "    x_train = train.loc[:, train.columns != 'binary']\n",
    "    y_train = train.binary.values\n",
    "    score = permutation_test_score(svm, x_train.values, y_train, groups=None, cv=None, \n",
    "                        n_permutations=10, n_jobs=1, random_state=0, \n",
    "                        verbose=2, scoring=None)\n",
    "    return score[2]\n",
    "\n",
    "\n",
    "def metric_fi_classification_f1(train, test, label_name):\n",
    "    print('average_classification')\n",
    "    x_train = train.loc[:, train.columns != 'binary']\n",
    "    y_train = train.binary.values\n",
    "    x_test = test.loc[:, test.columns != 'binary']\n",
    "    y_test = test.binary.values\n",
    "    seed = 7\n",
    "    knn = svm.SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "        decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
    "        max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "        tol=0.001, verbose=False)\n",
    "    knn.fit(x_train, y_train)\n",
    "    prediction = knn.predict(x_test)\n",
    "    return f1_score(y_test, prediction, average=None)\n",
    "\n",
    "\n",
    "def metric_fi_classification_mattcorr(train, test, label_name):\n",
    "    print('average_classification')\n",
    "    x_train = train.loc[:, train.columns != 'binary']\n",
    "    y_train = train.binary.values\n",
    "    x_test = test.loc[:, test.columns != 'binary']\n",
    "    y_test = test.binary.values\n",
    "    seed = 7\n",
    "    knn = svm.SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "        decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
    "        max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "        tol=0.001, verbose=False)\n",
    "    knn.fit(x_train, y_train)\n",
    "    prediction = knn.predict(x_test)\n",
    "    return matthews_corrcoef(y_test, prediction)\n",
    "\n",
    "metric_fisher_score = Metric('f-score', metric_fi_f_score)\n",
    "metric_permutation_test = Metric('f-score', metric_fi_permutation_test)\n",
    "metric_classification_f1 = Metric('f-score', metric_fi_classification_f1)\n",
    "metric_classification_mattcor = Metric('f-score', metric_fi_classification_mattcorr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4.2.1) Baseline Always Complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def always_complex_prediction(train, test):\n",
    "    y_test = test.binary.values\n",
    "    prediction = [1 for val in y_test]\n",
    "    f1score = precision_recall_fscore_support(y_test, prediction)\n",
    "    return f1score\n",
    "\n",
    "def baseline_always_complex(dataset):\n",
    "    results = [Result(ds.name, 'always_complex', agg_default[0],\n",
    "        always_complex_prediction(remove_labels_for_binary_df(ds.train), \n",
    "            remove_labels_for_binary_df(ds.test))) for ds in datasets]\n",
    "    evaluation = [{'dataset' : result.dataset, \n",
    "                        'zc' : result.fc, 'prec' : result.measure[0][1],\n",
    "                  'rec' : result.measure[1][1], 'f1' : result.measure[2][1]} \n",
    "                       for result in results]\n",
    "    counts = [(ds.name, Counter(ds.test.binary)) for ds in datasets]\n",
    "    return pd.DataFrame.from_records(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4.3.2) Baseline Memorize Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "'''\n",
    "Extract all words that are exactly identified as either complex\n",
    "or non-complex and use this as the vocabulary. Words that occur\n",
    "as both complex and non-complex are neglected for the vocabulary\n",
    "'''\n",
    "def build_clean_vocabulary(train):\n",
    "    targets_complex = set([mwe.strip().lower() for mwe in \n",
    "                train.loc[train['binary'] == 1,]['target'].tolist()])\n",
    "    targets_non_complex = set([mwe.strip().lower() for mwe in\n",
    "                train.loc[train['binary'] == 0,]['target'].tolist()])\n",
    "    targets_complex_cleaned = list(targets_complex.difference(targets_non_complex))\n",
    "    targets_non_complex_cleaned = list(targets_non_complex.difference(targets_complex))\n",
    "    vocabulary = {}\n",
    "    for target in targets_complex_cleaned:\n",
    "        vocabulary[target] = 1\n",
    "    for target in targets_non_complex_cleaned:\n",
    "        vocabulary[target] = 0\n",
    "    return vocabulary\n",
    "\n",
    "'''\n",
    "Extract all words that are identified as either complex\n",
    "or non-complex and use this as the vocabulary. Words that occur\n",
    "as both complex and non-complex are weighted based on the number\n",
    "of occurrences. If the word has been tagged more times as non-complex\n",
    "we save it as non-complex otherwise it is complex\n",
    "'''\n",
    "def build_weighted_vocabulary(train):\n",
    "    targets_complex = [mwe.strip().lower() for mwe in \n",
    "                train.loc[train['binary'] == 1,]['target'].tolist()]\n",
    "    targets_non_complex = [mwe.strip().lower() for mwe in\n",
    "                train.loc[train['binary'] == 0,]['target'].tolist()]\n",
    "    counts_complex = Counter(targets_complex)\n",
    "    counts_non_complex = Counter(targets_non_complex)\n",
    "    vocabulary = {}\n",
    "    for word, count in counts_complex.items():\n",
    "        count_nc = counts_non_complex.get(word, None)\n",
    "        if count_nc and count_nc > count:\n",
    "            vocabulary[word] = 0\n",
    "        else:\n",
    "            vocabulary[word] = 1\n",
    "    for word, count in counts_non_complex.items():\n",
    "        if word not in vocabulary:\n",
    "            vocabulary[word] = 0\n",
    "    return vocabulary\n",
    "\n",
    "def build_confidence_vocabulary_1(train, confidence):\n",
    "    targets_complex = [mwe.strip().lower() for mwe in \n",
    "                train.loc[train['prob'] >= confidence,]['target'].tolist()]\n",
    "    targets_non_complex = [mwe.strip().lower() for mwe in \n",
    "                train.loc[train['prob'] < confidence,]['target'].tolist()]\n",
    "    counts_complex = Counter(targets_complex)\n",
    "    counts_non_complex = Counter(targets_non_complex)\n",
    "    vocabulary = {}\n",
    "    for word, count in counts_complex.items():\n",
    "        count_nc = counts_non_complex.get(word, None)\n",
    "        if count_nc and count_nc > count:\n",
    "            vocabulary[word] = 0\n",
    "        else:\n",
    "            vocabulary[word] = 1\n",
    "    for word, count in counts_non_complex.items():\n",
    "        if word not in vocabulary:\n",
    "            vocabulary[word] = 0\n",
    "    return vocabulary\n",
    "\n",
    "def build_confidence_vocabulary_2(train, confidence):\n",
    "    targets_complex = [mwe.strip().lower() for mwe in \n",
    "                train.loc[train['prob'] >= confidence,]['target'].tolist()]\n",
    "    targets_non_complex = [mwe.strip().lower() for mwe in \n",
    "                train.loc[train['binary'] == 0,]['target'].tolist()]\n",
    "    counts_complex = Counter(targets_complex)\n",
    "    counts_non_complex = Counter(targets_non_complex)\n",
    "    vocabulary = {}\n",
    "    for word, count in counts_complex.items():\n",
    "        count_nc = counts_non_complex.get(word, None)\n",
    "        if count_nc and count_nc > count:\n",
    "            vocabulary[word] = 0\n",
    "        else:\n",
    "            vocabulary[word] = 1\n",
    "    for word, count in counts_non_complex.items():\n",
    "        if word not in vocabulary:\n",
    "            vocabulary[word] = 0\n",
    "    return vocabulary\n",
    "\n",
    "def build_confidence_vocabulary_mean(train):\n",
    "    train['target'] = train.target.apply(lambda target : target.strip().lower())\n",
    "    agg = train[['target', 'prob']].groupby('target',\n",
    "                        as_index=False).mean().values\n",
    "    tuples = [tuple(val) for val in agg]\n",
    "    vocabulary = {}\n",
    "    for target, confidence in tuples:\n",
    "        vocabulary[target] = confidence\n",
    "    return vocabulary\n",
    "\n",
    "def build_confidence_vocabulary_max(train):\n",
    "    train['target'] = train.target.apply(lambda target : target.strip().lower())\n",
    "    agg = train[['target', 'prob']].groupby('target',\n",
    "                        as_index=False).max().values\n",
    "    tuples = [tuple(val) for val in agg]\n",
    "    vocabulary = {}\n",
    "    for target, confidence in tuples:\n",
    "        vocabulary[target] = confidence\n",
    "    return vocabulary\n",
    "    \n",
    "\n",
    "def evaluate_label_target_predictions(test, vocabulary):\n",
    "    dict_test = list(zip(test.target, test.binary))\n",
    "    data = [(binary, (vocabulary[target.strip().lower()] if target.strip().lower() in vocabulary else 1)) \n",
    "            for target, binary in dict_test]\n",
    "    y_true = [vals[0] for vals in data]\n",
    "    prediction = [vals[1] for vals in data]\n",
    "    return precision_recall_fscore_support(y_true, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_vocab_clean(datasets):\n",
    "    evaluation_clean = [Result(ds.name, 'vocab_clean', agg_default[0], \n",
    "                    evaluate_label_target_predictions(ds.test, \n",
    "                    build_clean_vocabulary(ds.train))) for ds in datasets]\n",
    "    results_clean = [{'dataset' : result.dataset, \n",
    "                        'zc' : result.fc, 'prec' : result.measure[0][1],\n",
    "                  'rec' : result.measure[1][1], 'f1' : result.measure[2][1]} \n",
    "                       for result in evaluation_clean]\n",
    "    return pd.DataFrame.from_records(results_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_vocab_weighted(datasets):\n",
    "    evaluation_weighted = [Result(ds.name, 'vocab_weighted', agg_default[0], \n",
    "                        evaluate_label_target_predictions(ds.test, \n",
    "                    build_weighted_vocabulary(ds.train))) for ds in datasets]\n",
    "    results_weighted = [{'dataset' : result.dataset, \n",
    "                        'zc' : result.fc, 'prec' : result.measure[0][1],\n",
    "                  'rec' : result.measure[1][1], 'f1' : result.measure[2][1]} \n",
    "                       for result in evaluation_weighted]\n",
    "    return pd.DataFrame.from_records(results_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_vocab_conf(datasets, confidence = 0.5):\n",
    "    evaluation_conf = [Result(ds.name, 'vocab_conf', agg_default[0], \n",
    "                            evaluate_label_target_predictions(ds.test, \n",
    "                        build_confidence_vocabulary_2(ds.train, confidence))) for ds in datasets]\n",
    "    results_conf = [{'dataset' : result.dataset, \n",
    "                        'zc' : result.fc, 'prec' : result.measure[0][1],\n",
    "                  'rec' : result.measure[1][1], 'f1' : result.measure[2][1]} \n",
    "                       for result in evaluation_conf]\n",
    "    return pd.DataFrame.from_records(results_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn import model_selection\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "def xgboost(train, test):\n",
    "    x_train, y_train, x_test, y_test = prep_data(train, test)\n",
    "    xgtrain = xgb.DMatrix(x_train.values, label=y_train)\n",
    "    xgtest = xgb.DMatrix(x_test.values, label=y_test)\n",
    "    xg_test_x = xgb.DMatrix(x_test.values)\n",
    "    param = {'max_depth': 30, 'eta': 1, 'silent': 1, \\\n",
    "             'objective': 'binary:logistic',  'n_estimators':5000}\n",
    "    evallist = [(xgtest, 'eval'), (xgtrain, 'train')]\n",
    "    num_round = 70\n",
    "    bst = xgb.train(param, xgtrain, num_round, evallist)\n",
    "    prediction = bst.predict(xg_test_x)\n",
    "    prediction_binary = list(map(lambda val: 1 if val>0.5 else 0, prediction))\n",
    "    return y_test, prediction_binary\n",
    "\n",
    "def adaboost(train, test):\n",
    "    x_train, y_train, x_test, y_test = prep_data(train, test)\n",
    "    adab = AdaBoostClassifier(base_estimator=None, n_estimators=5000, \n",
    "                          learning_rate=1.0, algorithm='SAMME.R',\n",
    "                          random_state=None)\n",
    "    adab.fit(x_train, y_train) \n",
    "    prediction = adab.predict(x_test)\n",
    "    return y_test, prediction\n",
    "\n",
    "def random_forest(train, test):\n",
    "    x_train, y_train, x_test, y_test = prep_data(train, test)\n",
    "    x_train = x_train.values.astype(np.float)\n",
    "    x_test = x_test.values.astype(np.float)\n",
    "    clf = RandomForestClassifier(max_depth=10, random_state=14521, n_estimators=1800, \\\n",
    "                    verbose=1, min_samples_split=5, min_samples_leaf=4, bootstrap=False)\n",
    "    clf.fit(x_train, y_train)\n",
    "    prediction = clf.predict(x_test)\n",
    "    prediction_binary = list(map(lambda val: 1 if val>0.5 else 0, prediction))\n",
    "    return y_test, prediction_binary\n",
    "\n",
    "def random_forest_extra(train, test):\n",
    "    x_train, y_train, x_test, y_test = prep_data(train, test)\n",
    "    x_train = x_train.values.astype(np.float)\n",
    "    x_test = x_test.values.astype(np.float)\n",
    "    clf = ExtraTreesClassifier(n_estimators=1800, criterion='gini', max_depth=None,\n",
    "                     min_samples_split=5, min_samples_leaf=4, min_weight_fraction_leaf=0.0,\n",
    "                     max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                     min_impurity_split=None, bootstrap=False, oob_score=False,\n",
    "                     random_state=15325, verbose=0, warm_start=False)\n",
    "    clf.fit(x_train, y_train)\n",
    "    prediction = clf.predict(x_test)\n",
    "    prediction_binary = list(map(lambda val: 1 if val>0.5 else 0, prediction))\n",
    "    return y_test, prediction_binary\n",
    "\n",
    "def svm(train, test):\n",
    "    x_train, y_train, x_test, y_test = prep_data(train, test)\n",
    "    seed = 7\n",
    "    svc = SVC(C=10, kernel='rbf', degree=3, gamma='auto', \n",
    "            coef0=0.0, shrinking=True, probability=False, tol=0.001, \n",
    "            cache_size=200, class_weight=None, verbose=False, max_iter=-1, \n",
    "            decision_function_shape='ovr', random_state=None)\n",
    "    svc.fit(x_train, y_train) \n",
    "    prediction = svc.predict(x_test)\n",
    "    f1score = f1_score(y_test, prediction)\n",
    "    return y_test, prediction\n",
    "\n",
    "\n",
    "def mlp(train, test):\n",
    "    x_train, y_train, x_test, y_test = prep_data(train, test)\n",
    "    mlp = MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto',\n",
    "          beta_1=0.9, beta_2=0.999, early_stopping=False,\n",
    "          epsilon=1e-08, hidden_layer_sizes=(50, 20), learning_rate='constant',\n",
    "          learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
    "          nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
    "          solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
    "          warm_start=False)\n",
    "    mlp.fit(x_train, y_train) \n",
    "    prediction = mlp.predict(x_test)\n",
    "    return y_test, prediction\n",
    "\n",
    "def decision_tree(train, test):\n",
    "    x_train, y_train, x_test, y_test = prep_data(train, test)\n",
    "    seed = 7\n",
    "    dt = DecisionTreeClassifier(criterion='gini', splitter='best', \n",
    "                                 max_depth=None, min_samples_split=2, \n",
    "                                 min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "                                 max_features=None, random_state=None, max_leaf_nodes=None, \n",
    "                                 min_impurity_decrease=0.0, min_impurity_split=None, \n",
    "                                 class_weight=None, presort=False)\n",
    "    dt.fit(x_train, y_train) \n",
    "    prediction = dt.predict(x_test)\n",
    "    return y_test, prediction\n",
    "\n",
    "\n",
    "def knn(train, test):\n",
    "    x_train, y_train, x_test, y_test = prep_data(train, test)\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', \n",
    "                     leaf_size=30, p=2, metric='minkowski')\n",
    "    knn.fit(x_train, y_train) \n",
    "    prediction = knn.predict(x_test)\n",
    "    return y_test, prediction\n",
    "\n",
    "def naive_bayes(train, test):\n",
    "    x_train, y_train, x_test, y_test = prep_data(train, test)\n",
    "    naive_bayes = GaussianNB(priors=None)\n",
    "    naive_bayes.fit(x_train, y_train) \n",
    "    prediction = naive_bayes.predict(x_test)\n",
    "    return y_test, prediction\n",
    "\n",
    "def logistic_regression(train, test):\n",
    "    x_train, y_train, x_test, y_test = prep_data(train, test)\n",
    "    logistic_regression = LogisticRegression(penalty='l2', dual=False, tol=0.0001,\n",
    "                                     C=1.0, fit_intercept=True, intercept_scaling=1, \n",
    "                                     class_weight=None, random_state=None, solver='lbfgs',\n",
    "                                     max_iter=100, verbose=0, \n",
    "                                     warm_start=False)\n",
    "    logistic_regression.fit(x_train, y_train) \n",
    "    prediction = logistic_regression.predict(x_test)\n",
    "    return y_test, prediction\n",
    "\n",
    "\n",
    "def xgboost_with_bst(train, test, silent):\n",
    "    x_train, y_train, x_test, y_test = prep_data(train, test)\n",
    "    xgtrain = xgb.DMatrix(x_train.values, label=y_train, feature_names=x_train.columns.values)\n",
    "    xgtest = xgb.DMatrix(x_test.values, label=y_test, feature_names=x_test.columns.values)\n",
    "    xg_test_x = xgb.DMatrix(x_test.values, feature_names=x_test.columns.values)\n",
    "    param = {'max_depth': 30, 'eta': 1, 'silent': silent, 'objective': 'binary:logistic',  'n_estimators':5000}\n",
    "    evallist = [(xgtest, 'eval'), (xgtrain, 'train')]\n",
    "    num_round = 70\n",
    "    bst = xgb.train(param, xgtrain, num_round, evallist)\n",
    "    prediction = bst.predict(xg_test_x)\n",
    "    prediction_binary = list(map(lambda val: 1 if val>0.5 else 0, prediction))\n",
    "    f1score = precision_recall_fscore_support(y_test, prediction_binary)\n",
    "    return f1score, bst\n",
    "\n",
    "def random_forest_with_forest(train, test, label):\n",
    "    x_train, y_train, x_test, y_test = prep_data(train, test)\n",
    "    x_train = x_train.as_matrix().astype(np.float)\n",
    "    x_test = x_test.as_matrix().astype(np.float)\n",
    "    clf = RandomForestClassifier(max_depth=10, random_state=0, n_estimators=1800, \\\n",
    "                            verbose=1, min_samples_split=5, min_samples_leaf=4, bootstrap=False)\n",
    "    clf.fit(x_train, y_train)\n",
    "    prediction = clf.predict(x_test)\n",
    "    prediction_binary = list(map(lambda val: 1 if val>0.5 else 0, prediction))\n",
    "    f1score = precision_recall_fscore_support(y_test, prediction_binary)\n",
    "    return f1score, clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X.x Complex Phrase Identifcation ML Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.1 A1 Prediction Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we conduct the experiments for A1 word prediction aggregation to obtain the complexity of a phrase. We predict the complexity of each word of the phrase individually, and aggregate the predictions with three different simple A1 aggregation functions (min, max, majority voting). We train our models on the single words training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_starts(start, target):\n",
    "    curr_start = start\n",
    "    starts = [curr_start]\n",
    "    tokens = target.split()\n",
    "    for token in tokens:\n",
    "        curr_start = curr_start + len(token) + 1\n",
    "        starts.append(curr_start)\n",
    "    return ' '.join([str(start) for start in starts[:len(tokens)]])\n",
    "\n",
    "def compute_ends(start, target):\n",
    "    curr_start = start\n",
    "    ends = []\n",
    "    tokens = target.split()\n",
    "    for index, token in enumerate(tokens):\n",
    "        if index > 0:\n",
    "            curr_start = curr_start + len(token) + 1\n",
    "        else:\n",
    "            curr_start = curr_start + len(token)\n",
    "        ends.append(curr_start)\n",
    "    return ' '.join([str(end) for end in ends])\n",
    "\n",
    "def phrase_splitter(dataframe):\n",
    "    df = dataframe.copy()\n",
    "    df['starts'] = df[['start', 'target']].apply(lambda vals : compute_starts(*vals), axis=1)\n",
    "    df['ends'] = df[['start', 'target']].apply(lambda vals : compute_ends(*vals), axis=1)\n",
    "    df['phrase_index'] = df.apply(lambda x : x.name, axis=1)\n",
    "    s1 = df.target.str.split(' ', expand=True).stack().str.strip().reset_index(level=1, drop=True)\n",
    "    s2 = df.p_target.str.split(' ', expand=True).stack().str.strip().reset_index(level=1, drop=True)\n",
    "    s3 = df.starts.str.split(' ', expand=True).stack().str.strip().reset_index(level=1, drop=True)\n",
    "    s4 = df.ends.str.split(' ', expand=True).stack().str.strip().reset_index(level=1, drop=True)\n",
    "    s5 = df.p_lemma.str.split(' ', expand=True).stack().str.strip().reset_index(level=1, drop=True)\n",
    "    s6 = df.lemma.str.split(' ', expand=True).stack().str.strip().reset_index(level=1, drop=True)\n",
    "    df['pos_tags'] = df.pos_tags.apply(lambda tags : ' '.join(tags))\n",
    "    s7 = df.pos_tags.str.split(' ', expand=True).stack().str.strip().reset_index(level=1, drop=True)\n",
    "    df['pos_tags_pt'] = df.pos_tags_pt.apply(lambda tags : ' '.join(tags))\n",
    "    s8 = df.pos_tags_pt.str.split(' ', expand=True).stack().str.strip().reset_index(level=1, drop=True)\n",
    "    df1 = pd.concat([s1, s2, s3, s4, s5, s6, s7, s8], axis=1, keys=['target','p_target', \\\n",
    "                                        'start', 'end', 'p_lemma', 'lemma','pos_tags', 'pos_tags_pt'])\n",
    "    splitted_df = df.drop(['target', 'p_target', 'starts', \\\n",
    "                'start', 'ends', 'end', 'p_lemma','lemma', \n",
    "                        'pos_tags', 'pos_tags_pt'], axis=1).join(df1).reset_index(drop=True)\n",
    "    splitted_df['start'] = pd.to_numeric(splitted_df.start, errors='coerce')\n",
    "    splitted_df['end'] = pd.to_numeric(splitted_df.end, errors='coerce')\n",
    "    return splitted_df\n",
    "\n",
    "def phrase_splitting_datasets(datasets):\n",
    "    return [Dataset(ds.name, ds.train, phrase_splitter(ds.test)) for ds in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_prediction_agg_majority_vote(predictions):\n",
    "    positive_sum = np.sum(predictions)\n",
    "    ratio = positive_sum / len(predictions)\n",
    "    return int(ratio + 0.5)\n",
    "\n",
    "def phrase_prediction_agg_max(predictions):\n",
    "    return np.max(predictions)\n",
    "\n",
    "def phrase_prediction_agg_min(predictions):\n",
    "    return np.min(predictions)\n",
    "\n",
    "phrase_agg_mv = Aggregation('phrase_mv', phrase_prediction_agg_majority_vote)\n",
    "phrase_agg_max = Aggregation('phrase_max', phrase_prediction_agg_max)\n",
    "phrase_agg_min = Aggregation('phrase_min', phrase_prediction_agg_min)\n",
    "\n",
    "phrase_aggs = [phrase_agg_mv, phrase_agg_max, phrase_agg_min]\n",
    "\n",
    "def phrase_merger(df_test, result, agg):\n",
    "    df_test = df_test.copy()\n",
    "    prediction = result[1]\n",
    "    df_test['prediction'] = prediction\n",
    "    pred_binary = df_test.groupby('phrase_index').apply(lambda row : \\\n",
    "                            (agg(row['prediction']), agg(row['binary']))).values\n",
    "    predictions = [pred for pred, binary in pred_binary]\n",
    "    binary = [binary for pred, binary in pred_binary]\n",
    "    score = precision_recall_fscore_support(binary, predictions)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eval_df_from_results_phrase(results):\n",
    "    evaluation = [{'dataset' : result.dataset.name, 'agg' : result.agg[0],\n",
    "                     'prec' : result.measure[0][1],\n",
    "                   'rec' : result.measure[1][1], 'f1' : result.measure[2][1]} \n",
    "                       for result in results]\n",
    "    return pd.DataFrame.from_records(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: think about isntead of using all features use best feature sets from CWI experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_datasets(['Wikipedia', 'WikiNews', 'News'], 'Train', 'Dev', type_train='word', type_test='phrase')\n",
    "datasets = preprocess_datasets(datasets)\n",
    "# 1. Linguistic Features\n",
    "#datasets_fc_linguistic = compute_features_linguistic(datasets, drop_features=['phrase_length (lin)'], aggs=aggs_all)\n",
    "# 2. Corpus Features\n",
    "# datasets_fc_frequency = compute_features_frequency(datasets, aggs=aggs_all)\n",
    "# datasets_fc_language_model = compute_features_language_model(datasets, aggs=aggs_all)\n",
    "# datasets_fc_corpus = compute_features_corpus([datasets_fc_frequency, datasets_fc_language_model])\n",
    "# # 3. Psycholinguistic\n",
    "# datasets_fc_psycholinguistic = compute_features_psycholinguistic(datasets, aggs=aggs_all)\n",
    "# # 4. Semantic Features\n",
    "# datasets_fc_wordnet = compute_features_wordnet(datasets, aggs=aggs_all)\n",
    "# datasets_fc_dbpedia = compute_features_dbpedia(datasets, aggs=aggs_all)\n",
    "# datasets_fc_brown_clustering = compute_features_brown_clustering(datasets, aggs=aggs_all)\n",
    "# datasets_fc_semantic = compute_features_semantic([datasets_fc_wordnet, datasets_fc_dbpedia, datasets_fc_brown_clustering])\n",
    "# # 5. Dictionary Features\n",
    "# datasets_fc_dictionary = compute_features_dictionary(datasets, aggs=aggs_all)\n",
    "# # 6. Concatentation of feature categories\n",
    "# # (1) Corpus + Semantic\n",
    "# datasets_fc_corpus_semantic = concat_feature_datasets(datasets_fc_corpus, datasets_fc_semantic, name='corpus+semantic')\n",
    "# # (2) WordNet + Psycholinguistic\n",
    "# datasets_fc_wordnet_psycholinguistic = concat_feature_datasets(datasets_fc_wordnet, \\\n",
    "#                             datasets_fc_psycholinguistic, name='wordnet+psycholinguistic')\n",
    "# (3) All categories\n",
    "# datasets_fc_all = concat_feature_datasets(datasets_fc_linguistic, datasets_fc_psycholinguistic, \\\n",
    "#                             datasets_fc_semantic, datasets_fc_corpus, datasets_fc_dictionary, name='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_splitted_datasets = phrase_splitting_datasets(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Studio\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\core\\fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\Studio\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "datasets_fc_linguistic = compute_features_linguistic(phrase_splitted_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_all_fc = datasets_fc_linguistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.1 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_xgboost = [Result(fs, '', agg, phrase_merger(fs.test, xgboost(*transform_feat_to_num(\n",
    "                remove_labels_for_binary_df(fs.train), \n",
    "                remove_labels_phrase_for_binary_df(fs.test))), agg.agg)) \n",
    "          for fs in datasets_all_fc\n",
    "          for agg in phrase_aggs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.672131</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0.554054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.703125</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.608108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.162162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.638655</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.527778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.138889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.715084</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.621359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.746803</td>\n",
       "      <td>0.789189</td>\n",
       "      <td>0.708738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.169903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          agg    dataset        f1      prec       rec\n",
       "0   phrase_mv  Wikipedia  0.672131  0.854167  0.554054\n",
       "1  phrase_max  Wikipedia  0.703125  0.833333  0.608108\n",
       "2  phrase_min  Wikipedia  0.275862  0.923077  0.162162\n",
       "3   phrase_mv   WikiNews  0.638655  0.808511  0.527778\n",
       "4  phrase_max   WikiNews  0.727273  0.800000  0.666667\n",
       "5  phrase_min   WikiNews  0.235294  0.769231  0.138889\n",
       "6   phrase_mv       News  0.715084  0.842105  0.621359\n",
       "7  phrase_max       News  0.746803  0.789189  0.708738\n",
       "8  phrase_min       News  0.285714  0.897436  0.169903"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_eval_df_from_results_phrase(results_xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   33.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   33.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   33.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   26.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   24.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   42.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.4min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.6s finished\n"
     ]
    }
   ],
   "source": [
    "results_rf = [Result(fs, '', agg, phrase_merger(fs.test, random_forest(*transform_feat_to_num(\n",
    "                remove_labels_for_binary_df(fs.train), \n",
    "                remove_labels_phrase_for_binary_df(fs.test))), agg.agg)) \n",
    "          for fs in datasets_all_fc\n",
    "          for agg in phrase_aggs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.661157</td>\n",
       "      <td>0.851064</td>\n",
       "      <td>0.540541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.621622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.292135</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.175676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.614035</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.486111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.728682</td>\n",
       "      <td>0.824561</td>\n",
       "      <td>0.652778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.279070</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.578616</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.446602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.649860</td>\n",
       "      <td>0.768212</td>\n",
       "      <td>0.563107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.183406</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.101942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          agg    dataset        f1      prec       rec\n",
       "0   phrase_mv  Wikipedia  0.661157  0.851064  0.540541\n",
       "1  phrase_max  Wikipedia  0.718750  0.851852  0.621622\n",
       "2  phrase_min  Wikipedia  0.292135  0.866667  0.175676\n",
       "3   phrase_mv   WikiNews  0.614035  0.833333  0.486111\n",
       "4  phrase_max   WikiNews  0.728682  0.824561  0.652778\n",
       "5  phrase_min   WikiNews  0.279070  0.857143  0.166667\n",
       "6   phrase_mv       News  0.578616  0.821429  0.446602\n",
       "7  phrase_max       News  0.649860  0.768212  0.563107\n",
       "8  phrase_min       News  0.183406  0.913043  0.101942"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_eval_df_from_results_phrase(results_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.3 Random Forest (Extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rfe = [Result(fs, '', agg, phrase_merger(fs.test, random_forest_extra(*transform_feat_to_num(\n",
    "                remove_labels_for_binary_df(fs.train), \n",
    "                remove_labels_phrase_for_binary_df(fs.test))), agg.agg)) \n",
    "          for fs in datasets_all_fc\n",
    "          for agg in phrase_aggs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_eval_df_from_results_phrase(results_rfe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.4 AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ada = [Result(fs, '', agg, phrase_merger(fs.test, adaboost(*transform_feat_to_num(\n",
    "                remove_labels_for_binary_df(fs.train), \n",
    "                remove_labels_phrase_for_binary_df(fs.test))), agg.agg)) \n",
    "          for fs in datasets_all_fc\n",
    "          for agg in phrase_aggs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_eval_df_from_results_phrase(results_ada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.5 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dt = [Result(fs, '', agg, phrase_merger(fs.test, decision_tree(*transform_feat_to_num(\n",
    "                remove_labels_for_binary_df(fs.train), \n",
    "                remove_labels_phrase_for_binary_df(fs.test))), agg.agg)) \n",
    "          for fs in datasets_all_fc\n",
    "          for agg in phrase_aggs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_eval_df_from_results_phrase(results_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.6 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lr = [Result(fs, '', agg, phrase_merger(fs.test, logistic_regression(*transform_feat_to_num(\n",
    "                remove_labels_for_binary_df(fs.train), \n",
    "                remove_labels_phrase_for_binary_df(fs.test))), agg.agg)) \n",
    "          for fs in datasets_all_fc\n",
    "          for agg in phrase_aggs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_eval_df_from_results_phrase(results_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.7 SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_svm = [Result(fs, '', agg, phrase_merger(fs.test, svm(*transform_feat_to_num(\n",
    "                remove_labels_for_binary_df(fs.train), \n",
    "                remove_labels_phrase_for_binary_df(fs.test))), agg.agg)) \n",
    "          for fs in datasets_all_fc\n",
    "          for agg in phrase_aggs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_eval_df_from_results_phrase(results_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.8 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_nb = [Result(fs, '', agg, phrase_merger(fs.test, naive_bayes(*transform_feat_to_num(\n",
    "                remove_labels_for_binary_df(fs.train), \n",
    "                remove_labels_phrase_for_binary_df(fs.test))), agg.agg)) \n",
    "          for fs in datasets_all_fc\n",
    "          for agg in phrase_aggs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_eval_df_from_results_phrase(results_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.9 kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_knn = [Result(fs, '', agg, phrase_merger(fs.test, knn(*transform_feat_to_num(\n",
    "                remove_labels_for_binary_df(fs.train), \n",
    "                remove_labels_phrase_for_binary_df(fs.test))), agg.agg)) \n",
    "          for fs in datasets_all_fc\n",
    "          for agg in phrase_aggs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_eval_df_from_results_phrase(results_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.10 MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_mlp = [Result(fs, '', agg, phrase_merger(fs.test, mlp(*transform_feat_to_num(\n",
    "                remove_labels_for_binary_df(fs.train), \n",
    "                remove_labels_phrase_for_binary_df(fs.test))), agg.agg)) \n",
    "          for fs in datasets_all_fc\n",
    "          for agg in phrase_aggs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_eval_df_from_results_phrase(results_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.2 A2 Feature Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test both training set inputs (DS-P and DS-WP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Studio\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\core\\fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\Studio\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-294-675e46383dbb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_datasets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# 1. Linguistic Features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdatasets_fc_linguistic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_features_linguistic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maggs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maggs_all\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m# 2. Corpus Features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# datasets_fc_frequency = compute_features_frequency(datasets, aggs=aggs_all)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-5bfb10e52021>\u001b[0m in \u001b[0;36mcompute_features_linguistic\u001b[1;34m(datasets, aggs, drop_features)\u001b[0m\n\u001b[0;32m      3\u001b[0m                         \u001b[0mfc_linguistic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                         fc_linguistic.func(ds.test, agg.agg, drop_features)) \n\u001b[1;32m----> 5\u001b[1;33m                         \u001b[1;32mfor\u001b[0m \u001b[0mds\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m                         for agg in aggs]\n",
      "\u001b[1;32m<ipython-input-24-5bfb10e52021>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m                         fc_linguistic.func(ds.test, agg.agg, drop_features)) \n\u001b[0;32m      5\u001b[0m                         \u001b[1;32mfor\u001b[0m \u001b[0mds\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m                         for agg in aggs]\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-8960ee4f63c0>\u001b[0m in \u001b[0;36mfeatures_linguistic\u001b[1;34m(dataframe, agg, drop_features)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cal_ngram_2_sim_min (lin)'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp_target\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcognate_across_languages_sim\u001b[0m\u001b[1;33m,\u001b[0m                                 \u001b[1;32mlambda\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbigram_dist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranslations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cal_ngram_2_sim_max (lin)'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp_target\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcognate_across_languages_sim\u001b[0m\u001b[1;33m,\u001b[0m                                 \u001b[1;32mlambda\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbigram_dist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranslations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cal_ngram_2_sim_mean (lin)'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp_target\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcognate_across_languages_sim\u001b[0m\u001b[1;33m,\u001b[0m                                 \u001b[1;32mlambda\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbigram_dist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranslations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cal_ngram_3_sim_min (lin)'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp_target\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcognate_across_languages_sim\u001b[0m\u001b[1;33m,\u001b[0m                                 \u001b[1;32mlambda\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtrigram_dist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranslations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cal_ngram_3_sim_max (lin)'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp_target\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcognate_across_languages_sim\u001b[0m\u001b[1;33m,\u001b[0m                                 \u001b[1;32mlambda\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtrigram_dist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranslations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3190\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3191\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3192\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3194\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/src\\inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-8960ee4f63c0>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(target)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cal_ngram_2_sim_min (lin)'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp_target\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcognate_across_languages_sim\u001b[0m\u001b[1;33m,\u001b[0m                                 \u001b[1;32mlambda\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbigram_dist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranslations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cal_ngram_2_sim_max (lin)'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp_target\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcognate_across_languages_sim\u001b[0m\u001b[1;33m,\u001b[0m                                 \u001b[1;32mlambda\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbigram_dist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranslations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cal_ngram_2_sim_mean (lin)'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp_target\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcognate_across_languages_sim\u001b[0m\u001b[1;33m,\u001b[0m                                 \u001b[1;32mlambda\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbigram_dist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranslations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cal_ngram_3_sim_min (lin)'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp_target\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcognate_across_languages_sim\u001b[0m\u001b[1;33m,\u001b[0m                                 \u001b[1;32mlambda\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtrigram_dist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranslations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cal_ngram_3_sim_max (lin)'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp_target\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcognate_across_languages_sim\u001b[0m\u001b[1;33m,\u001b[0m                                 \u001b[1;32mlambda\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtrigram_dist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranslations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-784cdc4ea2e8>\u001b[0m in \u001b[0;36magg_feat_num_average\u001b[1;34m(target, func_feature, *args, **kwargs)\u001b[0m\n\u001b[0;32m      6\u001b[0m         return np.mean([func_feature(token, *args, pos=poss) \n\u001b[0;32m      7\u001b[0m                 for token, poss in zip(word_tokenize(target), pos)])\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfunc_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0magg_feat_num_weighted_average\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc_feature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-784cdc4ea2e8>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      6\u001b[0m         return np.mean([func_feature(token, *args, pos=poss) \n\u001b[0;32m      7\u001b[0m                 for token, poss in zip(word_tokenize(target), pos)])\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfunc_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0magg_feat_num_weighted_average\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc_feature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-2245147fb7d8>\u001b[0m in \u001b[0;36mcognate_across_languages_sim\u001b[1;34m(target, sim_func, agg_func, translations)\u001b[0m\n\u001b[0;32m    109\u001b[0m     similarities = [sim_func(targ,trans_text) \n\u001b[0;32m    110\u001b[0m                     for trans_text in trans_texts]\n\u001b[1;32m--> 111\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0magg_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msimilarities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mporter_stem_len\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[1;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[0;32m   2955\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2956\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[1;32m-> 2957\u001b[1;33m                           out=out, **kwargs)\n\u001b[0m\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2959\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m         ret = um.true_divide(\n\u001b[0;32m     73\u001b[0m                 ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "datasets = load_datasets(['Wikipedia', 'WikiNews', 'News'], 'Train', 'Dev', type_train='phrase', type_test='phrase')\n",
    "datasets = preprocess_datasets(datasets)\n",
    "# 1. Linguistic Features\n",
    "datasets_fc_linguistic = compute_features_linguistic(datasets, aggs=aggs_all)\n",
    "# 2. Corpus Features\n",
    "# datasets_fc_frequency = compute_features_frequency(datasets, aggs=aggs_all)\n",
    "# datasets_fc_language_model = compute_features_language_model(datasets, aggs=aggs_all)\n",
    "# datasets_fc_corpus = compute_features_corpus([datasets_fc_frequency, datasets_fc_language_model])\n",
    "# # 3. Psycholinguistic\n",
    "# datasets_fc_psycholinguistic = compute_features_psycholinguistic(datasets, aggs=aggs_all)\n",
    "# # 4. Semantic Features\n",
    "# datasets_fc_wordnet = compute_features_wordnet(datasets, aggs=aggs_all)\n",
    "# datasets_fc_dbpedia = compute_features_dbpedia(datasets, aggs=aggs_all)\n",
    "# datasets_fc_brown_clustering = compute_features_brown_clustering(datasets, aggs=aggs_all)\n",
    "# datasets_fc_semantic = compute_features_semantic([datasets_fc_wordnet, datasets_fc_dbpedia, datasets_fc_brown_clustering])\n",
    "# # 5. Dictionary Features\n",
    "# datasets_fc_dictionary = compute_features_dictionary(datasets, aggs=aggs_all)\n",
    "# # 6. Concatentation of feature categories\n",
    "# # (1) Corpus + Semantic\n",
    "# datasets_fc_corpus_semantic = concat_feature_datasets(datasets_fc_corpus, datasets_fc_semantic, name='corpus+semantic')\n",
    "# # (2) WordNet + Psycholinguistic\n",
    "# datasets_fc_wordnet_psycholinguistic = concat_feature_datasets(datasets_fc_wordnet, \\\n",
    "#                             datasets_fc_psycholinguistic, name='wordnet+psycholinguistic')\n",
    "# (3) All categories\n",
    "# datasets_fc_all = concat_feature_datasets(datasets_fc_linguistic, datasets_fc_psycholinguistic, \\\n",
    "#                             datasets_fc_semantic, datasets_fc_corpus, datasets_fc_dictionary, name='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets_fc_linguistic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-169-6125d2e9f658>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdatasets_fc_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasets_fc_linguistic\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'datasets_fc_linguistic' is not defined"
     ]
    }
   ],
   "source": [
    "datasets_fc_all = datasets_fc_linguistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>target</th>\n",
       "      <th>nat</th>\n",
       "      <th>non_nat</th>\n",
       "      <th>nat_marked</th>\n",
       "      <th>non_nat_marked</th>\n",
       "      <th>binary</th>\n",
       "      <th>prob</th>\n",
       "      <th>p_target</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>pos_tags_pt</th>\n",
       "      <th>lemma</th>\n",
       "      <th>p_lemma</th>\n",
       "      <th>length (lin)</th>\n",
       "      <th>phrase_length (lin)</th>\n",
       "      <th>target_num_words (lin)</th>\n",
       "      <th>relative_position_left (lin)</th>\n",
       "      <th>relative_position_centered (lin)</th>\n",
       "      <th>relative_position_right (lin)</th>\n",
       "      <th>ratio_cap_letters (lin)</th>\n",
       "      <th>all_caps (lin)</th>\n",
       "      <th>ratio_num_letters (lin)</th>\n",
       "      <th>ratio_non_ascii_letters (lin)</th>\n",
       "      <th>ratio_non_alpha (lin)</th>\n",
       "      <th>grapheme_to_phoneme_ratio (lin)</th>\n",
       "      <th>num_pronounciations (lin)</th>\n",
       "      <th>hyphenated (lin)</th>\n",
       "      <th>is_title (lin)</th>\n",
       "      <th>mrc_nphon (lin)</th>\n",
       "      <th>cal_ngram_2_sim_min (lin)</th>\n",
       "      <th>cal_ngram_2_sim_max (lin)</th>\n",
       "      <th>cal_ngram_2_sim_mean (lin)</th>\n",
       "      <th>cal_ngram_3_sim_min (lin)</th>\n",
       "      <th>cal_ngram_3_sim_max (lin)</th>\n",
       "      <th>cal_ngram_3_sim_mean (lin)</th>\n",
       "      <th>num_syllables (lin)</th>\n",
       "      <th>num_vowels (lin)</th>\n",
       "      <th>vowel_consonant_ratio (lin)</th>\n",
       "      <th>porter_stem_len (lin)</th>\n",
       "      <th>porter_stemmer_num_steps (lin)</th>\n",
       "      <th>diff_len_stem_len (lin)</th>\n",
       "      <th>reduction_stem_len (lin)</th>\n",
       "      <th>norm_num_syllables (lin)</th>\n",
       "      <th>norm_num_vowels (lin)</th>\n",
       "      <th>lemma_len (lin)</th>\n",
       "      <th>reduction_lemma_len (lin)</th>\n",
       "      <th>diff_len_lemma_len (lin)</th>\n",
       "      <th>dep_dist_to_head (lin)</th>\n",
       "      <th>dep_dist_to_root (lin)</th>\n",
       "      <th>dep_dist_to_root_norm (lin)</th>\n",
       "      <th>dep_relation_to_head (lin)</th>\n",
       "      <th>dep_num_dependents (lin)</th>\n",
       "      <th>dep_max_num_dependents (lin)</th>\n",
       "      <th>dep_num_dependents_norm (lin)</th>\n",
       "      <th>dep_head_word_len (lin)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .</td>\n",
       "      <td>43</td>\n",
       "      <td>61</td>\n",
       "      <td>future generations</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.15</td>\n",
       "      <td>future generations</td>\n",
       "      <td>[JJ, NNS]</td>\n",
       "      <td>[a, n]</td>\n",
       "      <td>future generation</td>\n",
       "      <td>future generation</td>\n",
       "      <td>9.386500</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>0.298611</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.423611</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.215051</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1.936200</td>\n",
       "      <td>0.383803</td>\n",
       "      <td>0.832004</td>\n",
       "      <td>0.605328</td>\n",
       "      <td>0.361227</td>\n",
       "      <td>0.850375</td>\n",
       "      <td>0.603351</td>\n",
       "      <td>3.354600</td>\n",
       "      <td>4.354600</td>\n",
       "      <td>0.887117</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.354600</td>\n",
       "      <td>4.386500</td>\n",
       "      <td>0.467320</td>\n",
       "      <td>0.357386</td>\n",
       "      <td>0.463922</td>\n",
       "      <td>8.412099</td>\n",
       "      <td>0.103809</td>\n",
       "      <td>0.974401</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>misc</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>8.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .</td>\n",
       "      <td>104</td>\n",
       "      <td>142</td>\n",
       "      <td>traditional connection to that country</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>traditional connection to that country</td>\n",
       "      <td>[JJ, NN, TO, DT, NN]</td>\n",
       "      <td>[a, n, n, n, n]</td>\n",
       "      <td>traditional connection to that country</td>\n",
       "      <td>traditional connection to that country</td>\n",
       "      <td>9.672044</td>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.188375</td>\n",
       "      <td>1.019187</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>7.870827</td>\n",
       "      <td>0.260069</td>\n",
       "      <td>0.709980</td>\n",
       "      <td>0.510124</td>\n",
       "      <td>0.272206</td>\n",
       "      <td>0.683987</td>\n",
       "      <td>0.502785</td>\n",
       "      <td>3.101048</td>\n",
       "      <td>4.085329</td>\n",
       "      <td>0.730719</td>\n",
       "      <td>6.633825</td>\n",
       "      <td>1.296364</td>\n",
       "      <td>3.038218</td>\n",
       "      <td>0.314124</td>\n",
       "      <td>0.320620</td>\n",
       "      <td>0.422385</td>\n",
       "      <td>9.672044</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>misc</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>8.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...</td>\n",
       "      <td>15</td>\n",
       "      <td>48</td>\n",
       "      <td>Aboriginal land rights legislaton</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>aboriginal land rights legislaton</td>\n",
       "      <td>[NNP, NN, NNS, NN]</td>\n",
       "      <td>[n, n, n, n]</td>\n",
       "      <td>Aboriginal land right legislaton</td>\n",
       "      <td>aboriginal land right legislaton</td>\n",
       "      <td>8.901884</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>0.041899</td>\n",
       "      <td>0.087989</td>\n",
       "      <td>0.134078</td>\n",
       "      <td>0.038567</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.111314</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>3.534519</td>\n",
       "      <td>0.119064</td>\n",
       "      <td>0.705680</td>\n",
       "      <td>0.458919</td>\n",
       "      <td>0.111010</td>\n",
       "      <td>0.705104</td>\n",
       "      <td>0.454485</td>\n",
       "      <td>3.623244</td>\n",
       "      <td>3.623244</td>\n",
       "      <td>0.683154</td>\n",
       "      <td>7.993241</td>\n",
       "      <td>0.494528</td>\n",
       "      <td>0.908643</td>\n",
       "      <td>0.102073</td>\n",
       "      <td>0.407020</td>\n",
       "      <td>0.407020</td>\n",
       "      <td>8.852055</td>\n",
       "      <td>0.005598</td>\n",
       "      <td>0.049829</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.073770</td>\n",
       "      <td>misc</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>9.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...</td>\n",
       "      <td>26</td>\n",
       "      <td>48</td>\n",
       "      <td>land rights legislaton</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>land rights legislaton</td>\n",
       "      <td>[NN, NNS, NN]</td>\n",
       "      <td>[n, n, n]</td>\n",
       "      <td>land right legislaton</td>\n",
       "      <td>land right legislaton</td>\n",
       "      <td>8.212491</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>0.072626</td>\n",
       "      <td>0.103352</td>\n",
       "      <td>0.134078</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.111441</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.597319</td>\n",
       "      <td>0.134194</td>\n",
       "      <td>0.708732</td>\n",
       "      <td>0.456383</td>\n",
       "      <td>0.121812</td>\n",
       "      <td>0.689934</td>\n",
       "      <td>0.446581</td>\n",
       "      <td>2.883363</td>\n",
       "      <td>2.883363</td>\n",
       "      <td>0.512878</td>\n",
       "      <td>7.989608</td>\n",
       "      <td>0.222883</td>\n",
       "      <td>0.222883</td>\n",
       "      <td>0.027139</td>\n",
       "      <td>0.351095</td>\n",
       "      <td>0.351095</td>\n",
       "      <td>8.103843</td>\n",
       "      <td>0.013230</td>\n",
       "      <td>0.108648</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.065574</td>\n",
       "      <td>misc</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...</td>\n",
       "      <td>100</td>\n",
       "      <td>119</td>\n",
       "      <td>Aboriginal protests</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>aboriginal protests</td>\n",
       "      <td>[NNP, NNS]</td>\n",
       "      <td>[n, n]</td>\n",
       "      <td>Aboriginal protest</td>\n",
       "      <td>aboriginal protest</td>\n",
       "      <td>9.109893</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>0.279330</td>\n",
       "      <td>0.305866</td>\n",
       "      <td>0.332402</td>\n",
       "      <td>0.055495</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.125240</td>\n",
       "      <td>2.335160</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>4.646699</td>\n",
       "      <td>0.325994</td>\n",
       "      <td>0.764493</td>\n",
       "      <td>0.587681</td>\n",
       "      <td>0.332195</td>\n",
       "      <td>0.790661</td>\n",
       "      <td>0.592243</td>\n",
       "      <td>3.548900</td>\n",
       "      <td>3.548900</td>\n",
       "      <td>0.677533</td>\n",
       "      <td>7.516300</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.593593</td>\n",
       "      <td>0.174930</td>\n",
       "      <td>0.389565</td>\n",
       "      <td>0.389565</td>\n",
       "      <td>8.695126</td>\n",
       "      <td>0.045529</td>\n",
       "      <td>0.414767</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.090164</td>\n",
       "      <td>misc</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...</td>\n",
       "      <td>182</td>\n",
       "      <td>202</td>\n",
       "      <td>Yolngu Bark Petition</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>yolngu bark petition</td>\n",
       "      <td>[NNP, NNP, NNP]</td>\n",
       "      <td>[n, n, n]</td>\n",
       "      <td>Yolngu Bark Petition</td>\n",
       "      <td>yolngu bark petition</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>0.508380</td>\n",
       "      <td>0.536313</td>\n",
       "      <td>0.564246</td>\n",
       "      <td>0.180556</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.047619</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>3.156110</td>\n",
       "      <td>0.556801</td>\n",
       "      <td>0.729545</td>\n",
       "      <td>0.645013</td>\n",
       "      <td>0.524845</td>\n",
       "      <td>0.689989</td>\n",
       "      <td>0.609565</td>\n",
       "      <td>1.977733</td>\n",
       "      <td>2.646535</td>\n",
       "      <td>0.779201</td>\n",
       "      <td>5.028673</td>\n",
       "      <td>0.308931</td>\n",
       "      <td>0.971327</td>\n",
       "      <td>0.161888</td>\n",
       "      <td>0.329622</td>\n",
       "      <td>0.441089</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.311475</td>\n",
       "      <td>misc</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>7.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...</td>\n",
       "      <td>218</td>\n",
       "      <td>237</td>\n",
       "      <td>Wave Hill Walk-Off</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>wave hill walk-off</td>\n",
       "      <td>[NNP, NNP, NNP]</td>\n",
       "      <td>[n, n, n]</td>\n",
       "      <td>Wave Hill Walk-Off</td>\n",
       "      <td>wave hill walk-off</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>0.608939</td>\n",
       "      <td>0.635475</td>\n",
       "      <td>0.662011</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>1.222222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1.564889</td>\n",
       "      <td>0.018746</td>\n",
       "      <td>0.618791</td>\n",
       "      <td>0.244032</td>\n",
       "      <td>0.012498</td>\n",
       "      <td>0.587586</td>\n",
       "      <td>0.228008</td>\n",
       "      <td>1.478370</td>\n",
       "      <td>1.812536</td>\n",
       "      <td>0.556110</td>\n",
       "      <td>5.913482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.580149</td>\n",
       "      <td>-0.108778</td>\n",
       "      <td>0.277194</td>\n",
       "      <td>0.339850</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>26.5</td>\n",
       "      <td>0.434426</td>\n",
       "      <td>misc</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>7.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...</td>\n",
       "      <td>254</td>\n",
       "      <td>280</td>\n",
       "      <td>Aboriginal Lands Trust Act</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>aboriginal lands trust act</td>\n",
       "      <td>[NNP, NNP, NNP, NNP]</td>\n",
       "      <td>[n, n, n, n]</td>\n",
       "      <td>Aboriginal Lands Trust Act</td>\n",
       "      <td>aboriginal lands trust act</td>\n",
       "      <td>5.750000</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>0.709497</td>\n",
       "      <td>0.745810</td>\n",
       "      <td>0.782123</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.027778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>4.850412</td>\n",
       "      <td>0.066272</td>\n",
       "      <td>0.610947</td>\n",
       "      <td>0.333734</td>\n",
       "      <td>0.062258</td>\n",
       "      <td>0.625619</td>\n",
       "      <td>0.332800</td>\n",
       "      <td>2.431241</td>\n",
       "      <td>2.431241</td>\n",
       "      <td>0.547293</td>\n",
       "      <td>5.572077</td>\n",
       "      <td>0.627679</td>\n",
       "      <td>0.177923</td>\n",
       "      <td>0.030943</td>\n",
       "      <td>0.422825</td>\n",
       "      <td>0.422825</td>\n",
       "      <td>5.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0.565574</td>\n",
       "      <td>misc</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>4.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>However , it was not until the 1970s , when indigenous Australians ( both Australian Aborigines and Torres Strait Islanders ) became more politically active , that there emerged powerful movement ...</td>\n",
       "      <td>44</td>\n",
       "      <td>66</td>\n",
       "      <td>indigenous Australians</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>indigenous australians</td>\n",
       "      <td>[JJ, NNPS]</td>\n",
       "      <td>[a, n]</td>\n",
       "      <td>indigenous Australians</td>\n",
       "      <td>indigenous australians</td>\n",
       "      <td>10.575889</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>0.181070</td>\n",
       "      <td>0.226337</td>\n",
       "      <td>0.271605</td>\n",
       "      <td>0.052354</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.104712</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>3.998279</td>\n",
       "      <td>0.434430</td>\n",
       "      <td>0.775043</td>\n",
       "      <td>0.642560</td>\n",
       "      <td>0.449582</td>\n",
       "      <td>0.790166</td>\n",
       "      <td>0.649506</td>\n",
       "      <td>3.444253</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.907376</td>\n",
       "      <td>8.667240</td>\n",
       "      <td>1.444253</td>\n",
       "      <td>1.908648</td>\n",
       "      <td>0.180472</td>\n",
       "      <td>0.325670</td>\n",
       "      <td>0.472774</td>\n",
       "      <td>10.575889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.065789</td>\n",
       "      <td>misc</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>8.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>However , it was not until the 1970s , when indigenous Australians ( both Australian Aborigines and Torres Strait Islanders ) became more politically active , that there emerged powerful movement ...</td>\n",
       "      <td>74</td>\n",
       "      <td>95</td>\n",
       "      <td>Australian Aborigines</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>australian aborigines</td>\n",
       "      <td>[NNP, NNPS]</td>\n",
       "      <td>[n, n]</td>\n",
       "      <td>Australian Aborigines</td>\n",
       "      <td>australian aborigines</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>0.304527</td>\n",
       "      <td>0.347737</td>\n",
       "      <td>0.390947</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.111111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>9.746446</td>\n",
       "      <td>0.793085</td>\n",
       "      <td>0.976950</td>\n",
       "      <td>0.890011</td>\n",
       "      <td>0.804610</td>\n",
       "      <td>0.983096</td>\n",
       "      <td>0.896670</td>\n",
       "      <td>4.492891</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.507109</td>\n",
       "      <td>1.492891</td>\n",
       "      <td>1.492891</td>\n",
       "      <td>0.149289</td>\n",
       "      <td>0.449289</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.171053</td>\n",
       "      <td>misc</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>10.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>However , it was not until the 1970s , when indigenous Australians ( both Australian Aborigines and Torres Strait Islanders ) became more politically active , that there emerged powerful movement ...</td>\n",
       "      <td>74</td>\n",
       "      <td>123</td>\n",
       "      <td>Australian Aborigines and Torres Strait Islanders</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>australian aborigines and torres strait islanders</td>\n",
       "      <td>[NNP, NNPS, CC, NNP, NNP, NNP]</td>\n",
       "      <td>[n, n, n, n, n, n]</td>\n",
       "      <td>Australian Aborigines and Torres Strait Islanders</td>\n",
       "      <td>australian aborigines and torres strait islanders</td>\n",
       "      <td>8.196748</td>\n",
       "      <td>49</td>\n",
       "      <td>6</td>\n",
       "      <td>0.304527</td>\n",
       "      <td>0.405350</td>\n",
       "      <td>0.506173</td>\n",
       "      <td>0.128808</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.181474</td>\n",
       "      <td>1.000625</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>4.230849</td>\n",
       "      <td>0.452201</td>\n",
       "      <td>0.755893</td>\n",
       "      <td>0.581770</td>\n",
       "      <td>0.430871</td>\n",
       "      <td>0.772451</td>\n",
       "      <td>0.570711</td>\n",
       "      <td>2.817979</td>\n",
       "      <td>3.198301</td>\n",
       "      <td>0.661347</td>\n",
       "      <td>6.348989</td>\n",
       "      <td>1.401451</td>\n",
       "      <td>1.847759</td>\n",
       "      <td>0.225426</td>\n",
       "      <td>0.343792</td>\n",
       "      <td>0.390191</td>\n",
       "      <td>8.196748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.166667</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.223684</td>\n",
       "      <td>misc</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>9.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>However , it was not until the 1970s , when indigenous Australians ( both Australian Aborigines and Torres Strait Islanders ) became more politically active , that there emerged powerful movement ...</td>\n",
       "      <td>100</td>\n",
       "      <td>113</td>\n",
       "      <td>Torres Strait</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>torres strait</td>\n",
       "      <td>[NNP, NNP]</td>\n",
       "      <td>[n, n]</td>\n",
       "      <td>Torres Strait</td>\n",
       "      <td>torres strait</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0.411523</td>\n",
       "      <td>0.438272</td>\n",
       "      <td>0.465021</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.431656</td>\n",
       "      <td>0.296523</td>\n",
       "      <td>0.756834</td>\n",
       "      <td>0.474980</td>\n",
       "      <td>0.287630</td>\n",
       "      <td>0.770344</td>\n",
       "      <td>0.460183</td>\n",
       "      <td>1.513669</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>4.972662</td>\n",
       "      <td>1.027338</td>\n",
       "      <td>1.027338</td>\n",
       "      <td>0.171223</td>\n",
       "      <td>0.252278</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>misc</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>3XU9MCX6VODXPI3L8I02CM94TFB2R7</td>\n",
       "      <td>However , it was not until the 1970s , when indigenous Australians ( both Australian Aborigines and Torres Strait Islanders ) became more politically active , that there emerged powerful movement ...</td>\n",
       "      <td>219</td>\n",
       "      <td>241</td>\n",
       "      <td>Aboriginal land rights</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>aboriginal land rights</td>\n",
       "      <td>[NNP, NN, NNS]</td>\n",
       "      <td>[n, n, n]</td>\n",
       "      <td>Aboriginal land right</td>\n",
       "      <td>aboriginal land right</td>\n",
       "      <td>8.212505</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>0.901235</td>\n",
       "      <td>0.946502</td>\n",
       "      <td>0.991770</td>\n",
       "      <td>0.062779</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.181195</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>5.973915</td>\n",
       "      <td>0.114967</td>\n",
       "      <td>0.659406</td>\n",
       "      <td>0.401809</td>\n",
       "      <td>0.101354</td>\n",
       "      <td>0.668890</td>\n",
       "      <td>0.394315</td>\n",
       "      <td>3.363221</td>\n",
       "      <td>3.363221</td>\n",
       "      <td>0.694533</td>\n",
       "      <td>6.608249</td>\n",
       "      <td>0.835833</td>\n",
       "      <td>1.604256</td>\n",
       "      <td>0.195343</td>\n",
       "      <td>0.409524</td>\n",
       "      <td>0.409524</td>\n",
       "      <td>8.103859</td>\n",
       "      <td>0.013229</td>\n",
       "      <td>0.108647</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>misc</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>7.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>3Y7LTZE0YTNFBLYE1J4L486VLNEUZB</td>\n",
       "      <td>Her novel The Good Earth was the best-selling fiction book in the U.S. in 1931 and 1932 and won the Pulitzer Prize in 1932 .</td>\n",
       "      <td>100</td>\n",
       "      <td>114</td>\n",
       "      <td>Pulitzer Prize</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>pulitzer prize</td>\n",
       "      <td>[NNP, NNP]</td>\n",
       "      <td>[n, n]</td>\n",
       "      <td>Pulitzer Prize</td>\n",
       "      <td>pulitzer prize</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0.862903</td>\n",
       "      <td>0.919355</td>\n",
       "      <td>0.162500</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.196429</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1.419331</td>\n",
       "      <td>0.612907</td>\n",
       "      <td>0.893550</td>\n",
       "      <td>0.746329</td>\n",
       "      <td>0.586025</td>\n",
       "      <td>0.905378</td>\n",
       "      <td>0.746687</td>\n",
       "      <td>2.290334</td>\n",
       "      <td>2.645167</td>\n",
       "      <td>0.623656</td>\n",
       "      <td>5.645167</td>\n",
       "      <td>0.645167</td>\n",
       "      <td>0.854833</td>\n",
       "      <td>0.131513</td>\n",
       "      <td>0.352359</td>\n",
       "      <td>0.406949</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>misc</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>3Y7LTZE0YTNFBLYE1J4L486VLNEUZB</td>\n",
       "      <td>In 1921 , Buck 's mother died of a tropical disease , sprue , and shortly afterward her father moved in .</td>\n",
       "      <td>35</td>\n",
       "      <td>51</td>\n",
       "      <td>tropical disease</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>tropical disease</td>\n",
       "      <td>[JJ, NN]</td>\n",
       "      <td>[a, n]</td>\n",
       "      <td>tropical disease</td>\n",
       "      <td>tropical disease</td>\n",
       "      <td>7.509948</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.409524</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.196021</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>6.529845</td>\n",
       "      <td>0.377815</td>\n",
       "      <td>0.583456</td>\n",
       "      <td>0.475514</td>\n",
       "      <td>0.400611</td>\n",
       "      <td>0.575289</td>\n",
       "      <td>0.481813</td>\n",
       "      <td>2.509948</td>\n",
       "      <td>3.490052</td>\n",
       "      <td>0.959371</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.509948</td>\n",
       "      <td>0.201060</td>\n",
       "      <td>0.334216</td>\n",
       "      <td>0.464724</td>\n",
       "      <td>7.509948</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>misc</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>3Y7LTZE0YTNFBLYE1J4L486VLNEUZB</td>\n",
       "      <td>She wrote on a diverse variety of topics including women 's rights , Asian cultures , immigration , adoption , missionary work , and war .</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "      <td>diverse variety</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>diverse variety</td>\n",
       "      <td>[JJ, NN]</td>\n",
       "      <td>[a, n]</td>\n",
       "      <td>diverse variety</td>\n",
       "      <td>diverse variety</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.108696</td>\n",
       "      <td>0.163043</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.313467</td>\n",
       "      <td>1.629143</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>5.741715</td>\n",
       "      <td>0.062914</td>\n",
       "      <td>0.920531</td>\n",
       "      <td>0.607538</td>\n",
       "      <td>0.073400</td>\n",
       "      <td>0.929361</td>\n",
       "      <td>0.626696</td>\n",
       "      <td>2.741715</td>\n",
       "      <td>3.370857</td>\n",
       "      <td>0.966333</td>\n",
       "      <td>6.370857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.629143</td>\n",
       "      <td>0.089878</td>\n",
       "      <td>0.391674</td>\n",
       "      <td>0.481551</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>misc</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>3Y7LTZE0YTNFBLYE1J4L486VLNEUZB</td>\n",
       "      <td>She wrote on a diverse variety of topics including women 's rights , Asian cultures , immigration , adoption , missionary work , and war .</td>\n",
       "      <td>111</td>\n",
       "      <td>126</td>\n",
       "      <td>missionary work</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>missionary work</td>\n",
       "      <td>[JJ, NN]</td>\n",
       "      <td>[a, n]</td>\n",
       "      <td>missionary work</td>\n",
       "      <td>missionary work</td>\n",
       "      <td>9.199682</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.804348</td>\n",
       "      <td>0.858696</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.261116</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>6.466454</td>\n",
       "      <td>0.649960</td>\n",
       "      <td>0.649960</td>\n",
       "      <td>0.649960</td>\n",
       "      <td>0.621073</td>\n",
       "      <td>0.635517</td>\n",
       "      <td>0.627492</td>\n",
       "      <td>3.599841</td>\n",
       "      <td>4.466454</td>\n",
       "      <td>0.911076</td>\n",
       "      <td>9.199682</td>\n",
       "      <td>0.866614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.391301</td>\n",
       "      <td>0.485501</td>\n",
       "      <td>9.199682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>18.5</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>misc</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>3VJ4PFXFJ38EADQ6PROMNFFMC27AUQ</td>\n",
       "      <td>Another Baroque piece that stands is the Chapel of Nuestra Señora del Rosario , which is a recognizable feature of the architecture of Quito .</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>Baroque piece</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>baroque piece</td>\n",
       "      <td>[NNP, NN]</td>\n",
       "      <td>[n, n]</td>\n",
       "      <td>Baroque piece</td>\n",
       "      <td>baroque piece</td>\n",
       "      <td>6.239649</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0.056338</td>\n",
       "      <td>0.102113</td>\n",
       "      <td>0.147887</td>\n",
       "      <td>0.088546</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.718319</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>4.175339</td>\n",
       "      <td>0.334324</td>\n",
       "      <td>0.917534</td>\n",
       "      <td>0.611441</td>\n",
       "      <td>0.320828</td>\n",
       "      <td>0.917534</td>\n",
       "      <td>0.634679</td>\n",
       "      <td>1.587670</td>\n",
       "      <td>3.587670</td>\n",
       "      <td>1.402055</td>\n",
       "      <td>5.175339</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.064309</td>\n",
       "      <td>0.170572</td>\n",
       "      <td>0.254449</td>\n",
       "      <td>0.574979</td>\n",
       "      <td>6.239649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>misc</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>3VJ4PFXFJ38EADQ6PROMNFFMC27AUQ</td>\n",
       "      <td>Another Baroque piece that stands is the Chapel of Nuestra Señora del Rosario , which is a recognizable feature of the architecture of Quito .</td>\n",
       "      <td>22</td>\n",
       "      <td>33</td>\n",
       "      <td>that stands</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>that stands</td>\n",
       "      <td>[IN, VBZ]</td>\n",
       "      <td>[n, v]</td>\n",
       "      <td>that stand</td>\n",
       "      <td>that stand</td>\n",
       "      <td>5.953959</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0.154930</td>\n",
       "      <td>0.193662</td>\n",
       "      <td>0.232394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.007674</td>\n",
       "      <td>1.023021</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.069062</td>\n",
       "      <td>0.407075</td>\n",
       "      <td>0.982735</td>\n",
       "      <td>0.643150</td>\n",
       "      <td>0.488490</td>\n",
       "      <td>0.980816</td>\n",
       "      <td>0.653110</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.203069</td>\n",
       "      <td>4.976979</td>\n",
       "      <td>0.976979</td>\n",
       "      <td>0.976979</td>\n",
       "      <td>0.164089</td>\n",
       "      <td>0.167955</td>\n",
       "      <td>0.167955</td>\n",
       "      <td>4.975714</td>\n",
       "      <td>0.164302</td>\n",
       "      <td>0.978244</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>misc</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>5.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>3VJ4PFXFJ38EADQ6PROMNFFMC27AUQ</td>\n",
       "      <td>Another Baroque piece that stands is the Chapel of Nuestra Señora del Rosario , which is a recognizable feature of the architecture of Quito .</td>\n",
       "      <td>41</td>\n",
       "      <td>77</td>\n",
       "      <td>Chapel of Nuestra Señora del Rosario</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>chapel of nuestra señora del rosario</td>\n",
       "      <td>[NNP, IN, NNP, NNP, FW, NNP]</td>\n",
       "      <td>[n, n, n, n, n, n]</td>\n",
       "      <td>Chapel of Nuestra Señora del Rosario</td>\n",
       "      <td>chapel of nuestra señora del rosario</td>\n",
       "      <td>6.040444</td>\n",
       "      <td>36</td>\n",
       "      <td>6</td>\n",
       "      <td>0.288732</td>\n",
       "      <td>0.415493</td>\n",
       "      <td>0.542254</td>\n",
       "      <td>0.134467</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.036203</td>\n",
       "      <td>0.036203</td>\n",
       "      <td>1.043443</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.939943</td>\n",
       "      <td>0.180825</td>\n",
       "      <td>0.960459</td>\n",
       "      <td>0.491541</td>\n",
       "      <td>0.163431</td>\n",
       "      <td>0.960889</td>\n",
       "      <td>0.476108</td>\n",
       "      <td>2.547061</td>\n",
       "      <td>2.781606</td>\n",
       "      <td>0.866948</td>\n",
       "      <td>6.027149</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013295</td>\n",
       "      <td>0.002201</td>\n",
       "      <td>0.421668</td>\n",
       "      <td>0.460497</td>\n",
       "      <td>6.040444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>misc</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>6.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>3VJ4PFXFJ38EADQ6PROMNFFMC27AUQ</td>\n",
       "      <td>Another Baroque piece that stands is the Chapel of Nuestra Señora del Rosario , which is a recognizable feature of the architecture of Quito .</td>\n",
       "      <td>51</td>\n",
       "      <td>77</td>\n",
       "      <td>Nuestra Señora del Rosario</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>nuestra señora del rosario</td>\n",
       "      <td>[NNP, NNP, FW, NNP]</td>\n",
       "      <td>[n, n, n, n]</td>\n",
       "      <td>Nuestra Señora del Rosario</td>\n",
       "      <td>nuestra señora del rosario</td>\n",
       "      <td>6.054692</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>0.359155</td>\n",
       "      <td>0.450704</td>\n",
       "      <td>0.542254</td>\n",
       "      <td>0.125626</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.046283</td>\n",
       "      <td>0.046283</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148239</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.492429</td>\n",
       "      <td>0.140096</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.477901</td>\n",
       "      <td>2.649778</td>\n",
       "      <td>2.928033</td>\n",
       "      <td>0.934950</td>\n",
       "      <td>6.035270</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019423</td>\n",
       "      <td>0.003208</td>\n",
       "      <td>0.437640</td>\n",
       "      <td>0.483597</td>\n",
       "      <td>6.054692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>misc</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>3VJ4PFXFJ38EADQ6PROMNFFMC27AUQ</td>\n",
       "      <td>Another Baroque piece that stands is the Chapel of Nuestra Señora del Rosario , which is a recognizable feature of the architecture of Quito .</td>\n",
       "      <td>91</td>\n",
       "      <td>111</td>\n",
       "      <td>recognizable feature</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>recognizable feature</td>\n",
       "      <td>[JJ, NN]</td>\n",
       "      <td>[a, n]</td>\n",
       "      <td>recognizable feature</td>\n",
       "      <td>recognizable feature</td>\n",
       "      <td>10.394020</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>0.640845</td>\n",
       "      <td>0.711268</td>\n",
       "      <td>0.781690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.240897</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>9.072824</td>\n",
       "      <td>0.227057</td>\n",
       "      <td>0.712985</td>\n",
       "      <td>0.473884</td>\n",
       "      <td>0.206158</td>\n",
       "      <td>0.673447</td>\n",
       "      <td>0.453145</td>\n",
       "      <td>4.036412</td>\n",
       "      <td>4.678804</td>\n",
       "      <td>0.913121</td>\n",
       "      <td>7.357608</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.036412</td>\n",
       "      <td>0.292131</td>\n",
       "      <td>0.388340</td>\n",
       "      <td>0.450144</td>\n",
       "      <td>10.394020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>misc</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>3VJ4PFXFJ38EADQ6PROMNFFMC27AUQ</td>\n",
       "      <td>El Panecillo is a hill located in the middle west of the city at an altitude of about 3,016 metres ( 9,895 ft ) above sea level .</td>\n",
       "      <td>118</td>\n",
       "      <td>127</td>\n",
       "      <td>sea level</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>sea level</td>\n",
       "      <td>[NN, NN]</td>\n",
       "      <td>[n, n]</td>\n",
       "      <td>sea level</td>\n",
       "      <td>sea level</td>\n",
       "      <td>3.926780</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.914729</td>\n",
       "      <td>0.949612</td>\n",
       "      <td>0.984496</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.268305</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>2.926780</td>\n",
       "      <td>0.288616</td>\n",
       "      <td>0.768305</td>\n",
       "      <td>0.463418</td>\n",
       "      <td>0.240513</td>\n",
       "      <td>0.721966</td>\n",
       "      <td>0.413419</td>\n",
       "      <td>1.463390</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.382146</td>\n",
       "      <td>3.926780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.372669</td>\n",
       "      <td>0.509323</td>\n",
       "      <td>3.926780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>20.5</td>\n",
       "      <td>0.759259</td>\n",
       "      <td>misc</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>6.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>3VJ4PFXFJ38EADQ6PROMNFFMC27AUQ</td>\n",
       "      <td>Axis is the nerve of the public space known as Independence Square or Plaza Grande ( colonial name ) , around which were built in addition the Archbishop 's Palace , the Municipal Palace , the Hot...</td>\n",
       "      <td>220</td>\n",
       "      <td>242</td>\n",
       "      <td>Metropolitan Cathedral</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>metropolitan cathedral</td>\n",
       "      <td>[NNP, NNP]</td>\n",
       "      <td>[n, n]</td>\n",
       "      <td>Metropolitan Cathedral</td>\n",
       "      <td>metropolitan cathedral</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>0.901639</td>\n",
       "      <td>0.946721</td>\n",
       "      <td>0.991803</td>\n",
       "      <td>0.097222</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.062500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>9.967753</td>\n",
       "      <td>0.397335</td>\n",
       "      <td>0.877482</td>\n",
       "      <td>0.692867</td>\n",
       "      <td>0.384721</td>\n",
       "      <td>0.868073</td>\n",
       "      <td>0.685526</td>\n",
       "      <td>3.983877</td>\n",
       "      <td>3.983877</td>\n",
       "      <td>0.605415</td>\n",
       "      <td>9.459692</td>\n",
       "      <td>0.508062</td>\n",
       "      <td>1.040308</td>\n",
       "      <td>0.099077</td>\n",
       "      <td>0.379417</td>\n",
       "      <td>0.379417</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>37.5</td>\n",
       "      <td>0.872093</td>\n",
       "      <td>misc</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>7.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>3VJ4PFXFJ38EADQ6PROMNFFMC27AUQ</td>\n",
       "      <td>A monument to the Virgin Mary is located on top of El Panecillo and is visible from most of the city of Quito .</td>\n",
       "      <td>18</td>\n",
       "      <td>29</td>\n",
       "      <td>Virgin Mary</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>virgin mary</td>\n",
       "      <td>[NNP, NNP]</td>\n",
       "      <td>[n, n]</td>\n",
       "      <td>Virgin Mary</td>\n",
       "      <td>virgin mary</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0.162162</td>\n",
       "      <td>0.211712</td>\n",
       "      <td>0.261261</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>3.087560</td>\n",
       "      <td>0.230184</td>\n",
       "      <td>0.513211</td>\n",
       "      <td>0.401715</td>\n",
       "      <td>0.217435</td>\n",
       "      <td>0.500461</td>\n",
       "      <td>0.388966</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.691244</td>\n",
       "      <td>5.235024</td>\n",
       "      <td>0.382488</td>\n",
       "      <td>-0.235024</td>\n",
       "      <td>-0.047005</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>misc</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>3VJ4PFXFJ38EADQ6PROMNFFMC27AUQ</td>\n",
       "      <td>To this end , areas were organized to locate objects within their cultural contexts , to make them accessible to the world , which used several rooms and spaces within the palace .</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>To this end</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>to this end</td>\n",
       "      <td>[TO, DT, NN]</td>\n",
       "      <td>[n, n, n]</td>\n",
       "      <td>To this end</td>\n",
       "      <td>to this end</td>\n",
       "      <td>2.193108</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030556</td>\n",
       "      <td>0.061111</td>\n",
       "      <td>0.417100</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.009103</td>\n",
       "      <td>2.695709</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>2.978000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.348480</td>\n",
       "      <td>0.174551</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>0.421349</td>\n",
       "      <td>0.199302</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.484153</td>\n",
       "      <td>2.978000</td>\n",
       "      <td>0.161081</td>\n",
       "      <td>-0.784892</td>\n",
       "      <td>-0.357891</td>\n",
       "      <td>0.455974</td>\n",
       "      <td>0.455974</td>\n",
       "      <td>2.193108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>misc</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>3VJ4PFXFJ38EADQ6PROMNFFMC27AUQ</td>\n",
       "      <td>To this end , areas were organized to locate objects within their cultural contexts , to make them accessible to the world , which used several rooms and spaces within the palace .</td>\n",
       "      <td>66</td>\n",
       "      <td>83</td>\n",
       "      <td>cultural contexts</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.30</td>\n",
       "      <td>cultural contexts</td>\n",
       "      <td>[JJ, NN]</td>\n",
       "      <td>[a, n]</td>\n",
       "      <td>cultural context</td>\n",
       "      <td>cultural context</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.413889</td>\n",
       "      <td>0.461111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.972337</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>2.957210</td>\n",
       "      <td>0.704364</td>\n",
       "      <td>0.888096</td>\n",
       "      <td>0.813162</td>\n",
       "      <td>0.690376</td>\n",
       "      <td>0.900530</td>\n",
       "      <td>0.821352</td>\n",
       "      <td>2.328579</td>\n",
       "      <td>2.328579</td>\n",
       "      <td>0.420954</td>\n",
       "      <td>6.671421</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.328579</td>\n",
       "      <td>0.166072</td>\n",
       "      <td>0.291072</td>\n",
       "      <td>0.291072</td>\n",
       "      <td>7.392208</td>\n",
       "      <td>0.075974</td>\n",
       "      <td>0.607792</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>misc</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>3HYV4299H0XJ2T052TP3N6NO0QD8E3</td>\n",
       "      <td>Beethoven 's Symphony No. 7 , Bruckner 's Symphony No. 6 and Mendelssohn 's Symphony No. 4 comprise a nearly complete list of symphonies in this key in the Romantic era .</td>\n",
       "      <td>156</td>\n",
       "      <td>168</td>\n",
       "      <td>Romantic era</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>romantic era</td>\n",
       "      <td>[NNP, NN]</td>\n",
       "      <td>[n, n]</td>\n",
       "      <td>Romantic era</td>\n",
       "      <td>romantic era</td>\n",
       "      <td>6.377716</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0.917647</td>\n",
       "      <td>0.952941</td>\n",
       "      <td>0.988235</td>\n",
       "      <td>0.084443</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.324457</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>6.090003</td>\n",
       "      <td>0.501700</td>\n",
       "      <td>0.862667</td>\n",
       "      <td>0.634756</td>\n",
       "      <td>0.499267</td>\n",
       "      <td>0.862667</td>\n",
       "      <td>0.633134</td>\n",
       "      <td>2.618001</td>\n",
       "      <td>2.618001</td>\n",
       "      <td>1.134799</td>\n",
       "      <td>4.854002</td>\n",
       "      <td>0.618001</td>\n",
       "      <td>1.523714</td>\n",
       "      <td>0.238912</td>\n",
       "      <td>0.410492</td>\n",
       "      <td>0.410492</td>\n",
       "      <td>6.377716</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>misc</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>3HYV4299H0XJ2T052TP3N6NO0QD8E3</td>\n",
       "      <td>Mozart 's Clarinet Concerto and Clarinet Quintet are both in A major , and generally Mozart was more likely to use clarinets in A major than in any other key besides E-flat major .</td>\n",
       "      <td>10</td>\n",
       "      <td>48</td>\n",
       "      <td>Clarinet Concerto and Clarinet Quintet</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>clarinet concerto and clarinet quintet</td>\n",
       "      <td>[NNP, NNP, CC, NNP, NNP]</td>\n",
       "      <td>[n, n, n, n, n]</td>\n",
       "      <td>Clarinet Concerto and Clarinet Quintet</td>\n",
       "      <td>clarinet concerto and clarinet quintet</td>\n",
       "      <td>7.746287</td>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.161111</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.129363</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000782</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>7.740838</td>\n",
       "      <td>0.637900</td>\n",
       "      <td>0.913962</td>\n",
       "      <td>0.792928</td>\n",
       "      <td>0.600781</td>\n",
       "      <td>0.913962</td>\n",
       "      <td>0.779204</td>\n",
       "      <td>2.743332</td>\n",
       "      <td>2.998337</td>\n",
       "      <td>0.638168</td>\n",
       "      <td>7.740838</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005449</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>0.354148</td>\n",
       "      <td>0.387068</td>\n",
       "      <td>7.746287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>misc</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>7.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>3HYV4299H0XJ2T052TP3N6NO0QD8E3</td>\n",
       "      <td>Mozart 's Clarinet Concerto and Clarinet Quintet are both in A major , and generally Mozart was more likely to use clarinets in A major than in any other key besides E-flat major .</td>\n",
       "      <td>85</td>\n",
       "      <td>100</td>\n",
       "      <td>Mozart was more</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>mozart was more</td>\n",
       "      <td>[NNP, VBD, RBR]</td>\n",
       "      <td>[n, v, r]</td>\n",
       "      <td>Mozart be more</td>\n",
       "      <td>mozart be more</td>\n",
       "      <td>5.839749</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>0.472222</td>\n",
       "      <td>0.513889</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.153887</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.952234</td>\n",
       "      <td>1.930224</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.169277</td>\n",
       "      <td>0.920220</td>\n",
       "      <td>0.925079</td>\n",
       "      <td>0.921921</td>\n",
       "      <td>0.919816</td>\n",
       "      <td>0.925484</td>\n",
       "      <td>0.921759</td>\n",
       "      <td>1.919006</td>\n",
       "      <td>1.992712</td>\n",
       "      <td>0.536853</td>\n",
       "      <td>5.823434</td>\n",
       "      <td>0.007288</td>\n",
       "      <td>0.016315</td>\n",
       "      <td>0.002794</td>\n",
       "      <td>0.328611</td>\n",
       "      <td>0.341232</td>\n",
       "      <td>5.739476</td>\n",
       "      <td>0.017171</td>\n",
       "      <td>0.100273</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.121212</td>\n",
       "      <td>misc</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5300</th>\n",
       "      <td>34KYK9TV2R93EA3U60TI3PH49N7SBN</td>\n",
       "      <td>The other DNA strand ( the non-template strand ) is called the coding strand , because its sequence is the same as the newly created RNA transcript ( except for the substitution of uracil for thym...</td>\n",
       "      <td>63</td>\n",
       "      <td>76</td>\n",
       "      <td>coding strand</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.20</td>\n",
       "      <td>coding strand</td>\n",
       "      <td>[NN, NN]</td>\n",
       "      <td>[n, n]</td>\n",
       "      <td>coding strand</td>\n",
       "      <td>coding strand</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.342365</td>\n",
       "      <td>0.374384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.100982</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>2.970526</td>\n",
       "      <td>0.306920</td>\n",
       "      <td>0.789620</td>\n",
       "      <td>0.470158</td>\n",
       "      <td>0.278869</td>\n",
       "      <td>0.831696</td>\n",
       "      <td>0.477170</td>\n",
       "      <td>1.504912</td>\n",
       "      <td>1.504912</td>\n",
       "      <td>0.351474</td>\n",
       "      <td>4.990175</td>\n",
       "      <td>0.504912</td>\n",
       "      <td>1.009825</td>\n",
       "      <td>0.168304</td>\n",
       "      <td>0.250819</td>\n",
       "      <td>0.250819</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.040541</td>\n",
       "      <td>misc</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5310</th>\n",
       "      <td>34KYK9TV2R93EA3U60TI3PH49N7SBN</td>\n",
       "      <td>The other DNA strand ( the non-template strand ) is called the coding strand , because its sequence is the same as the newly created RNA transcript ( except for the substitution of uracil for thym...</td>\n",
       "      <td>165</td>\n",
       "      <td>199</td>\n",
       "      <td>substitution of uracil for thymine</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>substitution of uracil for thymine</td>\n",
       "      <td>[NN, IN, NN, IN, NN]</td>\n",
       "      <td>[n, n, n, n, n]</td>\n",
       "      <td>substitution of uracil for thymine</td>\n",
       "      <td>substitution of uracil for thymine</td>\n",
       "      <td>8.249764</td>\n",
       "      <td>34</td>\n",
       "      <td>5</td>\n",
       "      <td>0.812808</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>0.980296</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.029655</td>\n",
       "      <td>1.007283</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>3.863466</td>\n",
       "      <td>0.449512</td>\n",
       "      <td>0.948504</td>\n",
       "      <td>0.698568</td>\n",
       "      <td>0.420100</td>\n",
       "      <td>0.948959</td>\n",
       "      <td>0.679010</td>\n",
       "      <td>2.974951</td>\n",
       "      <td>3.633292</td>\n",
       "      <td>0.822123</td>\n",
       "      <td>6.949029</td>\n",
       "      <td>0.658341</td>\n",
       "      <td>1.300735</td>\n",
       "      <td>0.157669</td>\n",
       "      <td>0.360610</td>\n",
       "      <td>0.440412</td>\n",
       "      <td>8.249764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>misc</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5316</th>\n",
       "      <td>34KYK9TV2R93EA3U60TI3PH49N7SBN</td>\n",
       "      <td>During transcription , a DNA sequence is read by an RNA polymerase , which produces a complementary , antiparallel RNA strand called a primary transcript .</td>\n",
       "      <td>52</td>\n",
       "      <td>66</td>\n",
       "      <td>RNA polymerase</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>rna polymerase</td>\n",
       "      <td>[NNP, NN]</td>\n",
       "      <td>[n, n]</td>\n",
       "      <td>RNA polymerase</td>\n",
       "      <td>rna polymerase</td>\n",
       "      <td>6.459276</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>0.335484</td>\n",
       "      <td>0.380645</td>\n",
       "      <td>0.425806</td>\n",
       "      <td>0.505818</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.921218</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.440941</td>\n",
       "      <td>0.466879</td>\n",
       "      <td>0.449587</td>\n",
       "      <td>0.423649</td>\n",
       "      <td>0.466879</td>\n",
       "      <td>0.446705</td>\n",
       "      <td>2.556262</td>\n",
       "      <td>3.075016</td>\n",
       "      <td>0.759377</td>\n",
       "      <td>6.112523</td>\n",
       "      <td>0.518754</td>\n",
       "      <td>0.346753</td>\n",
       "      <td>0.053683</td>\n",
       "      <td>0.395751</td>\n",
       "      <td>0.476062</td>\n",
       "      <td>6.459276</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>misc</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5323</th>\n",
       "      <td>34KYK9TV2R93EA3U60TI3PH49N7SBN</td>\n",
       "      <td>During transcription , a DNA sequence is read by an RNA polymerase , which produces a complementary , antiparallel RNA strand called a primary transcript .</td>\n",
       "      <td>133</td>\n",
       "      <td>153</td>\n",
       "      <td>a primary transcript</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>a primary transcript</td>\n",
       "      <td>[DT, JJ, NN]</td>\n",
       "      <td>[n, a, n]</td>\n",
       "      <td>a primary transcript</td>\n",
       "      <td>a primary transcript</td>\n",
       "      <td>5.685786</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>0.858065</td>\n",
       "      <td>0.922581</td>\n",
       "      <td>0.987097</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.424953</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>5.685786</td>\n",
       "      <td>0.665084</td>\n",
       "      <td>0.874359</td>\n",
       "      <td>0.795920</td>\n",
       "      <td>0.657185</td>\n",
       "      <td>0.877759</td>\n",
       "      <td>0.796828</td>\n",
       "      <td>1.738260</td>\n",
       "      <td>1.738260</td>\n",
       "      <td>inf</td>\n",
       "      <td>5.685786</td>\n",
       "      <td>0.163212</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.305720</td>\n",
       "      <td>0.305720</td>\n",
       "      <td>5.685786</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>misc</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>8.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5327</th>\n",
       "      <td>34KYK9TV2R93EA3U60TI3PH49N7SBN</td>\n",
       "      <td>During transcription , a DNA sequence is read by an RNA polymerase , which produces a complementary , antiparallel RNA strand called a primary transcript .</td>\n",
       "      <td>135</td>\n",
       "      <td>153</td>\n",
       "      <td>primary transcript</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>primary transcript</td>\n",
       "      <td>[JJ, NN]</td>\n",
       "      <td>[a, n]</td>\n",
       "      <td>primary transcript</td>\n",
       "      <td>primary transcript</td>\n",
       "      <td>9.148526</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.929032</td>\n",
       "      <td>0.987097</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>9.148526</td>\n",
       "      <td>0.417584</td>\n",
       "      <td>0.781512</td>\n",
       "      <td>0.645108</td>\n",
       "      <td>0.403849</td>\n",
       "      <td>0.787425</td>\n",
       "      <td>0.646687</td>\n",
       "      <td>2.283825</td>\n",
       "      <td>2.283825</td>\n",
       "      <td>0.391912</td>\n",
       "      <td>9.148526</td>\n",
       "      <td>0.283825</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.249639</td>\n",
       "      <td>0.249639</td>\n",
       "      <td>9.148526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.5</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>misc</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5331</th>\n",
       "      <td>3S8APUMBJXKUA5Y80GTJWQYPKXDFBR</td>\n",
       "      <td>Alchemical symbolism has been used by psychologists such as Carl Jung who reexamined alchemical symbolism and theory and presented the inner meaning of alchemical work as a spiritual path .</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>Alchemical symbolism</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.30</td>\n",
       "      <td>alchemical symbolism</td>\n",
       "      <td>[JJ, NN]</td>\n",
       "      <td>[a, n]</td>\n",
       "      <td>Alchemical symbolism</td>\n",
       "      <td>alchemical symbolism</td>\n",
       "      <td>9.512813</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052910</td>\n",
       "      <td>0.105820</td>\n",
       "      <td>0.051281</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.951281</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>9.488895</td>\n",
       "      <td>0.426266</td>\n",
       "      <td>0.772224</td>\n",
       "      <td>0.628158</td>\n",
       "      <td>0.395963</td>\n",
       "      <td>0.797779</td>\n",
       "      <td>0.630412</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.511105</td>\n",
       "      <td>0.585184</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.511105</td>\n",
       "      <td>3.512813</td>\n",
       "      <td>0.369272</td>\n",
       "      <td>0.420486</td>\n",
       "      <td>0.369092</td>\n",
       "      <td>9.512813</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.086207</td>\n",
       "      <td>misc</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>6.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5342</th>\n",
       "      <td>3S8APUMBJXKUA5Y80GTJWQYPKXDFBR</td>\n",
       "      <td>Alchemical symbolism has been used by psychologists such as Carl Jung who reexamined alchemical symbolism and theory and presented the inner meaning of alchemical work as a spiritual path .</td>\n",
       "      <td>135</td>\n",
       "      <td>148</td>\n",
       "      <td>inner meaning</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>inner meaning</td>\n",
       "      <td>[JJ, NN]</td>\n",
       "      <td>[a, n]</td>\n",
       "      <td>inner meaning</td>\n",
       "      <td>inner meaning</td>\n",
       "      <td>5.814814</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.748677</td>\n",
       "      <td>0.783069</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.558025</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>3.814814</td>\n",
       "      <td>0.383598</td>\n",
       "      <td>0.610229</td>\n",
       "      <td>0.481874</td>\n",
       "      <td>0.364198</td>\n",
       "      <td>0.619930</td>\n",
       "      <td>0.473610</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.407407</td>\n",
       "      <td>0.700617</td>\n",
       "      <td>4.592593</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>1.222221</td>\n",
       "      <td>0.210191</td>\n",
       "      <td>0.343949</td>\n",
       "      <td>0.414013</td>\n",
       "      <td>5.814814</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.5</td>\n",
       "      <td>0.534483</td>\n",
       "      <td>misc</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5347</th>\n",
       "      <td>3S8APUMBJXKUA5Y80GTJWQYPKXDFBR</td>\n",
       "      <td>Alchemical symbolism has been used by psychologists such as Carl Jung who reexamined alchemical symbolism and theory and presented the inner meaning of alchemical work as a spiritual path .</td>\n",
       "      <td>173</td>\n",
       "      <td>187</td>\n",
       "      <td>spiritual path</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>spiritual path</td>\n",
       "      <td>[JJ, NN]</td>\n",
       "      <td>[a, n]</td>\n",
       "      <td>spiritual path</td>\n",
       "      <td>spiritual path</td>\n",
       "      <td>6.572385</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>0.915344</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.989418</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.161841</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>5.144770</td>\n",
       "      <td>0.114328</td>\n",
       "      <td>0.538233</td>\n",
       "      <td>0.363289</td>\n",
       "      <td>0.095274</td>\n",
       "      <td>0.570775</td>\n",
       "      <td>0.362068</td>\n",
       "      <td>2.286192</td>\n",
       "      <td>2.543431</td>\n",
       "      <td>0.573423</td>\n",
       "      <td>5.543431</td>\n",
       "      <td>0.514477</td>\n",
       "      <td>1.028954</td>\n",
       "      <td>0.156557</td>\n",
       "      <td>0.347848</td>\n",
       "      <td>0.386988</td>\n",
       "      <td>6.572385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>22.5</td>\n",
       "      <td>0.775862</td>\n",
       "      <td>misc</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>6.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5363</th>\n",
       "      <td>3S8APUMBJXKUA5Y80GTJWQYPKXDFBR</td>\n",
       "      <td>The plot was originally taken from - but partly altered for better conformity - Ludovico Ariosto 's Orlando furioso ( like those of the Handel operas Orlando and Ariodante ) , an epic poem set in ...</td>\n",
       "      <td>179</td>\n",
       "      <td>188</td>\n",
       "      <td>epic poem</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>epic poem</td>\n",
       "      <td>[NN, NN]</td>\n",
       "      <td>[n, n]</td>\n",
       "      <td>epic poem</td>\n",
       "      <td>epic poem</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.736626</td>\n",
       "      <td>0.755144</td>\n",
       "      <td>0.773663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.134577</td>\n",
       "      <td>0.638508</td>\n",
       "      <td>0.409845</td>\n",
       "      <td>0.157006</td>\n",
       "      <td>0.602621</td>\n",
       "      <td>0.410635</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>27.5</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>misc</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>5.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5371</th>\n",
       "      <td>3S8APUMBJXKUA5Y80GTJWQYPKXDFBR</td>\n",
       "      <td>The opera contains several musical sequences with opportunity for dance : these were composed for dancer Marie Sallé .</td>\n",
       "      <td>27</td>\n",
       "      <td>44</td>\n",
       "      <td>musical sequences</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>musical sequences</td>\n",
       "      <td>[JJ, NNS]</td>\n",
       "      <td>[a, n]</td>\n",
       "      <td>musical sequence</td>\n",
       "      <td>musical sequence</td>\n",
       "      <td>8.341567</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>0.228814</td>\n",
       "      <td>0.300847</td>\n",
       "      <td>0.372881</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.958848</td>\n",
       "      <td>1.670784</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>2.633732</td>\n",
       "      <td>0.705859</td>\n",
       "      <td>0.925468</td>\n",
       "      <td>0.798273</td>\n",
       "      <td>0.677760</td>\n",
       "      <td>0.913047</td>\n",
       "      <td>0.782153</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.670784</td>\n",
       "      <td>0.783539</td>\n",
       "      <td>6.341567</td>\n",
       "      <td>1.670784</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.239763</td>\n",
       "      <td>0.359645</td>\n",
       "      <td>0.440059</td>\n",
       "      <td>7.620023</td>\n",
       "      <td>0.086500</td>\n",
       "      <td>0.721544</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>misc</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>8.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5398</th>\n",
       "      <td>308KJXFUJR7ZA0BHPMYCIJTKTO0ATZ</td>\n",
       "      <td>These mountain chains are separated from each other by several tributaries of the Rhine : the Mosel , the Lahn , and the Nahe .</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>mountain chains</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>mountain chains</td>\n",
       "      <td>[NN, NNS]</td>\n",
       "      <td>[n, n]</td>\n",
       "      <td>mountain chain</td>\n",
       "      <td>mountain chain</td>\n",
       "      <td>6.650321</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0.047244</td>\n",
       "      <td>0.106299</td>\n",
       "      <td>0.165354</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.445807</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1.950963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.492442</td>\n",
       "      <td>0.285131</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.489923</td>\n",
       "      <td>0.282033</td>\n",
       "      <td>1.325160</td>\n",
       "      <td>2.650321</td>\n",
       "      <td>0.662580</td>\n",
       "      <td>5.975481</td>\n",
       "      <td>0.674840</td>\n",
       "      <td>0.674840</td>\n",
       "      <td>0.101475</td>\n",
       "      <td>0.199263</td>\n",
       "      <td>0.398525</td>\n",
       "      <td>6.166791</td>\n",
       "      <td>0.072708</td>\n",
       "      <td>0.483530</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>misc</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>7.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5402</th>\n",
       "      <td>308KJXFUJR7ZA0BHPMYCIJTKTO0ATZ</td>\n",
       "      <td>These mountain chains are separated from each other by several tributaries of the Rhine : the Mosel , the Lahn , and the Nahe .</td>\n",
       "      <td>55</td>\n",
       "      <td>74</td>\n",
       "      <td>several tributaries</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>several tributaries</td>\n",
       "      <td>[JJ, NNS]</td>\n",
       "      <td>[a, n]</td>\n",
       "      <td>several tributary</td>\n",
       "      <td>several tributary</td>\n",
       "      <td>10.496949</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>0.433071</td>\n",
       "      <td>0.507874</td>\n",
       "      <td>0.582677</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.020960</td>\n",
       "      <td>1.125763</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.754576</td>\n",
       "      <td>0.052314</td>\n",
       "      <td>0.830693</td>\n",
       "      <td>0.340063</td>\n",
       "      <td>0.039068</td>\n",
       "      <td>0.848202</td>\n",
       "      <td>0.337068</td>\n",
       "      <td>3.811356</td>\n",
       "      <td>4.748475</td>\n",
       "      <td>0.822853</td>\n",
       "      <td>8.496949</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.190532</td>\n",
       "      <td>0.363092</td>\n",
       "      <td>0.452367</td>\n",
       "      <td>8.722464</td>\n",
       "      <td>0.169048</td>\n",
       "      <td>1.774486</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>misc</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5428</th>\n",
       "      <td>308KJXFUJR7ZA0BHPMYCIJTKTO0ATZ</td>\n",
       "      <td>The Rhine Valley is bounded by mountain chains and it containins several of the historically significant places in Germany .</td>\n",
       "      <td>80</td>\n",
       "      <td>104</td>\n",
       "      <td>historically significant</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>historically significant</td>\n",
       "      <td>[RB, JJ]</td>\n",
       "      <td>[r, a]</td>\n",
       "      <td>historically significant</td>\n",
       "      <td>historically significant</td>\n",
       "      <td>11.646241</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.092320</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>3.891354</td>\n",
       "      <td>0.373413</td>\n",
       "      <td>0.662417</td>\n",
       "      <td>0.513583</td>\n",
       "      <td>0.387422</td>\n",
       "      <td>0.702950</td>\n",
       "      <td>0.535884</td>\n",
       "      <td>4.323120</td>\n",
       "      <td>4.646241</td>\n",
       "      <td>0.663749</td>\n",
       "      <td>6.707519</td>\n",
       "      <td>2.938722</td>\n",
       "      <td>4.938722</td>\n",
       "      <td>0.424061</td>\n",
       "      <td>0.371203</td>\n",
       "      <td>0.398948</td>\n",
       "      <td>11.646241</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>misc</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>8.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5429</th>\n",
       "      <td>308KJXFUJR7ZA0BHPMYCIJTKTO0ATZ</td>\n",
       "      <td>The Rhine Valley is bounded by mountain chains and it containins several of the historically significant places in Germany .</td>\n",
       "      <td>80</td>\n",
       "      <td>111</td>\n",
       "      <td>historically significant places</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>historically significant places</td>\n",
       "      <td>[RB, JJ, NNS]</td>\n",
       "      <td>[r, a, n]</td>\n",
       "      <td>historically significant place</td>\n",
       "      <td>historically significant place</td>\n",
       "      <td>10.159291</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.770161</td>\n",
       "      <td>0.895161</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.068007</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>2.866558</td>\n",
       "      <td>0.275074</td>\n",
       "      <td>0.751320</td>\n",
       "      <td>0.497465</td>\n",
       "      <td>0.285393</td>\n",
       "      <td>0.781179</td>\n",
       "      <td>0.507623</td>\n",
       "      <td>3.711322</td>\n",
       "      <td>3.949347</td>\n",
       "      <td>0.620625</td>\n",
       "      <td>6.257840</td>\n",
       "      <td>2.428155</td>\n",
       "      <td>3.901451</td>\n",
       "      <td>0.384028</td>\n",
       "      <td>0.365313</td>\n",
       "      <td>0.388742</td>\n",
       "      <td>10.800481</td>\n",
       "      <td>-0.063114</td>\n",
       "      <td>-0.641190</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>misc</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5434</th>\n",
       "      <td>308KJXFUJR7ZA0BHPMYCIJTKTO0ATZ</td>\n",
       "      <td>Pygmalion is a play by George Bernard Shaw , named after a Greek mythological character .</td>\n",
       "      <td>59</td>\n",
       "      <td>87</td>\n",
       "      <td>Greek mythological character</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>greek mythological character</td>\n",
       "      <td>[JJ, JJ, NN]</td>\n",
       "      <td>[a, a, n]</td>\n",
       "      <td>Greek mythological character</td>\n",
       "      <td>greek mythological character</td>\n",
       "      <td>8.501684</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>0.662921</td>\n",
       "      <td>0.820225</td>\n",
       "      <td>0.977528</td>\n",
       "      <td>0.087297</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.189110</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>8.395024</td>\n",
       "      <td>0.409446</td>\n",
       "      <td>0.754253</td>\n",
       "      <td>0.582572</td>\n",
       "      <td>0.406237</td>\n",
       "      <td>0.778514</td>\n",
       "      <td>0.599580</td>\n",
       "      <td>3.566715</td>\n",
       "      <td>3.828309</td>\n",
       "      <td>0.660374</td>\n",
       "      <td>7.021764</td>\n",
       "      <td>1.283358</td>\n",
       "      <td>1.479920</td>\n",
       "      <td>0.174074</td>\n",
       "      <td>0.419530</td>\n",
       "      <td>0.450300</td>\n",
       "      <td>8.501684</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>misc</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>7.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5442</th>\n",
       "      <td>308KJXFUJR7ZA0BHPMYCIJTKTO0ATZ</td>\n",
       "      <td>Pygmalion is a play by George Bernard Shaw , named after a Greek mythological character .</td>\n",
       "      <td>65</td>\n",
       "      <td>87</td>\n",
       "      <td>mythological character</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.15</td>\n",
       "      <td>mythological character</td>\n",
       "      <td>[JJ, NN]</td>\n",
       "      <td>[a, n]</td>\n",
       "      <td>mythological character</td>\n",
       "      <td>mythological character</td>\n",
       "      <td>11.214029</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>0.730337</td>\n",
       "      <td>0.853933</td>\n",
       "      <td>0.977528</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.141946</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>9.952039</td>\n",
       "      <td>0.430506</td>\n",
       "      <td>0.773473</td>\n",
       "      <td>0.596079</td>\n",
       "      <td>0.420255</td>\n",
       "      <td>0.794520</td>\n",
       "      <td>0.609928</td>\n",
       "      <td>4.476019</td>\n",
       "      <td>4.476019</td>\n",
       "      <td>0.658145</td>\n",
       "      <td>7.738010</td>\n",
       "      <td>1.738010</td>\n",
       "      <td>3.476019</td>\n",
       "      <td>0.309971</td>\n",
       "      <td>0.399145</td>\n",
       "      <td>0.399145</td>\n",
       "      <td>11.214029</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>misc</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5455</th>\n",
       "      <td>308KJXFUJR7ZA0BHPMYCIJTKTO0ATZ</td>\n",
       "      <td>Santiago de Cuba province has been the site of many battles , both during the war for independence and the 1959 Cuban Revolution , where much of the guerrilla fighting took place in the mountainou...</td>\n",
       "      <td>149</td>\n",
       "      <td>167</td>\n",
       "      <td>guerrilla fighting</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.15</td>\n",
       "      <td>guerrilla fighting</td>\n",
       "      <td>[NN, VBG]</td>\n",
       "      <td>[n, v]</td>\n",
       "      <td>guerrilla fight</td>\n",
       "      <td>guerrilla fight</td>\n",
       "      <td>8.621250</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>0.716346</td>\n",
       "      <td>0.759615</td>\n",
       "      <td>0.802885</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.724250</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>3.727498</td>\n",
       "      <td>0.448680</td>\n",
       "      <td>0.668593</td>\n",
       "      <td>0.533489</td>\n",
       "      <td>0.402662</td>\n",
       "      <td>0.652812</td>\n",
       "      <td>0.512889</td>\n",
       "      <td>2.621250</td>\n",
       "      <td>3.242499</td>\n",
       "      <td>0.623250</td>\n",
       "      <td>7.484998</td>\n",
       "      <td>0.378750</td>\n",
       "      <td>1.136251</td>\n",
       "      <td>0.131797</td>\n",
       "      <td>0.304045</td>\n",
       "      <td>0.376105</td>\n",
       "      <td>7.615451</td>\n",
       "      <td>0.116665</td>\n",
       "      <td>1.005798</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.5</td>\n",
       "      <td>0.569444</td>\n",
       "      <td>misc</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5459</th>\n",
       "      <td>308KJXFUJR7ZA0BHPMYCIJTKTO0ATZ</td>\n",
       "      <td>Santiago de Cuba province has been the site of many battles , both during the war for independence and the 1959 Cuban Revolution , where much of the guerrilla fighting took place in the mountainou...</td>\n",
       "      <td>186</td>\n",
       "      <td>206</td>\n",
       "      <td>mountainous province</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>mountainous province</td>\n",
       "      <td>[JJ, NN]</td>\n",
       "      <td>[a, n]</td>\n",
       "      <td>mountainous province</td>\n",
       "      <td>mountainous province</td>\n",
       "      <td>10.237800</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>0.894231</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.990385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.316020</td>\n",
       "      <td>1.254067</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>7.745933</td>\n",
       "      <td>0.226604</td>\n",
       "      <td>0.728751</td>\n",
       "      <td>0.524650</td>\n",
       "      <td>0.198707</td>\n",
       "      <td>0.728751</td>\n",
       "      <td>0.516919</td>\n",
       "      <td>2.745933</td>\n",
       "      <td>5.237800</td>\n",
       "      <td>1.047560</td>\n",
       "      <td>7.745933</td>\n",
       "      <td>1.745933</td>\n",
       "      <td>2.491867</td>\n",
       "      <td>0.243399</td>\n",
       "      <td>0.268215</td>\n",
       "      <td>0.511614</td>\n",
       "      <td>10.237800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>26.5</td>\n",
       "      <td>0.736111</td>\n",
       "      <td>misc</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5465</th>\n",
       "      <td>308KJXFUJR7ZA0BHPMYCIJTKTO0ATZ</td>\n",
       "      <td>Santiago de Cuba Province is the second most populated province in the island of Cuba .</td>\n",
       "      <td>33</td>\n",
       "      <td>54</td>\n",
       "      <td>second most populated</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>second most populated</td>\n",
       "      <td>[JJ, RBS, JJ]</td>\n",
       "      <td>[a, r, a]</td>\n",
       "      <td>second most populated</td>\n",
       "      <td>second most populated</td>\n",
       "      <td>8.295541</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.929459</td>\n",
       "      <td>1.078444</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.938255</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.537171</td>\n",
       "      <td>0.280517</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.555578</td>\n",
       "      <td>0.284672</td>\n",
       "      <td>3.556508</td>\n",
       "      <td>3.556508</td>\n",
       "      <td>0.732169</td>\n",
       "      <td>5.025636</td>\n",
       "      <td>1.634952</td>\n",
       "      <td>3.269905</td>\n",
       "      <td>0.394176</td>\n",
       "      <td>0.428725</td>\n",
       "      <td>0.428725</td>\n",
       "      <td>8.295541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>misc</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>8.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5471</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>It occurs as far east as India and western Saudi Arabia , and as far west as the Canary Islands and Azores .</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>far east</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>far east</td>\n",
       "      <td>[RB, JJ]</td>\n",
       "      <td>[r, a]</td>\n",
       "      <td>far east</td>\n",
       "      <td>far east</td>\n",
       "      <td>3.306965</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.120370</td>\n",
       "      <td>0.157407</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.102322</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>2.306965</td>\n",
       "      <td>0.030696</td>\n",
       "      <td>0.191853</td>\n",
       "      <td>0.112554</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166273</td>\n",
       "      <td>0.102322</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.306965</td>\n",
       "      <td>0.653482</td>\n",
       "      <td>3.306965</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.302392</td>\n",
       "      <td>0.395216</td>\n",
       "      <td>3.306965</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>misc</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5472</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>It occurs as far east as India and western Saudi Arabia , and as far west as the Canary Islands and Azores .</td>\n",
       "      <td>65</td>\n",
       "      <td>73</td>\n",
       "      <td>far west</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>far west</td>\n",
       "      <td>[RB, NN]</td>\n",
       "      <td>[r, n]</td>\n",
       "      <td>far west</td>\n",
       "      <td>far west</td>\n",
       "      <td>3.284271</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.601852</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.675926</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>2.568542</td>\n",
       "      <td>0.118446</td>\n",
       "      <td>0.142136</td>\n",
       "      <td>0.134239</td>\n",
       "      <td>0.102653</td>\n",
       "      <td>0.123184</td>\n",
       "      <td>0.113182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.452621</td>\n",
       "      <td>3.284271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.304482</td>\n",
       "      <td>0.304482</td>\n",
       "      <td>3.284271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>misc</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5480</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>It occurs as far east as India and western Saudi Arabia , and as far west as the Canary Islands and Azores .</td>\n",
       "      <td>81</td>\n",
       "      <td>95</td>\n",
       "      <td>Canary Islands</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>canary islands</td>\n",
       "      <td>[NNP, NNP]</td>\n",
       "      <td>[n, n]</td>\n",
       "      <td>Canary Islands</td>\n",
       "      <td>canary islands</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.879630</td>\n",
       "      <td>0.154762</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.083333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>4.049356</td>\n",
       "      <td>0.230881</td>\n",
       "      <td>0.827649</td>\n",
       "      <td>0.552823</td>\n",
       "      <td>0.214488</td>\n",
       "      <td>0.854137</td>\n",
       "      <td>0.553804</td>\n",
       "      <td>2.674893</td>\n",
       "      <td>2.674893</td>\n",
       "      <td>0.804936</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.411522</td>\n",
       "      <td>0.411522</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>16.5</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>misc</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>5.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5485</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>There are several generations of Acherontia atropos per year , with continuous broods in Africa .</td>\n",
       "      <td>33</td>\n",
       "      <td>51</td>\n",
       "      <td>Acherontia atropos</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.30</td>\n",
       "      <td>acherontia atropos</td>\n",
       "      <td>[NNP, FW]</td>\n",
       "      <td>[n, n]</td>\n",
       "      <td>Acherontia atropos</td>\n",
       "      <td>acherontia atropos</td>\n",
       "      <td>8.500814</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>0.340206</td>\n",
       "      <td>0.432990</td>\n",
       "      <td>0.525773</td>\n",
       "      <td>0.050027</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.767850</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.883925</td>\n",
       "      <td>0.727377</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.863688</td>\n",
       "      <td>3.500204</td>\n",
       "      <td>4.000409</td>\n",
       "      <td>0.875051</td>\n",
       "      <td>8.000818</td>\n",
       "      <td>0.499796</td>\n",
       "      <td>0.499997</td>\n",
       "      <td>0.058818</td>\n",
       "      <td>0.411749</td>\n",
       "      <td>0.470591</td>\n",
       "      <td>8.500814</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>misc</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5489</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>There are several generations of Acherontia atropos per year , with continuous broods in Africa .</td>\n",
       "      <td>68</td>\n",
       "      <td>85</td>\n",
       "      <td>continuous broods</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>continuous broods</td>\n",
       "      <td>[JJ, NNS]</td>\n",
       "      <td>[a, n]</td>\n",
       "      <td>continuous brood</td>\n",
       "      <td>continuous brood</td>\n",
       "      <td>7.746818</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>0.701031</td>\n",
       "      <td>0.788660</td>\n",
       "      <td>0.876289</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>4.367045</td>\n",
       "      <td>0.227394</td>\n",
       "      <td>0.490188</td>\n",
       "      <td>0.372386</td>\n",
       "      <td>0.219182</td>\n",
       "      <td>0.490188</td>\n",
       "      <td>0.374864</td>\n",
       "      <td>2.310114</td>\n",
       "      <td>3.310114</td>\n",
       "      <td>0.718352</td>\n",
       "      <td>5.873409</td>\n",
       "      <td>1.436705</td>\n",
       "      <td>1.873409</td>\n",
       "      <td>0.241829</td>\n",
       "      <td>0.298202</td>\n",
       "      <td>0.427287</td>\n",
       "      <td>7.208487</td>\n",
       "      <td>0.069491</td>\n",
       "      <td>0.538332</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>misc</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>8.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5501</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>The newly-hatched larva starts out a light shade of green but darkens after feeding , with yellow stripes diagonally on the sides .</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>newly-hatched larva</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>newly-hatched larva</td>\n",
       "      <td>[JJ, NN]</td>\n",
       "      <td>[a, n]</td>\n",
       "      <td>newly-hatched larva</td>\n",
       "      <td>newly-hatched larva</td>\n",
       "      <td>9.057497</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>0.030534</td>\n",
       "      <td>0.103053</td>\n",
       "      <td>0.175573</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039014</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1.971252</td>\n",
       "      <td>0.373717</td>\n",
       "      <td>0.619610</td>\n",
       "      <td>0.509142</td>\n",
       "      <td>0.363792</td>\n",
       "      <td>0.619610</td>\n",
       "      <td>0.517348</td>\n",
       "      <td>3.014374</td>\n",
       "      <td>3.014374</td>\n",
       "      <td>0.553958</td>\n",
       "      <td>8.043122</td>\n",
       "      <td>0.507187</td>\n",
       "      <td>1.014374</td>\n",
       "      <td>0.111993</td>\n",
       "      <td>0.332804</td>\n",
       "      <td>0.332804</td>\n",
       "      <td>9.057497</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>misc</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>5.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5511</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>The newly-hatched larva starts out a light shade of green but darkens after feeding , with yellow stripes diagonally on the sides .</td>\n",
       "      <td>98</td>\n",
       "      <td>116</td>\n",
       "      <td>stripes diagonally</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>stripes diagonally</td>\n",
       "      <td>[NNS, RB]</td>\n",
       "      <td>[n, r]</td>\n",
       "      <td>stripe diagonally</td>\n",
       "      <td>stripe diagonally</td>\n",
       "      <td>8.551908</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>0.748092</td>\n",
       "      <td>0.816794</td>\n",
       "      <td>0.885496</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.137928</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103435</td>\n",
       "      <td>0.549416</td>\n",
       "      <td>0.352682</td>\n",
       "      <td>0.068957</td>\n",
       "      <td>0.549200</td>\n",
       "      <td>0.332497</td>\n",
       "      <td>3.069211</td>\n",
       "      <td>3.551908</td>\n",
       "      <td>0.710382</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.034606</td>\n",
       "      <td>2.551908</td>\n",
       "      <td>0.298402</td>\n",
       "      <td>0.358892</td>\n",
       "      <td>0.415335</td>\n",
       "      <td>8.035877</td>\n",
       "      <td>0.060341</td>\n",
       "      <td>0.516032</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>13.5</td>\n",
       "      <td>0.613636</td>\n",
       "      <td>misc</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>5.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5514</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>In the third instar , purple or blue edging develops on the yellow stripes and the tail horn turns from black to yellow .</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>third instar</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>third instar</td>\n",
       "      <td>[JJ, NN]</td>\n",
       "      <td>[a, n]</td>\n",
       "      <td>third instar</td>\n",
       "      <td>third instar</td>\n",
       "      <td>5.825473</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0.057851</td>\n",
       "      <td>0.107438</td>\n",
       "      <td>0.157025</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.116352</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.523582</td>\n",
       "      <td>0.029088</td>\n",
       "      <td>0.887804</td>\n",
       "      <td>0.459369</td>\n",
       "      <td>0.029088</td>\n",
       "      <td>0.887804</td>\n",
       "      <td>0.459369</td>\n",
       "      <td>1.825473</td>\n",
       "      <td>1.825473</td>\n",
       "      <td>0.456368</td>\n",
       "      <td>5.825473</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.313360</td>\n",
       "      <td>0.313360</td>\n",
       "      <td>5.825473</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.239130</td>\n",
       "      <td>misc</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5516</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>In the third instar , purple or blue edging develops on the yellow stripes and the tail horn turns from black to yellow .</td>\n",
       "      <td>32</td>\n",
       "      <td>43</td>\n",
       "      <td>blue edging</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>blue edging</td>\n",
       "      <td>[JJ, VBG]</td>\n",
       "      <td>[a, v]</td>\n",
       "      <td>blue edge</td>\n",
       "      <td>blue edge</td>\n",
       "      <td>5.404028</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0.264463</td>\n",
       "      <td>0.309917</td>\n",
       "      <td>0.355372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.450336</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>3.702014</td>\n",
       "      <td>0.074496</td>\n",
       "      <td>0.291543</td>\n",
       "      <td>0.146845</td>\n",
       "      <td>0.049664</td>\n",
       "      <td>0.293691</td>\n",
       "      <td>0.149923</td>\n",
       "      <td>1.702014</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.648993</td>\n",
       "      <td>3.297986</td>\n",
       "      <td>0.702014</td>\n",
       "      <td>2.106042</td>\n",
       "      <td>0.389717</td>\n",
       "      <td>0.314953</td>\n",
       "      <td>0.370094</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.259811</td>\n",
       "      <td>1.404028</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>misc</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5523</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>In the third instar , purple or blue edging develops on the yellow stripes and the tail horn turns from black to yellow .</td>\n",
       "      <td>83</td>\n",
       "      <td>92</td>\n",
       "      <td>tail horn</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>tail horn</td>\n",
       "      <td>[NN, NN]</td>\n",
       "      <td>[n, n]</td>\n",
       "      <td>tail horn</td>\n",
       "      <td>tail horn</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.685950</td>\n",
       "      <td>0.723140</td>\n",
       "      <td>0.760331</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.158727</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.043652</td>\n",
       "      <td>0.591845</td>\n",
       "      <td>0.255484</td>\n",
       "      <td>0.029101</td>\n",
       "      <td>0.591845</td>\n",
       "      <td>0.236083</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.476181</td>\n",
       "      <td>0.650787</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.369045</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.282609</td>\n",
       "      <td>misc</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>5.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5534</th>\n",
       "      <td>3ZUE82NE0A2B8701X4995O8OCRS8FL</td>\n",
       "      <td>The larva grows to about 120-130 mm , and pupates in an underground chamber .</td>\n",
       "      <td>56</td>\n",
       "      <td>75</td>\n",
       "      <td>underground chamber</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>underground chamber</td>\n",
       "      <td>[JJ, NN]</td>\n",
       "      <td>[a, n]</td>\n",
       "      <td>underground chamber</td>\n",
       "      <td>underground chamber</td>\n",
       "      <td>9.034016</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.850649</td>\n",
       "      <td>0.974026</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.309599</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>7.525512</td>\n",
       "      <td>0.267990</td>\n",
       "      <td>0.594200</td>\n",
       "      <td>0.407835</td>\n",
       "      <td>0.252580</td>\n",
       "      <td>0.652420</td>\n",
       "      <td>0.410404</td>\n",
       "      <td>2.508504</td>\n",
       "      <td>3.017008</td>\n",
       "      <td>0.487172</td>\n",
       "      <td>9.034016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.277673</td>\n",
       "      <td>0.333961</td>\n",
       "      <td>9.034016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>9.5</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>misc</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>718 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id  \\\n",
       "4     3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "9     3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "14    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "15    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "24    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "31    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "34    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "38    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "52    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "53    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "56    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "57    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "69    3XU9MCX6VODXPI3L8I02CM94TFB2R7   \n",
       "80    3Y7LTZE0YTNFBLYE1J4L486VLNEUZB   \n",
       "86    3Y7LTZE0YTNFBLYE1J4L486VLNEUZB   \n",
       "124   3Y7LTZE0YTNFBLYE1J4L486VLNEUZB   \n",
       "135   3Y7LTZE0YTNFBLYE1J4L486VLNEUZB   \n",
       "139   3VJ4PFXFJ38EADQ6PROMNFFMC27AUQ   \n",
       "140   3VJ4PFXFJ38EADQ6PROMNFFMC27AUQ   \n",
       "144   3VJ4PFXFJ38EADQ6PROMNFFMC27AUQ   \n",
       "145   3VJ4PFXFJ38EADQ6PROMNFFMC27AUQ   \n",
       "151   3VJ4PFXFJ38EADQ6PROMNFFMC27AUQ   \n",
       "163   3VJ4PFXFJ38EADQ6PROMNFFMC27AUQ   \n",
       "187   3VJ4PFXFJ38EADQ6PROMNFFMC27AUQ   \n",
       "190   3VJ4PFXFJ38EADQ6PROMNFFMC27AUQ   \n",
       "200   3VJ4PFXFJ38EADQ6PROMNFFMC27AUQ   \n",
       "207   3VJ4PFXFJ38EADQ6PROMNFFMC27AUQ   \n",
       "228   3HYV4299H0XJ2T052TP3N6NO0QD8E3   \n",
       "232   3HYV4299H0XJ2T052TP3N6NO0QD8E3   \n",
       "239   3HYV4299H0XJ2T052TP3N6NO0QD8E3   \n",
       "...                              ...   \n",
       "5300  34KYK9TV2R93EA3U60TI3PH49N7SBN   \n",
       "5310  34KYK9TV2R93EA3U60TI3PH49N7SBN   \n",
       "5316  34KYK9TV2R93EA3U60TI3PH49N7SBN   \n",
       "5323  34KYK9TV2R93EA3U60TI3PH49N7SBN   \n",
       "5327  34KYK9TV2R93EA3U60TI3PH49N7SBN   \n",
       "5331  3S8APUMBJXKUA5Y80GTJWQYPKXDFBR   \n",
       "5342  3S8APUMBJXKUA5Y80GTJWQYPKXDFBR   \n",
       "5347  3S8APUMBJXKUA5Y80GTJWQYPKXDFBR   \n",
       "5363  3S8APUMBJXKUA5Y80GTJWQYPKXDFBR   \n",
       "5371  3S8APUMBJXKUA5Y80GTJWQYPKXDFBR   \n",
       "5398  308KJXFUJR7ZA0BHPMYCIJTKTO0ATZ   \n",
       "5402  308KJXFUJR7ZA0BHPMYCIJTKTO0ATZ   \n",
       "5428  308KJXFUJR7ZA0BHPMYCIJTKTO0ATZ   \n",
       "5429  308KJXFUJR7ZA0BHPMYCIJTKTO0ATZ   \n",
       "5434  308KJXFUJR7ZA0BHPMYCIJTKTO0ATZ   \n",
       "5442  308KJXFUJR7ZA0BHPMYCIJTKTO0ATZ   \n",
       "5455  308KJXFUJR7ZA0BHPMYCIJTKTO0ATZ   \n",
       "5459  308KJXFUJR7ZA0BHPMYCIJTKTO0ATZ   \n",
       "5465  308KJXFUJR7ZA0BHPMYCIJTKTO0ATZ   \n",
       "5471  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5472  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5480  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5485  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5489  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5501  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5511  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5514  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5516  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5523  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "5534  3ZUE82NE0A2B8701X4995O8OCRS8FL   \n",
       "\n",
       "                                                                                                                                                                                                     sentence  \\\n",
       "4                                                            Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .   \n",
       "9                                                            Normally , the land will be passed down to future generations in a way that recognizes the community 's traditional connection to that country .   \n",
       "14    The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...   \n",
       "15    The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...   \n",
       "24    The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...   \n",
       "31    The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...   \n",
       "34    The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...   \n",
       "38    The passing of Aboriginal land rights legislaton in Australia was preceded by a number of important Aboriginal protests , including the 1946 Aboriginal Stockmen 's Strike , the 1963 Yolngu Bark Pe...   \n",
       "52    However , it was not until the 1970s , when indigenous Australians ( both Australian Aborigines and Torres Strait Islanders ) became more politically active , that there emerged powerful movement ...   \n",
       "53    However , it was not until the 1970s , when indigenous Australians ( both Australian Aborigines and Torres Strait Islanders ) became more politically active , that there emerged powerful movement ...   \n",
       "56    However , it was not until the 1970s , when indigenous Australians ( both Australian Aborigines and Torres Strait Islanders ) became more politically active , that there emerged powerful movement ...   \n",
       "57    However , it was not until the 1970s , when indigenous Australians ( both Australian Aborigines and Torres Strait Islanders ) became more politically active , that there emerged powerful movement ...   \n",
       "69    However , it was not until the 1970s , when indigenous Australians ( both Australian Aborigines and Torres Strait Islanders ) became more politically active , that there emerged powerful movement ...   \n",
       "80                                                                               Her novel The Good Earth was the best-selling fiction book in the U.S. in 1931 and 1932 and won the Pulitzer Prize in 1932 .   \n",
       "86                                                                                                  In 1921 , Buck 's mother died of a tropical disease , sprue , and shortly afterward her father moved in .   \n",
       "124                                                                She wrote on a diverse variety of topics including women 's rights , Asian cultures , immigration , adoption , missionary work , and war .   \n",
       "135                                                                She wrote on a diverse variety of topics including women 's rights , Asian cultures , immigration , adoption , missionary work , and war .   \n",
       "139                                                            Another Baroque piece that stands is the Chapel of Nuestra Señora del Rosario , which is a recognizable feature of the architecture of Quito .   \n",
       "140                                                            Another Baroque piece that stands is the Chapel of Nuestra Señora del Rosario , which is a recognizable feature of the architecture of Quito .   \n",
       "144                                                            Another Baroque piece that stands is the Chapel of Nuestra Señora del Rosario , which is a recognizable feature of the architecture of Quito .   \n",
       "145                                                            Another Baroque piece that stands is the Chapel of Nuestra Señora del Rosario , which is a recognizable feature of the architecture of Quito .   \n",
       "151                                                            Another Baroque piece that stands is the Chapel of Nuestra Señora del Rosario , which is a recognizable feature of the architecture of Quito .   \n",
       "163                                                                         El Panecillo is a hill located in the middle west of the city at an altitude of about 3,016 metres ( 9,895 ft ) above sea level .   \n",
       "187   Axis is the nerve of the public space known as Independence Square or Plaza Grande ( colonial name ) , around which were built in addition the Archbishop 's Palace , the Municipal Palace , the Hot...   \n",
       "190                                                                                           A monument to the Virgin Mary is located on top of El Panecillo and is visible from most of the city of Quito .   \n",
       "200                      To this end , areas were organized to locate objects within their cultural contexts , to make them accessible to the world , which used several rooms and spaces within the palace .   \n",
       "207                      To this end , areas were organized to locate objects within their cultural contexts , to make them accessible to the world , which used several rooms and spaces within the palace .   \n",
       "228                                Beethoven 's Symphony No. 7 , Bruckner 's Symphony No. 6 and Mendelssohn 's Symphony No. 4 comprise a nearly complete list of symphonies in this key in the Romantic era .   \n",
       "232                      Mozart 's Clarinet Concerto and Clarinet Quintet are both in A major , and generally Mozart was more likely to use clarinets in A major than in any other key besides E-flat major .   \n",
       "239                      Mozart 's Clarinet Concerto and Clarinet Quintet are both in A major , and generally Mozart was more likely to use clarinets in A major than in any other key besides E-flat major .   \n",
       "...                                                                                                                                                                                                       ...   \n",
       "5300  The other DNA strand ( the non-template strand ) is called the coding strand , because its sequence is the same as the newly created RNA transcript ( except for the substitution of uracil for thym...   \n",
       "5310  The other DNA strand ( the non-template strand ) is called the coding strand , because its sequence is the same as the newly created RNA transcript ( except for the substitution of uracil for thym...   \n",
       "5316                                              During transcription , a DNA sequence is read by an RNA polymerase , which produces a complementary , antiparallel RNA strand called a primary transcript .   \n",
       "5323                                              During transcription , a DNA sequence is read by an RNA polymerase , which produces a complementary , antiparallel RNA strand called a primary transcript .   \n",
       "5327                                              During transcription , a DNA sequence is read by an RNA polymerase , which produces a complementary , antiparallel RNA strand called a primary transcript .   \n",
       "5331            Alchemical symbolism has been used by psychologists such as Carl Jung who reexamined alchemical symbolism and theory and presented the inner meaning of alchemical work as a spiritual path .   \n",
       "5342            Alchemical symbolism has been used by psychologists such as Carl Jung who reexamined alchemical symbolism and theory and presented the inner meaning of alchemical work as a spiritual path .   \n",
       "5347            Alchemical symbolism has been used by psychologists such as Carl Jung who reexamined alchemical symbolism and theory and presented the inner meaning of alchemical work as a spiritual path .   \n",
       "5363  The plot was originally taken from - but partly altered for better conformity - Ludovico Ariosto 's Orlando furioso ( like those of the Handel operas Orlando and Ariodante ) , an epic poem set in ...   \n",
       "5371                                                                                   The opera contains several musical sequences with opportunity for dance : these were composed for dancer Marie Sallé .   \n",
       "5398                                                                          These mountain chains are separated from each other by several tributaries of the Rhine : the Mosel , the Lahn , and the Nahe .   \n",
       "5402                                                                          These mountain chains are separated from each other by several tributaries of the Rhine : the Mosel , the Lahn , and the Nahe .   \n",
       "5428                                                                             The Rhine Valley is bounded by mountain chains and it containins several of the historically significant places in Germany .   \n",
       "5429                                                                             The Rhine Valley is bounded by mountain chains and it containins several of the historically significant places in Germany .   \n",
       "5434                                                                                                                Pygmalion is a play by George Bernard Shaw , named after a Greek mythological character .   \n",
       "5442                                                                                                                Pygmalion is a play by George Bernard Shaw , named after a Greek mythological character .   \n",
       "5455  Santiago de Cuba province has been the site of many battles , both during the war for independence and the 1959 Cuban Revolution , where much of the guerrilla fighting took place in the mountainou...   \n",
       "5459  Santiago de Cuba province has been the site of many battles , both during the war for independence and the 1959 Cuban Revolution , where much of the guerrilla fighting took place in the mountainou...   \n",
       "5465                                                                                                                  Santiago de Cuba Province is the second most populated province in the island of Cuba .   \n",
       "5471                                                                                             It occurs as far east as India and western Saudi Arabia , and as far west as the Canary Islands and Azores .   \n",
       "5472                                                                                             It occurs as far east as India and western Saudi Arabia , and as far west as the Canary Islands and Azores .   \n",
       "5480                                                                                             It occurs as far east as India and western Saudi Arabia , and as far west as the Canary Islands and Azores .   \n",
       "5485                                                                                                        There are several generations of Acherontia atropos per year , with continuous broods in Africa .   \n",
       "5489                                                                                                        There are several generations of Acherontia atropos per year , with continuous broods in Africa .   \n",
       "5501                                                                      The newly-hatched larva starts out a light shade of green but darkens after feeding , with yellow stripes diagonally on the sides .   \n",
       "5511                                                                      The newly-hatched larva starts out a light shade of green but darkens after feeding , with yellow stripes diagonally on the sides .   \n",
       "5514                                                                                In the third instar , purple or blue edging develops on the yellow stripes and the tail horn turns from black to yellow .   \n",
       "5516                                                                                In the third instar , purple or blue edging develops on the yellow stripes and the tail horn turns from black to yellow .   \n",
       "5523                                                                                In the third instar , purple or blue edging develops on the yellow stripes and the tail horn turns from black to yellow .   \n",
       "5534                                                                                                                            The larva grows to about 120-130 mm , and pupates in an underground chamber .   \n",
       "\n",
       "      start  end                                             target  nat  \\\n",
       "4        43   61                                 future generations   10   \n",
       "9       104  142             traditional connection to that country   10   \n",
       "14       15   48                  Aboriginal land rights legislaton   10   \n",
       "15       26   48                             land rights legislaton   10   \n",
       "24      100  119                                Aboriginal protests   10   \n",
       "31      182  202                               Yolngu Bark Petition   10   \n",
       "34      218  237                                Wave Hill Walk-Off    10   \n",
       "38      254  280                         Aboriginal Lands Trust Act   10   \n",
       "52       44   66                             indigenous Australians   10   \n",
       "53       74   95                              Australian Aborigines   10   \n",
       "56       74  123  Australian Aborigines and Torres Strait Islanders   10   \n",
       "57      100  113                                      Torres Strait   10   \n",
       "69      219  241                             Aboriginal land rights   10   \n",
       "80      100  114                                     Pulitzer Prize   10   \n",
       "86       35   51                                   tropical disease   10   \n",
       "124      15   30                                    diverse variety   10   \n",
       "135     111  126                                    missionary work   10   \n",
       "139       8   21                                      Baroque piece   10   \n",
       "140      22   33                                        that stands   10   \n",
       "144      41   77               Chapel of Nuestra Señora del Rosario   10   \n",
       "145      51   77                         Nuestra Señora del Rosario   10   \n",
       "151      91  111                               recognizable feature   10   \n",
       "163     118  127                                          sea level   10   \n",
       "187     220  242                             Metropolitan Cathedral   10   \n",
       "190      18   29                                        Virgin Mary   10   \n",
       "200       0   11                                        To this end   10   \n",
       "207      66   83                                  cultural contexts   10   \n",
       "228     156  168                                       Romantic era   10   \n",
       "232      10   48             Clarinet Concerto and Clarinet Quintet   10   \n",
       "239      85  100                                    Mozart was more   10   \n",
       "...     ...  ...                                                ...  ...   \n",
       "5300     63   76                                      coding strand   10   \n",
       "5310    165  199                 substitution of uracil for thymine   10   \n",
       "5316     52   66                                     RNA polymerase   10   \n",
       "5323    133  153                               a primary transcript   10   \n",
       "5327    135  153                                 primary transcript   10   \n",
       "5331      0   20                               Alchemical symbolism   10   \n",
       "5342    135  148                                      inner meaning   10   \n",
       "5347    173  187                                     spiritual path   10   \n",
       "5363    179  188                                          epic poem   10   \n",
       "5371     27   44                                  musical sequences   10   \n",
       "5398      6   21                                    mountain chains   10   \n",
       "5402     55   74                                several tributaries   10   \n",
       "5428     80  104                           historically significant   10   \n",
       "5429     80  111                    historically significant places   10   \n",
       "5434     59   87                       Greek mythological character   10   \n",
       "5442     65   87                             mythological character   10   \n",
       "5455    149  167                                 guerrilla fighting   10   \n",
       "5459    186  206                               mountainous province   10   \n",
       "5465     33   54                              second most populated   10   \n",
       "5471     13   21                                           far east   10   \n",
       "5472     65   73                                           far west   10   \n",
       "5480     81   95                                     Canary Islands   10   \n",
       "5485     33   51                                 Acherontia atropos   10   \n",
       "5489     68   85                                  continuous broods   10   \n",
       "5501      4   23                                newly-hatched larva   10   \n",
       "5511     98  116                                 stripes diagonally   10   \n",
       "5514      7   19                                       third instar   10   \n",
       "5516     32   43                                        blue edging   10   \n",
       "5523     83   92                                          tail horn   10   \n",
       "5534     56   75                                underground chamber   10   \n",
       "\n",
       "      non_nat  nat_marked  non_nat_marked  binary  prob  \\\n",
       "4          10           1               2       1  0.15   \n",
       "9          10           0               0       0  0.00   \n",
       "14         10           0               1       1  0.05   \n",
       "15         10           1               0       1  0.05   \n",
       "24         10           0               1       1  0.05   \n",
       "31         10           0               0       0  0.00   \n",
       "34         10           0               0       0  0.00   \n",
       "38         10           0               0       0  0.00   \n",
       "52         10           0               0       0  0.00   \n",
       "53         10           0               0       0  0.00   \n",
       "56         10           0               0       0  0.00   \n",
       "57         10           0               0       0  0.00   \n",
       "69         10           0               0       0  0.00   \n",
       "80         10           1               0       1  0.05   \n",
       "86         10           1               1       1  0.10   \n",
       "124        10           1               0       1  0.05   \n",
       "135        10           3               2       1  0.25   \n",
       "139        10           1               0       1  0.05   \n",
       "140        10           0               1       1  0.05   \n",
       "144        10           1               0       1  0.05   \n",
       "145        10           1               0       1  0.05   \n",
       "151        10           2               0       1  0.10   \n",
       "163        10           0               0       0  0.00   \n",
       "187        10           2               0       1  0.10   \n",
       "190        10           0               0       0  0.00   \n",
       "200        10           1               0       1  0.05   \n",
       "207        10           3               3       1  0.30   \n",
       "228        10           1               1       1  0.10   \n",
       "232        10           1               0       1  0.05   \n",
       "239        10           0               0       0  0.00   \n",
       "...       ...         ...             ...     ...   ...   \n",
       "5300       10           3               1       1  0.20   \n",
       "5310       10           0               0       0  0.00   \n",
       "5316       10           1               1       1  0.10   \n",
       "5323       10           0               0       0  0.00   \n",
       "5327       10           1               0       1  0.05   \n",
       "5331       10           4               2       1  0.30   \n",
       "5342       10           1               0       1  0.05   \n",
       "5347       10           1               1       1  0.10   \n",
       "5363       10           1               1       1  0.10   \n",
       "5371       10           0               1       1  0.05   \n",
       "5398       10           1               0       1  0.05   \n",
       "5402       10           0               0       0  0.00   \n",
       "5428       10           2               0       1  0.10   \n",
       "5429       10           0               0       0  0.00   \n",
       "5434       10           0               0       0  0.00   \n",
       "5442       10           2               1       1  0.15   \n",
       "5455       10           2               1       1  0.15   \n",
       "5459       10           1               0       1  0.05   \n",
       "5465       10           0               0       0  0.00   \n",
       "5471       10           0               0       0  0.00   \n",
       "5472       10           0               0       0  0.00   \n",
       "5480       10           1               0       1  0.05   \n",
       "5485       10           4               2       1  0.30   \n",
       "5489       10           1               1       1  0.10   \n",
       "5501       10           1               0       1  0.05   \n",
       "5511       10           0               1       1  0.05   \n",
       "5514       10           0               1       1  0.05   \n",
       "5516       10           1               0       1  0.05   \n",
       "5523       10           0               2       1  0.10   \n",
       "5534       10           0               1       1  0.05   \n",
       "\n",
       "                                               p_target  \\\n",
       "4                                    future generations   \n",
       "9                traditional connection to that country   \n",
       "14                    aboriginal land rights legislaton   \n",
       "15                               land rights legislaton   \n",
       "24                                  aboriginal protests   \n",
       "31                                 yolngu bark petition   \n",
       "34                                   wave hill walk-off   \n",
       "38                           aboriginal lands trust act   \n",
       "52                               indigenous australians   \n",
       "53                                australian aborigines   \n",
       "56    australian aborigines and torres strait islanders   \n",
       "57                                        torres strait   \n",
       "69                               aboriginal land rights   \n",
       "80                                       pulitzer prize   \n",
       "86                                     tropical disease   \n",
       "124                                     diverse variety   \n",
       "135                                     missionary work   \n",
       "139                                       baroque piece   \n",
       "140                                         that stands   \n",
       "144                chapel of nuestra señora del rosario   \n",
       "145                          nuestra señora del rosario   \n",
       "151                                recognizable feature   \n",
       "163                                           sea level   \n",
       "187                              metropolitan cathedral   \n",
       "190                                         virgin mary   \n",
       "200                                         to this end   \n",
       "207                                   cultural contexts   \n",
       "228                                        romantic era   \n",
       "232              clarinet concerto and clarinet quintet   \n",
       "239                                     mozart was more   \n",
       "...                                                 ...   \n",
       "5300                                      coding strand   \n",
       "5310                 substitution of uracil for thymine   \n",
       "5316                                     rna polymerase   \n",
       "5323                               a primary transcript   \n",
       "5327                                 primary transcript   \n",
       "5331                               alchemical symbolism   \n",
       "5342                                      inner meaning   \n",
       "5347                                     spiritual path   \n",
       "5363                                          epic poem   \n",
       "5371                                  musical sequences   \n",
       "5398                                    mountain chains   \n",
       "5402                                several tributaries   \n",
       "5428                           historically significant   \n",
       "5429                    historically significant places   \n",
       "5434                       greek mythological character   \n",
       "5442                             mythological character   \n",
       "5455                                 guerrilla fighting   \n",
       "5459                               mountainous province   \n",
       "5465                              second most populated   \n",
       "5471                                           far east   \n",
       "5472                                           far west   \n",
       "5480                                     canary islands   \n",
       "5485                                 acherontia atropos   \n",
       "5489                                  continuous broods   \n",
       "5501                                newly-hatched larva   \n",
       "5511                                 stripes diagonally   \n",
       "5514                                       third instar   \n",
       "5516                                        blue edging   \n",
       "5523                                          tail horn   \n",
       "5534                                underground chamber   \n",
       "\n",
       "                            pos_tags         pos_tags_pt  \\\n",
       "4                          [JJ, NNS]              [a, n]   \n",
       "9               [JJ, NN, TO, DT, NN]     [a, n, n, n, n]   \n",
       "14                [NNP, NN, NNS, NN]        [n, n, n, n]   \n",
       "15                     [NN, NNS, NN]           [n, n, n]   \n",
       "24                        [NNP, NNS]              [n, n]   \n",
       "31                   [NNP, NNP, NNP]           [n, n, n]   \n",
       "34                   [NNP, NNP, NNP]           [n, n, n]   \n",
       "38              [NNP, NNP, NNP, NNP]        [n, n, n, n]   \n",
       "52                        [JJ, NNPS]              [a, n]   \n",
       "53                       [NNP, NNPS]              [n, n]   \n",
       "56    [NNP, NNPS, CC, NNP, NNP, NNP]  [n, n, n, n, n, n]   \n",
       "57                        [NNP, NNP]              [n, n]   \n",
       "69                    [NNP, NN, NNS]           [n, n, n]   \n",
       "80                        [NNP, NNP]              [n, n]   \n",
       "86                          [JJ, NN]              [a, n]   \n",
       "124                         [JJ, NN]              [a, n]   \n",
       "135                         [JJ, NN]              [a, n]   \n",
       "139                        [NNP, NN]              [n, n]   \n",
       "140                        [IN, VBZ]              [n, v]   \n",
       "144     [NNP, IN, NNP, NNP, FW, NNP]  [n, n, n, n, n, n]   \n",
       "145              [NNP, NNP, FW, NNP]        [n, n, n, n]   \n",
       "151                         [JJ, NN]              [a, n]   \n",
       "163                         [NN, NN]              [n, n]   \n",
       "187                       [NNP, NNP]              [n, n]   \n",
       "190                       [NNP, NNP]              [n, n]   \n",
       "200                     [TO, DT, NN]           [n, n, n]   \n",
       "207                         [JJ, NN]              [a, n]   \n",
       "228                        [NNP, NN]              [n, n]   \n",
       "232         [NNP, NNP, CC, NNP, NNP]     [n, n, n, n, n]   \n",
       "239                  [NNP, VBD, RBR]           [n, v, r]   \n",
       "...                              ...                 ...   \n",
       "5300                        [NN, NN]              [n, n]   \n",
       "5310            [NN, IN, NN, IN, NN]     [n, n, n, n, n]   \n",
       "5316                       [NNP, NN]              [n, n]   \n",
       "5323                    [DT, JJ, NN]           [n, a, n]   \n",
       "5327                        [JJ, NN]              [a, n]   \n",
       "5331                        [JJ, NN]              [a, n]   \n",
       "5342                        [JJ, NN]              [a, n]   \n",
       "5347                        [JJ, NN]              [a, n]   \n",
       "5363                        [NN, NN]              [n, n]   \n",
       "5371                       [JJ, NNS]              [a, n]   \n",
       "5398                       [NN, NNS]              [n, n]   \n",
       "5402                       [JJ, NNS]              [a, n]   \n",
       "5428                        [RB, JJ]              [r, a]   \n",
       "5429                   [RB, JJ, NNS]           [r, a, n]   \n",
       "5434                    [JJ, JJ, NN]           [a, a, n]   \n",
       "5442                        [JJ, NN]              [a, n]   \n",
       "5455                       [NN, VBG]              [n, v]   \n",
       "5459                        [JJ, NN]              [a, n]   \n",
       "5465                   [JJ, RBS, JJ]           [a, r, a]   \n",
       "5471                        [RB, JJ]              [r, a]   \n",
       "5472                        [RB, NN]              [r, n]   \n",
       "5480                      [NNP, NNP]              [n, n]   \n",
       "5485                       [NNP, FW]              [n, n]   \n",
       "5489                       [JJ, NNS]              [a, n]   \n",
       "5501                        [JJ, NN]              [a, n]   \n",
       "5511                       [NNS, RB]              [n, r]   \n",
       "5514                        [JJ, NN]              [a, n]   \n",
       "5516                       [JJ, VBG]              [a, v]   \n",
       "5523                        [NN, NN]              [n, n]   \n",
       "5534                        [JJ, NN]              [a, n]   \n",
       "\n",
       "                                                  lemma  \\\n",
       "4                                     future generation   \n",
       "9                traditional connection to that country   \n",
       "14                     Aboriginal land right legislaton   \n",
       "15                                land right legislaton   \n",
       "24                                   Aboriginal protest   \n",
       "31                                 Yolngu Bark Petition   \n",
       "34                                   Wave Hill Walk-Off   \n",
       "38                           Aboriginal Lands Trust Act   \n",
       "52                               indigenous Australians   \n",
       "53                                Australian Aborigines   \n",
       "56    Australian Aborigines and Torres Strait Islanders   \n",
       "57                                        Torres Strait   \n",
       "69                                Aboriginal land right   \n",
       "80                                       Pulitzer Prize   \n",
       "86                                     tropical disease   \n",
       "124                                     diverse variety   \n",
       "135                                     missionary work   \n",
       "139                                       Baroque piece   \n",
       "140                                          that stand   \n",
       "144                Chapel of Nuestra Señora del Rosario   \n",
       "145                          Nuestra Señora del Rosario   \n",
       "151                                recognizable feature   \n",
       "163                                           sea level   \n",
       "187                              Metropolitan Cathedral   \n",
       "190                                         Virgin Mary   \n",
       "200                                         To this end   \n",
       "207                                    cultural context   \n",
       "228                                        Romantic era   \n",
       "232              Clarinet Concerto and Clarinet Quintet   \n",
       "239                                      Mozart be more   \n",
       "...                                                 ...   \n",
       "5300                                      coding strand   \n",
       "5310                 substitution of uracil for thymine   \n",
       "5316                                     RNA polymerase   \n",
       "5323                               a primary transcript   \n",
       "5327                                 primary transcript   \n",
       "5331                               Alchemical symbolism   \n",
       "5342                                      inner meaning   \n",
       "5347                                     spiritual path   \n",
       "5363                                          epic poem   \n",
       "5371                                   musical sequence   \n",
       "5398                                     mountain chain   \n",
       "5402                                  several tributary   \n",
       "5428                           historically significant   \n",
       "5429                     historically significant place   \n",
       "5434                       Greek mythological character   \n",
       "5442                             mythological character   \n",
       "5455                                    guerrilla fight   \n",
       "5459                               mountainous province   \n",
       "5465                              second most populated   \n",
       "5471                                           far east   \n",
       "5472                                           far west   \n",
       "5480                                     Canary Islands   \n",
       "5485                                 Acherontia atropos   \n",
       "5489                                   continuous brood   \n",
       "5501                                newly-hatched larva   \n",
       "5511                                  stripe diagonally   \n",
       "5514                                       third instar   \n",
       "5516                                          blue edge   \n",
       "5523                                          tail horn   \n",
       "5534                                underground chamber   \n",
       "\n",
       "                                                p_lemma  length (lin)  \\\n",
       "4                                     future generation      9.386500   \n",
       "9                traditional connection to that country      9.672044   \n",
       "14                     aboriginal land right legislaton      8.901884   \n",
       "15                                land right legislaton      8.212491   \n",
       "24                                   aboriginal protest      9.109893   \n",
       "31                                 yolngu bark petition      6.000000   \n",
       "34                                   wave hill walk-off      5.333333   \n",
       "38                           aboriginal lands trust act      5.750000   \n",
       "52                               indigenous australians     10.575889   \n",
       "53                                australian aborigines     10.000000   \n",
       "56    australian aborigines and torres strait islanders      8.196748   \n",
       "57                                        torres strait      6.000000   \n",
       "69                                aboriginal land right      8.212505   \n",
       "80                                       pulitzer prize      6.500000   \n",
       "86                                     tropical disease      7.509948   \n",
       "124                                     diverse variety      7.000000   \n",
       "135                                     missionary work      9.199682   \n",
       "139                                       baroque piece      6.239649   \n",
       "140                                          that stand      5.953959   \n",
       "144                chapel of nuestra señora del rosario      6.040444   \n",
       "145                          nuestra señora del rosario      6.054692   \n",
       "151                                recognizable feature     10.394020   \n",
       "163                                           sea level      3.926780   \n",
       "187                              metropolitan cathedral     10.500000   \n",
       "190                                         virgin mary      5.000000   \n",
       "200                                         to this end      2.193108   \n",
       "207                                    cultural context      8.000000   \n",
       "228                                        romantic era      6.377716   \n",
       "232              clarinet concerto and clarinet quintet      7.746287   \n",
       "239                                      mozart be more      5.839749   \n",
       "...                                                 ...           ...   \n",
       "5300                                      coding strand      6.000000   \n",
       "5310                 substitution of uracil for thymine      8.249764   \n",
       "5316                                     rna polymerase      6.459276   \n",
       "5323                               a primary transcript      5.685786   \n",
       "5327                                 primary transcript      9.148526   \n",
       "5331                               alchemical symbolism      9.512813   \n",
       "5342                                      inner meaning      5.814814   \n",
       "5347                                     spiritual path      6.572385   \n",
       "5363                                          epic poem      4.000000   \n",
       "5371                                   musical sequence      8.341567   \n",
       "5398                                     mountain chain      6.650321   \n",
       "5402                                  several tributary     10.496949   \n",
       "5428                           historically significant     11.646241   \n",
       "5429                     historically significant place     10.159291   \n",
       "5434                       greek mythological character      8.501684   \n",
       "5442                             mythological character     11.214029   \n",
       "5455                                    guerrilla fight      8.621250   \n",
       "5459                               mountainous province     10.237800   \n",
       "5465                              second most populated      8.295541   \n",
       "5471                                           far east      3.306965   \n",
       "5472                                           far west      3.284271   \n",
       "5480                                     canary islands      6.500000   \n",
       "5485                                 acherontia atropos      8.500814   \n",
       "5489                                   continuous brood      7.746818   \n",
       "5501                                newly-hatched larva      9.057497   \n",
       "5511                                  stripe diagonally      8.551908   \n",
       "5514                                       third instar      5.825473   \n",
       "5516                                          blue edge      5.404028   \n",
       "5523                                          tail horn      4.000000   \n",
       "5534                                underground chamber      9.034016   \n",
       "\n",
       "      phrase_length (lin)  target_num_words (lin)  \\\n",
       "4                      18                       2   \n",
       "9                      38                       5   \n",
       "14                     33                       4   \n",
       "15                     22                       3   \n",
       "24                     19                       2   \n",
       "31                     20                       3   \n",
       "34                     19                       3   \n",
       "38                     26                       4   \n",
       "52                     22                       2   \n",
       "53                     21                       2   \n",
       "56                     49                       6   \n",
       "57                     13                       2   \n",
       "69                     22                       3   \n",
       "80                     14                       2   \n",
       "86                     16                       2   \n",
       "124                    15                       2   \n",
       "135                    15                       2   \n",
       "139                    13                       2   \n",
       "140                    11                       2   \n",
       "144                    36                       6   \n",
       "145                    26                       4   \n",
       "151                    20                       2   \n",
       "163                     9                       2   \n",
       "187                    22                       2   \n",
       "190                    11                       2   \n",
       "200                    11                       3   \n",
       "207                    17                       2   \n",
       "228                    12                       2   \n",
       "232                    38                       5   \n",
       "239                    15                       3   \n",
       "...                   ...                     ...   \n",
       "5300                   13                       2   \n",
       "5310                   34                       5   \n",
       "5316                   14                       2   \n",
       "5323                   20                       3   \n",
       "5327                   18                       2   \n",
       "5331                   20                       2   \n",
       "5342                   13                       2   \n",
       "5347                   14                       2   \n",
       "5363                    9                       2   \n",
       "5371                   17                       2   \n",
       "5398                   15                       2   \n",
       "5402                   19                       2   \n",
       "5428                   24                       2   \n",
       "5429                   31                       3   \n",
       "5434                   28                       3   \n",
       "5442                   22                       2   \n",
       "5455                   18                       2   \n",
       "5459                   20                       2   \n",
       "5465                   21                       3   \n",
       "5471                    8                       2   \n",
       "5472                    8                       2   \n",
       "5480                   14                       2   \n",
       "5485                   18                       2   \n",
       "5489                   17                       2   \n",
       "5501                   19                       2   \n",
       "5511                   18                       2   \n",
       "5514                   12                       2   \n",
       "5516                   11                       2   \n",
       "5523                    9                       2   \n",
       "5534                   19                       2   \n",
       "\n",
       "      relative_position_left (lin)  relative_position_centered (lin)  \\\n",
       "4                         0.298611                          0.361111   \n",
       "9                         0.722222                          0.854167   \n",
       "14                        0.041899                          0.087989   \n",
       "15                        0.072626                          0.103352   \n",
       "24                        0.279330                          0.305866   \n",
       "31                        0.508380                          0.536313   \n",
       "34                        0.608939                          0.635475   \n",
       "38                        0.709497                          0.745810   \n",
       "52                        0.181070                          0.226337   \n",
       "53                        0.304527                          0.347737   \n",
       "56                        0.304527                          0.405350   \n",
       "57                        0.411523                          0.438272   \n",
       "69                        0.901235                          0.946502   \n",
       "80                        0.806452                          0.862903   \n",
       "86                        0.333333                          0.409524   \n",
       "124                       0.108696                          0.163043   \n",
       "135                       0.804348                          0.858696   \n",
       "139                       0.056338                          0.102113   \n",
       "140                       0.154930                          0.193662   \n",
       "144                       0.288732                          0.415493   \n",
       "145                       0.359155                          0.450704   \n",
       "151                       0.640845                          0.711268   \n",
       "163                       0.914729                          0.949612   \n",
       "187                       0.901639                          0.946721   \n",
       "190                       0.162162                          0.211712   \n",
       "200                       0.000000                          0.030556   \n",
       "207                       0.366667                          0.413889   \n",
       "228                       0.917647                          0.952941   \n",
       "232                       0.055556                          0.161111   \n",
       "239                       0.472222                          0.513889   \n",
       "...                            ...                               ...   \n",
       "5300                      0.310345                          0.342365   \n",
       "5310                      0.812808                          0.896552   \n",
       "5316                      0.335484                          0.380645   \n",
       "5323                      0.858065                          0.922581   \n",
       "5327                      0.870968                          0.929032   \n",
       "5331                      0.000000                          0.052910   \n",
       "5342                      0.714286                          0.748677   \n",
       "5347                      0.915344                          0.952381   \n",
       "5363                      0.736626                          0.755144   \n",
       "5371                      0.228814                          0.300847   \n",
       "5398                      0.047244                          0.106299   \n",
       "5402                      0.433071                          0.507874   \n",
       "5428                      0.645161                          0.741935   \n",
       "5429                      0.645161                          0.770161   \n",
       "5434                      0.662921                          0.820225   \n",
       "5442                      0.730337                          0.853933   \n",
       "5455                      0.716346                          0.759615   \n",
       "5459                      0.894231                          0.942308   \n",
       "5465                      0.379310                          0.500000   \n",
       "5471                      0.120370                          0.157407   \n",
       "5472                      0.601852                          0.638889   \n",
       "5480                      0.750000                          0.814815   \n",
       "5485                      0.340206                          0.432990   \n",
       "5489                      0.701031                          0.788660   \n",
       "5501                      0.030534                          0.103053   \n",
       "5511                      0.748092                          0.816794   \n",
       "5514                      0.057851                          0.107438   \n",
       "5516                      0.264463                          0.309917   \n",
       "5523                      0.685950                          0.723140   \n",
       "5534                      0.727273                          0.850649   \n",
       "\n",
       "      relative_position_right (lin)  ratio_cap_letters (lin)  all_caps (lin)  \\\n",
       "4                          0.423611                 0.000000           False   \n",
       "9                          0.986111                 0.000000           False   \n",
       "14                         0.134078                 0.038567           False   \n",
       "15                         0.134078                 0.000000           False   \n",
       "24                         0.332402                 0.055495           False   \n",
       "31                         0.564246                 0.180556           False   \n",
       "34                         0.662011                 0.250000           False   \n",
       "38                         0.782123                 0.208333           False   \n",
       "52                         0.271605                 0.052354           False   \n",
       "53                         0.390947                 0.100000           False   \n",
       "56                         0.506173                 0.128808           False   \n",
       "57                         0.465021                 0.166667           False   \n",
       "69                         0.991770                 0.062779           False   \n",
       "80                         0.919355                 0.162500           False   \n",
       "86                         0.485714                 0.000000           False   \n",
       "124                        0.217391                 0.000000           False   \n",
       "135                        0.913043                 0.000000           False   \n",
       "139                        0.147887                 0.088546           False   \n",
       "140                        0.232394                 0.000000           False   \n",
       "144                        0.542254                 0.134467           False   \n",
       "145                        0.542254                 0.125626           False   \n",
       "151                        0.781690                 0.000000           False   \n",
       "163                        0.984496                 0.000000           False   \n",
       "187                        0.991803                 0.097222           False   \n",
       "190                        0.261261                 0.208333           False   \n",
       "200                        0.061111                 0.417100           False   \n",
       "207                        0.461111                 0.000000           False   \n",
       "228                        0.988235                 0.084443           False   \n",
       "232                        0.266667                 0.129363           False   \n",
       "239                        0.555556                 0.153887           False   \n",
       "...                             ...                      ...             ...   \n",
       "5300                       0.374384                 0.000000           False   \n",
       "5310                       0.980296                 0.000000           False   \n",
       "5316                       0.425806                 0.505818           False   \n",
       "5323                       0.987097                 0.000000           False   \n",
       "5327                       0.987097                 0.000000           False   \n",
       "5331                       0.105820                 0.051281           False   \n",
       "5342                       0.783069                 0.000000           False   \n",
       "5347                       0.989418                 0.000000           False   \n",
       "5363                       0.773663                 0.000000           False   \n",
       "5371                       0.372881                 0.000000           False   \n",
       "5398                       0.165354                 0.000000           False   \n",
       "5402                       0.582677                 0.000000           False   \n",
       "5428                       0.838710                 0.000000           False   \n",
       "5429                       0.895161                 0.000000           False   \n",
       "5434                       0.977528                 0.087297           False   \n",
       "5442                       0.977528                 0.000000           False   \n",
       "5455                       0.802885                 0.000000           False   \n",
       "5459                       0.990385                 0.000000           False   \n",
       "5465                       0.620690                 0.000000           False   \n",
       "5471                       0.194444                 0.000000           False   \n",
       "5472                       0.675926                 0.000000           False   \n",
       "5480                       0.879630                 0.154762           False   \n",
       "5485                       0.525773                 0.050027           False   \n",
       "5489                       0.876289                 0.000000           False   \n",
       "5501                       0.175573                 0.000000           False   \n",
       "5511                       0.885496                 0.000000           False   \n",
       "5514                       0.157025                 0.000000           False   \n",
       "5516                       0.355372                 0.000000           False   \n",
       "5523                       0.760331                 0.000000           False   \n",
       "5534                       0.974026                 0.000000           False   \n",
       "\n",
       "      ratio_num_letters (lin)  ratio_non_ascii_letters (lin)  \\\n",
       "4                         0.0                       0.000000   \n",
       "9                         0.0                       0.000000   \n",
       "14                        0.0                       0.000000   \n",
       "15                        0.0                       0.000000   \n",
       "24                        0.0                       0.000000   \n",
       "31                        0.0                       0.000000   \n",
       "34                        0.0                       0.000000   \n",
       "38                        0.0                       0.000000   \n",
       "52                        0.0                       0.000000   \n",
       "53                        0.0                       0.000000   \n",
       "56                        0.0                       0.000000   \n",
       "57                        0.0                       0.000000   \n",
       "69                        0.0                       0.000000   \n",
       "80                        0.0                       0.000000   \n",
       "86                        0.0                       0.000000   \n",
       "124                       0.0                       0.000000   \n",
       "135                       0.0                       0.000000   \n",
       "139                       0.0                       0.000000   \n",
       "140                       0.0                       0.000000   \n",
       "144                       0.0                       0.036203   \n",
       "145                       0.0                       0.046283   \n",
       "151                       0.0                       0.000000   \n",
       "163                       0.0                       0.000000   \n",
       "187                       0.0                       0.000000   \n",
       "190                       0.0                       0.000000   \n",
       "200                       0.0                       0.000000   \n",
       "207                       0.0                       0.000000   \n",
       "228                       0.0                       0.000000   \n",
       "232                       0.0                       0.000000   \n",
       "239                       0.0                       0.000000   \n",
       "...                       ...                            ...   \n",
       "5300                      0.0                       0.000000   \n",
       "5310                      0.0                       0.000000   \n",
       "5316                      0.0                       0.000000   \n",
       "5323                      0.0                       0.000000   \n",
       "5327                      0.0                       0.000000   \n",
       "5331                      0.0                       0.000000   \n",
       "5342                      0.0                       0.000000   \n",
       "5347                      0.0                       0.000000   \n",
       "5363                      0.0                       0.000000   \n",
       "5371                      0.0                       0.000000   \n",
       "5398                      0.0                       0.000000   \n",
       "5402                      0.0                       0.000000   \n",
       "5428                      0.0                       0.000000   \n",
       "5429                      0.0                       0.000000   \n",
       "5434                      0.0                       0.000000   \n",
       "5442                      0.0                       0.000000   \n",
       "5455                      0.0                       0.000000   \n",
       "5459                      0.0                       0.000000   \n",
       "5465                      0.0                       0.000000   \n",
       "5471                      0.0                       0.000000   \n",
       "5472                      0.0                       0.000000   \n",
       "5480                      0.0                       0.000000   \n",
       "5485                      0.0                       0.000000   \n",
       "5489                      0.0                       0.000000   \n",
       "5501                      0.0                       0.000000   \n",
       "5511                      0.0                       0.000000   \n",
       "5514                      0.0                       0.000000   \n",
       "5516                      0.0                       0.000000   \n",
       "5523                      0.0                       0.000000   \n",
       "5534                      0.0                       0.000000   \n",
       "\n",
       "      ratio_non_alpha (lin)  grapheme_to_phoneme_ratio (lin)  \\\n",
       "4                  0.000000                         1.215051   \n",
       "9                  0.000000                         1.188375   \n",
       "14                 0.000000                         1.111314   \n",
       "15                 0.000000                         1.111441   \n",
       "24                 0.000000                         1.125240   \n",
       "31                 0.000000                         1.047619   \n",
       "34                 0.041667                         1.222222   \n",
       "38                 0.000000                         1.027778   \n",
       "52                 0.000000                         1.104712   \n",
       "53                 0.000000                         1.111111   \n",
       "56                 0.000000                         1.181474   \n",
       "57                 0.000000                         1.200000   \n",
       "69                 0.000000                         1.181195   \n",
       "80                 0.000000                         1.196429   \n",
       "86                 0.000000                         1.196021   \n",
       "124                0.000000                         1.313467   \n",
       "135                0.000000                         1.261116   \n",
       "139                0.000000                         1.718319   \n",
       "140                0.000000                         1.007674   \n",
       "144                0.036203                         1.043443   \n",
       "145                0.046283                         1.000000   \n",
       "151                0.000000                         1.240897   \n",
       "163                0.000000                         1.268305   \n",
       "187                0.000000                         1.062500   \n",
       "190                0.000000                         1.100000   \n",
       "200                0.000000                         1.009103   \n",
       "207                0.000000                         0.972337   \n",
       "228                0.000000                         1.000000   \n",
       "232                0.000000                         1.000000   \n",
       "239                0.000000                         0.952234   \n",
       "...                     ...                              ...   \n",
       "5300               0.000000                         1.100982   \n",
       "5310               0.000000                         1.029655   \n",
       "5316               0.000000                         0.921218   \n",
       "5323               0.000000                         1.000000   \n",
       "5327               0.000000                         1.000000   \n",
       "5331               0.000000                         0.951281   \n",
       "5342               0.000000                         1.558025   \n",
       "5347               0.000000                         1.161841   \n",
       "5363               0.000000                         1.000000   \n",
       "5371               0.000000                         0.958848   \n",
       "5398               0.000000                         1.445807   \n",
       "5402               0.000000                         1.020960   \n",
       "5428               0.000000                         1.092320   \n",
       "5429               0.000000                         1.068007   \n",
       "5434               0.000000                         1.189110   \n",
       "5442               0.000000                         1.141946   \n",
       "5455               0.000000                         1.724250   \n",
       "5459               0.000000                         1.316020   \n",
       "5465               0.000000                         0.929459   \n",
       "5471               0.000000                         1.102322   \n",
       "5472               0.000000                         1.000000   \n",
       "5480               0.000000                         1.083333   \n",
       "5485               0.000000                         1.000000   \n",
       "5489               0.000000                         1.000000   \n",
       "5501               0.039014                         1.000000   \n",
       "5511               0.000000                         1.137928   \n",
       "5514               0.000000                         1.116352   \n",
       "5516               0.000000                         1.450336   \n",
       "5523               0.000000                         1.158727   \n",
       "5534               0.000000                         1.309599   \n",
       "\n",
       "      num_pronounciations (lin)  hyphenated (lin)  is_title (lin)  \\\n",
       "4                      1.000000                 0           False   \n",
       "9                      1.019187                 0           False   \n",
       "14                     1.000000                 0           False   \n",
       "15                     1.000000                 0           False   \n",
       "24                     2.335160                 0           False   \n",
       "31                     1.000000                 0            True   \n",
       "34                     1.000000                 1            True   \n",
       "38                     1.000000                 0            True   \n",
       "52                     1.000000                 0           False   \n",
       "53                     1.000000                 0            True   \n",
       "56                     1.000625                 0           False   \n",
       "57                     1.000000                 0            True   \n",
       "69                     1.000000                 0           False   \n",
       "80                     1.000000                 0            True   \n",
       "86                     1.000000                 0           False   \n",
       "124                    1.629143                 0           False   \n",
       "135                    1.000000                 0           False   \n",
       "139                    1.000000                 0           False   \n",
       "140                    1.023021                 0           False   \n",
       "144                    1.000000                 0           False   \n",
       "145                    1.000000                 0           False   \n",
       "151                    1.000000                 0           False   \n",
       "163                    1.000000                 0           False   \n",
       "187                    1.000000                 0            True   \n",
       "190                    1.000000                 0            True   \n",
       "200                    2.695709                 0           False   \n",
       "207                    1.000000                 0           False   \n",
       "228                    1.324457                 0           False   \n",
       "232                    1.000782                 0           False   \n",
       "239                    1.930224                 0           False   \n",
       "...                         ...               ...             ...   \n",
       "5300                   1.000000                 0           False   \n",
       "5310                   1.007283                 0           False   \n",
       "5316                   1.000000                 0           False   \n",
       "5323                   1.424953                 0           False   \n",
       "5327                   1.000000                 0           False   \n",
       "5331                   1.000000                 0           False   \n",
       "5342                   1.000000                 0           False   \n",
       "5347                   1.000000                 0           False   \n",
       "5363                   1.000000                 0           False   \n",
       "5371                   1.670784                 0           False   \n",
       "5398                   1.000000                 0           False   \n",
       "5402                   1.125763                 0           False   \n",
       "5428                   2.000000                 0           False   \n",
       "5429                   2.000000                 0           False   \n",
       "5434                   1.000000                 0           False   \n",
       "5442                   1.000000                 0           False   \n",
       "5455                   1.000000                 0           False   \n",
       "5459                   1.254067                 0           False   \n",
       "5465                   1.078444                 0           False   \n",
       "5471                   1.000000                 0           False   \n",
       "5472                   1.000000                 0           False   \n",
       "5480                   1.000000                 0            True   \n",
       "5485                   1.000000                 0           False   \n",
       "5489                   1.000000                 0           False   \n",
       "5501                   1.000000                 1           False   \n",
       "5511                   1.000000                 0           False   \n",
       "5514                   1.000000                 0           False   \n",
       "5516                   1.000000                 0           False   \n",
       "5523                   1.000000                 0           False   \n",
       "5534                   1.000000                 0           False   \n",
       "\n",
       "      mrc_nphon (lin)  cal_ngram_2_sim_min (lin)  cal_ngram_2_sim_max (lin)  \\\n",
       "4            1.936200                   0.383803                   0.832004   \n",
       "9            7.870827                   0.260069                   0.709980   \n",
       "14           3.534519                   0.119064                   0.705680   \n",
       "15           0.597319                   0.134194                   0.708732   \n",
       "24           4.646699                   0.325994                   0.764493   \n",
       "31           3.156110                   0.556801                   0.729545   \n",
       "34           1.564889                   0.018746                   0.618791   \n",
       "38           4.850412                   0.066272                   0.610947   \n",
       "52           3.998279                   0.434430                   0.775043   \n",
       "53           9.746446                   0.793085                   0.976950   \n",
       "56           4.230849                   0.452201                   0.755893   \n",
       "57           2.431656                   0.296523                   0.756834   \n",
       "69           5.973915                   0.114967                   0.659406   \n",
       "80           1.419331                   0.612907                   0.893550   \n",
       "86           6.529845                   0.377815                   0.583456   \n",
       "124          5.741715                   0.062914                   0.920531   \n",
       "135          6.466454                   0.649960                   0.649960   \n",
       "139          4.175339                   0.334324                   0.917534   \n",
       "140          0.069062                   0.407075                   0.982735   \n",
       "144          0.939943                   0.180825                   0.960459   \n",
       "145          0.000000                   0.148239                   1.000000   \n",
       "151          9.072824                   0.227057                   0.712985   \n",
       "163          2.926780                   0.288616                   0.768305   \n",
       "187          9.967753                   0.397335                   0.877482   \n",
       "190          3.087560                   0.230184                   0.513211   \n",
       "200          2.978000                   0.000000                   0.348480   \n",
       "207          2.957210                   0.704364                   0.888096   \n",
       "228          6.090003                   0.501700                   0.862667   \n",
       "232          7.740838                   0.637900                   0.913962   \n",
       "239          0.169277                   0.920220                   0.925079   \n",
       "...               ...                        ...                        ...   \n",
       "5300         2.970526                   0.306920                   0.789620   \n",
       "5310         3.863466                   0.449512                   0.948504   \n",
       "5316         0.000000                   0.440941                   0.466879   \n",
       "5323         5.685786                   0.665084                   0.874359   \n",
       "5327         9.148526                   0.417584                   0.781512   \n",
       "5331         9.488895                   0.426266                   0.772224   \n",
       "5342         3.814814                   0.383598                   0.610229   \n",
       "5347         5.144770                   0.114328                   0.538233   \n",
       "5363         4.000000                   0.134577                   0.638508   \n",
       "5371         2.633732                   0.705859                   0.925468   \n",
       "5398         1.950963                   0.000000                   0.492442   \n",
       "5402         0.754576                   0.052314                   0.830693   \n",
       "5428         3.891354                   0.373413                   0.662417   \n",
       "5429         2.866558                   0.275074                   0.751320   \n",
       "5434         8.395024                   0.409446                   0.754253   \n",
       "5442         9.952039                   0.430506                   0.773473   \n",
       "5455         3.727498                   0.448680                   0.668593   \n",
       "5459         7.745933                   0.226604                   0.728751   \n",
       "5465         0.938255                   0.000000                   0.537171   \n",
       "5471         2.306965                   0.030696                   0.191853   \n",
       "5472         2.568542                   0.118446                   0.142136   \n",
       "5480         4.049356                   0.230881                   0.827649   \n",
       "5485         0.000000                   0.767850                   1.000000   \n",
       "5489         4.367045                   0.227394                   0.490188   \n",
       "5501         1.971252                   0.373717                   0.619610   \n",
       "5511         0.000000                   0.103435                   0.549416   \n",
       "5514         0.523582                   0.029088                   0.887804   \n",
       "5516         3.702014                   0.074496                   0.291543   \n",
       "5523         3.000000                   0.043652                   0.591845   \n",
       "5534         7.525512                   0.267990                   0.594200   \n",
       "\n",
       "      cal_ngram_2_sim_mean (lin)  cal_ngram_3_sim_min (lin)  \\\n",
       "4                       0.605328                   0.361227   \n",
       "9                       0.510124                   0.272206   \n",
       "14                      0.458919                   0.111010   \n",
       "15                      0.456383                   0.121812   \n",
       "24                      0.587681                   0.332195   \n",
       "31                      0.645013                   0.524845   \n",
       "34                      0.244032                   0.012498   \n",
       "38                      0.333734                   0.062258   \n",
       "52                      0.642560                   0.449582   \n",
       "53                      0.890011                   0.804610   \n",
       "56                      0.581770                   0.430871   \n",
       "57                      0.474980                   0.287630   \n",
       "69                      0.401809                   0.101354   \n",
       "80                      0.746329                   0.586025   \n",
       "86                      0.475514                   0.400611   \n",
       "124                     0.607538                   0.073400   \n",
       "135                     0.649960                   0.621073   \n",
       "139                     0.611441                   0.320828   \n",
       "140                     0.643150                   0.488490   \n",
       "144                     0.491541                   0.163431   \n",
       "145                     0.492429                   0.140096   \n",
       "151                     0.473884                   0.206158   \n",
       "163                     0.463418                   0.240513   \n",
       "187                     0.692867                   0.384721   \n",
       "190                     0.401715                   0.217435   \n",
       "200                     0.174551                   0.022000   \n",
       "207                     0.813162                   0.690376   \n",
       "228                     0.634756                   0.499267   \n",
       "232                     0.792928                   0.600781   \n",
       "239                     0.921921                   0.919816   \n",
       "...                          ...                        ...   \n",
       "5300                    0.470158                   0.278869   \n",
       "5310                    0.698568                   0.420100   \n",
       "5316                    0.449587                   0.423649   \n",
       "5323                    0.795920                   0.657185   \n",
       "5327                    0.645108                   0.403849   \n",
       "5331                    0.628158                   0.395963   \n",
       "5342                    0.481874                   0.364198   \n",
       "5347                    0.363289                   0.095274   \n",
       "5363                    0.409845                   0.157006   \n",
       "5371                    0.798273                   0.677760   \n",
       "5398                    0.285131                   0.000000   \n",
       "5402                    0.340063                   0.039068   \n",
       "5428                    0.513583                   0.387422   \n",
       "5429                    0.497465                   0.285393   \n",
       "5434                    0.582572                   0.406237   \n",
       "5442                    0.596079                   0.420255   \n",
       "5455                    0.533489                   0.402662   \n",
       "5459                    0.524650                   0.198707   \n",
       "5465                    0.280517                   0.000000   \n",
       "5471                    0.112554                   0.000000   \n",
       "5472                    0.134239                   0.102653   \n",
       "5480                    0.552823                   0.214488   \n",
       "5485                    0.883925                   0.727377   \n",
       "5489                    0.372386                   0.219182   \n",
       "5501                    0.509142                   0.363792   \n",
       "5511                    0.352682                   0.068957   \n",
       "5514                    0.459369                   0.029088   \n",
       "5516                    0.146845                   0.049664   \n",
       "5523                    0.255484                   0.029101   \n",
       "5534                    0.407835                   0.252580   \n",
       "\n",
       "      cal_ngram_3_sim_max (lin)  cal_ngram_3_sim_mean (lin)  \\\n",
       "4                      0.850375                    0.603351   \n",
       "9                      0.683987                    0.502785   \n",
       "14                     0.705104                    0.454485   \n",
       "15                     0.689934                    0.446581   \n",
       "24                     0.790661                    0.592243   \n",
       "31                     0.689989                    0.609565   \n",
       "34                     0.587586                    0.228008   \n",
       "38                     0.625619                    0.332800   \n",
       "52                     0.790166                    0.649506   \n",
       "53                     0.983096                    0.896670   \n",
       "56                     0.772451                    0.570711   \n",
       "57                     0.770344                    0.460183   \n",
       "69                     0.668890                    0.394315   \n",
       "80                     0.905378                    0.746687   \n",
       "86                     0.575289                    0.481813   \n",
       "124                    0.929361                    0.626696   \n",
       "135                    0.635517                    0.627492   \n",
       "139                    0.917534                    0.634679   \n",
       "140                    0.980816                    0.653110   \n",
       "144                    0.960889                    0.476108   \n",
       "145                    1.000000                    0.477901   \n",
       "151                    0.673447                    0.453145   \n",
       "163                    0.721966                    0.413419   \n",
       "187                    0.868073                    0.685526   \n",
       "190                    0.500461                    0.388966   \n",
       "200                    0.421349                    0.199302   \n",
       "207                    0.900530                    0.821352   \n",
       "228                    0.862667                    0.633134   \n",
       "232                    0.913962                    0.779204   \n",
       "239                    0.925484                    0.921759   \n",
       "...                         ...                         ...   \n",
       "5300                   0.831696                    0.477170   \n",
       "5310                   0.948959                    0.679010   \n",
       "5316                   0.466879                    0.446705   \n",
       "5323                   0.877759                    0.796828   \n",
       "5327                   0.787425                    0.646687   \n",
       "5331                   0.797779                    0.630412   \n",
       "5342                   0.619930                    0.473610   \n",
       "5347                   0.570775                    0.362068   \n",
       "5363                   0.602621                    0.410635   \n",
       "5371                   0.913047                    0.782153   \n",
       "5398                   0.489923                    0.282033   \n",
       "5402                   0.848202                    0.337068   \n",
       "5428                   0.702950                    0.535884   \n",
       "5429                   0.781179                    0.507623   \n",
       "5434                   0.778514                    0.599580   \n",
       "5442                   0.794520                    0.609928   \n",
       "5455                   0.652812                    0.512889   \n",
       "5459                   0.728751                    0.516919   \n",
       "5465                   0.555578                    0.284672   \n",
       "5471                   0.166273                    0.102322   \n",
       "5472                   0.123184                    0.113182   \n",
       "5480                   0.854137                    0.553804   \n",
       "5485                   1.000000                    0.863688   \n",
       "5489                   0.490188                    0.374864   \n",
       "5501                   0.619610                    0.517348   \n",
       "5511                   0.549200                    0.332497   \n",
       "5514                   0.887804                    0.459369   \n",
       "5516                   0.293691                    0.149923   \n",
       "5523                   0.591845                    0.236083   \n",
       "5534                   0.652420                    0.410404   \n",
       "\n",
       "      num_syllables (lin)  num_vowels (lin)  vowel_consonant_ratio (lin)  \\\n",
       "4                3.354600          4.354600                     0.887117   \n",
       "9                3.101048          4.085329                     0.730719   \n",
       "14               3.623244          3.623244                     0.683154   \n",
       "15               2.883363          2.883363                     0.512878   \n",
       "24               3.548900          3.548900                     0.677533   \n",
       "31               1.977733          2.646535                     0.779201   \n",
       "34               1.478370          1.812536                     0.556110   \n",
       "38               2.431241          2.431241                     0.547293   \n",
       "52               3.444253          5.000000                     0.907376   \n",
       "53               4.492891          5.000000                     1.000000   \n",
       "56               2.817979          3.198301                     0.661347   \n",
       "57               1.513669          2.000000                     0.500000   \n",
       "69               3.363221          3.363221                     0.694533   \n",
       "80               2.290334          2.645167                     0.623656   \n",
       "86               2.509948          3.490052                     0.959371   \n",
       "124              2.741715          3.370857                     0.966333   \n",
       "135              3.599841          4.466454                     0.911076   \n",
       "139              1.587670          3.587670                     1.402055   \n",
       "140              1.000000          1.000000                     0.203069   \n",
       "144              2.547061          2.781606                     0.866948   \n",
       "145              2.649778          2.928033                     0.934950   \n",
       "151              4.036412          4.678804                     0.913121   \n",
       "163              1.463390          2.000000                     1.382146   \n",
       "187              3.983877          3.983877                     0.605415   \n",
       "190              2.000000          2.000000                     0.691244   \n",
       "200              1.000000          1.000000                     0.484153   \n",
       "207              2.328579          2.328579                     0.420954   \n",
       "228              2.618001          2.618001                     1.134799   \n",
       "232              2.743332          2.998337                     0.638168   \n",
       "239              1.919006          1.992712                     0.536853   \n",
       "...                   ...               ...                          ...   \n",
       "5300             1.504912          1.504912                     0.351474   \n",
       "5310             2.974951          3.633292                     0.822123   \n",
       "5316             2.556262          3.075016                     0.759377   \n",
       "5323             1.738260          1.738260                          inf   \n",
       "5327             2.283825          2.283825                     0.391912   \n",
       "5331             4.000000          3.511105                     0.585184   \n",
       "5342             2.000000          2.407407                     0.700617   \n",
       "5347             2.286192          2.543431                     0.573423   \n",
       "5363             2.000000          2.000000                     1.000000   \n",
       "5371             3.000000          3.670784                     0.783539   \n",
       "5398             1.325160          2.650321                     0.662580   \n",
       "5402             3.811356          4.748475                     0.822853   \n",
       "5428             4.323120          4.646241                     0.663749   \n",
       "5429             3.711322          3.949347                     0.620625   \n",
       "5434             3.566715          3.828309                     0.660374   \n",
       "5442             4.476019          4.476019                     0.658145   \n",
       "5455             2.621250          3.242499                     0.623250   \n",
       "5459             2.745933          5.237800                     1.047560   \n",
       "5465             3.556508          3.556508                     0.732169   \n",
       "5471             1.000000          1.306965                     0.653482   \n",
       "5472             1.000000          1.000000                     0.452621   \n",
       "5480             2.674893          2.674893                     0.804936   \n",
       "5485             3.500204          4.000409                     0.875051   \n",
       "5489             2.310114          3.310114                     0.718352   \n",
       "5501             3.014374          3.014374                     0.553958   \n",
       "5511             3.069211          3.551908                     0.710382   \n",
       "5514             1.825473          1.825473                     0.456368   \n",
       "5516             1.702014          2.000000                     0.648993   \n",
       "5523             1.000000          1.476181                     0.650787   \n",
       "5534             2.508504          3.017008                     0.487172   \n",
       "\n",
       "      porter_stem_len (lin)  porter_stemmer_num_steps (lin)  \\\n",
       "4                  5.000000                        2.354600   \n",
       "9                  6.633825                        1.296364   \n",
       "14                 7.993241                        0.494528   \n",
       "15                 7.989608                        0.222883   \n",
       "24                 7.516300                        1.000000   \n",
       "31                 5.028673                        0.308931   \n",
       "34                 5.913482                        0.000000   \n",
       "38                 5.572077                        0.627679   \n",
       "52                 8.667240                        1.444253   \n",
       "53                 8.507109                        1.492891   \n",
       "56                 6.348989                        1.401451   \n",
       "57                 4.972662                        1.027338   \n",
       "69                 6.608249                        0.835833   \n",
       "80                 5.645167                        0.645167   \n",
       "86                 6.000000                        1.000000   \n",
       "124                6.370857                        1.000000   \n",
       "135                9.199682                        0.866614   \n",
       "139                5.175339                        1.000000   \n",
       "140                4.976979                        0.976979   \n",
       "144                6.027149                        0.000000   \n",
       "145                6.035270                        0.000000   \n",
       "151                7.357608                        1.000000   \n",
       "163                3.926780                        0.000000   \n",
       "187                9.459692                        0.508062   \n",
       "190                5.235024                        0.382488   \n",
       "200                2.978000                        0.161081   \n",
       "207                6.671421                        1.000000   \n",
       "228                4.854002                        0.618001   \n",
       "232                7.740838                        0.000000   \n",
       "239                5.823434                        0.007288   \n",
       "...                     ...                             ...   \n",
       "5300               4.990175                        0.504912   \n",
       "5310               6.949029                        0.658341   \n",
       "5316               6.112523                        0.518754   \n",
       "5323               5.685786                        0.163212   \n",
       "5327               9.148526                        0.283825   \n",
       "5331               6.000000                        1.511105   \n",
       "5342               4.592593                        0.407407   \n",
       "5347               5.543431                        0.514477   \n",
       "5363               4.000000                        0.000000   \n",
       "5371               6.341567                        1.670784   \n",
       "5398               5.975481                        0.674840   \n",
       "5402               8.496949                        1.000000   \n",
       "5428               6.707519                        2.938722   \n",
       "5429               6.257840                        2.428155   \n",
       "5434               7.021764                        1.283358   \n",
       "5442               7.738010                        1.738010   \n",
       "5455               7.484998                        0.378750   \n",
       "5459               7.745933                        1.745933   \n",
       "5465               5.025636                        1.634952   \n",
       "5471               3.306965                        0.000000   \n",
       "5472               3.284271                        0.000000   \n",
       "5480               6.000000                        1.000000   \n",
       "5485               8.000818                        0.499796   \n",
       "5489               5.873409                        1.436705   \n",
       "5501               8.043122                        0.507187   \n",
       "5511               6.000000                        2.034606   \n",
       "5514               5.825473                        0.000000   \n",
       "5516               3.297986                        0.702014   \n",
       "5523               4.000000                        0.000000   \n",
       "5534               9.034016                        0.000000   \n",
       "\n",
       "      diff_len_stem_len (lin)  reduction_stem_len (lin)  \\\n",
       "4                    4.386500                  0.467320   \n",
       "9                    3.038218                  0.314124   \n",
       "14                   0.908643                  0.102073   \n",
       "15                   0.222883                  0.027139   \n",
       "24                   1.593593                  0.174930   \n",
       "31                   0.971327                  0.161888   \n",
       "34                  -0.580149                 -0.108778   \n",
       "38                   0.177923                  0.030943   \n",
       "52                   1.908648                  0.180472   \n",
       "53                   1.492891                  0.149289   \n",
       "56                   1.847759                  0.225426   \n",
       "57                   1.027338                  0.171223   \n",
       "69                   1.604256                  0.195343   \n",
       "80                   0.854833                  0.131513   \n",
       "86                   1.509948                  0.201060   \n",
       "124                  0.629143                  0.089878   \n",
       "135                  0.000000                  0.000000   \n",
       "139                  1.064309                  0.170572   \n",
       "140                  0.976979                  0.164089   \n",
       "144                  0.013295                  0.002201   \n",
       "145                  0.019423                  0.003208   \n",
       "151                  3.036412                  0.292131   \n",
       "163                  0.000000                  0.000000   \n",
       "187                  1.040308                  0.099077   \n",
       "190                 -0.235024                 -0.047005   \n",
       "200                 -0.784892                 -0.357891   \n",
       "207                  1.328579                  0.166072   \n",
       "228                  1.523714                  0.238912   \n",
       "232                  0.005449                  0.000703   \n",
       "239                  0.016315                  0.002794   \n",
       "...                       ...                       ...   \n",
       "5300                 1.009825                  0.168304   \n",
       "5310                 1.300735                  0.157669   \n",
       "5316                 0.346753                  0.053683   \n",
       "5323                 0.000000                  0.000000   \n",
       "5327                 0.000000                  0.000000   \n",
       "5331                 3.512813                  0.369272   \n",
       "5342                 1.222221                  0.210191   \n",
       "5347                 1.028954                  0.156557   \n",
       "5363                 0.000000                  0.000000   \n",
       "5371                 2.000000                  0.239763   \n",
       "5398                 0.674840                  0.101475   \n",
       "5402                 2.000000                  0.190532   \n",
       "5428                 4.938722                  0.424061   \n",
       "5429                 3.901451                  0.384028   \n",
       "5434                 1.479920                  0.174074   \n",
       "5442                 3.476019                  0.309971   \n",
       "5455                 1.136251                  0.131797   \n",
       "5459                 2.491867                  0.243399   \n",
       "5465                 3.269905                  0.394176   \n",
       "5471                 0.000000                  0.000000   \n",
       "5472                 0.000000                  0.000000   \n",
       "5480                 0.500000                  0.076923   \n",
       "5485                 0.499997                  0.058818   \n",
       "5489                 1.873409                  0.241829   \n",
       "5501                 1.014374                  0.111993   \n",
       "5511                 2.551908                  0.298402   \n",
       "5514                 0.000000                  0.000000   \n",
       "5516                 2.106042                  0.389717   \n",
       "5523                 0.000000                  0.000000   \n",
       "5534                 0.000000                  0.000000   \n",
       "\n",
       "      norm_num_syllables (lin)  norm_num_vowels (lin)  lemma_len (lin)  \\\n",
       "4                     0.357386               0.463922         8.412099   \n",
       "9                     0.320620               0.422385         9.672044   \n",
       "14                    0.407020               0.407020         8.852055   \n",
       "15                    0.351095               0.351095         8.103843   \n",
       "24                    0.389565               0.389565         8.695126   \n",
       "31                    0.329622               0.441089         6.000000   \n",
       "34                    0.277194               0.339850         5.333333   \n",
       "38                    0.422825               0.422825         5.750000   \n",
       "52                    0.325670               0.472774        10.575889   \n",
       "53                    0.449289               0.500000        10.000000   \n",
       "56                    0.343792               0.390191         8.196748   \n",
       "57                    0.252278               0.333333         6.000000   \n",
       "69                    0.409524               0.409524         8.103859   \n",
       "80                    0.352359               0.406949         6.500000   \n",
       "86                    0.334216               0.464724         7.509948   \n",
       "124                   0.391674               0.481551         7.000000   \n",
       "135                   0.391301               0.485501         9.199682   \n",
       "139                   0.254449               0.574979         6.239649   \n",
       "140                   0.167955               0.167955         4.975714   \n",
       "144                   0.421668               0.460497         6.040444   \n",
       "145                   0.437640               0.483597         6.054692   \n",
       "151                   0.388340               0.450144        10.394020   \n",
       "163                   0.372669               0.509323         3.926780   \n",
       "187                   0.379417               0.379417        10.500000   \n",
       "190                   0.400000               0.400000         5.000000   \n",
       "200                   0.455974               0.455974         2.193108   \n",
       "207                   0.291072               0.291072         7.392208   \n",
       "228                   0.410492               0.410492         6.377716   \n",
       "232                   0.354148               0.387068         7.746287   \n",
       "239                   0.328611               0.341232         5.739476   \n",
       "...                        ...                    ...              ...   \n",
       "5300                  0.250819               0.250819         6.000000   \n",
       "5310                  0.360610               0.440412         8.249764   \n",
       "5316                  0.395751               0.476062         6.459276   \n",
       "5323                  0.305720               0.305720         5.685786   \n",
       "5327                  0.249639               0.249639         9.148526   \n",
       "5331                  0.420486               0.369092         9.512813   \n",
       "5342                  0.343949               0.414013         5.814814   \n",
       "5347                  0.347848               0.386988         6.572385   \n",
       "5363                  0.500000               0.500000         4.000000   \n",
       "5371                  0.359645               0.440059         7.620023   \n",
       "5398                  0.199263               0.398525         6.166791   \n",
       "5402                  0.363092               0.452367         8.722464   \n",
       "5428                  0.371203               0.398948        11.646241   \n",
       "5429                  0.365313               0.388742        10.800481   \n",
       "5434                  0.419530               0.450300         8.501684   \n",
       "5442                  0.399145               0.399145        11.214029   \n",
       "5455                  0.304045               0.376105         7.615451   \n",
       "5459                  0.268215               0.511614        10.237800   \n",
       "5465                  0.428725               0.428725         8.295541   \n",
       "5471                  0.302392               0.395216         3.306965   \n",
       "5472                  0.304482               0.304482         3.284271   \n",
       "5480                  0.411522               0.411522         6.500000   \n",
       "5485                  0.411749               0.470591         8.500814   \n",
       "5489                  0.298202               0.427287         7.208487   \n",
       "5501                  0.332804               0.332804         9.057497   \n",
       "5511                  0.358892               0.415335         8.035877   \n",
       "5514                  0.313360               0.313360         5.825473   \n",
       "5516                  0.314953               0.370094         4.000000   \n",
       "5523                  0.250000               0.369045         4.000000   \n",
       "5534                  0.277673               0.333961         9.034016   \n",
       "\n",
       "      reduction_lemma_len (lin)  diff_len_lemma_len (lin)  \\\n",
       "4                      0.103809                  0.974401   \n",
       "9                      0.000000                  0.000000   \n",
       "14                     0.005598                  0.049829   \n",
       "15                     0.013230                  0.108648   \n",
       "24                     0.045529                  0.414767   \n",
       "31                     0.000000                  0.000000   \n",
       "34                     0.000000                  0.000000   \n",
       "38                     0.000000                  0.000000   \n",
       "52                     0.000000                  0.000000   \n",
       "53                     0.000000                  0.000000   \n",
       "56                     0.000000                  0.000000   \n",
       "57                     0.000000                  0.000000   \n",
       "69                     0.013229                  0.108647   \n",
       "80                     0.000000                  0.000000   \n",
       "86                     0.000000                  0.000000   \n",
       "124                    0.000000                  0.000000   \n",
       "135                    0.000000                  0.000000   \n",
       "139                    0.000000                  0.000000   \n",
       "140                    0.164302                  0.978244   \n",
       "144                    0.000000                  0.000000   \n",
       "145                    0.000000                  0.000000   \n",
       "151                    0.000000                  0.000000   \n",
       "163                    0.000000                  0.000000   \n",
       "187                    0.000000                  0.000000   \n",
       "190                    0.000000                  0.000000   \n",
       "200                    0.000000                  0.000000   \n",
       "207                    0.075974                  0.607792   \n",
       "228                    0.000000                  0.000000   \n",
       "232                    0.000000                  0.000000   \n",
       "239                    0.017171                  0.100273   \n",
       "...                         ...                       ...   \n",
       "5300                   0.000000                  0.000000   \n",
       "5310                   0.000000                  0.000000   \n",
       "5316                   0.000000                  0.000000   \n",
       "5323                   0.000000                  0.000000   \n",
       "5327                   0.000000                  0.000000   \n",
       "5331                   0.000000                  0.000000   \n",
       "5342                   0.000000                  0.000000   \n",
       "5347                   0.000000                  0.000000   \n",
       "5363                   0.000000                  0.000000   \n",
       "5371                   0.086500                  0.721544   \n",
       "5398                   0.072708                  0.483530   \n",
       "5402                   0.169048                  1.774486   \n",
       "5428                   0.000000                  0.000000   \n",
       "5429                  -0.063114                 -0.641190   \n",
       "5434                   0.000000                  0.000000   \n",
       "5442                   0.000000                  0.000000   \n",
       "5455                   0.116665                  1.005798   \n",
       "5459                   0.000000                  0.000000   \n",
       "5465                   0.000000                  0.000000   \n",
       "5471                   0.000000                  0.000000   \n",
       "5472                   0.000000                  0.000000   \n",
       "5480                   0.000000                  0.000000   \n",
       "5485                   0.000000                  0.000000   \n",
       "5489                   0.069491                  0.538332   \n",
       "5501                   0.000000                  0.000000   \n",
       "5511                   0.060341                  0.516032   \n",
       "5514                   0.000000                  0.000000   \n",
       "5516                   0.259811                  1.404028   \n",
       "5523                   0.000000                  0.000000   \n",
       "5534                   0.000000                  0.000000   \n",
       "\n",
       "      dep_dist_to_head (lin)  dep_dist_to_root (lin)  \\\n",
       "4                   1.500000                     2.5   \n",
       "9                   1.400000                    14.0   \n",
       "14                  1.750000                     4.5   \n",
       "15                  1.666667                     4.0   \n",
       "24                  1.500000                     5.5   \n",
       "31                  2.000000                    19.0   \n",
       "34                  3.250000                    26.5   \n",
       "38                  2.750000                    34.5   \n",
       "52                  4.500000                     2.5   \n",
       "53                  1.500000                     6.5   \n",
       "56                  1.166667                     8.5   \n",
       "57                  0.500000                     9.5   \n",
       "69                  1.333333                    28.0   \n",
       "80                  1.000000                    10.5   \n",
       "86                  1.500000                     2.5   \n",
       "124                 1.500000                     2.5   \n",
       "135                 4.500000                    18.5   \n",
       "139                 2.000000                     4.5   \n",
       "140                 0.500000                     2.5   \n",
       "144                 2.000000                     1.5   \n",
       "145                 1.750000                     2.5   \n",
       "151                 2.500000                     9.5   \n",
       "163                 5.000000                    20.5   \n",
       "187                 1.500000                    37.5   \n",
       "190                 1.500000                     1.5   \n",
       "200                 1.333333                     4.0   \n",
       "207                 2.000000                     5.5   \n",
       "228                 0.000000                     0.0   \n",
       "232                 1.800000                     6.0   \n",
       "239                 1.000000                     4.0   \n",
       "...                      ...                     ...   \n",
       "5300                1.000000                     1.5   \n",
       "5310                2.400000                    22.0   \n",
       "5316                1.500000                     2.5   \n",
       "5323                1.000000                    15.0   \n",
       "5327                1.000000                    15.5   \n",
       "5331                1.000000                     2.5   \n",
       "5342                1.000000                    15.5   \n",
       "5347                4.500000                    22.5   \n",
       "5363                7.000000                    27.5   \n",
       "5371                1.000000                     1.5   \n",
       "5398                0.500000                     1.5   \n",
       "5402                2.500000                     4.5   \n",
       "5428                0.000000                     9.5   \n",
       "5429                1.333333                    10.0   \n",
       "5434                1.666667                     9.0   \n",
       "5442                2.000000                     9.5   \n",
       "5455                1.000000                    20.5   \n",
       "5459                2.000000                    26.5   \n",
       "5465                0.666667                     1.0   \n",
       "5471                1.000000                     1.5   \n",
       "5472                5.000000                    12.5   \n",
       "5480                1.500000                    16.5   \n",
       "5485                1.000000                     3.5   \n",
       "5489                4.000000                     9.5   \n",
       "5501                0.000000                     0.5   \n",
       "5511                7.500000                    13.5   \n",
       "5514                2.500000                     5.5   \n",
       "5516                1.500000                     0.5   \n",
       "5523                1.500000                     6.5   \n",
       "5534                1.500000                     9.5   \n",
       "\n",
       "      dep_dist_to_root_norm (lin) dep_relation_to_head (lin)  \\\n",
       "4                        0.104167                       misc   \n",
       "9                        0.583333                       misc   \n",
       "14                       0.073770                       misc   \n",
       "15                       0.065574                       misc   \n",
       "24                       0.090164                       misc   \n",
       "31                       0.311475                       misc   \n",
       "34                       0.434426                       misc   \n",
       "38                       0.565574                       misc   \n",
       "52                       0.065789                       misc   \n",
       "53                       0.171053                       misc   \n",
       "56                       0.223684                       misc   \n",
       "57                       0.250000                       misc   \n",
       "69                       0.736842                       misc   \n",
       "80                       0.437500                       misc   \n",
       "86                       0.119048                       misc   \n",
       "124                      0.100000                       misc   \n",
       "135                      0.740000                       misc   \n",
       "139                      0.187500                       misc   \n",
       "140                      0.104167                       misc   \n",
       "144                      0.062500                       misc   \n",
       "145                      0.104167                       misc   \n",
       "151                      0.395833                       misc   \n",
       "163                      0.759259                       misc   \n",
       "187                      0.872093                       misc   \n",
       "190                      0.065217                       misc   \n",
       "200                      0.125000                       misc   \n",
       "207                      0.171875                       misc   \n",
       "228                      0.000000                       misc   \n",
       "232                      0.181818                       misc   \n",
       "239                      0.121212                       misc   \n",
       "...                           ...                        ...   \n",
       "5300                     0.040541                       misc   \n",
       "5310                     0.594595                       misc   \n",
       "5316                     0.100000                       misc   \n",
       "5323                     0.600000                       misc   \n",
       "5327                     0.620000                       misc   \n",
       "5331                     0.086207                       misc   \n",
       "5342                     0.534483                       misc   \n",
       "5347                     0.775862                       misc   \n",
       "5363                     0.625000                       misc   \n",
       "5371                     0.083333                       misc   \n",
       "5398                     0.062500                       misc   \n",
       "5402                     0.187500                       misc   \n",
       "5428                     0.500000                       misc   \n",
       "5429                     0.526316                       misc   \n",
       "5434                     0.600000                       misc   \n",
       "5442                     0.633333                       misc   \n",
       "5455                     0.569444                       misc   \n",
       "5459                     0.736111                       misc   \n",
       "5465                     0.066667                       misc   \n",
       "5471                     0.068182                       misc   \n",
       "5472                     0.568182                       misc   \n",
       "5480                     0.750000                       misc   \n",
       "5485                     0.233333                       misc   \n",
       "5489                     0.633333                       misc   \n",
       "5501                     0.022727                       misc   \n",
       "5511                     0.613636                       misc   \n",
       "5514                     0.239130                       misc   \n",
       "5516                     0.021739                       misc   \n",
       "5523                     0.282609                       misc   \n",
       "5534                     0.678571                       misc   \n",
       "\n",
       "      dep_num_dependents (lin)  dep_max_num_dependents (lin)  \\\n",
       "4                            2                             9   \n",
       "9                            5                             9   \n",
       "14                           4                             9   \n",
       "15                           4                             9   \n",
       "24                           5                             9   \n",
       "31                           4                             9   \n",
       "34                           7                             9   \n",
       "38                           8                             9   \n",
       "52                           2                            10   \n",
       "53                           6                            10   \n",
       "56                           8                            10   \n",
       "57                           0                            10   \n",
       "69                           3                            10   \n",
       "80                           2                            10   \n",
       "86                           8                             8   \n",
       "124                          4                            13   \n",
       "135                          1                            13   \n",
       "139                          3                             6   \n",
       "140                          1                             6   \n",
       "144                         11                             6   \n",
       "145                          6                             6   \n",
       "151                          5                             6   \n",
       "163                          2                             5   \n",
       "187                          2                             7   \n",
       "190                          3                             6   \n",
       "200                          2                             6   \n",
       "207                          3                             6   \n",
       "228                          0                            13   \n",
       "232                          5                            14   \n",
       "239                          1                            14   \n",
       "...                        ...                           ...   \n",
       "5300                         2                            10   \n",
       "5310                         9                            10   \n",
       "5316                         5                             6   \n",
       "5323                         2                             6   \n",
       "5327                         2                             6   \n",
       "5331                         1                             7   \n",
       "5342                         3                             7   \n",
       "5347                         3                             7   \n",
       "5363                         3                             5   \n",
       "5371                         3                             5   \n",
       "5398                         2                             7   \n",
       "5402                         3                             7   \n",
       "5428                         1                             6   \n",
       "5429                         5                             6   \n",
       "5434                         4                             5   \n",
       "5442                         4                             5   \n",
       "5455                         3                             6   \n",
       "5459                         3                             6   \n",
       "5465                         1                             7   \n",
       "5471                         6                             6   \n",
       "5472                         3                             6   \n",
       "5480                         5                             6   \n",
       "5485                         2                             5   \n",
       "5489                         3                             5   \n",
       "5501                         2                             8   \n",
       "5511                         3                             8   \n",
       "5514                         3                             6   \n",
       "5516                         0                             6   \n",
       "5523                         2                             6   \n",
       "5534                         3                             6   \n",
       "\n",
       "      dep_num_dependents_norm (lin)  dep_head_word_len (lin)  \n",
       "4                          0.222222                 8.500000  \n",
       "9                          0.555556                 8.800000  \n",
       "14                         0.444444                 9.250000  \n",
       "15                         0.444444                 9.000000  \n",
       "24                         0.555556                 7.000000  \n",
       "31                         0.444444                 7.333333  \n",
       "34                         0.777778                 7.500000  \n",
       "38                         0.888889                 4.250000  \n",
       "52                         0.200000                 8.500000  \n",
       "53                         0.600000                10.500000  \n",
       "56                         0.800000                 9.833333  \n",
       "57                         0.000000                 9.000000  \n",
       "69                         0.300000                 7.666667  \n",
       "80                         0.200000                 4.000000  \n",
       "86                         1.000000                 5.500000  \n",
       "124                        0.307692                 6.000000  \n",
       "135                        0.076923                 5.000000  \n",
       "139                        0.500000                 5.500000  \n",
       "140                        0.166667                 5.500000  \n",
       "144                        1.833333                 6.800000  \n",
       "145                        1.000000                 6.750000  \n",
       "151                        0.833333                 7.000000  \n",
       "163                        0.400000                 6.500000  \n",
       "187                        0.285714                 7.500000  \n",
       "190                        0.500000                 6.000000  \n",
       "200                        0.333333                 5.000000  \n",
       "207                        0.500000                 7.000000  \n",
       "228                        0.000000                 0.000000  \n",
       "232                        0.357143                 7.200000  \n",
       "239                        0.071429                 6.000000  \n",
       "...                             ...                      ...  \n",
       "5300                       0.200000                 6.000000  \n",
       "5310                       0.900000                 7.000000  \n",
       "5316                       0.833333                 7.000000  \n",
       "5323                       0.333333                 8.666667  \n",
       "5327                       0.333333                 8.000000  \n",
       "5331                       0.142857                 6.500000  \n",
       "5342                       0.428571                 8.000000  \n",
       "5347                       0.428571                 6.500000  \n",
       "5363                       0.600000                 5.500000  \n",
       "5371                       0.600000                 8.500000  \n",
       "5398                       0.285714                 7.500000  \n",
       "5402                       0.428571                10.000000  \n",
       "5428                       0.166667                 8.500000  \n",
       "5429                       0.833333                 8.000000  \n",
       "5434                       0.800000                 7.666667  \n",
       "5442                       0.800000                 7.000000  \n",
       "5455                       0.500000                 4.000000  \n",
       "5459                       0.500000                 6.000000  \n",
       "5465                       0.142857                 8.333333  \n",
       "5471                       1.000000                 5.000000  \n",
       "5472                       0.500000                 4.000000  \n",
       "5480                       0.833333                 5.500000  \n",
       "5485                       0.400000                 9.000000  \n",
       "5489                       0.600000                 8.500000  \n",
       "5501                       0.250000                 5.500000  \n",
       "5511                       0.375000                 5.500000  \n",
       "5514                       0.500000                 7.000000  \n",
       "5516                       0.000000                 6.000000  \n",
       "5523                       0.333333                 5.500000  \n",
       "5534                       0.500000                 7.000000  \n",
       "\n",
       "[718 rows x 58 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_fc_all[3].train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [Result(fs, fs.fc, fs.agg,\n",
    "        precision_recall_fscore_support(*mlp(*transform_feat_to_num(remove_labels_for_binary_df(fs.train), \n",
    "        remove_labels_for_binary_df(fs.test))))) for fs in all_fc_datasets]\n",
    "feature_eval_data = create_eval_df_from_results(results)\n",
    "feature_eval_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.x Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.608496</td>\n",
       "      <td>0.437294</td>\n",
       "      <td>1.0</td>\n",
       "      <td>always_complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.531693</td>\n",
       "      <td>0.362113</td>\n",
       "      <td>1.0</td>\n",
       "      <td>always_complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>News</td>\n",
       "      <td>0.491968</td>\n",
       "      <td>0.326232</td>\n",
       "      <td>1.0</td>\n",
       "      <td>always_complex</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset        f1      prec  rec              zc\n",
       "0  Wikipedia  0.608496  0.437294  1.0  always_complex\n",
       "1   WikiNews  0.531693  0.362113  1.0  always_complex\n",
       "2       News  0.491968  0.326232  1.0  always_complex"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_always_complex(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.695527</td>\n",
       "      <td>0.563084</td>\n",
       "      <td>0.909434</td>\n",
       "      <td>vocab_clean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.684280</td>\n",
       "      <td>0.544118</td>\n",
       "      <td>0.921708</td>\n",
       "      <td>vocab_clean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>News</td>\n",
       "      <td>0.696108</td>\n",
       "      <td>0.549645</td>\n",
       "      <td>0.948980</td>\n",
       "      <td>vocab_clean</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset        f1      prec       rec           zc\n",
       "0  Wikipedia  0.695527  0.563084  0.909434  vocab_clean\n",
       "1   WikiNews  0.684280  0.544118  0.921708  vocab_clean\n",
       "2       News  0.696108  0.549645  0.948980  vocab_clean"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_vocab_clean(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.706587</td>\n",
       "      <td>0.585608</td>\n",
       "      <td>0.890566</td>\n",
       "      <td>vocab_weighted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.708571</td>\n",
       "      <td>0.591885</td>\n",
       "      <td>0.882562</td>\n",
       "      <td>vocab_weighted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>News</td>\n",
       "      <td>0.780261</td>\n",
       "      <td>0.717466</td>\n",
       "      <td>0.855102</td>\n",
       "      <td>vocab_weighted</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset        f1      prec       rec              zc\n",
       "0  Wikipedia  0.706587  0.585608  0.890566  vocab_weighted\n",
       "1   WikiNews  0.708571  0.591885  0.882562  vocab_weighted\n",
       "2       News  0.780261  0.717466  0.855102  vocab_weighted"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_vocab_weighted(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix = datasets_fc_all[1].train.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.80) and column not in columns]\n",
    "# Drop features \n",
    "#df.drop(df.columns[to_drop], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_fc_all_corr_feats = [FeatureDataset(ds.name, ds.fc, ds.agg, ds.train.drop(to_drop, axis=1),\n",
    "                 ds.test.drop(to_drop, axis=1)) for ds in datasets_fc_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [Result(fs, fs.fc, fs.agg,\n",
    "        precision_recall_fscore_support(*xgboost(*transform_feat_to_num(remove_labels_for_binary_df(fs.train), \n",
    "        remove_labels_for_binary_df(fs.test))))) for fs in datasets_fc_all_corr_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.718929</td>\n",
       "      <td>0.728682</td>\n",
       "      <td>0.709434</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.773619</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.772242</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>News</td>\n",
       "      <td>0.806551</td>\n",
       "      <td>0.809035</td>\n",
       "      <td>0.804082</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset        f1      prec       rec   zc\n",
       "0  Wikipedia  0.718929  0.728682  0.709434  all\n",
       "1   WikiNews  0.773619  0.775000  0.772242  all\n",
       "2       News  0.806551  0.809035  0.804082  all"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_eval_data = create_eval_df_from_results(results)\n",
    "feature_eval_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
