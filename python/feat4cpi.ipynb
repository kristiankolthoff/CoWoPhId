{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1) Load Data\n",
    "First, we load all the data we need into pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.wsd import lesk\n",
    "import nltk\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from collections import defaultdict\n",
    "\n",
    "Dataset = namedtuple('Dataset', 'name, train, test')\n",
    "FeatureDataset = namedtuple('FeatureDataset', 'name, fc, agg, train, test')\n",
    "FeatureCategory = namedtuple('FeatureCategory', 'name, func')\n",
    "Aggregation = namedtuple('Aggregation', 'name, agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "columns = ['id', 'sentence', \"start\", \"end\", \"target\", \n",
    "           \"nat\", \"non_nat\", \"nat_marked\", \"non_nat_marked\", \"binary\", \"prob\"]\n",
    "\n",
    "def load_df(path, d_type, header):\n",
    "    df = pd.read_csv(path, header=header, sep = \"\\t\")\n",
    "    if len(df.columns) == len(columns):\n",
    "        df.columns = columns\n",
    "    if d_type == 'word':\n",
    "        df = df.loc[df.target.map(lambda target : len(word_tokenize(target)))<=1,]\n",
    "    elif d_type == 'phrase':\n",
    "        df = df.loc[df.target.map(lambda target : len(word_tokenize(target)))>1,]\n",
    "    return df\n",
    "\n",
    "def load_datasets(names, train_name, test_name, type_train = None, type_test = None, header=None):\n",
    "    MAIN_PATH_DATASET = \"../cwishareddataset/traindevset/english/\"\n",
    "    datasets = [Dataset(name, load_df(MAIN_PATH_DATASET + name + '_' + train_name + '.tsv', type_train, header),\n",
    "                              load_df(MAIN_PATH_DATASET + name + '_' + test_name + '.tsv', type_test, header))\n",
    "                              for name in names]\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2.1) Preprocessing\n",
    "Here we compute preprocessed variants of the target words. We provide a preprocessed target word with whitespace removel, lowercasing etc. In addition, we provide the lemma of the target and the preprocessed versions of the lemma. Finall, we also compute the POS tags and the PennTreebank POS tags, so later feature functions requiring POS tags can easily access the precomputed tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import *\n",
    "from nltk import word_tokenize\n",
    "from functools import lru_cache\n",
    "from utils import penn_to_wn\n",
    "import re\n",
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "wordNetLemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def overlaps(start1, end1, start2, end2):\n",
    "    return bool(range(max(start1, start2), min(end1, end2)+1))\n",
    "\n",
    "tbl = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "                      if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(tbl)\n",
    "\n",
    "def ratio_non_alpha(target):\n",
    "    return 1 - (np.sum([1 for letter in target if (ord(letter)>=65 and ord(letter)<=90) \n",
    "             or (ord(letter)>=97 and ord(letter)<=122)]) / len(target))\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def targets_with_index(start, end, context):\n",
    "    curr_pos = 0\n",
    "    targets = []\n",
    "    j = 0\n",
    "    w = 0\n",
    "    curr_split = ''\n",
    "    ctx_split = context.split()\n",
    "    whitespaces = re.findall('\\s+', context)\n",
    "    num_whitespaces = [len(token) for token in whitespaces]\n",
    "    num_whitespaces.append(1)\n",
    "    tokens = word_tokenize(context)\n",
    "    tokens = ['\"' if token not in context else token for token in tokens]\n",
    "    for index, token in enumerate(tokens, 1):\n",
    "        targets.append((token, index, curr_pos, (curr_pos + len(token))))\n",
    "        curr_pos += len(token)\n",
    "        curr_split += token\n",
    "        if ctx_split[j] == curr_split:\n",
    "            curr_pos += num_whitespaces[w]\n",
    "            j += 1\n",
    "            w += 1\n",
    "            curr_split = ''\n",
    "    vals = [(target[0], target[1]) for target in targets \\\n",
    "            if overlaps(start, end, target[2], target[3])]\n",
    "    return [val for val in vals if val[0] != '\"']\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def wordnet_pos_tagging(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    return nltk.pos_tag(tokens)\n",
    "\n",
    "def pos_tags(start, end, target, sentence):\n",
    "    wordPOSPairs = wordnet_pos_tagging(sentence)\n",
    "    targets_index = targets_with_index(start, end, sentence)\n",
    "    results = [wordPOSPairs[tpl[1]-1][1] for tpl in targets_index]\n",
    "    filtered_results = [result for result in results \n",
    "                        if remove_punctuation(result).strip() and result != 'POS']\n",
    "    if len(nltk.word_tokenize(target)) != len(filtered_results):\n",
    "            return ['n' for word in target.split()]\n",
    "    return filtered_results if len(filtered_results) > 0 else None\n",
    "\n",
    "def wordnet_lemma(target, pos):\n",
    "    #tokens = nltk.word_tokenize(target)\n",
    "    tokens = target.split()\n",
    "    if pos:\n",
    "        if len(pos) != len(tokens):\n",
    "            return target\n",
    "        pos = [penn_to_wn(poss) if penn_to_wn(poss) else 'n' for poss in pos]\n",
    "        lemmas = [wordNetLemmatizer.lemmatize(token, poss)\n",
    "                     for token, poss in zip(tokens, pos)]\n",
    "        return ' '.join(lemmas)\n",
    "    return target\n",
    "\n",
    "def preprocessing(dataframe):\n",
    "    df = dataframe.copy()\n",
    "    df['sentence'] = df.sentence.apply(lambda sent : sent.replace(\"''\", \"``\"))\n",
    "    df['p_target'] = df.target.apply(lambda target : target.strip().lower())\n",
    "    df['pos_tags'] = df[['start', 'end', 'target', 'sentence']].apply(lambda vals : pos_tags(*vals), axis = 1)\n",
    "    df['pos_tags_pt'] = df.pos_tags.apply(lambda pos : [penn_to_wn(poss) if penn_to_wn(poss) else 'n' for poss in pos] \n",
    "                                          if pos else [])\n",
    "    df['lemma'] = df[['target', 'pos_tags']].apply(lambda vals : wordnet_lemma(*vals), axis = 1)\n",
    "    df['p_lemma'] = df.lemma.apply(lambda lemma : lemma.strip().lower())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_datasets(datasets):\n",
    "    return [Dataset(ds.name, preprocessing(ds.train), \n",
    "                             preprocessing(ds.test)) \n",
    "                             for ds in datasets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2.1.2) Regularization\n",
    "Here we provide some functions to compute a regularized binary label based on thresholds of the probability, the number of native annotations, the number of non-native annotations and the sum of native and non-native annotations. Setting the threshold up in order to require more than a single mark for a word to be complex, may help in regularizing the model. Note that this regularized binary label of course should only used on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_regularied_label_prob(dataframe, prob_thresh = 0.05):\n",
    "    df = dataframe.copy()\n",
    "    df['binary'] = df.prob.apply(lambda prob : 1 if prob >= prob_thresh else 0)\n",
    "    return df\n",
    "\n",
    "def create_regularized_label_nat(dataframe, nat_thresh = 1):\n",
    "    df = dataframe.copy()\n",
    "    df['binary'] = df.nat_marked.apply(lambda nat : 1 if nat >= nat_thresh else 0)\n",
    "    return df\n",
    "\n",
    "def create_regularized_label_non_nat(dataframe, non_nat_thresh = 1):\n",
    "    df = dataframe.copy()\n",
    "    df['binary'] = df.non_nat_marked.apply(lambda nat : 1 if nat >= non_nat_thresh else 0)\n",
    "    return df\n",
    "\n",
    "def create_regularized_label_marks_sum(dataframe, sum_thresh = 1):\n",
    "    df = dataframe.copy()\n",
    "    df['binary'] = df[['nat_marked','non_nat_marked']].apply(lambda marks : 1 \\\n",
    "                                        if sum(marks) > sum_thresh else 0, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_regularization(datasets, regularizer, val):\n",
    "     return [Dataset(ds.name, regularizer(ds.train, val), \n",
    "                        ds.test) for ds in datasets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2.2) Aggregation (A2)\n",
    "Since many labels are multi-word expression, we first of all define some aggregation functions that aggregate feature values over multiple tokens. Implementing this seperately allows to easily exchange the used aggregation function and keeps the feature computation functions clean. These feature computation functions should only compute features for a single target word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_wiki = {}\n",
    "sum_counts = 0\n",
    "with open(\"resources/word-freq-dumps/enwiki-20150602-words-frequency.txt\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        word, freq = line.partition(\" \")[::2]\n",
    "        sum_counts+=int(freq)\n",
    "        word_freq_wiki[word.strip()] = int(freq)\n",
    "        \n",
    "def get_unigram_probability(word):\n",
    "    return word_freq_wiki.get(word,1) / (sum_counts + len(word_freq_wiki))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def agg_feat_num_average(target, func_feature, *args, **kwargs):\n",
    "    if 'pos' in kwargs:\n",
    "        pos = kwargs.pop('pos')\n",
    "        return np.mean([func_feature(token, *args, pos=poss) \n",
    "                for token, poss in zip(word_tokenize(target), pos)])\n",
    "    return np.mean([func_feature(token, *args) for token in word_tokenize(target)])\n",
    "\n",
    "def agg_feat_num_weighted_average(target, func_feature, alpha, *args, **kwargs):\n",
    "    if 'pos' in kwargs:\n",
    "        pos = kwargs.pop('pos')\n",
    "        prob_sum = np.sum([(alpha/(alpha+get_unigram_probability(token))) for token in word_tokenize(target)])\n",
    "        return np.mean([((alpha/(alpha+get_unigram_probability(token)))/prob_sum) * \n",
    "                func_feature(token, *args, pos=poss) for token, poss in zip(word_tokenize(target), pos)])\n",
    "    prob_sum = np.sum([(alpha/(alpha+get_unigram_probability(token))) for token in word_tokenize(target)])\n",
    "    return np.sum([((alpha/(alpha+get_unigram_probability(token)))/prob_sum) * \n",
    "                func_feature(token, *args) for token in word_tokenize(target)])\n",
    "\n",
    "agg_feat_num_weighted_average_medium = lambda target, func_feature, *args, **kwargs: \\\n",
    "                        agg_feat_num_weighted_average(target, func_feature, 0.0001, *args, **kwargs)\n",
    "\n",
    "def agg_feat_num_median(target, func_feature, *args, **kwargs):\n",
    "    if 'pos' in kwargs:\n",
    "        pos = kwargs.pop('pos')\n",
    "        return np.median([func_feature(token, *args, pos=poss) \n",
    "                for token, poss in zip(word_tokenize(target), pos)])\n",
    "    return np.median([func_feature(token, *args) for token in word_tokenize(target)])\n",
    "\n",
    "def agg_feat_num_max(target, func_feature, *args, **kwargs):\n",
    "    if 'pos' in kwargs:\n",
    "        pos = kwargs.pop('pos')\n",
    "        return np.max([func_feature(token, *args, pos=poss) for token, poss in zip(word_tokenize(target), pos)])\n",
    "    return np.max([func_feature(token, *args) for token in word_tokenize(target)])\n",
    "\n",
    "def agg_feat_num_min(target, func_feature, *args, **kwargs):\n",
    "    if 'pos' in kwargs:\n",
    "        pos = kwargs.pop('pos')\n",
    "        return np.min([func_feature(token, *args, pos=poss) for token, poss in zip(word_tokenize(target), pos)])\n",
    "    return np.min([func_feature(token, *args) for token in word_tokenize(target)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.400640363656745"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simple(target):\n",
    "    return len(target)\n",
    "\n",
    "agg_feat_num_weighted_average_medium('and web science group', simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_mean = [Aggregation('mean', agg_feat_num_average)]\n",
    "agg_max = [Aggregation('max', agg_feat_num_max)]\n",
    "agg_weighted = [Aggregation('weighted_mean', agg_feat_num_weighted_average_medium)]\n",
    "agg_default = [Aggregation('mean', agg_feat_num_average)]\n",
    "aggs_small = [Aggregation('mean', agg_feat_num_average), Aggregation('max', agg_feat_num_max)]\n",
    "aggs_all = [Aggregation('mean', agg_feat_num_average),\n",
    "            Aggregation('max', agg_feat_num_max), Aggregation('min', agg_feat_num_min),\n",
    "           Aggregation('weighted_mean', agg_feat_num_weighted_average_medium)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggs = agg_default "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_feature_datasets(*args, name=None):\n",
    "    zipped = zip(*args)\n",
    "    concat_features = []\n",
    "    for dataset in zipped:\n",
    "        df_train = None\n",
    "        df_test = None\n",
    "        fcs = []\n",
    "        aggs = []\n",
    "        for tpl in dataset:\n",
    "            if not fcs:\n",
    "                df_train = tpl.train.copy()\n",
    "                df_test = tpl.test.copy()\n",
    "            else:\n",
    "                df_train = pd.concat([df_train, tpl.train.copy()], axis = 1)\n",
    "                df_test = pd.concat([df_test, tpl.test.copy()], axis = 1)\n",
    "            fcs.append(tpl.fc)\n",
    "            aggs.append(tpl.agg)\n",
    "        if name:\n",
    "            data_name = (name,)\n",
    "        else:\n",
    "            data_name = fcs\n",
    "        concat_features.append(FeatureDataset(tpl.name, data_name, aggs,\n",
    "                    df_train.loc[:,~df_train.columns.duplicated()], \n",
    "                    df_test.loc[:,~df_test.columns.duplicated()]))\n",
    "    return concat_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3) Features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3.0.1) Baseline I\n",
    "The baseline I feature set covers only the two most relevant features as previous work has been shown. In many research work, only these two features, namely the word length and the word frequency are employed as features to compute complexity. Hence, we set this as our first feature baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_wiki = {}\n",
    "freq_sum_wiki = 0\n",
    "with open(\"resources/word-freq-dumps/enwiki-20150602-words-frequency.txt\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        word, freq = line.partition(\" \")[::2]\n",
    "        word_freq_wiki[word.strip()] = int(freq)\n",
    "        freq_sum_wiki+=int(freq)\n",
    "        \n",
    "def get_dict_count(target, freqs):\n",
    "    return freqs.get(target.strip().lower(), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_baseline_1(dataframe, agg, drop_features):\n",
    "    df = dataframe.copy()\n",
    "    df['length (bl1)'] = df.target.apply(lambda target : agg(target, len))\n",
    "    df['freq_wiki (bl1)'] = df.p_target.apply(lambda target : agg(target, get_dict_count, word_freq_wiki))\n",
    "    df['log_freq_wiki (bl1)'] = df['freq_wiki (bl1)'].apply(lambda freq : np.log(freq))\n",
    "    df = df.drop(drop_features, axis = 1)\n",
    "    return df\n",
    "\n",
    "fc_baseline_1 = FeatureCategory('baseline_1', features_baseline_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features_baseline_1(datasets, aggs = agg_default, drop_features = []):\n",
    "    return [FeatureDataset(ds.name, fc_baseline_1, agg,\n",
    "                        fc_baseline_1.func(ds.train, agg.agg, drop_features), \n",
    "                        fc_baseline_1.func(ds.test, agg.agg, drop_features)) \n",
    "                        for ds in datasets\n",
    "                        for agg in aggs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3.0.2) Basline II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordmodel import Word\n",
    "\n",
    "words_mrc_database = {}\n",
    "with open(\"resources/mrc-database/mrc2.dct\", encoding=\"utf8\") as file:\n",
    "    for index, line in enumerate(file):\n",
    "        line = line.strip()\n",
    "        word, phon, dphon, stress = line[51:].split('|')\n",
    "        w = Word(\n",
    "                wid = index,\n",
    "                nlet = int(line[0:2]),\n",
    "                nphon = int(line[2:4]),\n",
    "                nsyl = int(line[4]),\n",
    "                kf_freq = int(line[5:10]),\n",
    "                kf_ncats = int(line[10:12]),\n",
    "                kf_nsamp = int(line[12:15]),\n",
    "                tl_freq = int(line[15:21]),\n",
    "                brown_freq = int(line[21:25]),\n",
    "                fam = int(line[25:28]),\n",
    "                conc = int(line[28:31]),\n",
    "                imag = int(line[31:34]),\n",
    "                meanc = int(line[34:37]),\n",
    "                meanp = int(line[37:40]),\n",
    "                aoa = int(line[40:43]),\n",
    "                tq2 = line[43],\n",
    "                wtype = line[44],\n",
    "                pdwtype = line[45],\n",
    "                alphasyl = line[46],\n",
    "                status = line[47],\n",
    "                var = line[48],\n",
    "                cap = line[49],\n",
    "                irreg = line[50],\n",
    "                word=word,\n",
    "                phon=phon,\n",
    "                dphon=dphon,\n",
    "                stress=stress)\n",
    "        words_mrc_database[w.word.strip().lower()] = w\n",
    "\n",
    "def mrc_database(target, func, missing_val):\n",
    "    word = words_mrc_database.get(target.strip().lower())\n",
    "    val = func(word) if word else missing_val\n",
    "    return val if val != 0 else missing_val\n",
    "\n",
    "word_concreteness = {}\n",
    "with open(\"resources/word-freq-dumps/concreteness_brysbaert_et_al.txt\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        word, bigram, conc_m, conc_sd, \\\n",
    "        unknown, total, percent_known, \\\n",
    "        subtlex, dom_pos = line.split('\\t')\n",
    "        word_concreteness[word.strip()] = float(conc_m)\n",
    "\n",
    "word_freq_wiki = {}\n",
    "freq_sum_wiki = 0\n",
    "with open(\"resources/word-freq-dumps/enwiki-20150602-words-frequency.txt\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        word, freq = line.partition(\" \")[::2]\n",
    "        word_freq_wiki[word.strip()] = int(freq)\n",
    "        freq_sum_wiki+=int(freq)\n",
    "        \n",
    "def get_dict_count(target, freqs):\n",
    "    return freqs.get(target.strip().lower(), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_baseline_2(dataframe, agg, drop_features):\n",
    "    df = dataframe.copy()\n",
    "    df['length (bl2)'] = df.target.apply(lambda target : agg(target, len))\n",
    "    df['freq_wiki (bl2)'] = df.p_target.apply(lambda target : agg(target, get_dict_count, word_freq_wiki))\n",
    "    df['log_freq_wiki (bl2)'] = df[['freq_wiki (bl2)']].apply(lambda freq : np.log(freq))\n",
    "    df['mrc_fam (bl2)'] = df.p_target.apply(lambda target : agg(target, mrc_database, lambda word : word.fam, 400))\n",
    "    df['mrc_conc (bl2)'] = df.p_target.apply(lambda target : agg(target, mrc_database, lambda word : word.conc, 400))\n",
    "    df['mrc_imag (bl2)'] = df.p_target.apply(lambda target : agg(target, mrc_database, lambda word : word.imag, 400))\n",
    "    df['mrc_meanc (bl2)'] = df.p_target.apply(lambda target : agg(target, mrc_database, lambda word : word.meanc, 400))\n",
    "    df['concreteness (bl2)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                                lambda target : word_concreteness.get(target, 2.5)))\n",
    "    df = df.drop(drop_features, axis = 1)\n",
    "    return df\n",
    "\n",
    "fc_baseline_2 = FeatureCategory('baseline_2', features_baseline_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features_baseline_2(datasets, aggs = agg_default, drop_features = []):\n",
    "     return [FeatureDataset(ds.name, fc_baseline_2, agg,\n",
    "                        fc_baseline_2.func(ds.train, agg.agg, drop_features), \n",
    "                        fc_baseline_2.func(ds.test, agg.agg, drop_features)) \n",
    "                        for ds in datasets\n",
    "                        for agg in aggs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3.1) Linguistic Features\n",
    "Here we compute linguistic word features like the number of vowels the word has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import cmudict\n",
    "import numpy as np\n",
    "import pronouncing as pnc\n",
    "from wordmodel import Word\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.wordnet import *\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "from functools import lru_cache\n",
    "from collections import Counter\n",
    "from mezmorize import Cache\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import pronouncing as pnc\n",
    "from utils import penn_to_wn\n",
    "from nltk.parse.corenlp import *\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "\n",
    "java_path = \"C:/Program Files (x86)/Java/jdk1.8.0_144/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "path_to_jar = 'resources/stanford-dependency-parser/stanford-parser.jar'\n",
    "path_to_models_jar = 'resources/stanford-dependency-parser/stanford-parser-3.9.1-models.jar'\n",
    "\n",
    "porterStemmer = PorterStemmer()\n",
    "wordNetLemmatizer = WordNetLemmatizer()\n",
    "nerTagger = StanfordNERTagger('resources/stanford-ner-tagger/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "               'resources/stanford-ner-tagger/stanford-ner.jar',\n",
    "               encoding='utf-8')\n",
    "\n",
    "words_mrc_database = {}\n",
    "with open(\"resources/mrc-database/mrc2.dct\", encoding=\"utf8\") as file:\n",
    "    for index, line in enumerate(file):\n",
    "        line = line.strip()\n",
    "        word, phon, dphon, stress = line[51:].split('|')\n",
    "        w = Word(\n",
    "                wid = index,\n",
    "                nlet = int(line[0:2]),\n",
    "                nphon = int(line[2:4]),\n",
    "                nsyl = int(line[4]),\n",
    "                kf_freq = int(line[5:10]),\n",
    "                kf_ncats = int(line[10:12]),\n",
    "                kf_nsamp = int(line[12:15]),\n",
    "                tl_freq = int(line[15:21]),\n",
    "                brown_freq = int(line[21:25]),\n",
    "                fam = int(line[25:28]),\n",
    "                conc = int(line[28:31]),\n",
    "                imag = int(line[31:34]),\n",
    "                meanc = int(line[34:37]),\n",
    "                meanp = int(line[37:40]),\n",
    "                aoa = int(line[40:43]),\n",
    "                tq2 = line[43],\n",
    "                wtype = line[44],\n",
    "                pdwtype = line[45],\n",
    "                alphasyl = line[46],\n",
    "                status = line[47],\n",
    "                var = line[48],\n",
    "                cap = line[49],\n",
    "                irreg = line[50],\n",
    "                word=word,\n",
    "                phon=phon,\n",
    "                dphon=dphon,\n",
    "                stress=stress)\n",
    "        words_mrc_database[w.word.strip().lower()] = w\n",
    "\n",
    "def mrc_database(target, func, missing_val):\n",
    "    word = words_mrc_database.get(target.strip().lower())\n",
    "    val = func(word) if word else missing_val\n",
    "    return val if val != 0 else missing_val\n",
    "\n",
    "d = cmudict.dict()\n",
    "\n",
    "def num_syllables_rule_based(target):\n",
    "    vowels = \"aeiouy\"\n",
    "    numVowels = 0\n",
    "    lastWasVowel = False\n",
    "    for wc in target:\n",
    "        foundVowel = False\n",
    "        for v in vowels:\n",
    "            if v == wc:\n",
    "                if not lastWasVowel: numVowels+=1  \n",
    "                foundVowel = lastWasVowel = True\n",
    "                break\n",
    "        if not foundVowel:  \n",
    "            lastWasVowel = False\n",
    "    if len(target) > 2 and target[-2:] == \"es\":\n",
    "        numVowels-=1\n",
    "    elif len(target) > 1 and target[-1:] == \"e\":\n",
    "        numVowels-=1\n",
    "    return numVowels\n",
    "\n",
    "def num_syllables(target):\n",
    "    if target in d:\n",
    "        return np.mean([len(list(y for y in x if y[-1].isdigit())) for x in d[target.lower()]])\n",
    "    else:\n",
    "        return num_syllables_rule_based(target)\n",
    "    \n",
    "def num_vowels(target):\n",
    "    return np.sum([target.lower().count(vowel) for vowel in 'aeiouy'])\n",
    "\n",
    "def cognate_across_languages_sim(target, sim_func, agg_func, translations):\n",
    "    targ = target.strip().lower()\n",
    "    translated = translations.get(targ)\n",
    "    if not translated:\n",
    "        return 0\n",
    "    trans_texts = set([trans_word.text for trans_word in translated])\n",
    "    similarities = [sim_func(targ,trans_text) \n",
    "                    for trans_text in trans_texts]\n",
    "    return agg_func(similarities)\n",
    "\n",
    "def porter_stem_len(target):\n",
    "    return len(str(porterStemmer.stem(target)))\n",
    "\n",
    "def porter_stemmer_num_steps(target):\n",
    "    stem = target.lower()\n",
    "    applied_steps = 0\n",
    "    if porterStemmer.mode == porterStemmer.NLTK_EXTENSIONS and target in porterStemmer.pool:\n",
    "            return applied_steps\n",
    "    if porterStemmer.mode != porterStemmer.ORIGINAL_ALGORITHM and len(target) <= 2:\n",
    "            return applied_steps\n",
    "    step_funcs = [porterStemmer._step1a, porterStemmer._step1b, porterStemmer._step1c,\n",
    "                  porterStemmer._step2, porterStemmer._step3, porterStemmer._step3,\n",
    "                  porterStemmer._step4, porterStemmer._step5a, porterStemmer._step5b]\n",
    "    for step_func in step_funcs:\n",
    "        stem_step = step_func(stem)\n",
    "        if stem_step != stem:\n",
    "            stem = stem_step\n",
    "            applied_steps += 1\n",
    "    return applied_steps\n",
    "\n",
    "def is_named_entity(sentence, target):\n",
    "    tokenized_sent = word_tokenize(sentence)\n",
    "    tagged_sent = nerTagger.tag(tokenized_sent)\n",
    "    for token, tag in tagged_sent:\n",
    "        if token == target and tag != 'O':\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def named_entity_type(sentence, target):\n",
    "    tokenized_sent = word_tokenize(sentence)\n",
    "    tagged_sent = nerTagger.tag(tokenized_sent)\n",
    "    return [tag for token, tag in tagged_sent if token == target][0]\n",
    "\n",
    "def ratio_cap_letters(target):\n",
    "    return np.sum([1 for letter in target if letter.isupper()]) / len(target)\n",
    "\n",
    "def ratio_num_letters(target):\n",
    "    return np.sum([1 for letter in target if letter.isdigit()]) / len(target)\n",
    "\n",
    "def ratio_non_ascii_letters(target):\n",
    "    ascii = set(string.printable)   \n",
    "    return 1 - (np.sum([1 for letter in target if letter in ascii]) / len(target))\n",
    "\n",
    "def ratio_non_alpha(target):\n",
    "    return 1 - (np.sum([1 for letter in target if (ord(letter)>=65 and ord(letter)<=90) \n",
    "             or (ord(letter)>=97 and ord(letter)<=122)]) / len(target))\n",
    "\n",
    "def grapheme_to_phoneme_ratio(target):\n",
    "    phoneme_lengths = [len(prons.split()) \n",
    "            for prons in pnc.phones_for_word(target)]\n",
    "    if phoneme_lengths:\n",
    "        return len(target) / np.mean(phoneme_lengths)\n",
    "    return 1\n",
    "\n",
    "def num_pronounciations(target):\n",
    "    length = len(pnc.phones_for_word(target))\n",
    "    return length if length != 0 else 1\n",
    "\n",
    "# First make sure that the StanfordCoreNLP Server is running under port 9011\n",
    "# cd to stanfordCoreNLP directory\n",
    "# java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9011 -timeout 15000\n",
    "parser = CoreNLPDependencyParser(url='http://localhost:9011/')\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def dependency_parse_with_root(sentence):\n",
    "    try:\n",
    "        dependency_parser = parser.raw_parse(sentence)\n",
    "        dependencies = []\n",
    "        parsetree = list(dependency_parser)[0]\n",
    "        for index, node in parsetree.nodes.items():\n",
    "            for relation, dependant in parsetree.nodes[index]['deps'].items():\n",
    "                for dep in dependant:\n",
    "                    triple = ((node['word'], index), relation, \\\n",
    "                              (parsetree.nodes[dep]['word'], dep))\n",
    "                    dependencies.append(triple)\n",
    "        return dependencies\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def dependency_parse(sentence):\n",
    "    dependencies = dependency_parse_with_root(sentence)\n",
    "    filtered_dependencies = [triple for triple in dependencies if triple[1] != 'ROOT']\n",
    "    return filtered_dependencies\n",
    "\n",
    "\n",
    "def dep_dist_to_head(target, start, end, context):\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    return np.nan_to_num(np.mean([np.abs(triple[0][1] - triple[2][1])-1 \n",
    "                                for triple in triples if triple[2] in targets]))\n",
    "\n",
    "def dep_dist_to_root(target, start, end, context):\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse_with_root(context)\n",
    "    root_nodes = list(filter(lambda triple : triple[1] == 'ROOT' , triples))\n",
    "    if root_nodes: \n",
    "        root_node = root_nodes[0]\n",
    "    else:\n",
    "        return 0\n",
    "    dist = np.nan_to_num(np.mean([np.abs(root_node[2][1] - triple[2][1])-1 \n",
    "                                for triple in triples if triple[2] in targets]))\n",
    "    return dist if dist != -1 else 0\n",
    "\n",
    "def dep_relation_to_head(target, start, end, context):\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse_with_root(context)\n",
    "    relations = [triple[1] for triple in triples if triple[2] in targets]\n",
    "    return relations[0] if len(relations) == 1 else 'misc'\n",
    "    \n",
    "def dep_head_word_len(target, start, end, context):\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse(context)\n",
    "    return np.nan_to_num(np.mean([len(triple[0][0]) \n",
    "        for triple in triples if triple[2] in targets]))\n",
    "\n",
    "def dep_num_dependents(target, start, end, context):\n",
    "    targets = targets_with_index(start, end, context)\n",
    "    triples = dependency_parse_with_root(context)\n",
    "    return len([triple[1] for triple in triples if triple[0] in targets])\n",
    "\n",
    "def dep_max_num_dependents(context):\n",
    "    triples = dependency_parse_with_root(context)\n",
    "    most = Counter([triple[0][0] for triple in triples]).most_common(1)\n",
    "    return most[0][1] if most else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len ta_train : 8251\n",
      "Len ta_test : 2097\n",
      "Len targets : 10348\n"
     ]
    }
   ],
   "source": [
    "datasets = load_datasets(['Wikipedia', 'WikiNews', 'News'], 'TrainDev', 'Test')\n",
    "targets_train = list(set([ngram for ds in datasets for mwe in ds.train['target'].tolist() for ngram in mwe.split()]))\n",
    "targets_test = list(set([ngram for ds in datasets for mwe in ds.test['target'].tolist() for ngram in mwe.split()]))\n",
    "targets = targets_train.copy()\n",
    "targets.extend(targets_test)\n",
    "print('Len ta_train : {}'.format(len(targets_train)))\n",
    "print('Len ta_test : {}'.format(len(targets_test)))\n",
    "print('Len targets : {}'.format(len(targets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from googletrans import Translator\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "translator = Translator()\n",
    "targets = [target.strip().lower() for target in targets]\n",
    "\n",
    "trans_word = translator.translate(word, dest='de')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from googletrans import Translator\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "translator = Translator()\n",
    "targets = [target.strip().lower() for target in targets]\n",
    "languages = ['fr', 'de', 'es']\n",
    "translations = defaultdict(list)\n",
    "for index, word in enumerate(targets):\n",
    "    print(word)\n",
    "    translator = Translator()\n",
    "    for lang in languages:\n",
    "        trans_word = translator.translate(word, dest=lang)\n",
    "        translations[word].append(trans_word)\n",
    "        print(str(index) + \" \" + word + \" \" + trans_word.text)\n",
    "with open('resources/translations/translations.json', 'wb') as fp:\n",
    "    pickle.dump(translations, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "with open('resources/translations/data.json', 'rb') as fp:\n",
    "    data = pickle.load(fp)\n",
    "\n",
    "if not data:\n",
    "    translator = Translator()\n",
    "    targets = [target.strip().lower() for target in targets]\n",
    "    languages = ['fr', 'de', 'es']\n",
    "    translations = defaultdict(list)\n",
    "    for index, word in enumerate(targets):\n",
    "        translator = Translator()\n",
    "        for lang in languages:\n",
    "            trans_word = translator.translate(word, dest=lang)\n",
    "            translations[word].append(trans_word)\n",
    "            print(str(index) + \" \" + word + \" \" + trans_word.text)\n",
    "    with open('resources/translations/data.json', 'wb') as fp:\n",
    "        pickle.dump(translations, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    translations = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from similarity.ngram import NGram\n",
    "bigram_dist = NGram(2)\n",
    "trigram_dist = NGram(3)\n",
    "\n",
    "def features_linguistic(dataframe, agg, drop_features):\n",
    "    df = dataframe.copy()\n",
    "    df['length (lin)'] = df.target.apply(lambda target : agg(target, len))\n",
    "    df['phrase_length (lin)'] = df.target.apply(lambda target : len(target))\n",
    "    df['target_num_words (lin)'] = df.target.apply(lambda target : len(word_tokenize(target)))\n",
    "    # Relative positions of the target word based on character counting\n",
    "    df['relative_position_left (lin)'] = df[['sentence', 'start']].apply(lambda vals : vals[1] / len(vals[0]), axis = 1)\n",
    "    df['relative_position_centered (lin)'] = df[['sentence', 'start', 'end']].apply(lambda vals : \n",
    "                ((vals[1] + vals[2]) / 2) / len(vals[0]), axis = 1)\n",
    "    df['relative_position_right (lin)'] = df[['sentence', 'end']].apply(lambda vals : vals[1] / len(vals[0]), axis = 1)\n",
    "    df['ratio_cap_letters (lin)'] = df.target.apply(lambda target : agg(target, ratio_cap_letters))\n",
    "    df['all_caps (lin)'] = df[['ratio_cap_letters (lin)']] == 1\n",
    "    df['ratio_num_letters (lin)'] = df.target.apply(lambda target : agg(target, ratio_num_letters))\n",
    "    df['ratio_non_ascii_letters (lin)'] = df.target.apply(lambda target : agg(target, ratio_non_ascii_letters))\n",
    "    df['ratio_non_alpha (lin)'] = df.target.apply(lambda target : agg(target, ratio_non_alpha))\n",
    "    df['grapheme_to_phoneme_ratio (lin)'] = df.target.apply(lambda target : agg(target, grapheme_to_phoneme_ratio))\n",
    "    df['num_pronounciations (lin)'] = df.target.apply(lambda target : agg(target, num_pronounciations))\n",
    "    df['hyphenated (lin)'] = df.target.apply(lambda target : int('-' in target))\n",
    "    df['is_title (lin)'] = df.target.apply(lambda target : target.istitle())\n",
    "    df['mrc_nphon (lin)'] = df.p_target.apply(lambda target : agg(target, mrc_database, lambda word : word.nphon, 0))\n",
    "    df['cal_ngram_2_sim_min (lin)'] = df.p_target.apply(lambda target : agg(target, cognate_across_languages_sim, \\\n",
    "                                lambda source, dest : 1 - bigram_dist.distance(source, dest), np.min, translations))\n",
    "    df['cal_ngram_2_sim_max (lin)'] = df.p_target.apply(lambda target : agg(target, cognate_across_languages_sim, \\\n",
    "                                lambda source, dest : 1 - bigram_dist.distance(source, dest), np.max, translations))\n",
    "    df['cal_ngram_2_sim_mean (lin)'] = df.p_target.apply(lambda target : agg(target, cognate_across_languages_sim, \\\n",
    "                                lambda source, dest : 1 - bigram_dist.distance(source, dest), np.mean, translations))\n",
    "    df['cal_ngram_3_sim_min (lin)'] = df.p_target.apply(lambda target : agg(target, cognate_across_languages_sim, \\\n",
    "                                lambda source, dest : 1 - trigram_dist.distance(source, dest), np.min, translations))\n",
    "    df['cal_ngram_3_sim_max (lin)'] = df.p_target.apply(lambda target : agg(target, cognate_across_languages_sim, \\\n",
    "                                lambda source, dest : 1 - trigram_dist.distance(source, dest), np.max, translations))\n",
    "    df['cal_ngram_3_sim_mean (lin)'] = df.p_target.apply(lambda target : agg(target, cognate_across_languages_sim, \\\n",
    "                                lambda source, dest : 1 - trigram_dist.distance(source, dest), np.mean, translations))\n",
    "    df['num_syllables (lin)'] = df.p_target.apply(lambda target : agg(target, num_syllables))\n",
    "    df['num_vowels (lin)'] = df.p_target.apply(lambda target : agg(target, num_vowels))\n",
    "    df['vowel_consonant_ratio (lin)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                            lambda target : num_vowels(target) / (len(target) - num_vowels(target))))\n",
    "    # Porter stemmer stem length, number of applied steps,\n",
    "    # difference of stem length to target and reduction ratio\n",
    "    df['porter_stem_len (lin)'] = df.p_target.apply(lambda target : agg(target, porter_stem_len))\n",
    "    df['porter_stemmer_num_steps (lin)'] = df.p_target.apply(lambda target : agg(target, porter_stemmer_num_steps))\n",
    "    df['diff_len_stem_len (lin)'] = df['length (lin)'] - df['porter_stem_len (lin)']\n",
    "    df['reduction_stem_len (lin)'] = 1 - df['porter_stem_len (lin)'] / df['length (lin)']\n",
    "    df['norm_num_syllables (lin)'] = df['num_syllables (lin)'] / df['length (lin)']\n",
    "    df['norm_num_vowels (lin)'] = df['num_vowels (lin)'] / df['length (lin)']\n",
    "    df['lemma_len (lin)'] = df.lemma.apply(lambda lemma : agg(lemma, len))\n",
    "    df['reduction_lemma_len (lin)'] = 1 - df['lemma_len (lin)'] / df['length (lin)']\n",
    "    df['diff_len_lemma_len (lin)'] = df['length (lin)'] - df['lemma_len (lin)']\n",
    "    df['dep_dist_to_head (lin)'] = df[['target', 'start', 'end', 'sentence']].apply(lambda vals : \n",
    "                                               dep_dist_to_head(*vals), axis=1)\n",
    "    df['dep_dist_to_root (lin)'] = df[['target', 'start', 'end', 'sentence']].apply(lambda vals : \n",
    "                                                dep_dist_to_root(*vals), axis=1)\n",
    "    df['dep_dist_to_root_norm (lin)'] = df[['dep_dist_to_root (lin)', 'sentence']].apply(lambda vals : \\\n",
    "                                                float(vals[0]) / (len(word_tokenize(vals[1]))-1), axis=1)\n",
    "    df['dep_relation_to_head (lin)'] = df[['target', 'start', 'end', 'sentence']].apply(lambda vals : \\\n",
    "                                                dep_relation_to_head(*vals), axis = 1)\n",
    "    df['dep_num_dependents (lin)'] = df[['target', 'start', 'end', 'sentence']].apply(lambda vals : \\\n",
    "                                                dep_num_dependents(*vals), axis = 1)\n",
    "    df['dep_max_num_dependents (lin)'] = df.sentence.apply(lambda sentence : dep_max_num_dependents(sentence))\n",
    "    df['dep_num_dependents_norm (lin)'] = df['dep_num_dependents (lin)'] / df['dep_max_num_dependents (lin)']\n",
    "    df['dep_head_word_len (lin)'] = df[['target', 'start', 'end', 'sentence']].apply(lambda vals : \\\n",
    "                                                dep_head_word_len(*vals), axis = 1)\n",
    "    df = df.drop(drop_features, axis = 1)\n",
    "    return df\n",
    "    \n",
    "fc_linguistic = FeatureCategory('linguistic', features_linguistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features_linguistic(datasets, aggs = agg_default, drop_features = []):\n",
    "    return [FeatureDataset(ds.name, fc_linguistic, agg,\n",
    "                        fc_linguistic.func(ds.train, agg.agg, drop_features), \n",
    "                        fc_linguistic.func(ds.test, agg.agg, drop_features)) \n",
    "                        for ds in datasets\n",
    "                        for agg in aggs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3.3) Corpus-Based Features\n",
    "Here we compute features which are based on larger corpora. In this category we distinguish e.g. between frequency counts and N-Gram Language Model probabilites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (11,25) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import *\n",
    "from collections import defaultdict\n",
    "from wordmodel import Word\n",
    "import pandas as pd\n",
    "\n",
    "words_mrc_database = {}\n",
    "with open(\"resources/mrc-database/mrc2.dct\", encoding=\"utf8\") as file:\n",
    "    for index, line in enumerate(file):\n",
    "        line = line.strip()\n",
    "        word, phon, dphon, stress = line[51:].split('|')\n",
    "        w = Word(\n",
    "                wid = index,\n",
    "                nlet = int(line[0:2]),\n",
    "                nphon = int(line[2:4]),\n",
    "                nsyl = int(line[4]),\n",
    "                kf_freq = int(line[5:10]),\n",
    "                kf_ncats = int(line[10:12]),\n",
    "                kf_nsamp = int(line[12:15]),\n",
    "                tl_freq = int(line[15:21]),\n",
    "                brown_freq = int(line[21:25]),\n",
    "                fam = int(line[25:28]),\n",
    "                conc = int(line[28:31]),\n",
    "                imag = int(line[31:34]),\n",
    "                meanc = int(line[34:37]),\n",
    "                meanp = int(line[37:40]),\n",
    "                aoa = int(line[40:43]),\n",
    "                tq2 = line[43],\n",
    "                wtype = line[44],\n",
    "                pdwtype = line[45],\n",
    "                alphasyl = line[46],\n",
    "                status = line[47],\n",
    "                var = line[48],\n",
    "                cap = line[49],\n",
    "                irreg = line[50],\n",
    "                word=word,\n",
    "                phon=phon,\n",
    "                dphon=dphon,\n",
    "                stress=stress)\n",
    "        words_mrc_database[w.word.strip().lower()] = w\n",
    "\n",
    "def mrc_database(target, func, missing_val):\n",
    "    word = words_mrc_database.get(target.strip().lower())\n",
    "    val = func(word) if word else missing_val\n",
    "    return val if val != 0 else missing_val\n",
    "\n",
    "word_freq_wiki = {}\n",
    "freq_sum_wiki = 0\n",
    "with open(\"resources/word-freq-dumps/enwiki-20150602-words-frequency.txt\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        word, freq = line.partition(\" \")[::2]\n",
    "        word_freq_wiki[word.strip()] = int(freq)\n",
    "        freq_sum_wiki+=int(freq)\n",
    "        \n",
    "word_freq_simple_wiki = {}\n",
    "freq_sum_simple_wiki = 0\n",
    "with open(\"resources/word-freq-dumps/simple_wiki_word_freqs.txt\", encoding=\"ISO-8859-1\") as file:\n",
    "    for line in file:\n",
    "        word, freq = line.split()\n",
    "        word_freq_simple_wiki[word.strip()] = int(freq)\n",
    "        freq_sum_simple_wiki+=int(freq)\n",
    "        \n",
    "word_freq_lang8 = {}\n",
    "freq_sum_lang8 = 0\n",
    "with open(\"resources/word-freq-dumps/word_freqs_lang8.txt\", encoding=\"ISO-8859-1\") as file:\n",
    "    for line in file:\n",
    "        word, freq = line.split()\n",
    "        word_freq_lang8[word.strip()] = int(freq)\n",
    "        freq_sum_lang8+=int(freq)\n",
    "\n",
    "word_freq_bnc = {}\n",
    "with open(\"resources/word-freq-dumps/bnc_freq_all.al\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        freq, word, pos, num_files = line.split()\n",
    "        word_freq_bnc[word.strip()] = (int(freq), pos, int(num_files))\n",
    "\n",
    "word_freq_bnc_lemma = {}\n",
    "with open(\"resources/word-freq-dumps/bnc_lemma.al\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        sort_order, frequency, word, word_class = line.split()\n",
    "        word_freq_bnc_lemma[word.strip()] = (int(sort_order), word_class, int(frequency))\n",
    "\n",
    "        \n",
    "word_pknown_nobs_prev_freqZipf = {}\n",
    "with open(\"resources/word-freq-dumps/word_prevelance.csv\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        word, p_known, nobs, prevelance, freqZipf = line.split(\";\")\n",
    "        word_pknown_nobs_prev_freqZipf[word.strip()] = (float(p_known.replace(',','.')), \n",
    "                                                        float(nobs.replace(',','.')), \n",
    "                                                        float(prevelance.replace(',','.')), \n",
    "                                                        float(freqZipf.replace(',','.')))\n",
    "        \n",
    "subtlex_us = {}\n",
    "with open(\"resources/dictionaries/SUBTLEXus.txt\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        word, freq, cd_count, freq_low, cd_low, subtl_wf, lg10_wf, Subtlcd, lg10_cd = line.split('\\t')\n",
    "        subtlex_us[word.strip().lower()] = (int(freq), int(cd_count))\n",
    "        \n",
    "subtlex_uk = pd.read_csv(\"resources/dictionaries/SUBTLEXuk.txt\", sep = \"\\t\")\n",
    "subtlex_uk_dict = dict(zip(subtlex_uk['Spelling'], subtlex_uk['CD_count']))\n",
    "\n",
    "def get_dict_count(target, freqs):\n",
    "    return freqs.get(target.strip().lower(), 0)\n",
    "\n",
    "def freqZipf_func(target):\n",
    "    stats = word_pknown_nobs_prev_freqZipf.get(target)\n",
    "    return stats[3] if stats else 3.5\n",
    "\n",
    "\n",
    "WEIGHT_WIKI_SIMPLE_WIKI = freq_sum_wiki / freq_sum_simple_wiki\n",
    "WEIGHT_WIKI_LANG_8 = freq_sum_wiki / freq_sum_lang8\n",
    "\n",
    "def weighted_freq_ratio(target, word_freq_n, word_freq_m, weight):\n",
    "    freq_n = word_freq_n.get(target.strip().lower(), 1)\n",
    "    freq_m = word_freq_m.get(target.strip().lower(), 1)\n",
    "    return -1 + (2 * (freq_n / ((freq_m * weight) + freq_n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len ta_train : 11555\n",
      "Len ta_test : 2546\n",
      "Len targets : 14101\n"
     ]
    }
   ],
   "source": [
    "mwe_targets_train = list(set([mwe for ds in datasets for mwe in ds.train['target'].tolist()]))\n",
    "mwe_targets_test = list(set([mwe for ds in datasets for mwe in ds.test['target'].tolist()]))\n",
    "mwe_targets = mwe_targets_train.copy()\n",
    "mwe_targets.extend(mwe_targets_test)\n",
    "print('Len ta_train : {}'.format(len(mwe_targets_train)))\n",
    "print('Len ta_test : {}'.format(len(mwe_targets_test)))\n",
    "print('Len targets : {}'.format(len(mwe_targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phrasefinder as pf\n",
    "\n",
    "google_books_n_grams = {}\n",
    "options = pf.SearchOptions()\n",
    "options.topk = 10\n",
    "n_grams = mwe_targets\n",
    "\n",
    "with open('resources/word-freq-dumps/ngram_google.json', 'rb') as fp:\n",
    "    google_books_n_grams = pickle.load(fp)\n",
    "\n",
    "if not google_books_n_grams:\n",
    "    for index, n_gram in enumerate(n_grams):\n",
    "        try:\n",
    "            print(index, n_gram)\n",
    "            result = pf.search(pf.Corpus.AMERICAN_ENGLISH, n_gram, options)\n",
    "            vals = [(phrase.match_count, phrase.volume_count, phrase.first_year, phrase.last_year)\n",
    "                        for phrase in result.phrases]\n",
    "            mean_vals = [np.sum(elem) / len(elem) for elem in zip(*vals)]\n",
    "            google_books_n_grams[n_gram] = mean_vals\n",
    "            if result.status != pf.Status.Ok:\n",
    "                print('Request was not successful: {}'.format(result.status))\n",
    "        except Exception as error:\n",
    "            pass\n",
    "    with open('resources/word-freq-dumps/ngram_google.json', 'wb') as fp:\n",
    "        pickle.dump(google_books_n_grams, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_frequency(dataframe, agg, drop_features):   \n",
    "    df = dataframe.copy()\n",
    "    df['mrc_kf_freq (cor)'] = df.p_target.apply(lambda target : agg(target, mrc_database, \\\n",
    "                                                                    lambda word : word.kf_freq, 0))\n",
    "    df['mrc_kf_ncats (cor)'] = df.p_target.apply(lambda target : agg(target, mrc_database, \\\n",
    "                                                                    lambda word : word.kf_ncats, 0))\n",
    "    df['mrc_tl_freq (cor)'] = df.p_target.apply(lambda target : agg(target, mrc_database, \\\n",
    "                                                                    lambda word : word.tl_freq, 0))\n",
    "    df['mrc_brown_freq (cor)'] = df.p_target.apply(lambda target : agg(target, mrc_database, \\\n",
    "                                                                    lambda word : word.brown_freq, 0))\n",
    "    df['freq_wiki (cor)'] = df.p_target.apply(lambda target : agg(target, get_dict_count, word_freq_wiki))\n",
    "    df['log_freq_wiki (cor)'] = df['freq_wiki (cor)'].apply(lambda freq : np.log(freq))\n",
    "    df['freq_simple_wiki (cor)'] = df.p_target.apply(lambda target : agg(target, get_dict_count, word_freq_simple_wiki))\n",
    "    df['log_freq_simple_wiki (cor)'] = df['freq_simple_wiki (cor)'].apply(lambda freq : np.log(freq))\n",
    "    df['freq_bnc (cor)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                    lambda target : word_freq_bnc.get(target)[0] if word_freq_bnc.get(target) else 0))\n",
    "    df['log_freq_bnc (cor)'] = df['freq_bnc (cor)'].apply(lambda freq : np.log(freq))\n",
    "    df['freq_bnc_lemma (cor)'] = df.p_lemma.apply(lambda target : agg(target, \\\n",
    "                    lambda target : word_freq_bnc_lemma.get(target)[2] \\\n",
    "                                         if word_freq_bnc_lemma.get(target) else 0))\n",
    "    df['log_freq_bnc_lemma (cor)'] = df['freq_bnc_lemma (cor)'].apply(lambda freq : np.log(freq))\n",
    "    df['freqZipf (cor)'] = df.p_target.apply(lambda target : agg(target, freqZipf_func))\n",
    "    df['google_books_n_gram_freq (cor)'] = df.p_target.apply(lambda target : google_books_n_grams.get(target)[0] \\\n",
    "                                                     if google_books_n_grams.get(target) else 0)\n",
    "    df['log_google_books_n_gram_freq (cor)'] = df['google_books_n_gram_freq (cor)'].apply(lambda freq : np.log(freq))\n",
    "    df['google_books_n_gram_doc_freq (cor)'] = df.p_target.apply(lambda target : google_books_n_grams.get(target)[1] \\\n",
    "                                                        if google_books_n_grams.get(target)  else 0)\n",
    "    df['log_google_books_n_gram_doc_freq (cor)'] = df['google_books_n_gram_doc_freq (cor)'].apply(lambda freq : np.log(freq))\n",
    "    df['google_books_n_gram_first_year (cor)'] = df.p_target.apply(lambda target : google_books_n_grams.get(target)[2] \\\n",
    "                                                          if google_books_n_grams.get(target) else 1900)\n",
    "    df['google_books_n_gram_last_year (cor)'] = df.p_target.apply(lambda target : google_books_n_grams.get(target)[3] \\\n",
    "                                                         if google_books_n_grams.get(target)  else 1900)\n",
    "    df['subtlex_cd_us (cor)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                    lambda target : subtlex_us[target.strip().lower()][1] if subtlex_us.get(target.strip().lower()) else 0))\n",
    "    df['subtlex_cd_uk (cor)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                    lambda target : subtlex_uk_dict.get(target, 0)))\n",
    "    df['weighted_wiki_simple_wiki_ratio (cor)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                    weighted_freq_ratio, word_freq_wiki, word_freq_simple_wiki, WEIGHT_WIKI_SIMPLE_WIKI))\n",
    "    df['weighted_wiki_lang8_ratio (cor)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                    weighted_freq_ratio, word_freq_wiki, word_freq_lang8, WEIGHT_WIKI_SIMPLE_WIKI))\n",
    "    df = df.drop(drop_features, axis = 1)\n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "fc_frequency = FeatureCategory('frequency', features_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features_frequency(datasets, aggs = agg_default, drop_features = []):\n",
    "    return [FeatureDataset(ds.name, fc_frequency, agg,\n",
    "                        fc_frequency.func(ds.train, agg.agg, drop_features), \n",
    "                        fc_frequency.func(ds.test, agg.agg, drop_features)) \n",
    "                        for ds in datasets\n",
    "                        for agg in aggs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Language Model\n",
    "Here we load the different Kneser-Ney n-gram models we trained previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('resources/language-models/ngram_char_1.json', 'rb') as fp:\n",
    "    ngram_char_1 = pickle.load(fp)\n",
    "    \n",
    "with open('resources/language-models/ngram_word_1.json', 'rb') as fp:\n",
    "    ngram_word_1 = pickle.load(fp)\n",
    "\n",
    "with open('resources/language-models/ngram_char_2.json', 'rb') as fp:\n",
    "    ngram_char_2 = pickle.load(fp)\n",
    "    \n",
    "with open('resources/language-models/ngram_word_2.json', 'rb') as fp:\n",
    "    ngram_word_2 = pickle.load(fp)\n",
    "    \n",
    "with open('resources/language-models/ngram_char_3.json', 'rb') as fp:\n",
    "    ngram_char_3 = pickle.load(fp)\n",
    "    \n",
    "with open('resources/language-models/ngram_char_2_complex.json', 'rb') as fp:\n",
    "    ngram_char_2_complex = pickle.load(fp)\n",
    "    \n",
    "with open('resources/language-models/ngram_char_2_non_complex.json', 'rb') as fp:\n",
    "    ngram_char_2_non_complex = pickle.load(fp)\n",
    "    \n",
    "with open('resources/language-models/ngram_char_2_complex_cleaned.json', 'rb') as fp:\n",
    "    ngram_char_2_complex_cleaned = pickle.load(fp)\n",
    "    \n",
    "with open('resources/language-models/ngram_char_2_non_complex_cleaned.json', 'rb') as fp:\n",
    "    ngram_char_2_non_complex_cleaned = pickle.load(fp)\n",
    "    \n",
    "def kneser_ney_word_uni_gram(target):\n",
    "    return ngram_word_1.cond_prob(target)\n",
    "\n",
    "def kneser_ney_word_bi_gram(target):\n",
    "    words = target.split()\n",
    "    if len(words) <= 1:\n",
    "        return ngram_word_2.cond_prob(target)\n",
    "    return np.mean([ngram_word_2.cond_prob(words[index+1], (word,)) \n",
    "                for index, word in enumerate(words) \n",
    "                if index <= len(words)-2])\n",
    "    \n",
    "def kneser_ney_char_uni_gram_avg(target):\n",
    "    return np.mean([ngram_char_1.cond_prob(character) \n",
    "            for character in target])\n",
    "\n",
    "def kneser_ney_char_bi_gram_avg(target):\n",
    "    return np.mean([ngram_char_2.cond_prob(target[index+1], (character,)) \n",
    "            for index, character in enumerate(target) if index <= len(target)-2])\n",
    "\n",
    "def kneser_ney_char_bi_gram_avg_model(target, kn_model):\n",
    "    return np.mean([kn_model.cond_prob(target[index+1], (character,)) \n",
    "            for index, character in enumerate(target) if index <= len(target)-2])\n",
    "\n",
    "def kneser_ney_char_tri_gram_avg(target):\n",
    "    return np.mean([ngram_char_3.cond_prob(target[index+2], (character, target[index+1])) \n",
    "            for index, character in enumerate(target) if index <= len(target)-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import *\n",
    "wordNetLemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def features_language_model(dataframe, agg, drop_features):\n",
    "    df = dataframe.copy()\n",
    "    df['kneser_ney_word_uni_gram (cor)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                                    kneser_ney_word_uni_gram))\n",
    "    df['kneser_ney_word_bi_gram (cor)'] = df.p_target.apply(lambda target :  kneser_ney_word_bi_gram(target))\n",
    "    df['kneser_ney_char_uni_gram_avg (cor)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                                    kneser_ney_char_uni_gram_avg))\n",
    "    df['kneser_ney_char_bi_gram_avg (cor)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                                    kneser_ney_char_bi_gram_avg))\n",
    "    df['kneser_ney_char_tri_gram_avg (cor)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                                    kneser_ney_char_tri_gram_avg))\n",
    "    df['kneser_ney_char_bi_complex (cor)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                                    kneser_ney_char_bi_gram_avg_model, ngram_char_2_complex))\n",
    "    df['kneser_ney_char_bi_non_complex (cor)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                                    kneser_ney_char_bi_gram_avg_model, ngram_char_2_non_complex))\n",
    "    df['kneser_ney_char_bi_c_nc_ratio (cor)'] = df['kneser_ney_char_bi_complex (cor)'] / df['kneser_ney_char_bi_non_complex (cor)']\n",
    "    df['kneser_ney_char_bi_complex_cl (cor)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                                    kneser_ney_char_bi_gram_avg_model, ngram_char_2_complex_cleaned))\n",
    "    df['kneser_ney_char_bi_non_complex_cl (cor)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                                    kneser_ney_char_bi_gram_avg_model, ngram_char_2_non_complex_cleaned))\n",
    "    df['kneser_ney_char_bi_c_nc_ratio_cl (cor)'] = df['kneser_ney_char_bi_complex_cl (cor)'] / df['kneser_ney_char_bi_non_complex_cl (cor)']\n",
    "    df = df.fillna(0)\n",
    "    df = df.drop(drop_features, axis = 1)\n",
    "    return df\n",
    "\n",
    "fc_language_model = FeatureCategory('language_model', features_language_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features_language_model(datasets, aggs = agg_default, drop_features = []):\n",
    "    return [FeatureDataset(ds.name, fc_language_model, agg,\n",
    "                        fc_language_model.func(ds.train, agg.agg, drop_features), \n",
    "                        fc_language_model.func(ds.test, agg.agg, drop_features)) \n",
    "                        for ds in datasets\n",
    "                        for agg in aggs]\n",
    "\n",
    "fc_corpus = FeatureCategory('corpus', [fc_frequency, fc_language_model])\n",
    "\n",
    "def compute_features_corpus(datasets):\n",
    "    return [FeatureDataset(ds.name, fc_corpus,  ds.agg,\n",
    "            ds.train, ds.test) for ds in concat_feature_datasets(*datasets)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3.4) Psycholinguistic Features based on MRC Database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordmodel import Word\n",
    "\n",
    "words_mrc_database = {}\n",
    "with open(\"resources/mrc-database/mrc2.dct\", encoding=\"utf8\") as file:\n",
    "    for index, line in enumerate(file):\n",
    "        line = line.strip()\n",
    "        word, phon, dphon, stress = line[51:].split('|')\n",
    "        w = Word(\n",
    "                wid = index,\n",
    "                nlet = int(line[0:2]),\n",
    "                nphon = int(line[2:4]),\n",
    "                nsyl = int(line[4]),\n",
    "                kf_freq = int(line[5:10]),\n",
    "                kf_ncats = int(line[10:12]),\n",
    "                kf_nsamp = int(line[12:15]),\n",
    "                tl_freq = int(line[15:21]),\n",
    "                brown_freq = int(line[21:25]),\n",
    "                fam = int(line[25:28]),\n",
    "                conc = int(line[28:31]),\n",
    "                imag = int(line[31:34]),\n",
    "                meanc = int(line[34:37]),\n",
    "                meanp = int(line[37:40]),\n",
    "                aoa = int(line[40:43]),\n",
    "                tq2 = line[43],\n",
    "                wtype = line[44],\n",
    "                pdwtype = line[45],\n",
    "                alphasyl = line[46],\n",
    "                status = line[47],\n",
    "                var = line[48],\n",
    "                cap = line[49],\n",
    "                irreg = line[50],\n",
    "                word=word,\n",
    "                phon=phon,\n",
    "                dphon=dphon,\n",
    "                stress=stress)\n",
    "        words_mrc_database[w.word.strip().lower()] = w\n",
    "\n",
    "def mrc_database(target, func, missing_val):\n",
    "    word = words_mrc_database.get(target.strip().lower())\n",
    "    val = func(word) if word else missing_val\n",
    "    return val if val != 0 else missing_val\n",
    "\n",
    "word_concreteness = {}\n",
    "with open(\"resources/word-freq-dumps/concreteness_brysbaert_et_al.txt\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        word, bigram, conc_m, conc_sd, \\\n",
    "        unknown, total, percent_known, \\\n",
    "        subtlex, dom_pos = line.split('\\t')\n",
    "        word_concreteness[word.strip()] = float(conc_m)\n",
    "        \n",
    "word_age_of_aquisition = {}\n",
    "with open(\"resources/word-freq-dumps/AoA_ratings_Kuperman_et_al_BRM.csv\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        word, occur_total, occur_num, freq_pm, rating_Mean, rating_SD, dunno = line.split()\n",
    "        word_age_of_aquisition[word.strip()] = float(rating_Mean.replace(',', '.')) if rating_Mean != 'NA' else 0\n",
    "\n",
    "word_pknown_nobs_prev_freqZipf = {}\n",
    "with open(\"resources/word-freq-dumps/word_prevelance.csv\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        word, p_known, nobs, prevelance, freqZipf = line.split(\";\")\n",
    "        word_pknown_nobs_prev_freqZipf[word.strip()] = (float(p_known.replace(',','.')), \n",
    "                                                        float(nobs.replace(',','.')), \n",
    "                                                        float(prevelance.replace(',','.')), \n",
    "                                                        float(freqZipf.replace(',','.')))\n",
    "\n",
    "def perc_known_func(target, missing_value):\n",
    "    stats = word_pknown_nobs_prev_freqZipf.get(target)\n",
    "    return stats[0] if stats else missing_value\n",
    "\n",
    "def nobs_func(target):\n",
    "    stats = word_pknown_nobs_prev_freqZipf.get(target)\n",
    "    return stats[1] if stats else 0\n",
    "\n",
    "def prevelance_func(target):\n",
    "    stats = word_pknown_nobs_prev_freqZipf.get(target)\n",
    "    return stats[2] if stats else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_psycholingusitic(dataframe, agg, drop_features):\n",
    "    df = dataframe.copy()\n",
    "    df['mrc_fam (psy)'] = df.p_target.apply(lambda target : agg(target, mrc_database, lambda word : word.fam, 400))\n",
    "    df['mrc_conc (psy)'] = df.p_target.apply(lambda target : agg(target, mrc_database, lambda word : word.conc, 400))\n",
    "    df['mrc_imag (psy)'] = df.p_target.apply(lambda target : agg(target, mrc_database, lambda word : word.imag, 400))\n",
    "    df['mrc_meanc (psy)'] = df.p_target.apply(lambda target : agg(target, mrc_database, lambda word : word.meanc, 400))\n",
    "    df['mrc_meanp (psy)'] = df.p_target.apply(lambda target : agg(target, mrc_database, lambda word : word.meanp, 400))\n",
    "    df['mrc_aoa (psy)'] = df.p_target.apply(lambda target : agg(target, mrc_database, lambda word : word.aoa, 3.5))\n",
    "    df['perc_known (psy)'] = df.p_target.apply(lambda target : agg(target, perc_known_func, 0.5))\n",
    "    df['nobs (psy)'] = df.p_target.apply(lambda target : agg(target, nobs_func))\n",
    "    df['prevelance (psy)'] = df.p_target.apply(lambda target : agg(target, prevelance_func))\n",
    "    df['concreteness (psy)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                                lambda target : word_concreteness.get(target, 2.5)))\n",
    "    df['age_of_aquisition (psy)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                                    lambda target : word_age_of_aquisition.get(target, 8.5)))\n",
    "    df = df.drop(drop_features, axis = 1)\n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "fc_psycholinguistic = FeatureCategory('psycholinguistic', features_psycholingusitic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features_psycholinguistic(datasets, aggs = agg_default, drop_features = []):\n",
    "    return [FeatureDataset(ds.name, fc_psycholinguistic, agg,\n",
    "                        fc_psycholinguistic.func(ds.train, agg.agg, drop_features), \n",
    "                        fc_psycholinguistic.func(ds.test, agg.agg, drop_features)) \n",
    "                        for ds in datasets\n",
    "                        for agg in aggs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3.5) Semantic Features\n",
    "Here we implement all the relevant features based on WordNet and SentiWordNet. For example, the number of synsets the target word is contained in or the average length of the lemmas of all the synsets the target word is contained in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.5.1) WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.wsd import lesk\n",
    "from nltk.stem.wordnet import *\n",
    "from utils import penn_to_wn\n",
    "\n",
    "wordNetLemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def wn_synset_freq(target):\n",
    "    return len(wn.synsets(target))\n",
    "\n",
    "def wn_synset_avg_lemma_freq(target):\n",
    "    return np.nan_to_num(np.mean([len(synset.lemmas()) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_lemma_len(target):\n",
    "    return np.nan_to_num(np.nanmean([len(lemma.name()) \n",
    "            for synset in wn.synsets(target) \n",
    "            for lemma in synset.lemmas()]))\n",
    "\n",
    "def wn_synset_avg_hypernyms(target):\n",
    "    return np.nan_to_num(np.nanmean([len(synset.hypernyms()) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_hyponyms(target):\n",
    "    return np.nan_to_num(np.mean([len(synset.hyponyms()) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_sum_hypernyms(target):\n",
    "    return np.sum(([len(synset.hypernyms()) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_definition_len(target):\n",
    "    return np.nan_to_num(np.mean([len(str(synset.definition())) \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_hyptree_depth(target):\n",
    "    return np.nan_to_num(np.mean([synset.max_depth() \n",
    "            for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_num_distinct_pos(target):\n",
    "    return len(set([synset.pos() for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_num_relations(target):\n",
    "    return np.nan_to_num(np.mean([np.sum([len(synset.hypernyms()), len(synset.hyponyms()), \n",
    "             len(synset.instance_hypernyms()), len(synset.instance_hyponyms()),\n",
    "             len(synset.member_holonyms()), len(synset.substance_holonyms()),\n",
    "             len(synset.part_holonyms()), len(synset.member_meronyms()),\n",
    "             len(synset.substance_meronyms()), len(synset.part_meronyms())]) \n",
    "             for synset in wn.synsets(target)]))\n",
    "\n",
    "def wn_synset_avg_freq_pos(target, pos):\n",
    "    return len(wn.synsets(target, pos = pos))\n",
    "\n",
    "def wn_synset_pos_ratio_1(target, pos):\n",
    "    tokens = word_tokenize(target)\n",
    "    ratios = []\n",
    "    for token, poss in zip(tokens, pos):\n",
    "        synsets_freqs = len(wn.synsets(token))\n",
    "        ratios.append(len(wn.synsets(token, pos = poss)) / synsets_freqs \\\n",
    "                if synsets_freqs != 0 else 0.25)\n",
    "    return np.mean(ratios)\n",
    "\n",
    "def wn_synset_pos_ratio_2(target, pos):\n",
    "    tokens = word_tokenize(target)\n",
    "    ratios = []\n",
    "    for token, poss in zip(tokens, pos):\n",
    "        synsets_counts = np.sum([lemma.count() \n",
    "                for sn in wn.synsets(token) for lemma in sn.lemmas()])\n",
    "        ratios.append(np.sum([lemma.count() for sn in wn.synsets(token, pos = poss) \n",
    "                    for lemma in sn.lemmas()]) / synsets_counts if synsets_counts != 0 else 0.25)\n",
    "    return np.mean(ratios)\n",
    "\n",
    "def wn_synset_sense_entropy_uniform(target):\n",
    "    num_senses = len(wn.synsets(target))\n",
    "    return -np.sum([((1 / num_senses) * np.log2(1 / num_senses)) \n",
    "                     for index in range(0, num_senses)])\n",
    "\n",
    "def wn_synset_sense_entropy_pos_uniform(target):\n",
    "    num_senses = len(wn.synsets(target))\n",
    "    pos_distribution = [len(wn.synsets(target, pos = wn.NOUN)),\n",
    "                        len(wn.synsets(target, pos = wn.VERB)),\n",
    "                        len(wn.synsets(target, pos = wn.ADJ)),\n",
    "                        len(wn.synsets(target, pos = wn.ADV))]\n",
    "    return -np.sum([(np.nan_to_num((count / num_senses) * np.log2(count / num_senses))) \n",
    "            for count in pos_distribution]) if num_senses != 0 else 0\n",
    "\n",
    "def wn_synsets_sense_entropy_pos_central(target, pos):\n",
    "    num_senses_pos = len(wn.synsets(target, pos = pos))\n",
    "    return -np.sum([((1 / num_senses_pos) * np.log2(1 / num_senses_pos))\n",
    "                     for index in range(0, num_senses_pos)])\n",
    "\n",
    "def wn_synset_pos_probability_1(target, pos):\n",
    "    synsets = wn.synsets(target)\n",
    "    syn_freq_other_pos = np.sum([1 for synset in synsets if synset.pos() != pos])\n",
    "    return len(wn.synsets(target, pos = pos)) / syn_freq_other_pos\n",
    "\n",
    "def wn_synsets_avg_lemma_freq(target, freqs_func, freqs):\n",
    "    synsets = wn.synsets(target)\n",
    "    if not synsets:\n",
    "        return 0\n",
    "    return np.mean([np.nan_to_num(freqs_func(lemma.name(), freqs)) for synset in synsets\n",
    "                    for lemma in synset.lemmas()])\n",
    "\n",
    "def wn_synsets_freq_ratio_to_max_agg_min(target, freqs_func, freqs):\n",
    "    lemmas = [lemma.name().split('_') for synset in wn.synsets(target) \n",
    "                  for lemma in synset.lemmas()]\n",
    "    if not lemmas:\n",
    "        return 1\n",
    "    freqis = [np.min([freqs_func(lemma, freqs) for lemma in lemmata]) \n",
    "              for lemmata in lemmas]\n",
    "    target_freq = freqs_func(target, freqs)\n",
    "    if target_freq not in freqis: freqis.append(target_freq)\n",
    "    max_freq = np.max(freqis)\n",
    "    return target_freq / max_freq\n",
    "\n",
    "def wn_synsets_freq_ratio_to_max_agg_mean(target, freqs_func, freqs):\n",
    "    lemmas = [lemma.name().split('_') for synset in wn.synsets(target) \n",
    "                  for lemma in synset.lemmas()]\n",
    "    if not lemmas:\n",
    "        return 1\n",
    "    freqis = [np.mean([freqs_func(lemma, freqs) for lemma in lemmata]) \n",
    "              for lemmata in lemmas]\n",
    "    target_freq = freqs_func(target, freqs)\n",
    "    if target_freq not in freqis: freqis.append(target_freq)\n",
    "    max_freq = np.max(freqis)\n",
    "    return target_freq / max_freq\n",
    "\n",
    "def wn_synsets_freq_ratio_to_max_agg_median(target, freqs_func, freqs):\n",
    "    lemmas = [lemma.name().split('_') for synset in wn.synsets(target) \n",
    "                  for lemma in synset.lemmas()]\n",
    "    if not lemmas:\n",
    "        return 1\n",
    "    freqis = [np.median([freqs_func(lemma, freqs) for lemma in lemmata]) \n",
    "              for lemmata in lemmas]\n",
    "    target_freq = freqs_func(target, freqs)\n",
    "    if target_freq not in freqis: freqis.append(target_freq)\n",
    "    max_freq = np.max(freqis)\n",
    "    return target_freq / max_freq\n",
    "    \n",
    "def swn_avg_objective_score(target):\n",
    "    return np.nan_to_num(np.mean([senti_synset.obj_score() \n",
    "                for senti_synset in swn.senti_synsets(target)]))\n",
    "\n",
    "def wn_synset_lesk_wsd_ratio_hi_freq(target, sentence, freqs_func, freqs, pos):\n",
    "    wsd_synset = lesk(sentence.split(), target, pos)\n",
    "    if not wsd_synset:\n",
    "        return 0\n",
    "    lemmas = [lemma.name().split('_') for lemma in wsd_synset.lemmas()]\n",
    "    if not lemmas:\n",
    "        return 0\n",
    "    freqis = [np.min([freqs_func(lemma, freqs) for lemma in lemmata])\n",
    "              for lemmata in lemmas]\n",
    "    target_freq = freqs_func(target, freqs)\n",
    "    return np.sum([1 for freq in freqis if freq > target_freq]) / len(freqis)\n",
    "\n",
    "def wn_synset_lesk_wsd_ratio_hi_freq_sum(target, sentence, freqs_func, freqs, pos):\n",
    "    wsd_synset = lesk(sentence.split(), target, pos)\n",
    "    if not wsd_synset:\n",
    "        return 0\n",
    "    lemmas = [lemma.name().split('_') for lemma in wsd_synset.lemmas()]\n",
    "    if not lemmas:\n",
    "        return 0\n",
    "    freqis = [np.min([freqs_func(lemma, freqs) for lemma in lemmata])\n",
    "              for lemmata in lemmas]\n",
    "    target_freq = freqs_func(target, freqs)\n",
    "    return np.sum([freq for freq in freqis if freq > target_freq]) / np.sum(freqis)\n",
    "\n",
    "def wn_synset_lesk_wsd_ratio_hi_nopos_freq(target, sentence, freqs_func, freqs):\n",
    "    wsd_synset = lesk(sentence.split(), target)\n",
    "    if not wsd_synset:\n",
    "        return 0\n",
    "    lemmas = [lemma.name().split('_') for lemma in wsd_synset.lemmas()]\n",
    "    if not lemmas:\n",
    "        return 0\n",
    "    freqis = [np.min([freqs_func(lemma, freqs) for lemma in lemmata])\n",
    "              for lemmata in lemmas]\n",
    "    target_freq = freqs_func(target, freqs)\n",
    "    return np.sum([1 for freq in freqis if freq > target_freq]) / len(freqis)\n",
    "\n",
    "def wn_synset_lesk_wsd_ratio_low_freq(target, sentence, freqs_func, freqs, pos):\n",
    "    wsd_synset = lesk(sentence.split(), target, pos)\n",
    "    if not wsd_synset:\n",
    "        return 0\n",
    "    lemmas = [lemma.name().split('_') for lemma in wsd_synset.lemmas()]\n",
    "    if not lemmas:\n",
    "        return 0\n",
    "    freqis = [np.min([freqs_func(lemma, freqs) for lemma in lemmata])\n",
    "              for lemmata in lemmas]\n",
    "    target_freq = freqs_func(target, freqs)\n",
    "    return np.sum([1 for freq in freqis if freq < target_freq]) / len(freqis)\n",
    "\n",
    "def wn_synset_lesk_wsd_ratio_low_freq_sum(target, sentence, freqs_func, freqs, pos):\n",
    "    wsd_synset = lesk(sentence.split(), target, pos)\n",
    "    if not wsd_synset:\n",
    "        return 0\n",
    "    lemmas = [lemma.name().split('_') for lemma in wsd_synset.lemmas()]\n",
    "    if not lemmas:\n",
    "        return 0\n",
    "    freqis = [np.min([freqs_func(lemma, freqs) for lemma in lemmata])\n",
    "              for lemmata in lemmas]\n",
    "    target_freq = freqs_func(target, freqs)\n",
    "    return np.sum([freq for freq in freqis if freq < target_freq]) / np.sum(freqis)\n",
    "\n",
    "def wn_synset_lesk_wsd_ratio_low_nopos_freq(target, sentence, freqs_func, freqs):\n",
    "    wsd_synset = lesk(sentence.split(), target)\n",
    "    if not wsd_synset:\n",
    "        return 0\n",
    "    lemmas = [lemma.name().split('_') for lemma in wsd_synset.lemmas()]\n",
    "    if not lemmas:\n",
    "        return 0\n",
    "    freqis = [np.min([freqs_func(lemma, freqs) for lemma in lemmata])\n",
    "              for lemmata in lemmas]\n",
    "    target_freq = freqs_func(target, freqs)\n",
    "    return np.sum([1 for freq in freqis if freq < target_freq]) / len(freqis)\n",
    "\n",
    "def wn_synset_lesk_wsd_ratio_to_freq_sum(target, sentence, freqs_func, freqs, pos):\n",
    "    wsd_synset = lesk(sentence.split(), target, pos)\n",
    "    if not wsd_synset:\n",
    "        return 0\n",
    "    lemmas = [lemma.name().split('_') for lemma in wsd_synset.lemmas()]\n",
    "    if not lemmas:\n",
    "        return 0\n",
    "    freqis = [np.min([freqs_func(lemma, freqs) for lemma in lemmata])\n",
    "              for lemmata in lemmas]\n",
    "    target_freq = freqs_func(target, freqs)\n",
    "    return target_freq / np.sum(freqis)\n",
    "\n",
    "def wn_synset_lesk_wsd__norm_sense_rank(target, sentence, freqs_func, freqs, wsd_func, pos):\n",
    "    wsd_synset = wsd_func(sentence.split(), target, pos)\n",
    "    senses = wn.synsets(target)\n",
    "    if not wsd_synset:\n",
    "        return 0\n",
    "    wsd_synset = lesk(sentence.split(), target, pos)\n",
    "    sense_freqs = sorted([(sense, np.sum([lemma.count() for lemma in sense.lemmas()])) \n",
    "                   for sense in senses], key = lambda tpl : tpl[1], reverse=True)\n",
    "    sense_index = [sense for sense, cnt in sense_freqs].index(wsd_synset)\n",
    "    return sense_index / len(senses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warming up PyWSD (takes ~10 secs)... took 8.758508920669556 secs.\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "from pywsd.lesk import adapted_lesk\n",
    "\n",
    "def features_wordnet(dataframe, agg, drop_features):\n",
    "    df = dataframe.copy()\n",
    "    df['wn_synset_freq (sem)'] = df.p_target.apply(lambda target : agg(target, wn_synset_freq))\n",
    "    df['wn_synset_avg_lemma_freq (sem)'] = df.p_target.apply(lambda target : agg(target, wn_synset_avg_lemma_freq))\n",
    "    df['wn_synset_avg_lemma_len (sem)'] = df.p_target.apply(lambda target : agg(target, wn_synset_avg_lemma_len))\n",
    "    \n",
    "    df['length'] = df.target.apply(lambda target : agg(target, len))\n",
    "    df['wn_synset_diff_len_avg_lemma_len (sem)'] = df['wn_synset_avg_lemma_len (sem)'] - df.length\n",
    "    df['wn_synset_avg_hypernyms (sem)'] = df.p_target.apply(lambda target : agg(target, wn_synset_avg_hypernyms))\n",
    "    df['wn_synset_sum_hypernyms (sem)'] = df.p_target.apply(lambda target : agg(target, wn_synset_sum_hypernyms))\n",
    "    df['wn_synset_avg_hyponyms (sem)'] = df.p_target.apply(lambda target : agg(target, wn_synset_avg_hyponyms))\n",
    "\n",
    "    df['wn_synset_avg_definition_len (sem)'] = df.p_target.apply(lambda target : \n",
    "                                                         agg(target, wn_synset_avg_definition_len))\n",
    "    df['wn_synset_avg_hyptree_depth (sem)'] = df.p_target.apply(lambda target :\n",
    "                                                         agg(target, wn_synset_avg_hyptree_depth))\n",
    "    df['wn_synset_num_distinct_pos (sem)'] = df.p_target.apply(lambda target : \n",
    "                                                         agg(target, wn_synset_num_distinct_pos))\n",
    "    df['wn_synset_avg_num_relations (sem)'] = df.p_target.apply(lambda target : \n",
    "                                                         agg(target, wn_synset_avg_num_relations))\n",
    "\n",
    "    df['wn_synset_avg_freq_pos_noun (sem)'] = df.p_target.apply(lambda target : \n",
    "                                                        agg(target, wn_synset_avg_freq_pos, wn.NOUN))\n",
    "    df['wn_synset_avg_freq_pos_verb (sem)'] = df.p_target.apply(lambda target : \n",
    "                                                        agg(target, wn_synset_avg_freq_pos, wn.VERB))\n",
    "    df['wn_synset_avg_freq_pos_adj (sem)'] = df.p_target.apply(lambda target : \n",
    "                                                       agg(target, wn_synset_avg_freq_pos, wn.ADJ))\n",
    "    df['wn_synset_avg_freq_pos_adv (sem)'] = df.p_target.apply(lambda target : \n",
    "                                                       agg(target, wn_synset_avg_freq_pos, wn.ADV))\n",
    "\n",
    "    df['wn_synset_avg_freq_pos_noun_norm (sem)'] = np.nan_to_num(df['wn_synset_avg_freq_pos_noun (sem)'] / \\\n",
    "                                                                 df['wn_synset_freq (sem)'])\n",
    "    df['wn_synset_avg_freq_pos_verb_norm (sem)'] = np.nan_to_num(df['wn_synset_avg_freq_pos_verb (sem)'] / \\\n",
    "                                                                 df['wn_synset_freq (sem)'])\n",
    "    df['wn_synset_avg_freq_pos_adj_norm (sem)'] = np.nan_to_num(df['wn_synset_avg_freq_pos_adj (sem)'] / \\\n",
    "                                                                df['wn_synset_freq (sem)'])\n",
    "    df['wn_synset_avg_freq_pos_adv_norm (sem)'] = np.nan_to_num(df['wn_synset_avg_freq_pos_adv (sem)'] / \\\n",
    "                                                                df['wn_synset_freq (sem)'])\n",
    "\n",
    "    df['wn_synset_sense_entropy_uniform (sem)'] = df.p_target.apply(lambda target : \n",
    "                                            agg(target, wn_synset_sense_entropy_uniform))\n",
    "    df['wn_synset_sense_entropy_pos_uniform (sem)'] = df.p_target.apply(lambda target :\n",
    "                                            agg(target, wn_synset_sense_entropy_pos_uniform))\n",
    "    df['wn_synsets_sense_entropy_pos_central (sem)'] = df[['p_target', 'pos_tags_pt']].apply(\n",
    "        lambda vals : wn_synsets_sense_entropy_pos_central(vals[0], vals[1]), axis = 1)\n",
    "    \n",
    "    df['wn_synset_pos_ratio_1 (sem)'] = df[['p_target', 'pos_tags_pt']].apply(\n",
    "                    lambda vals : wn_synset_pos_ratio_1(vals[0], vals[1]), axis = 1)\n",
    "    \n",
    "    df['wn_synset_pos_ratio_2 (sem)'] = df[['p_target', 'pos_tags_pt']].apply(\n",
    "                    lambda vals : wn_synset_pos_ratio_2(vals[0], vals[1]), axis = 1)\n",
    "\n",
    "    df['swn_avg_objective_score (sem)'] = df.p_target.apply(lambda target : agg(target, swn_avg_objective_score))\n",
    "\n",
    "    df['wn_synsets_freq_ratio_to_max_agg_min (sem)'] = df.p_target.apply(lambda target : \\\n",
    "                                                    agg(target, wn_synsets_freq_ratio_to_max_agg_min, \\\n",
    "                                                                         get_dict_count, word_freq_wiki))\n",
    "    df['wn_synsets_freq_ratio_to_max_agg_mean (sem)'] = df.p_target.apply(lambda target : \\\n",
    "                                                    agg(target, wn_synsets_freq_ratio_to_max_agg_mean, \\\n",
    "                                                                         get_dict_count, word_freq_wiki))\n",
    "    df['wn_synsets_freq_ratio_to_max_agg_median (sem)'] = df.p_target.apply(lambda target : \\\n",
    "                                                    agg(target, wn_synsets_freq_ratio_to_max_agg_median, \\\n",
    "                                                                         get_dict_count, word_freq_wiki))\n",
    "    df['wn_synsets_avg_lemma_freq (sem)'] = df.p_target.apply(lambda target : \\\n",
    "                                                    agg(target, wn_synsets_avg_lemma_freq, \\\n",
    "                                                                         get_dict_count, word_freq_wiki))\n",
    "    df['freq_wiki'] = df.p_target.apply(lambda target : agg(target, get_dict_count, word_freq_wiki))\n",
    "    df['wn_synsets_freq_ratio_to_avg (sem)'] = df['wn_synsets_avg_lemma_freq (sem)'] / df.freq_wiki\n",
    "    df['wn_synset_lesk_wsd_ratio_hi_freq (sem)'] = df[['p_target','sentence', 'pos_tags_pt']].apply(lambda vals : \\\n",
    "                agg(vals[0], wn_synset_lesk_wsd_ratio_hi_freq, vals[1], \\\n",
    "                                     get_dict_count, word_freq_wiki, pos=vals[2]), axis = 1)\n",
    "    df['wn_synset_lesk_wsd_ratio_low_freq (sem)'] = df[['p_target','sentence', 'pos_tags_pt']].apply(lambda vals : \\\n",
    "                agg(vals[0], wn_synset_lesk_wsd_ratio_low_freq, vals[1], \\\n",
    "                                     get_dict_count, word_freq_wiki, pos=vals[2]), axis = 1)\n",
    "    df['wn_synset_lesk_wsd_ratio_hi_nopos_freq (sem)'] = df[['p_target','sentence']].apply(lambda vals : \\\n",
    "                agg(vals[0], wn_synset_lesk_wsd_ratio_hi_nopos_freq, vals[1], \\\n",
    "                                     get_dict_count, word_freq_wiki), axis = 1)\n",
    "    df['wn_synset_lesk_wsd_ratio_low_nopos_freq (sem)'] = df[['p_target','sentence']].apply(lambda vals : \\\n",
    "                agg(vals[0], wn_synset_lesk_wsd_ratio_low_nopos_freq, vals[1], \\\n",
    "                                     get_dict_count, word_freq_wiki), axis = 1)\n",
    "    df['wn_synset_lesk_wsd_ratio_hi_freq_sum (sem)'] = df[['p_target','sentence', 'pos_tags_pt']].apply(lambda vals : \\\n",
    "                agg(vals[0], wn_synset_lesk_wsd_ratio_hi_freq_sum, vals[1], \\\n",
    "                                     get_dict_count, word_freq_wiki, pos=vals[2]), axis = 1)\n",
    "    df['wn_synset_lesk_wsd_ratio_low_freq_sum (sem)'] = df[['p_target','sentence', 'pos_tags_pt']].apply(lambda vals : \\\n",
    "                agg(vals[0], wn_synset_lesk_wsd_ratio_low_freq_sum, vals[1], \\\n",
    "                                     get_dict_count, word_freq_wiki, pos=vals[2]), axis = 1)\n",
    "    df['wn_synset_lesk_wsd_ratio_to_freq_sum (sem)'] = df[['p_target','sentence', 'pos_tags_pt']].apply(lambda vals : \\\n",
    "                agg(vals[0], wn_synset_lesk_wsd_ratio_to_freq_sum, vals[1], \\\n",
    "                                     get_dict_count, word_freq_wiki, pos=vals[2]), axis = 1)\n",
    "    df['wn_synset_lesk_wsd__norm_sense_rank (sem)'] = df[['p_target','sentence', 'pos_tags_pt']].apply(lambda vals : \\\n",
    "                agg(vals[0], wn_synset_lesk_wsd__norm_sense_rank, vals[1], \\\n",
    "                                     get_dict_count, word_freq_wiki, lesk, pos=vals[2]), axis = 1)\n",
    "    df = df.drop(['length', 'freq_wiki'], axis = 1)\n",
    "    df = df.drop(drop_features, axis = 1)\n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "fc_wordnet = FeatureCategory('wordnet', features_wordnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features_wordnet(datasets, aggs = agg_default, drop_features = []):\n",
    "     return [FeatureDataset(ds.name, fc_wordnet, agg,\n",
    "                        fc_wordnet.func(ds.train, agg.agg, drop_features), \n",
    "                        fc_wordnet.func(ds.test, agg.agg, drop_features)) \n",
    "                        for ds in datasets\n",
    "                        for agg in aggs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.5.2) DBpedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('resources/dbpedia-cache/dbpedia_annotations_00.json', 'rb') as fp:\n",
    "    dbpedia_00 = pickle.load(fp)\n",
    "    \n",
    "with open('resources/dbpedia-cache/dbpedia_annotations_25.json', 'rb') as fp:\n",
    "    dbpedia_25 = pickle.load(fp)\n",
    "    \n",
    "with open('resources/dbpedia-cache/dbpedia_annotations_50.json', 'rb') as fp:\n",
    "    dbpedia_50 = pickle.load(fp)\n",
    "    \n",
    "with open('resources/dbpedia-cache/dbpedia_annotations_75.json', 'rb') as fp:\n",
    "    dbpedia_75 = pickle.load(fp)\n",
    "    \n",
    "with open('resources/dbpedia-cache/pagerank.json', 'rb') as fp:\n",
    "        page_rank = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def overlaps(start1, end1, start2, end2):\n",
    "    return bool(range(max(start1, start2), min(end1, end2)+1))\n",
    "\n",
    "def dbp_match_entities(sentence, target, start, end, annotations):\n",
    "    an_sents = annotations.get(sentence)\n",
    "    if an_sents:\n",
    "        ans = [(an['offset'], an['offset']+len(an['surfaceForm']), an) for an in an_sents]\n",
    "        return [an for s, e, an in ans if overlaps(start, end, s, e)]\n",
    "    return []\n",
    "\n",
    "def dbp_entity_ratio(sentence, target, start, end, annotations):\n",
    "    entities = dbp_match_entities(sentence, target, start, end, annotations)\n",
    "    if entities:\n",
    "        return np.min([np.sum([len(entity['surfaceForm']) \n",
    "                for entity in entities]) / len(target), 1])\n",
    "    return 0\n",
    "\n",
    "def dbp_support(sentence, target, start, end, annotations):\n",
    "    entities = dbp_match_entities(sentence, target, start, end, annotations)\n",
    "    if entities:\n",
    "        return np.mean([entity['support'] for entity in entities])\n",
    "    return 0\n",
    "\n",
    "def dbp_type_hierachy_depth(sentence, target, start, end, annotations):\n",
    "    entities = dbp_match_entities(sentence, target, start, end, annotations)\n",
    "    if entities:\n",
    "        return np.mean([np.sum([1 for cat in entity['types'].split(',') if 'DBpedia' in cat])\n",
    "                 for entity in entities])\n",
    "    return 0\n",
    "\n",
    "def dbp_freq_types(sentence, target, start, end, annotations):\n",
    "    entities = dbp_match_entities(sentence, target, start, end, annotations)\n",
    "    if entities:\n",
    "        return np.mean([len(entity['types'].split(',')) for entity in entities])\n",
    "    return 0\n",
    "\n",
    "def dbp_confidence(sentence, target, start, end):\n",
    "    entities = dbp_match_entities(sentence, target, start, end, dbpedia_75)\n",
    "    if entities:\n",
    "        return 0.75\n",
    "    entities = dbp_match_entities(sentence, target, start, end, dbpedia_50)\n",
    "    if entities:\n",
    "        return 0.5\n",
    "    entities = dbp_match_entities(sentence, target, start, end, dbpedia_25)\n",
    "    if entities:\n",
    "        return 0.25\n",
    "    return 0\n",
    "\n",
    "def dbp_pagerank(sentence, target, start, end, annotations):\n",
    "    entities = dbp_match_entities(sentence, target, start, end, annotations)\n",
    "    return np.nan_to_num(np.mean([page_rank.get(entity['URI'], 0) for entity in entities]))\n",
    "\n",
    "dbp_types = [('DBpedia:Place', 1, 'dbo:Place'), ('DBpedia:Person',2, 'dbo:Person'), \n",
    "             ('DBpedia:Organisation',3, 'dbo:Organisation'), ('DBpedia:Timeperiod', 4, 'dbo:Timeperiod')]\n",
    "\n",
    "def dbp_extract_type(entity):\n",
    "    types = [(cat, rank, name) for cat, rank, name in dbp_types if cat in entity['types']]\n",
    "    if not types and not entity['types']:\n",
    "        return ('dbo:notype', 0, 'dbo:notype')\n",
    "    if not types and entity['types']:\n",
    "        return ('dbo:misc', 5, 'dbo:misc')\n",
    "    else:\n",
    "        return types[0]\n",
    "\n",
    "def dbp_type(sentence, target, start, end, annotations):\n",
    "    entities = dbp_match_entities(sentence, target, start, end, annotations)\n",
    "    if not entities:\n",
    "        return 'dbo:missing'\n",
    "    types = [dbp_extract_type(entity) for entity in entities]\n",
    "    sorted(types, key=lambda tpl : tpl[1])\n",
    "    return types[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_dbpedia(dataframe, agg, drop_features):\n",
    "    df = dataframe.copy()\n",
    "    df['dbp_confidence (sem)'] = df[['sentence', 'target', 'start', 'end']].apply(lambda vals :\n",
    "                                                          dbp_confidence(*vals),axis = 1)\n",
    "    \n",
    "    df['dbp_entity_ratio_25 (sem)'] = df[['sentence', 'target', 'start', 'end']].apply(lambda vals :\n",
    "                                                          dbp_entity_ratio(*vals, dbpedia_25),axis = 1)\n",
    "    df['dbp_entity_support_25 (sem)'] = df[['sentence', 'target', 'start', 'end']].apply(lambda vals :\n",
    "                                                          dbp_support(*vals, dbpedia_25),axis = 1)\n",
    "    df['dbp_type_hierachy_depth_25 (sem)'] = df[['sentence', 'target', 'start', 'end']].apply(lambda vals :\n",
    "                                                          dbp_type_hierachy_depth(*vals, dbpedia_25),axis = 1)\n",
    "    df['dbp_freq_types_25 (sem)'] = df[['sentence', 'target', 'start', 'end']].apply(lambda vals :\n",
    "                                                          dbp_freq_types(*vals, dbpedia_25),axis = 1)\n",
    "    df['dbp_pagerank_25 (sem)'] = df[['sentence', 'target', 'start', 'end']].apply(lambda vals :\n",
    "                                                          dbp_pagerank(*vals, dbpedia_25),axis = 1)\n",
    "    max_page_rank = np.max(df['dbp_pagerank_25 (sem)'])\n",
    "    df['dbp_norm_pagerank_25 (sem)'] = df['dbp_pagerank_25 (sem)'] / max_page_rank\n",
    "    df['dbp_type_25 (sem)'] = df[['sentence', 'target', 'start', 'end']].apply(lambda vals :\n",
    "                                                          dbp_type(*vals, dbpedia_25),axis = 1)\n",
    "    \n",
    "    df['dbp_entity_ratio_00 (sem)'] = df[['sentence', 'target', 'start', 'end']].apply(lambda vals :\n",
    "                                                          dbp_entity_ratio(*vals, dbpedia_00),axis = 1)\n",
    "    df['dbp_entity_support_00 (sem)'] = df[['sentence', 'target', 'start', 'end']].apply(lambda vals :\n",
    "                                                          dbp_support(*vals, dbpedia_00),axis = 1)\n",
    "    df['dbp_type_hierachy_depth_00 (sem)'] = df[['sentence', 'target', 'start', 'end']].apply(lambda vals :\n",
    "                                                          dbp_type_hierachy_depth(*vals, dbpedia_00),axis = 1)\n",
    "    df['dbp_freq_types_00 (sem)'] = df[['sentence', 'target', 'start', 'end']].apply(lambda vals :\n",
    "                                                          dbp_freq_types(*vals, dbpedia_00),axis = 1)\n",
    "    df['dbp_pagerank_00 (sem)'] = df[['sentence', 'target', 'start', 'end']].apply(lambda vals :\n",
    "                                                          dbp_pagerank(*vals, dbpedia_00),axis = 1)\n",
    "    max_page_rank = np.max(df['dbp_pagerank_00 (sem)'])\n",
    "    df['dbp_norm_pagerank_00 (sem)'] = df['dbp_pagerank_00 (sem)'] / max_page_rank\n",
    "    df['dbp_type_00 (sem)'] = df[['sentence', 'target', 'start', 'end']].apply(lambda vals :\n",
    "                                                          dbp_type(*vals, dbpedia_00),axis = 1)\n",
    "    \n",
    "    df = df.drop(drop_features, axis = 1)\n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "fc_dbpedia = FeatureCategory('dbpedia', features_dbpedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features_dbpedia(datasets, aggs = agg_default, drop_features = []):\n",
    "    return [FeatureDataset(ds.name, fc_dbpedia, agg,\n",
    "                        fc_dbpedia.func(ds.train, agg.agg, drop_features), \n",
    "                        fc_dbpedia.func(ds.test, agg.agg, drop_features)) \n",
    "                        for ds in datasets\n",
    "                        for agg in aggs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3.5.3) Brown Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_cluster_word2cluster = {}\n",
    "brown_cluster_cluster2words = defaultdict(list)\n",
    "with open(\"resources/brown-clustering/paths/rcv1.clean-c6000-p1.paths\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        binary_cluster, word, _ = line.split()\n",
    "        brown_cluster_word2cluster[word] = binary_cluster\n",
    "        brown_cluster_cluster2words[binary_cluster].append(word)\n",
    "\n",
    "def brown_clustering_cluster_size(target):\n",
    "    cluster = brown_cluster_word2cluster.get(target)\n",
    "    return len(brown_cluster_cluster2words[cluster]) if cluster else 0\n",
    "\n",
    "def brown_clustering_cluster_depth_simple(target):\n",
    "    cluster = brown_cluster_word2cluster.get(target)\n",
    "    return int(cluster, 2) if cluster else 0\n",
    "\n",
    "def brown_clustering_cluster_depth_bit(target):\n",
    "    cluster = brown_cluster_word2cluster.get(target)\n",
    "    if not cluster:\n",
    "        return 8.75\n",
    "    return np.sum([1 for bit in cluster if bit == '1'])\n",
    "\n",
    "def brown_clustering_cluster_size_all(target):\n",
    "    cluster = brown_cluster_word2cluster.get(target)\n",
    "    if not cluster:\n",
    "        return 0\n",
    "    upper_clusters = [cluster[0:(len(cluster) - index)] + '0' * index \\\n",
    "         for index, bit in enumerate(reversed(cluster)) if bit == '1']\n",
    "    cluster_counts = [len(brown_cluster_cluster2words.get(clu, [])) \\\n",
    "                         for clu in upper_clusters]\n",
    "    return np.sum(cluster_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_brown_clustering(dataframe, agg, drop_features):\n",
    "    df = dataframe.copy()\n",
    "    df['brown_clustering_cluster_size (sem)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                                            brown_clustering_cluster_size))\n",
    "    df['brown_clustering_cluster_size_all (sem)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                                            brown_clustering_cluster_size_all))\n",
    "    df['brown_clustering_cluster_depth_simple (sem)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                                            brown_clustering_cluster_depth_simple))\n",
    "    df['brown_clustering_cluster_depth_bit (sem)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                                            brown_clustering_cluster_depth_bit))\n",
    "    df = df.drop(drop_features, axis = 1)\n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "fc_brown_clustering = FeatureCategory('brown_clustering', features_brown_clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features_brown_clustering(datasets, aggs = agg_default, drop_features = []):\n",
    "    return [FeatureDataset(ds.name, fc_brown_clustering, agg,\n",
    "                        fc_brown_clustering.func(ds.train, agg.agg, drop_features), \n",
    "                        fc_brown_clustering.func(ds.test, agg.agg, drop_features)) \n",
    "                        for ds in datasets\n",
    "                        for agg in aggs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_semantic = FeatureCategory('semantic', [fc_wordnet, fc_dbpedia, fc_brown_clustering])\n",
    "\n",
    "def compute_features_semantic(datasets):\n",
    "    return [FeatureDataset(ds.name, fc_semantic,  ds.agg,\n",
    "            ds.train, ds.test) for ds in concat_feature_datasets(*datasets)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3.6) Dictionary Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textatistic\n",
    "from collections import Counter\n",
    "\n",
    "academic_words = {}\n",
    "with open(\"resources/dictionaries/academic_word_list.txt\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        word, rank = line.split()\n",
    "        academic_words[word.strip()] = rank\n",
    "\n",
    "prefixes = {}\n",
    "with open(\"resources/dictionaries/prefixes.txt\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        prefix, definition, examples = line.split('\\t')\n",
    "        prefixes[prefix.replace('-', '').strip()] = definition\n",
    "\n",
    "suffixes = {}\n",
    "with open(\"resources/dictionaries/suffixes.txt\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        suffix, definition, examples = line.split('\\t')\n",
    "        suffixes[suffix.replace('-', '').strip()] = definition\n",
    "\n",
    "with open(\"resources/dictionaries/biology_glossary.csv\", encoding=\"utf8\") as file:\n",
    "    content = [line.strip().lower() for line in file.readlines()]\n",
    "    gloss_biology = set(content)\n",
    "\n",
    "with open(\"resources/dictionaries/geography_glossary.csv\", encoding=\"utf8\") as file:\n",
    "    content = [line.strip().lower() for line in file.readlines()]\n",
    "    gloss_geography = set(content)\n",
    "    \n",
    "with open(\"resources/dictionaries/physics_glossary.csv\", encoding=\"utf8\") as file:\n",
    "    content = [line.strip().lower() for line in file.readlines()]\n",
    "    gloss_physics = set(content)\n",
    "    \n",
    "with open(\"resources/dictionaries/stopwords_en.txt\", encoding=\"utf8\") as file:\n",
    "    content = [line.strip().lower() for line in file.readlines()]\n",
    "    stop_words = set(content)\n",
    "    \n",
    "with open(\"resources/dictionaries/most_freq_used_3000_words.txt\", encoding=\"utf8\") as file:\n",
    "    content = [line.strip().lower() for line in file.readlines()]\n",
    "    most_freq_used_3000_words = set(content)\n",
    "    \n",
    "with open(\"resources/dictionaries/most_freq_used_5000_words.txt\", encoding=\"utf8\") as file:\n",
    "    content = [line.split()[1].strip().lower() for line in file.readlines()]\n",
    "    most_freq_used_5000_words = set(content)\n",
    "    \n",
    "\n",
    "'''\n",
    "Extract all words that are exactly identified as either complex\n",
    "or non-complex and use this as the vocabulary. Words that occur\n",
    "as both complex and non-complex are neglected for the vocabulary\n",
    "'''\n",
    "def build_clean_vocabulary(train):\n",
    "    targets_complex = set([mwe.strip().lower() for mwe in \n",
    "                train.loc[train['binary'] == 1,]['target'].tolist()])\n",
    "    targets_non_complex = set([mwe.strip().lower() for mwe in\n",
    "                train.loc[train['binary'] == 0,]['target'].tolist()])\n",
    "    targets_complex_cleaned = list(targets_complex.difference(targets_non_complex))\n",
    "    targets_non_complex_cleaned = list(targets_non_complex.difference(targets_complex))\n",
    "    vocabulary = {}\n",
    "    for target in targets_complex_cleaned:\n",
    "        vocabulary[target] = 1\n",
    "    for target in targets_non_complex_cleaned:\n",
    "        vocabulary[target] = 0\n",
    "    return vocabulary\n",
    "\n",
    "'''\n",
    "Extract all words that are identified as either complex\n",
    "or non-complex and use this as the vocabulary. Words that occur\n",
    "as both complex and non-complex are weighted based on the number\n",
    "of occurrences. If the word has been tagged more times as non-complex\n",
    "we save it as non-complex otherwise it is complex\n",
    "'''\n",
    "def build_weighted_vocabulary(train):\n",
    "    targets_complex = [mwe.strip().lower() for mwe in \n",
    "                train.loc[train['binary'] == 1,]['target'].tolist()]\n",
    "    targets_non_complex = [mwe.strip().lower() for mwe in\n",
    "                train.loc[train['binary'] == 0,]['target'].tolist()]\n",
    "    counts_complex = Counter(targets_complex)\n",
    "    counts_non_complex = Counter(targets_non_complex)\n",
    "    vocabulary = {}\n",
    "    for word, count in counts_complex.items():\n",
    "        count_nc = counts_non_complex.get(word, None)\n",
    "        if count_nc and count_nc > count:\n",
    "            vocabulary[word] = 0\n",
    "        else:\n",
    "            vocabulary[word] = 1\n",
    "    for word, count in counts_non_complex.items():\n",
    "        if word not in vocabulary:\n",
    "            vocabulary[word] = 0\n",
    "    return vocabulary\n",
    "\n",
    "def build_confidence_vocabulary_1(train, confidence):\n",
    "    targets_complex = [mwe.strip().lower() for mwe in \n",
    "                train.loc[train['prob'] >= confidence,]['target'].tolist()]\n",
    "    targets_non_complex = [mwe.strip().lower() for mwe in \n",
    "                train.loc[train['prob'] < confidence,]['target'].tolist()]\n",
    "    counts_complex = Counter(targets_complex)\n",
    "    counts_non_complex = Counter(targets_non_complex)\n",
    "    vocabulary = {}\n",
    "    for word, count in counts_complex.items():\n",
    "        count_nc = counts_non_complex.get(word, None)\n",
    "        if count_nc and count_nc > count:\n",
    "            vocabulary[word] = 0\n",
    "        else:\n",
    "            vocabulary[word] = 1\n",
    "    for word, count in counts_non_complex.items():\n",
    "        if word not in vocabulary:\n",
    "            vocabulary[word] = 0\n",
    "    return vocabulary\n",
    "\n",
    "def build_confidence_vocabulary_2(train, confidence):\n",
    "    targets_complex = [mwe.strip().lower() for mwe in \n",
    "                train.loc[train['prob'] >= confidence,]['target'].tolist()]\n",
    "    targets_non_complex = [mwe.strip().lower() for mwe in \n",
    "                train.loc[train['binary'] == 0,]['target'].tolist()]\n",
    "    counts_complex = Counter(targets_complex)\n",
    "    counts_non_complex = Counter(targets_non_complex)\n",
    "    vocabulary = {}\n",
    "    for word, count in counts_complex.items():\n",
    "        count_nc = counts_non_complex.get(word, None)\n",
    "        if count_nc and count_nc > count:\n",
    "            vocabulary[word] = 0\n",
    "        else:\n",
    "            vocabulary[word] = 1\n",
    "    for word, count in counts_non_complex.items():\n",
    "        if word not in vocabulary:\n",
    "            vocabulary[word] = 0\n",
    "    return vocabulary\n",
    "\n",
    "def build_confidence_vocabulary_mean(train):\n",
    "    train['target'] = train.target.apply(lambda target : target.strip().lower())\n",
    "    agg = train[['target', 'prob']].groupby('target',\n",
    "                        as_index=False).mean().values\n",
    "    tuples = [tuple(val) for val in agg]\n",
    "    vocabulary = {}\n",
    "    for target, confidence in tuples:\n",
    "        vocabulary[target] = confidence\n",
    "    return vocabulary\n",
    "\n",
    "def build_confidence_vocabulary_max(train):\n",
    "    train['target'] = train.target.apply(lambda target : target.strip().lower())\n",
    "    agg = train[['target', 'prob']].groupby('target',\n",
    "                        as_index=False).max().values\n",
    "    tuples = [tuple(val) for val in agg]\n",
    "    vocabulary = {}\n",
    "    for target, confidence in tuples:\n",
    "        vocabulary[target] = confidence\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_dictionary(dataframe, agg, drop_features):\n",
    "    df = dataframe.copy()\n",
    "    df['dict_dale_chall (dic)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                            lambda target :  0 if textatistic.notdalechall_count(target) >= 1 else 1))\n",
    "    df['dict_570_academic_words (dic)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                                    lambda target : int(target in academic_words)))\n",
    "    df['common_prefix (dic)'] = df.p_target.apply(lambda target : int(np.sum([1 for prefix in prefixes if target.startswith(prefix)]) > 0))\n",
    "    df['common_suffix (dic)'] = df.p_target.apply(lambda target : int(np.sum([1 for suffix in suffixes if target.endswith(suffix)]) > 0))\n",
    "    df['gloss_biology (dic)'] = df.p_target.apply(lambda target : int(target in gloss_biology))\n",
    "    df['gloss_physics (dic)'] = df.p_target.apply(lambda target : int(target in gloss_physics))\n",
    "    df['gloss_geography (dic)'] = df.p_target.apply(lambda target : int(target in gloss_geography))\n",
    "    df['stop_word (dic)'] = df.p_target.apply(lambda target : int(target in stop_words))\n",
    "    df['most_freq_used_3000_words (dic)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                                    lambda target : int(target in most_freq_used_3000_words)))\n",
    "    df['most_freq_used_5000_words (dic)'] = df.p_target.apply(lambda target : agg(target, \\\n",
    "                                                    lambda target : int(target in most_freq_used_5000_words)))\n",
    "    df = df.drop(drop_features, axis = 1)\n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "fc_dictionary = FeatureCategory('dictionary', features_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features_dictionary(datasets, aggs = agg_default, drop_features = []):\n",
    "    return [FeatureDataset(ds.name, fc_dictionary, agg,\n",
    "                        fc_dictionary.func(ds.train, agg.agg, drop_features), \n",
    "                        fc_dictionary.func(ds.test, agg.agg, drop_features)) \n",
    "                        for ds in datasets\n",
    "                        for agg in aggs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Classification Models\n",
    "Here we compute individual feature importance based on different metrics. For example, we implement and compute the F-Score, providing an idea of the discrimination power the feature has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Result = namedtuple('Result', 'dataset, fc, agg, measure')\n",
    "Dataset = namedtuple('Dataset', 'name, train, test')\n",
    "FeatureDataset = namedtuple('FeatureDataset', 'name, fc, agg, train, test')\n",
    "FeatureCategory = namedtuple('FeatureCategory', 'name, func')\n",
    "Feature = namedtuple('Feature', 'name, fc_name, train, test')\n",
    "Metric = namedtuple('Metric', 'name, func')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4.1) Utility Functions\n",
    "Here we provide several utility functions for working with the datasets and classification algorithms. For example, we provide functions to clean the datasets from all non-features (such as id, sentence, the annotator information etc.) and functions to transform the feature datasets into a proper representation for the algorithms (such as one-hot-encoding of categorical attributes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_labels_for_binary_df(dataframe, drop=[]):\n",
    "    drop_list = ['id', 'sentence', 'target', 'nat', 'non_nat', \n",
    "                  'nat_marked', 'non_nat_marked', 'prob', 'start', \n",
    "                  'end', 'p_target', 'lemma', 'p_lemma', 'pos_tags', 'pos_tags_pt']\n",
    "    drop_list.extend(drop)\n",
    "    df = dataframe.copy()\n",
    "    df = df.drop(drop_list, axis = 1)\n",
    "    return df\n",
    "\n",
    "def remove_labels_phrase_for_binary_df(dataframe, drop=[]):\n",
    "    drop_list = ['id', 'sentence', 'target', 'nat', 'non_nat', \n",
    "                  'nat_marked', 'non_nat_marked', 'prob', 'start', \n",
    "                  'end', 'p_target', 'lemma', 'p_lemma', 'pos_tags', 'pos_tags_pt', 'phrase_index']\n",
    "    drop_list.extend(drop)\n",
    "    df = dataframe.copy()\n",
    "    df = df.drop(drop_list, axis = 1)\n",
    "    return df\n",
    "\n",
    "def remove_labels_for_regr_df(dataframe, drop=[]):\n",
    "    drop_list = ['id', 'sentence', 'target', 'nat', 'non_nat', \n",
    "                  'nat_marked', 'non_nat_marked', 'binary', 'start', \n",
    "                  'end', 'p_target', 'lemma', 'p_lemma', 'pos_tags', 'pos_tags_pt']\n",
    "    drop_list.extend(drop)\n",
    "    df = dataframe.copy()\n",
    "    df = df.drop(drop_list, axis = 1)\n",
    "    return df\n",
    "    \n",
    "def transform_feat_to_num(train, test):\n",
    "    train_copy = train.copy()\n",
    "    test_copy = test.copy()\n",
    "    train_copy = train_copy.replace([np.inf, -np.inf], np.nan)\n",
    "    train_copy = train_copy.fillna(0)\n",
    "    test_copy = test_copy.replace([np.inf, -np.inf], np.nan)\n",
    "    test_copy = test_copy.fillna(0)\n",
    "    shape_train = train.shape\n",
    "    shape_test = test.shape\n",
    "    df = train_copy.append(test_copy, ignore_index=True)\n",
    "    df = pd.get_dummies(df)\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df = df.fillna(0)\n",
    "    df = df.applymap(lambda x: 1 if x == True else x)\n",
    "    df = df.applymap(lambda x: 0 if x == False else x)\n",
    "    return (df.loc[0:(shape_train[0]-1),], \n",
    "            df.loc[shape_train[0]:df.shape[0],])\n",
    "\n",
    "def prep_data(train, test):\n",
    "    x_train = train.loc[:, train.columns != 'binary']\n",
    "    y_train = train['binary'].values\n",
    "    x_test = test.loc[:, test.columns != 'binary']\n",
    "    y_test = test.binary.values\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def create_eval_df_from_results(results, remove_agg=True):\n",
    "    if remove_agg:\n",
    "        evaluation = [{'dataset' : result.dataset.name,\n",
    "                        'zc' : result.fc[0], 'prec' : result.measure[0][1],\n",
    "                   'rec' : result.measure[1][1], 'f1' : result.measure[2][1]} \n",
    "                       for result in results]\n",
    "    else:\n",
    "        evaluation = [{'dataset' : result.dataset.name, 'agg' : result.agg[0],\n",
    "                        'zc' : result.fc[0], 'prec' : result.measure[0][1],\n",
    "                   'rec' : result.measure[1][1], 'f1' : result.measure[2][1]} \n",
    "                       for result in results]\n",
    "    return pd.DataFrame.from_records(evaluation)\n",
    "\n",
    "def create_eval_df_from_results_macro(results, remove_agg=True):\n",
    "    if remove_agg:\n",
    "        evaluation = [{'dataset' : result.dataset.name,\n",
    "                        'zc' : result.fc[0], 'prec' : result.measure[0],\n",
    "                   'rec' : result.measure[1], 'f1' : result.measure[2]} \n",
    "                       for result in results]\n",
    "    else:\n",
    "        evaluation = [{'dataset' : result.dataset.name, 'agg' : result.agg[0],\n",
    "                        'zc' : result.fc[0], 'prec' : result.measure[0],\n",
    "                   'rec' : result.measure[1], 'f1' : result.measure[2]} \n",
    "                       for result in results]\n",
    "    return pd.DataFrame.from_records(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4.2.1) Baseline Always Complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def always_complex_prediction(train, test):\n",
    "    y_test = test.binary.values\n",
    "    prediction = [1 for val in y_test]\n",
    "    f1score = precision_recall_fscore_support(y_test, prediction, average='macro')\n",
    "    return f1score\n",
    "\n",
    "def baseline_always_complex(dataset):\n",
    "    results = [Result(ds.name, 'always_complex', agg_default[0],\n",
    "        always_complex_prediction(remove_labels_for_binary_df(ds.train), \n",
    "            remove_labels_for_binary_df(ds.test))) for ds in datasets]\n",
    "    evaluation = [{'dataset' : result.dataset, \n",
    "                        'zc' : result.fc, 'prec' : result.measure[0],\n",
    "                  'rec' : result.measure[1], 'f1' : result.measure[2]} \n",
    "                       for result in results]\n",
    "    counts = [(ds.name, Counter(ds.test.binary)) for ds in datasets]\n",
    "    return pd.DataFrame.from_records(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4.3.2) Baseline Memorize Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "'''\n",
    "Extract all words that are exactly identified as either complex\n",
    "or non-complex and use this as the vocabulary. Words that occur\n",
    "as both complex and non-complex are neglected for the vocabulary\n",
    "'''\n",
    "def build_clean_vocabulary(train):\n",
    "    targets_complex = set([mwe.strip().lower() for mwe in \n",
    "                train.loc[train['binary'] == 1,]['target'].tolist()])\n",
    "    targets_non_complex = set([mwe.strip().lower() for mwe in\n",
    "                train.loc[train['binary'] == 0,]['target'].tolist()])\n",
    "    targets_complex_cleaned = list(targets_complex.difference(targets_non_complex))\n",
    "    targets_non_complex_cleaned = list(targets_non_complex.difference(targets_complex))\n",
    "    vocabulary = {}\n",
    "    for target in targets_complex_cleaned:\n",
    "        vocabulary[target] = 1\n",
    "    for target in targets_non_complex_cleaned:\n",
    "        vocabulary[target] = 0\n",
    "    return vocabulary\n",
    "\n",
    "'''\n",
    "Extract all words that are identified as either complex\n",
    "or non-complex and use this as the vocabulary. Words that occur\n",
    "as both complex and non-complex are weighted based on the number\n",
    "of occurrences. If the word has been tagged more times as non-complex\n",
    "we save it as non-complex otherwise it is complex\n",
    "'''\n",
    "def build_weighted_vocabulary(train):\n",
    "    targets_complex = [mwe.strip().lower() for mwe in \n",
    "                train.loc[train['binary'] == 1,]['target'].tolist()]\n",
    "    targets_non_complex = [mwe.strip().lower() for mwe in\n",
    "                train.loc[train['binary'] == 0,]['target'].tolist()]\n",
    "    counts_complex = Counter(targets_complex)\n",
    "    counts_non_complex = Counter(targets_non_complex)\n",
    "    vocabulary = {}\n",
    "    for word, count in counts_complex.items():\n",
    "        count_nc = counts_non_complex.get(word, None)\n",
    "        if count_nc and count_nc > count:\n",
    "            vocabulary[word] = 0\n",
    "        else:\n",
    "            vocabulary[word] = 1\n",
    "    for word, count in counts_non_complex.items():\n",
    "        if word not in vocabulary:\n",
    "            vocabulary[word] = 0\n",
    "    return vocabulary\n",
    "\n",
    "def build_confidence_vocabulary_1(train, confidence):\n",
    "    targets_complex = [mwe.strip().lower() for mwe in \n",
    "                train.loc[train['prob'] >= confidence,]['target'].tolist()]\n",
    "    targets_non_complex = [mwe.strip().lower() for mwe in \n",
    "                train.loc[train['prob'] < confidence,]['target'].tolist()]\n",
    "    counts_complex = Counter(targets_complex)\n",
    "    counts_non_complex = Counter(targets_non_complex)\n",
    "    vocabulary = {}\n",
    "    for word, count in counts_complex.items():\n",
    "        count_nc = counts_non_complex.get(word, None)\n",
    "        if count_nc and count_nc > count:\n",
    "            vocabulary[word] = 0\n",
    "        else:\n",
    "            vocabulary[word] = 1\n",
    "    for word, count in counts_non_complex.items():\n",
    "        if word not in vocabulary:\n",
    "            vocabulary[word] = 0\n",
    "    return vocabulary\n",
    "\n",
    "def build_confidence_vocabulary_2(train, confidence):\n",
    "    targets_complex = [mwe.strip().lower() for mwe in \n",
    "                train.loc[train['prob'] >= confidence,]['target'].tolist()]\n",
    "    targets_non_complex = [mwe.strip().lower() for mwe in \n",
    "                train.loc[train['binary'] == 0,]['target'].tolist()]\n",
    "    counts_complex = Counter(targets_complex)\n",
    "    counts_non_complex = Counter(targets_non_complex)\n",
    "    vocabulary = {}\n",
    "    for word, count in counts_complex.items():\n",
    "        count_nc = counts_non_complex.get(word, None)\n",
    "        if count_nc and count_nc > count:\n",
    "            vocabulary[word] = 0\n",
    "        else:\n",
    "            vocabulary[word] = 1\n",
    "    for word, count in counts_non_complex.items():\n",
    "        if word not in vocabulary:\n",
    "            vocabulary[word] = 0\n",
    "    return vocabulary\n",
    "\n",
    "def build_confidence_vocabulary_mean(train):\n",
    "    train['target'] = train.target.apply(lambda target : target.strip().lower())\n",
    "    agg = train[['target', 'prob']].groupby('target',\n",
    "                        as_index=False).mean().values\n",
    "    tuples = [tuple(val) for val in agg]\n",
    "    vocabulary = {}\n",
    "    for target, confidence in tuples:\n",
    "        vocabulary[target] = confidence\n",
    "    return vocabulary\n",
    "\n",
    "def build_confidence_vocabulary_max(train):\n",
    "    train['target'] = train.target.apply(lambda target : target.strip().lower())\n",
    "    agg = train[['target', 'prob']].groupby('target',\n",
    "                        as_index=False).max().values\n",
    "    tuples = [tuple(val) for val in agg]\n",
    "    vocabulary = {}\n",
    "    for target, confidence in tuples:\n",
    "        vocabulary[target] = confidence\n",
    "    return vocabulary\n",
    "    \n",
    "\n",
    "def evaluate_label_target_predictions(test, vocabulary):\n",
    "    dict_test = list(zip(test.target, test.binary))\n",
    "    data = [(binary, (vocabulary[target.strip().lower()] if target.strip().lower() in vocabulary else 1)) \n",
    "            for target, binary in dict_test]\n",
    "    y_true = [vals[0] for vals in data]\n",
    "    prediction = [vals[1] for vals in data]\n",
    "    return precision_recall_fscore_support(y_true, prediction, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_vocab_clean(datasets):\n",
    "    evaluation_clean = [Result(ds.name, 'vocab_clean', agg_default[0], \n",
    "                    evaluate_label_target_predictions(ds.test, \n",
    "                    build_clean_vocabulary(ds.train))) for ds in datasets]\n",
    "    results_clean = [{'dataset' : result.dataset, \n",
    "                        'zc' : result.fc, 'prec' : result.measure[0],\n",
    "                  'rec' : result.measure[1], 'f1' : result.measure[2]} \n",
    "                       for result in evaluation_clean]\n",
    "    return pd.DataFrame.from_records(results_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_vocab_weighted(datasets):\n",
    "    evaluation_weighted = [Result(ds.name, 'vocab_weighted', agg_default[0], \n",
    "                        evaluate_label_target_predictions(ds.test, \n",
    "                    build_weighted_vocabulary(ds.train))) for ds in datasets]\n",
    "    results_weighted = [{'dataset' : result.dataset, \n",
    "                        'zc' : result.fc, 'prec' : result.measure[0],\n",
    "                  'rec' : result.measure[1], 'f1' : result.measure[2]} \n",
    "                       for result in evaluation_weighted]\n",
    "    return pd.DataFrame.from_records(results_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_vocab_conf(datasets, confidence = 0.5):\n",
    "    evaluation_conf = [Result(ds.name, 'vocab_conf', agg_default[0], \n",
    "                            evaluate_label_target_predictions(ds.test, \n",
    "                        build_confidence_vocabulary_2(ds.train, confidence))) for ds in datasets]\n",
    "    results_conf = [{'dataset' : result.dataset, \n",
    "                        'zc' : result.fc, 'prec' : result.measure[0],\n",
    "                  'rec' : result.measure[1], 'f1' : result.measure[2]} \n",
    "                       for result in evaluation_conf]\n",
    "    return pd.DataFrame.from_records(results_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn import model_selection\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "\n",
    "def xgboost(train, test):\n",
    "    x_train, y_train, x_test, y_test = prep_data(train, test)\n",
    "    xgtrain = xgb.DMatrix(x_train.values, label=y_train)\n",
    "    xgtest = xgb.DMatrix(x_test.values, label=y_test)\n",
    "    xg_test_x = xgb.DMatrix(x_test.values)\n",
    "    param = {'max_depth': 30, 'eta': 1, 'silent': 1, \\\n",
    "             'objective': 'binary:logistic',  'n_estimators':5000}\n",
    "    evallist = [(xgtest, 'eval'), (xgtrain, 'train')]\n",
    "    num_round = 70\n",
    "    bst = xgb.train(param, xgtrain, num_round, evallist)\n",
    "    prediction = bst.predict(xg_test_x)\n",
    "    prediction_binary = list(map(lambda val: 1 if val>0.5 else 0, prediction))\n",
    "    return y_test, prediction_binary\n",
    "\n",
    "def adaboost(train, test):\n",
    "    x_train, y_train, x_test, y_test = prep_data(train, test)\n",
    "    adab = AdaBoostClassifier(base_estimator=None, n_estimators=5000, \n",
    "                          learning_rate=1.0, algorithm='SAMME.R',\n",
    "                          random_state=None)\n",
    "    adab.fit(x_train, y_train) \n",
    "    prediction = adab.predict(x_test)\n",
    "    return y_test, prediction\n",
    "\n",
    "def random_forest(train, test):\n",
    "    x_train, y_train, x_test, y_test = prep_data(train, test)\n",
    "    x_train = x_train.values.astype(np.float)\n",
    "    x_test = x_test.values.astype(np.float)\n",
    "    clf = RandomForestClassifier(max_depth=10, random_state=14521, n_estimators=1800, \\\n",
    "                    verbose=1, min_samples_split=5, min_samples_leaf=4, bootstrap=False)\n",
    "    clf.fit(x_train, y_train)\n",
    "    prediction = clf.predict(x_test)\n",
    "    prediction_binary = list(map(lambda val: 1 if val>0.5 else 0, prediction))\n",
    "    return y_test, prediction_binary\n",
    "\n",
    "def random_forest_extra(train, test):\n",
    "    x_train, y_train, x_test, y_test = prep_data(train, test)\n",
    "    x_train = x_train.values.astype(np.float)\n",
    "    x_test = x_test.values.astype(np.float)\n",
    "    clf = ExtraTreesClassifier(n_estimators=1800, criterion='gini', max_depth=None,\n",
    "                     min_samples_split=5, min_samples_leaf=4, min_weight_fraction_leaf=0.0,\n",
    "                     max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                     min_impurity_split=None, bootstrap=False, oob_score=False,\n",
    "                     random_state=15325, verbose=0, warm_start=False)\n",
    "    clf.fit(x_train, y_train)\n",
    "    prediction = clf.predict(x_test)\n",
    "    prediction_binary = list(map(lambda val: 1 if val>0.5 else 0, prediction))\n",
    "    return y_test, prediction_binary\n",
    "\n",
    "def svm(train, test):\n",
    "    x_train, y_train, x_test, y_test = prep_data(train, test)\n",
    "    seed = 7\n",
    "    svc = SVC(C=10, kernel='rbf', degree=3, gamma='auto', \n",
    "            coef0=0.0, shrinking=True, probability=False, tol=0.001, \n",
    "            cache_size=200, class_weight=None, verbose=False, max_iter=-1, \n",
    "            decision_function_shape='ovr', random_state=None)\n",
    "    svc.fit(x_train, y_train) \n",
    "    prediction = svc.predict(x_test)\n",
    "    f1score = f1_score(y_test, prediction)\n",
    "    return y_test, prediction\n",
    "\n",
    "\n",
    "def mlp(train, test):\n",
    "    x_train, y_train, x_test, y_test = prep_data(train, test)\n",
    "    mlp = MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto',\n",
    "          beta_1=0.9, beta_2=0.999, early_stopping=False,\n",
    "          epsilon=1e-08, hidden_layer_sizes=(50, 20), learning_rate='constant',\n",
    "          learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
    "          nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
    "          solver='adam', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
    "          warm_start=False)\n",
    "    mlp.fit(x_train, y_train) \n",
    "    prediction = mlp.predict(x_test)\n",
    "    return y_test, prediction\n",
    "\n",
    "def decision_tree(train, test):\n",
    "    x_train, y_train, x_test, y_test = prep_data(train, test)\n",
    "    seed = 7\n",
    "    dt = DecisionTreeClassifier(criterion='gini', splitter='best', \n",
    "                                 max_depth=None, min_samples_split=2, \n",
    "                                 min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "                                 max_features=None, random_state=None, max_leaf_nodes=None, \n",
    "                                 min_impurity_decrease=0.0, min_impurity_split=None, \n",
    "                                 class_weight=None, presort=False)\n",
    "    dt.fit(x_train, y_train) \n",
    "    prediction = dt.predict(x_test)\n",
    "    return y_test, prediction\n",
    "\n",
    "\n",
    "def knn(train, test):\n",
    "    x_train, y_train, x_test, y_test = prep_data(train, test)\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', \n",
    "                     leaf_size=30, p=2, metric='minkowski')\n",
    "    knn.fit(x_train, y_train) \n",
    "    prediction = knn.predict(x_test)\n",
    "    return y_test, prediction\n",
    "\n",
    "def nn(train, test):\n",
    "    x_train, y_train, x_test, y_test = prep_data(train, test)\n",
    "    clf = NearestCentroid()\n",
    "    clf.fit(x_train, y_train)\n",
    "    prediction = clf.predict(x_test)\n",
    "    return y_test, prediction\n",
    "\n",
    "def naive_bayes(train, test):\n",
    "    x_train, y_train, x_test, y_test = prep_data(train, test)\n",
    "    naive_bayes = GaussianNB(priors=None)\n",
    "    naive_bayes.fit(x_train, y_train) \n",
    "    prediction = naive_bayes.predict(x_test)\n",
    "    return y_test, prediction\n",
    "\n",
    "def logistic_regression(train, test):\n",
    "    x_train, y_train, x_test, y_test = prep_data(train, test)\n",
    "    logistic_regression = LogisticRegression(penalty='l2', dual=False, tol=0.0001,\n",
    "                                     C=1.0, fit_intercept=True, intercept_scaling=1, \n",
    "                                     class_weight=None, random_state=None, solver='lbfgs',\n",
    "                                     max_iter=100, verbose=0, \n",
    "                                     warm_start=False)\n",
    "    logistic_regression.fit(x_train, y_train) \n",
    "    prediction = logistic_regression.predict(x_test)\n",
    "    return y_test, prediction\n",
    "\n",
    "\n",
    "def xgboost_with_bst(train, test, silent):\n",
    "    x_train, y_train, x_test, y_test = prep_data(train, test)\n",
    "    xgtrain = xgb.DMatrix(x_train.values, label=y_train, feature_names=x_train.columns.values)\n",
    "    xgtest = xgb.DMatrix(x_test.values, label=y_test, feature_names=x_test.columns.values)\n",
    "    xg_test_x = xgb.DMatrix(x_test.values, feature_names=x_test.columns.values)\n",
    "    param = {'max_depth': 30, 'eta': 1, 'silent': silent, 'objective': 'binary:logistic',  'n_estimators':5000}\n",
    "    evallist = [(xgtest, 'eval'), (xgtrain, 'train')]\n",
    "    num_round = 70\n",
    "    bst = xgb.train(param, xgtrain, num_round, evallist)\n",
    "    prediction = bst.predict(xg_test_x)\n",
    "    prediction_binary = list(map(lambda val: 1 if val>0.5 else 0, prediction))\n",
    "    f1score = precision_recall_fscore_support(y_test, prediction_binary)\n",
    "    return f1score, bst\n",
    "\n",
    "def random_forest_with_forest(train, test, label):\n",
    "    x_train, y_train, x_test, y_test = prep_data(train, test)\n",
    "    x_train = x_train.as_matrix().astype(np.float)\n",
    "    x_test = x_test.as_matrix().astype(np.float)\n",
    "    clf = RandomForestClassifier(max_depth=10, random_state=0, n_estimators=1800, \\\n",
    "                            verbose=1, min_samples_split=5, min_samples_leaf=4, bootstrap=False)\n",
    "    clf.fit(x_train, y_train)\n",
    "    prediction = clf.predict(x_test)\n",
    "    prediction_binary = list(map(lambda val: 1 if val>0.5 else 0, prediction))\n",
    "    f1score = precision_recall_fscore_support(y_test, prediction_binary)\n",
    "    return f1score, clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "def get_upsample_count(dataframe):\n",
    "    return dataframe.groupby('binary').size()[1]\n",
    "\n",
    "def balance_dataframe(dataframe):\n",
    "    df = dataframe.copy()\n",
    "    df_majority = df[df.binary==1]\n",
    "    df_minority = df[df.binary==0]\n",
    "    count = get_upsample_count(df)\n",
    "    df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,    \n",
    "                                 n_samples=count,\n",
    "                                 random_state=721) \n",
    "    return pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "def balance_phrase_label_dist(datasets):\n",
    "    return [FeatureDataset(ds.name, ds.fc, ds.agg, \n",
    "                    balance_dataframe(ds.train), ds.test) for ds in datasets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X.x Complex Phrase Identifcation ML Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.1 A1 Prediction Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we conduct the experiments for A1 word prediction aggregation to obtain the complexity of a phrase. We predict the complexity of each word of the phrase individually, and aggregate the predictions with three different simple A1 aggregation functions (min, max, majority voting). We train our models on the single words training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_starts(start, target):\n",
    "    curr_start = start\n",
    "    starts = [curr_start]\n",
    "    tokens = target.split()\n",
    "    for token in tokens:\n",
    "        curr_start = curr_start + len(token) + 1\n",
    "        starts.append(curr_start)\n",
    "    return ' '.join([str(start) for start in starts[:len(tokens)]])\n",
    "\n",
    "def compute_ends(start, target):\n",
    "    curr_start = start\n",
    "    ends = []\n",
    "    tokens = target.split()\n",
    "    for index, token in enumerate(tokens):\n",
    "        if index > 0:\n",
    "            curr_start = curr_start + len(token) + 1\n",
    "        else:\n",
    "            curr_start = curr_start + len(token)\n",
    "        ends.append(curr_start)\n",
    "    return ' '.join([str(end) for end in ends])\n",
    "\n",
    "def phrase_splitter(dataframe):\n",
    "    df = dataframe.copy()\n",
    "    df['starts'] = df[['start', 'target']].apply(lambda vals : compute_starts(*vals), axis=1)\n",
    "    df['ends'] = df[['start', 'target']].apply(lambda vals : compute_ends(*vals), axis=1)\n",
    "    df['phrase_index'] = df.apply(lambda x : x.name, axis=1)\n",
    "    s1 = df.target.str.split(' ', expand=True).stack().str.strip().reset_index(level=1, drop=True)\n",
    "    s2 = df.p_target.str.split(' ', expand=True).stack().str.strip().reset_index(level=1, drop=True)\n",
    "    s3 = df.starts.str.split(' ', expand=True).stack().str.strip().reset_index(level=1, drop=True)\n",
    "    s4 = df.ends.str.split(' ', expand=True).stack().str.strip().reset_index(level=1, drop=True)\n",
    "    s5 = df.p_lemma.str.split(' ', expand=True).stack().str.strip().reset_index(level=1, drop=True)\n",
    "    s6 = df.lemma.str.split(' ', expand=True).stack().str.strip().reset_index(level=1, drop=True)\n",
    "    df['pos_tags'] = df.pos_tags.apply(lambda tags : ' '.join(tags))\n",
    "    s7 = df.pos_tags.str.split(' ', expand=True).stack().str.strip().reset_index(level=1, drop=True)\n",
    "    df['pos_tags_pt'] = df.pos_tags_pt.apply(lambda tags : ' '.join(tags))\n",
    "    s8 = df.pos_tags_pt.str.split(' ', expand=True).stack().str.strip().reset_index(level=1, drop=True)\n",
    "    df1 = pd.concat([s1, s2, s3, s4, s5, s6, s7, s8], axis=1, keys=['target','p_target', \\\n",
    "                                        'start', 'end', 'p_lemma', 'lemma','pos_tags', 'pos_tags_pt'])\n",
    "    splitted_df = df.drop(['target', 'p_target', 'starts', \\\n",
    "                'start', 'ends', 'end', 'p_lemma','lemma', \n",
    "                        'pos_tags', 'pos_tags_pt'], axis=1).join(df1).reset_index(drop=True)\n",
    "    splitted_df['start'] = pd.to_numeric(splitted_df.start, errors='coerce')\n",
    "    splitted_df['end'] = pd.to_numeric(splitted_df.end, errors='coerce')\n",
    "    return splitted_df\n",
    "\n",
    "def phrase_splitting_datasets(datasets):\n",
    "    return [Dataset(ds.name, ds.train, phrase_splitter(ds.test)) for ds in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_prediction_agg_majority_vote(predictions):\n",
    "    positive_sum = np.sum(predictions)\n",
    "    ratio = positive_sum / len(predictions)\n",
    "    return int(ratio + 0.5)\n",
    "\n",
    "def phrase_prediction_agg_max(predictions):\n",
    "    return np.max(predictions)\n",
    "\n",
    "def phrase_prediction_agg_min(predictions):\n",
    "    return np.min(predictions)\n",
    "\n",
    "phrase_agg_mv = Aggregation('phrase_mv', phrase_prediction_agg_majority_vote)\n",
    "phrase_agg_max = Aggregation('phrase_max', phrase_prediction_agg_max)\n",
    "phrase_agg_min = Aggregation('phrase_min', phrase_prediction_agg_min)\n",
    "\n",
    "phrase_aggs = [phrase_agg_mv, phrase_agg_max, phrase_agg_min]\n",
    "\n",
    "def phrase_merger(df_test, result, agg):\n",
    "    df_test = df_test.copy()\n",
    "    prediction = result[1]\n",
    "    df_test['prediction'] = prediction\n",
    "    pred_binary = df_test.groupby('phrase_index').apply(lambda row : \\\n",
    "                            (agg(row['prediction']), agg(row['binary']))).values\n",
    "    predictions = [pred for pred, binary in pred_binary]\n",
    "    binary = [binary for pred, binary in pred_binary]\n",
    "    score = precision_recall_fscore_support(binary, predictions, average='macro')\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eval_df_from_results_phrase(results):\n",
    "    evaluation = [{'dataset' : result.dataset.name, 'agg' : result.agg[0],\n",
    "                        'zc' : result.fc[0], 'prec' : result.measure[0],\n",
    "                   'rec' : result.measure[1], 'f1' : result.measure[2]} \n",
    "                       for result in results]\n",
    "    return pd.DataFrame.from_records(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: think about isntead of using all features use best feature sets from CWI experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_datasets(['Wikipedia', 'WikiNews', 'News'], 'Train', 'Dev', type_train='word', type_test='phrase')\n",
    "datasets = preprocess_datasets(datasets)\n",
    "phrase_splitted_datasets = phrase_splitting_datasets(datasets)\n",
    "# 1. Linguistic Features\n",
    "datasets_fc_linguistic = compute_features_linguistic(phrase_splitted_datasets)\n",
    "# 2. Corpus Features\n",
    "datasets_fc_frequency = compute_features_frequency(phrase_splitted_datasets)\n",
    "datasets_fc_language_model = compute_features_language_model(phrase_splitted_datasets)\n",
    "datasets_fc_corpus = compute_features_corpus([datasets_fc_frequency, datasets_fc_language_model])\n",
    "# # 3. Psycholinguistic\n",
    "datasets_fc_psycholinguistic = compute_features_psycholinguistic(phrase_splitted_datasets)\n",
    "# # 4. Semantic Features\n",
    "datasets_fc_wordnet = compute_features_wordnet(phrase_splitted_datasets)\n",
    "datasets_fc_dbpedia = compute_features_dbpedia(phrase_splitted_datasets)\n",
    "datasets_fc_brown_clustering = compute_features_brown_clustering(phrase_splitted_datasets)\n",
    "datasets_fc_semantic = compute_features_semantic([datasets_fc_wordnet, datasets_fc_dbpedia, datasets_fc_brown_clustering])\n",
    "# # 5. Dictionary Features\n",
    "datasets_fc_dictionary = compute_features_dictionary(phrase_splitted_datasets)\n",
    "# # 6. Concatentation of feature categories\n",
    "# # (1) Corpus + Semantic\n",
    "datasets_fc_corpus_semantic = concat_feature_datasets(datasets_fc_corpus, datasets_fc_semantic, name='corpus+semantic')\n",
    "# # (2) WordNet + Psycholinguistic\n",
    "datasets_fc_wordnet_psycholinguistic = concat_feature_datasets(datasets_fc_wordnet, \\\n",
    "                             datasets_fc_psycholinguistic, name='wordnet+psycholinguistic')\n",
    "# (3) All categories\n",
    "datasets_fc_all = concat_feature_datasets(datasets_fc_linguistic, datasets_fc_psycholinguistic, \\\n",
    "                             datasets_fc_semantic, datasets_fc_corpus, datasets_fc_dictionary, name='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fc_datasets = []\n",
    "all_fc_datasets.extend(datasets_fc_linguistic)\n",
    "all_fc_datasets.extend(datasets_fc_frequency)\n",
    "all_fc_datasets.extend(datasets_fc_language_model)\n",
    "all_fc_datasets.extend(datasets_fc_corpus)\n",
    "all_fc_datasets.extend(datasets_fc_psycholinguistic)\n",
    "all_fc_datasets.extend(datasets_fc_wordnet)\n",
    "all_fc_datasets.extend(datasets_fc_dbpedia)\n",
    "all_fc_datasets.extend(datasets_fc_brown_clustering)\n",
    "all_fc_datasets.extend(datasets_fc_semantic)\n",
    "all_fc_datasets.extend(datasets_fc_dictionary)\n",
    "all_fc_datasets.extend(datasets_fc_corpus_semantic)\n",
    "all_fc_datasets.extend(datasets_fc_wordnet_psycholinguistic)\n",
    "all_fc_datasets.extend(datasets_fc_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.x Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.5</td>\n",
       "      <td>always_complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.5</td>\n",
       "      <td>always_complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.5</td>\n",
       "      <td>always_complex</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset        f1      prec  rec              zc\n",
       "0  Wikipedia  0.456790  0.420455  0.5  always_complex\n",
       "1   WikiNews  0.433735  0.382979  0.5  always_complex\n",
       "2       News  0.440171  0.393130  0.5  always_complex"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_always_complex(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.5</td>\n",
       "      <td>vocab_clean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.5</td>\n",
       "      <td>vocab_clean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.5</td>\n",
       "      <td>vocab_clean</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset        f1      prec  rec           zc\n",
       "0  Wikipedia  0.456790  0.420455  0.5  vocab_clean\n",
       "1   WikiNews  0.433735  0.382979  0.5  vocab_clean\n",
       "2       News  0.440171  0.393130  0.5  vocab_clean"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_vocab_clean(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.5</td>\n",
       "      <td>vocab_weighted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.5</td>\n",
       "      <td>vocab_weighted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.5</td>\n",
       "      <td>vocab_weighted</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset        f1      prec  rec              zc\n",
       "0  Wikipedia  0.456790  0.420455  0.5  vocab_weighted\n",
       "1   WikiNews  0.433735  0.382979  0.5  vocab_weighted\n",
       "2       News  0.440171  0.393130  0.5  vocab_weighted"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_vocab_weighted(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.1 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_xgboost = [Result(fs, fs.fc, agg, phrase_merger(fs.test, xgboost(*transform_feat_to_num(\n",
    "                remove_labels_for_binary_df(fs.train), \n",
    "                remove_labels_phrase_for_binary_df(fs.test))), agg.agg)) \n",
    "          for fs in all_fc_datasets\n",
    "          for agg in phrase_aggs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.465695</td>\n",
       "      <td>0.514583</td>\n",
       "      <td>0.527027</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.455729</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.482625</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.283998</td>\n",
       "      <td>0.548205</td>\n",
       "      <td>0.545367</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.507734</td>\n",
       "      <td>0.542553</td>\n",
       "      <td>0.559343</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.542208</td>\n",
       "      <td>0.547059</td>\n",
       "      <td>0.560606</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.302113</td>\n",
       "      <td>0.501899</td>\n",
       "      <td>0.501263</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.550313</td>\n",
       "      <td>0.566507</td>\n",
       "      <td>0.596394</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.501221</td>\n",
       "      <td>0.504984</td>\n",
       "      <td>0.506155</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.329237</td>\n",
       "      <td>0.565310</td>\n",
       "      <td>0.549237</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.497143</td>\n",
       "      <td>0.529915</td>\n",
       "      <td>0.554054</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.522366</td>\n",
       "      <td>0.528571</td>\n",
       "      <td>0.543436</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.271222</td>\n",
       "      <td>0.460317</td>\n",
       "      <td>0.451737</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.526260</td>\n",
       "      <td>0.535441</td>\n",
       "      <td>0.546717</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.568279</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.570707</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.413233</td>\n",
       "      <td>0.534273</td>\n",
       "      <td>0.539141</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.573984</td>\n",
       "      <td>0.579879</td>\n",
       "      <td>0.611737</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.484743</td>\n",
       "      <td>0.486759</td>\n",
       "      <td>0.485003</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.348716</td>\n",
       "      <td>0.596622</td>\n",
       "      <td>0.574376</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.529097</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>0.581081</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.538013</td>\n",
       "      <td>0.539465</td>\n",
       "      <td>0.556950</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.260504</td>\n",
       "      <td>0.452775</td>\n",
       "      <td>0.444981</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.502517</td>\n",
       "      <td>0.519114</td>\n",
       "      <td>0.525884</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.588652</td>\n",
       "      <td>0.585797</td>\n",
       "      <td>0.593434</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.414297</td>\n",
       "      <td>0.550435</td>\n",
       "      <td>0.554924</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.564937</td>\n",
       "      <td>0.580305</td>\n",
       "      <td>0.616678</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.480679</td>\n",
       "      <td>0.484779</td>\n",
       "      <td>0.481796</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.349287</td>\n",
       "      <td>0.584740</td>\n",
       "      <td>0.567874</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.462972</td>\n",
       "      <td>0.493939</td>\n",
       "      <td>0.489382</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.496568</td>\n",
       "      <td>0.505208</td>\n",
       "      <td>0.507722</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.237750</td>\n",
       "      <td>0.416321</td>\n",
       "      <td>0.402510</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.485938</td>\n",
       "      <td>0.502350</td>\n",
       "      <td>0.503157</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.576379</td>\n",
       "      <td>0.575321</td>\n",
       "      <td>0.577652</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.413233</td>\n",
       "      <td>0.534273</td>\n",
       "      <td>0.539141</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.574116</td>\n",
       "      <td>0.581863</td>\n",
       "      <td>0.615811</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.474120</td>\n",
       "      <td>0.476732</td>\n",
       "      <td>0.473648</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.361842</td>\n",
       "      <td>0.578377</td>\n",
       "      <td>0.568655</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.361607</td>\n",
       "      <td>0.454737</td>\n",
       "      <td>0.416988</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.434039</td>\n",
       "      <td>0.479332</td>\n",
       "      <td>0.462355</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.177997</td>\n",
       "      <td>0.381250</td>\n",
       "      <td>0.426641</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.505090</td>\n",
       "      <td>0.568531</td>\n",
       "      <td>0.592803</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.570988</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.632576</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.332875</td>\n",
       "      <td>0.586382</td>\n",
       "      <td>0.553662</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.533458</td>\n",
       "      <td>0.580411</td>\n",
       "      <td>0.619279</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.490690</td>\n",
       "      <td>0.510643</td>\n",
       "      <td>0.515257</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.257503</td>\n",
       "      <td>0.613821</td>\n",
       "      <td>0.538835</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.507379</td>\n",
       "      <td>0.526599</td>\n",
       "      <td>0.545367</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.514706</td>\n",
       "      <td>0.523573</td>\n",
       "      <td>0.536680</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.332287</td>\n",
       "      <td>0.494792</td>\n",
       "      <td>0.492278</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.534476</td>\n",
       "      <td>0.552002</td>\n",
       "      <td>0.571338</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.625842</td>\n",
       "      <td>0.622619</td>\n",
       "      <td>0.630051</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.446558</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.577010</td>\n",
       "      <td>0.587911</td>\n",
       "      <td>0.626387</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>0.505848</td>\n",
       "      <td>0.506935</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.328530</td>\n",
       "      <td>0.577237</td>\n",
       "      <td>0.555739</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.468082</td>\n",
       "      <td>0.526357</td>\n",
       "      <td>0.549228</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.454094</td>\n",
       "      <td>0.498947</td>\n",
       "      <td>0.498069</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.271222</td>\n",
       "      <td>0.460317</td>\n",
       "      <td>0.451737</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.434259</td>\n",
       "      <td>0.509579</td>\n",
       "      <td>0.512626</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.459770</td>\n",
       "      <td>0.506364</td>\n",
       "      <td>0.508838</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.332875</td>\n",
       "      <td>0.586382</td>\n",
       "      <td>0.553662</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.480308</td>\n",
       "      <td>0.554266</td>\n",
       "      <td>0.578017</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.464040</td>\n",
       "      <td>0.502161</td>\n",
       "      <td>0.503207</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.288380</td>\n",
       "      <td>0.575945</td>\n",
       "      <td>0.540395</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.457851</td>\n",
       "      <td>0.510898</td>\n",
       "      <td>0.520270</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.505747</td>\n",
       "      <td>0.509653</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.467181</td>\n",
       "      <td>0.467181</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.523068</td>\n",
       "      <td>0.577773</td>\n",
       "      <td>0.606692</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.582222</td>\n",
       "      <td>0.595260</td>\n",
       "      <td>0.630682</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.298910</td>\n",
       "      <td>0.529573</td>\n",
       "      <td>0.517045</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.582859</td>\n",
       "      <td>0.593855</td>\n",
       "      <td>0.635316</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.516062</td>\n",
       "      <td>0.519160</td>\n",
       "      <td>0.524012</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.325730</td>\n",
       "      <td>0.552365</td>\n",
       "      <td>0.540309</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>0.531034</td>\n",
       "      <td>0.552124</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.518541</td>\n",
       "      <td>0.520611</td>\n",
       "      <td>0.527992</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.299491</td>\n",
       "      <td>0.460535</td>\n",
       "      <td>0.443050</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.518689</td>\n",
       "      <td>0.535648</td>\n",
       "      <td>0.548611</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.576379</td>\n",
       "      <td>0.575321</td>\n",
       "      <td>0.577652</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.403986</td>\n",
       "      <td>0.585135</td>\n",
       "      <td>0.579545</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.576765</td>\n",
       "      <td>0.590012</td>\n",
       "      <td>0.630461</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.488634</td>\n",
       "      <td>0.493868</td>\n",
       "      <td>0.492372</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.349287</td>\n",
       "      <td>0.584740</td>\n",
       "      <td>0.567874</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.468082</td>\n",
       "      <td>0.526357</td>\n",
       "      <td>0.549228</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.482353</td>\n",
       "      <td>0.496278</td>\n",
       "      <td>0.494208</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.202899</td>\n",
       "      <td>0.420513</td>\n",
       "      <td>0.440154</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.434259</td>\n",
       "      <td>0.509579</td>\n",
       "      <td>0.512626</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.468159</td>\n",
       "      <td>0.511338</td>\n",
       "      <td>0.515783</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.286240</td>\n",
       "      <td>0.519048</td>\n",
       "      <td>0.510101</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.370377</td>\n",
       "      <td>0.492997</td>\n",
       "      <td>0.491418</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.347457</td>\n",
       "      <td>0.413321</td>\n",
       "      <td>0.372746</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.240012</td>\n",
       "      <td>0.575173</td>\n",
       "      <td>0.522625</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.507379</td>\n",
       "      <td>0.526599</td>\n",
       "      <td>0.545367</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.571096</td>\n",
       "      <td>0.566362</td>\n",
       "      <td>0.583977</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.281772</td>\n",
       "      <td>0.467201</td>\n",
       "      <td>0.458494</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.558349</td>\n",
       "      <td>0.573631</td>\n",
       "      <td>0.601010</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.618551</td>\n",
       "      <td>0.613095</td>\n",
       "      <td>0.631944</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.435595</td>\n",
       "      <td>0.577681</td>\n",
       "      <td>0.584596</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.586126</td>\n",
       "      <td>0.593841</td>\n",
       "      <td>0.633669</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.506337</td>\n",
       "      <td>0.507964</td>\n",
       "      <td>0.509362</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.345717</td>\n",
       "      <td>0.572210</td>\n",
       "      <td>0.558946</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.553768</td>\n",
       "      <td>0.561368</td>\n",
       "      <td>0.601351</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.562552</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.577220</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.322459</td>\n",
       "      <td>0.489967</td>\n",
       "      <td>0.485521</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.539216</td>\n",
       "      <td>0.568841</td>\n",
       "      <td>0.595960</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.619008</td>\n",
       "      <td>0.614183</td>\n",
       "      <td>0.640783</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.457385</td>\n",
       "      <td>0.626148</td>\n",
       "      <td>0.630051</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.577119</td>\n",
       "      <td>0.583882</td>\n",
       "      <td>0.618239</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.514126</td>\n",
       "      <td>0.515424</td>\n",
       "      <td>0.518291</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.348716</td>\n",
       "      <td>0.596622</td>\n",
       "      <td>0.574376</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.492001</td>\n",
       "      <td>0.518182</td>\n",
       "      <td>0.531853</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.518541</td>\n",
       "      <td>0.520611</td>\n",
       "      <td>0.527992</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.281772</td>\n",
       "      <td>0.467201</td>\n",
       "      <td>0.458494</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.547798</td>\n",
       "      <td>0.573923</td>\n",
       "      <td>0.602904</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.601244</td>\n",
       "      <td>0.597396</td>\n",
       "      <td>0.618056</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.403986</td>\n",
       "      <td>0.585135</td>\n",
       "      <td>0.579545</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.570742</td>\n",
       "      <td>0.575851</td>\n",
       "      <td>0.605236</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.500318</td>\n",
       "      <td>0.501093</td>\n",
       "      <td>0.501214</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.357843</td>\n",
       "      <td>0.576892</td>\n",
       "      <td>0.566227</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            agg    dataset        f1      prec       rec  \\\n",
       "0     phrase_mv  Wikipedia  0.465695  0.514583  0.527027   \n",
       "1    phrase_max  Wikipedia  0.455729  0.490196  0.482625   \n",
       "2    phrase_min  Wikipedia  0.283998  0.548205  0.545367   \n",
       "3     phrase_mv   WikiNews  0.507734  0.542553  0.559343   \n",
       "4    phrase_max   WikiNews  0.542208  0.547059  0.560606   \n",
       "5    phrase_min   WikiNews  0.302113  0.501899  0.501263   \n",
       "6     phrase_mv       News  0.550313  0.566507  0.596394   \n",
       "7    phrase_max       News  0.501221  0.504984  0.506155   \n",
       "8    phrase_min       News  0.329237  0.565310  0.549237   \n",
       "9     phrase_mv  Wikipedia  0.497143  0.529915  0.554054   \n",
       "10   phrase_max  Wikipedia  0.522366  0.528571  0.543436   \n",
       "11   phrase_min  Wikipedia  0.271222  0.460317  0.451737   \n",
       "12    phrase_mv   WikiNews  0.526260  0.535441  0.546717   \n",
       "13   phrase_max   WikiNews  0.568279  0.566667  0.570707   \n",
       "14   phrase_min   WikiNews  0.413233  0.534273  0.539141   \n",
       "15    phrase_mv       News  0.573984  0.579879  0.611737   \n",
       "16   phrase_max       News  0.484743  0.486759  0.485003   \n",
       "17   phrase_min       News  0.348716  0.596622  0.574376   \n",
       "18    phrase_mv  Wikipedia  0.529097  0.546875  0.581081   \n",
       "19   phrase_max  Wikipedia  0.538013  0.539465  0.556950   \n",
       "20   phrase_min  Wikipedia  0.260504  0.452775  0.444981   \n",
       "21    phrase_mv   WikiNews  0.502517  0.519114  0.525884   \n",
       "22   phrase_max   WikiNews  0.588652  0.585797  0.593434   \n",
       "23   phrase_min   WikiNews  0.414297  0.550435  0.554924   \n",
       "24    phrase_mv       News  0.564937  0.580305  0.616678   \n",
       "25   phrase_max       News  0.480679  0.484779  0.481796   \n",
       "26   phrase_min       News  0.349287  0.584740  0.567874   \n",
       "27    phrase_mv  Wikipedia  0.462972  0.493939  0.489382   \n",
       "28   phrase_max  Wikipedia  0.496568  0.505208  0.507722   \n",
       "29   phrase_min  Wikipedia  0.237750  0.416321  0.402510   \n",
       "30    phrase_mv   WikiNews  0.485938  0.502350  0.503157   \n",
       "31   phrase_max   WikiNews  0.576379  0.575321  0.577652   \n",
       "32   phrase_min   WikiNews  0.413233  0.534273  0.539141   \n",
       "33    phrase_mv       News  0.574116  0.581863  0.615811   \n",
       "34   phrase_max       News  0.474120  0.476732  0.473648   \n",
       "35   phrase_min       News  0.361842  0.578377  0.568655   \n",
       "36    phrase_mv  Wikipedia  0.361607  0.454737  0.416988   \n",
       "37   phrase_max  Wikipedia  0.434039  0.479332  0.462355   \n",
       "38   phrase_min  Wikipedia  0.177997  0.381250  0.426641   \n",
       "39    phrase_mv   WikiNews  0.505090  0.568531  0.592803   \n",
       "40   phrase_max   WikiNews  0.570988  0.595238  0.632576   \n",
       "41   phrase_min   WikiNews  0.332875  0.586382  0.553662   \n",
       "42    phrase_mv       News  0.533458  0.580411  0.619279   \n",
       "43   phrase_max       News  0.490690  0.510643  0.515257   \n",
       "44   phrase_min       News  0.257503  0.613821  0.538835   \n",
       "45    phrase_mv  Wikipedia  0.507379  0.526599  0.545367   \n",
       "46   phrase_max  Wikipedia  0.514706  0.523573  0.536680   \n",
       "47   phrase_min  Wikipedia  0.332287  0.494792  0.492278   \n",
       "48    phrase_mv   WikiNews  0.534476  0.552002  0.571338   \n",
       "49   phrase_max   WikiNews  0.625842  0.622619  0.630051   \n",
       "50   phrase_min   WikiNews  0.446558  0.648649  0.638889   \n",
       "51    phrase_mv       News  0.577010  0.587911  0.626387   \n",
       "52   phrase_max       News  0.503788  0.505848  0.506935   \n",
       "53   phrase_min       News  0.328530  0.577237  0.555739   \n",
       "54    phrase_mv  Wikipedia  0.468082  0.526357  0.549228   \n",
       "55   phrase_max  Wikipedia  0.454094  0.498947  0.498069   \n",
       "56   phrase_min  Wikipedia  0.271222  0.460317  0.451737   \n",
       "57    phrase_mv   WikiNews  0.434259  0.509579  0.512626   \n",
       "58   phrase_max   WikiNews  0.459770  0.506364  0.508838   \n",
       "59   phrase_min   WikiNews  0.332875  0.586382  0.553662   \n",
       "60    phrase_mv       News  0.480308  0.554266  0.578017   \n",
       "61   phrase_max       News  0.464040  0.502161  0.503207   \n",
       "62   phrase_min       News  0.288380  0.575945  0.540395   \n",
       "63    phrase_mv  Wikipedia  0.457851  0.510898  0.520270   \n",
       "64   phrase_max  Wikipedia  0.484848  0.505747  0.509653   \n",
       "65   phrase_min  Wikipedia  0.250000  0.467181  0.467181   \n",
       "66    phrase_mv   WikiNews  0.523068  0.577773  0.606692   \n",
       "67   phrase_max   WikiNews  0.582222  0.595260  0.630682   \n",
       "68   phrase_min   WikiNews  0.298910  0.529573  0.517045   \n",
       "69    phrase_mv       News  0.582859  0.593855  0.635316   \n",
       "70   phrase_max       News  0.516062  0.519160  0.524012   \n",
       "71   phrase_min       News  0.325730  0.552365  0.540309   \n",
       "72    phrase_mv  Wikipedia  0.515152  0.531034  0.552124   \n",
       "73   phrase_max  Wikipedia  0.518541  0.520611  0.527992   \n",
       "74   phrase_min  Wikipedia  0.299491  0.460535  0.443050   \n",
       "75    phrase_mv   WikiNews  0.518689  0.535648  0.548611   \n",
       "76   phrase_max   WikiNews  0.576379  0.575321  0.577652   \n",
       "77   phrase_min   WikiNews  0.403986  0.585135  0.579545   \n",
       "78    phrase_mv       News  0.576765  0.590012  0.630461   \n",
       "79   phrase_max       News  0.488634  0.493868  0.492372   \n",
       "80   phrase_min       News  0.349287  0.584740  0.567874   \n",
       "81    phrase_mv  Wikipedia  0.468082  0.526357  0.549228   \n",
       "82   phrase_max  Wikipedia  0.482353  0.496278  0.494208   \n",
       "83   phrase_min  Wikipedia  0.202899  0.420513  0.440154   \n",
       "84    phrase_mv   WikiNews  0.434259  0.509579  0.512626   \n",
       "85   phrase_max   WikiNews  0.468159  0.511338  0.515783   \n",
       "86   phrase_min   WikiNews  0.286240  0.519048  0.510101   \n",
       "87    phrase_mv       News  0.370377  0.492997  0.491418   \n",
       "88   phrase_max       News  0.347457  0.413321  0.372746   \n",
       "89   phrase_min       News  0.240012  0.575173  0.522625   \n",
       "90    phrase_mv  Wikipedia  0.507379  0.526599  0.545367   \n",
       "91   phrase_max  Wikipedia  0.571096  0.566362  0.583977   \n",
       "92   phrase_min  Wikipedia  0.281772  0.467201  0.458494   \n",
       "93    phrase_mv   WikiNews  0.558349  0.573631  0.601010   \n",
       "94   phrase_max   WikiNews  0.618551  0.613095  0.631944   \n",
       "95   phrase_min   WikiNews  0.435595  0.577681  0.584596   \n",
       "96    phrase_mv       News  0.586126  0.593841  0.633669   \n",
       "97   phrase_max       News  0.506337  0.507964  0.509362   \n",
       "98   phrase_min       News  0.345717  0.572210  0.558946   \n",
       "99    phrase_mv  Wikipedia  0.553768  0.561368  0.601351   \n",
       "100  phrase_max  Wikipedia  0.562552  0.558824  0.577220   \n",
       "101  phrase_min  Wikipedia  0.322459  0.489967  0.485521   \n",
       "102   phrase_mv   WikiNews  0.539216  0.568841  0.595960   \n",
       "103  phrase_max   WikiNews  0.619008  0.614183  0.640783   \n",
       "104  phrase_min   WikiNews  0.457385  0.626148  0.630051   \n",
       "105   phrase_mv       News  0.577119  0.583882  0.618239   \n",
       "106  phrase_max       News  0.514126  0.515424  0.518291   \n",
       "107  phrase_min       News  0.348716  0.596622  0.574376   \n",
       "108   phrase_mv  Wikipedia  0.492001  0.518182  0.531853   \n",
       "109  phrase_max  Wikipedia  0.518541  0.520611  0.527992   \n",
       "110  phrase_min  Wikipedia  0.281772  0.467201  0.458494   \n",
       "111   phrase_mv   WikiNews  0.547798  0.573923  0.602904   \n",
       "112  phrase_max   WikiNews  0.601244  0.597396  0.618056   \n",
       "113  phrase_min   WikiNews  0.403986  0.585135  0.579545   \n",
       "114   phrase_mv       News  0.570742  0.575851  0.605236   \n",
       "115  phrase_max       News  0.500318  0.501093  0.501214   \n",
       "116  phrase_min       News  0.357843  0.576892  0.566227   \n",
       "\n",
       "                           zc  \n",
       "0                  linguistic  \n",
       "1                  linguistic  \n",
       "2                  linguistic  \n",
       "3                  linguistic  \n",
       "4                  linguistic  \n",
       "5                  linguistic  \n",
       "6                  linguistic  \n",
       "7                  linguistic  \n",
       "8                  linguistic  \n",
       "9                   frequency  \n",
       "10                  frequency  \n",
       "11                  frequency  \n",
       "12                  frequency  \n",
       "13                  frequency  \n",
       "14                  frequency  \n",
       "15                  frequency  \n",
       "16                  frequency  \n",
       "17                  frequency  \n",
       "18             language_model  \n",
       "19             language_model  \n",
       "20             language_model  \n",
       "21             language_model  \n",
       "22             language_model  \n",
       "23             language_model  \n",
       "24             language_model  \n",
       "25             language_model  \n",
       "26             language_model  \n",
       "27                     corpus  \n",
       "28                     corpus  \n",
       "29                     corpus  \n",
       "30                     corpus  \n",
       "31                     corpus  \n",
       "32                     corpus  \n",
       "33                     corpus  \n",
       "34                     corpus  \n",
       "35                     corpus  \n",
       "36           psycholinguistic  \n",
       "37           psycholinguistic  \n",
       "38           psycholinguistic  \n",
       "39           psycholinguistic  \n",
       "40           psycholinguistic  \n",
       "41           psycholinguistic  \n",
       "42           psycholinguistic  \n",
       "43           psycholinguistic  \n",
       "44           psycholinguistic  \n",
       "45                    wordnet  \n",
       "46                    wordnet  \n",
       "47                    wordnet  \n",
       "48                    wordnet  \n",
       "49                    wordnet  \n",
       "50                    wordnet  \n",
       "51                    wordnet  \n",
       "52                    wordnet  \n",
       "53                    wordnet  \n",
       "54                    dbpedia  \n",
       "55                    dbpedia  \n",
       "56                    dbpedia  \n",
       "57                    dbpedia  \n",
       "58                    dbpedia  \n",
       "59                    dbpedia  \n",
       "60                    dbpedia  \n",
       "61                    dbpedia  \n",
       "62                    dbpedia  \n",
       "63           brown_clustering  \n",
       "64           brown_clustering  \n",
       "65           brown_clustering  \n",
       "66           brown_clustering  \n",
       "67           brown_clustering  \n",
       "68           brown_clustering  \n",
       "69           brown_clustering  \n",
       "70           brown_clustering  \n",
       "71           brown_clustering  \n",
       "72                   semantic  \n",
       "73                   semantic  \n",
       "74                   semantic  \n",
       "75                   semantic  \n",
       "76                   semantic  \n",
       "77                   semantic  \n",
       "78                   semantic  \n",
       "79                   semantic  \n",
       "80                   semantic  \n",
       "81                 dictionary  \n",
       "82                 dictionary  \n",
       "83                 dictionary  \n",
       "84                 dictionary  \n",
       "85                 dictionary  \n",
       "86                 dictionary  \n",
       "87                 dictionary  \n",
       "88                 dictionary  \n",
       "89                 dictionary  \n",
       "90            corpus+semantic  \n",
       "91            corpus+semantic  \n",
       "92            corpus+semantic  \n",
       "93            corpus+semantic  \n",
       "94            corpus+semantic  \n",
       "95            corpus+semantic  \n",
       "96            corpus+semantic  \n",
       "97            corpus+semantic  \n",
       "98            corpus+semantic  \n",
       "99   wordnet+psycholinguistic  \n",
       "100  wordnet+psycholinguistic  \n",
       "101  wordnet+psycholinguistic  \n",
       "102  wordnet+psycholinguistic  \n",
       "103  wordnet+psycholinguistic  \n",
       "104  wordnet+psycholinguistic  \n",
       "105  wordnet+psycholinguistic  \n",
       "106  wordnet+psycholinguistic  \n",
       "107  wordnet+psycholinguistic  \n",
       "108                       all  \n",
       "109                       all  \n",
       "110                       all  \n",
       "111                       all  \n",
       "112                       all  \n",
       "113                       all  \n",
       "114                       all  \n",
       "115                       all  \n",
       "116                       all  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_eval_data_xg = create_eval_df_from_results_phrase(results_xgboost)\n",
    "feature_eval_data_xg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>News</td>\n",
       "      <td>0.613904</td>\n",
       "      <td>0.613904</td>\n",
       "      <td>0.613904</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.575290</td>\n",
       "      <td>0.575290</td>\n",
       "      <td>0.575290</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.650831</td>\n",
       "      <td>0.727820</td>\n",
       "      <td>0.631313</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dataset        f1      prec       rec          zc\n",
       "11        News  0.613904  0.613904  0.613904  linguistic\n",
       "99   Wikipedia  0.575290  0.575290  0.575290    semantic\n",
       "151   WikiNews  0.650831  0.727820  0.631313         all"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = feature_eval_data_xg.groupby(['dataset'])['f1'].transform(max) == feature_eval_data_xg['f1']\n",
    "feature_eval_data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   17.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   19.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   23.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   28.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   32.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   33.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   54.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   44.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   52.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   28.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   31.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   30.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   40.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   40.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   30.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   50.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   50.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   53.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   29.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   29.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   28.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   36.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    4.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   35.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   33.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   50.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   55.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   44.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   39.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   41.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   32.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   47.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   54.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   40.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    9.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    9.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    9.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   13.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   13.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   12.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   19.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   17.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   18.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   27.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   28.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   26.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   38.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   28.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   35.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   58.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   51.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   51.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   12.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   14.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   15.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   19.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   19.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   14.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   22.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   29.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   26.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   13.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   13.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   13.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   17.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   15.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   13.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   20.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   23.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   23.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   29.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   29.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   30.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   39.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   39.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   36.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    3.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    3.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    3.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    5.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    5.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    5.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    8.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    8.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    8.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   53.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   45.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   49.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.8min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   29.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   29.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   29.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   39.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   39.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   34.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   56.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.0min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   58.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   56.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   57.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "results_rf = [Result(fs, fs.fc, agg, phrase_merger(fs.test, random_forest(*transform_feat_to_num(\n",
    "                remove_labels_for_binary_df(fs.train), \n",
    "                remove_labels_phrase_for_binary_df(fs.test))), agg.agg)) \n",
    "          for fs in all_fc_datasets\n",
    "          for agg in phrase_aggs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.457851</td>\n",
       "      <td>0.510898</td>\n",
       "      <td>0.520270</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.484375</td>\n",
       "      <td>0.514161</td>\n",
       "      <td>0.525097</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.283998</td>\n",
       "      <td>0.515525</td>\n",
       "      <td>0.516409</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.509720</td>\n",
       "      <td>0.560897</td>\n",
       "      <td>0.583965</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.567731</td>\n",
       "      <td>0.574443</td>\n",
       "      <td>0.599116</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.335613</td>\n",
       "      <td>0.553571</td>\n",
       "      <td>0.537879</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.464065</td>\n",
       "      <td>0.530714</td>\n",
       "      <td>0.544730</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.450678</td>\n",
       "      <td>0.478701</td>\n",
       "      <td>0.469053</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.274754</td>\n",
       "      <td>0.569492</td>\n",
       "      <td>0.533114</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.497143</td>\n",
       "      <td>0.529915</td>\n",
       "      <td>0.554054</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.522366</td>\n",
       "      <td>0.528571</td>\n",
       "      <td>0.543436</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.272351</td>\n",
       "      <td>0.482639</td>\n",
       "      <td>0.480695</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.517949</td>\n",
       "      <td>0.541496</td>\n",
       "      <td>0.557449</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.618551</td>\n",
       "      <td>0.613095</td>\n",
       "      <td>0.631944</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.403986</td>\n",
       "      <td>0.585135</td>\n",
       "      <td>0.579545</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.598360</td>\n",
       "      <td>0.598784</td>\n",
       "      <td>0.635229</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.512847</td>\n",
       "      <td>0.512706</td>\n",
       "      <td>0.513350</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.357843</td>\n",
       "      <td>0.576892</td>\n",
       "      <td>0.566227</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.553768</td>\n",
       "      <td>0.561368</td>\n",
       "      <td>0.601351</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.571096</td>\n",
       "      <td>0.566362</td>\n",
       "      <td>0.583977</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.248447</td>\n",
       "      <td>0.425397</td>\n",
       "      <td>0.409266</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.510603</td>\n",
       "      <td>0.530373</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.580357</td>\n",
       "      <td>0.577489</td>\n",
       "      <td>0.586490</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.393548</td>\n",
       "      <td>0.558708</td>\n",
       "      <td>0.556818</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.579973</td>\n",
       "      <td>0.584036</td>\n",
       "      <td>0.616592</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.479632</td>\n",
       "      <td>0.480281</td>\n",
       "      <td>0.479282</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.346176</td>\n",
       "      <td>0.561956</td>\n",
       "      <td>0.552445</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.505062</td>\n",
       "      <td>0.533962</td>\n",
       "      <td>0.560811</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.530130</td>\n",
       "      <td>0.533854</td>\n",
       "      <td>0.550193</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.258971</td>\n",
       "      <td>0.433638</td>\n",
       "      <td>0.416023</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.534476</td>\n",
       "      <td>0.552002</td>\n",
       "      <td>0.571338</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.588652</td>\n",
       "      <td>0.585797</td>\n",
       "      <td>0.593434</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.436106</td>\n",
       "      <td>0.597367</td>\n",
       "      <td>0.600379</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.592054</td>\n",
       "      <td>0.592702</td>\n",
       "      <td>0.626300</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.509205</td>\n",
       "      <td>0.509157</td>\n",
       "      <td>0.509275</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.361842</td>\n",
       "      <td>0.578377</td>\n",
       "      <td>0.568655</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.376906</td>\n",
       "      <td>0.485839</td>\n",
       "      <td>0.474903</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.431405</td>\n",
       "      <td>0.488064</td>\n",
       "      <td>0.477799</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.223664</td>\n",
       "      <td>0.518750</td>\n",
       "      <td>0.511583</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.514098</td>\n",
       "      <td>0.573148</td>\n",
       "      <td>0.599747</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.597278</td>\n",
       "      <td>0.611264</td>\n",
       "      <td>0.653409</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.281944</td>\n",
       "      <td>0.559593</td>\n",
       "      <td>0.525884</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.492667</td>\n",
       "      <td>0.577240</td>\n",
       "      <td>0.608877</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.483583</td>\n",
       "      <td>0.521814</td>\n",
       "      <td>0.532420</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.244867</td>\n",
       "      <td>0.578003</td>\n",
       "      <td>0.525052</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.538934</td>\n",
       "      <td>0.545537</td>\n",
       "      <td>0.572394</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.539683</td>\n",
       "      <td>0.548263</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.299491</td>\n",
       "      <td>0.460535</td>\n",
       "      <td>0.443050</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.542755</td>\n",
       "      <td>0.557407</td>\n",
       "      <td>0.578283</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.625842</td>\n",
       "      <td>0.622619</td>\n",
       "      <td>0.630051</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.401818</td>\n",
       "      <td>0.641026</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.550313</td>\n",
       "      <td>0.566507</td>\n",
       "      <td>0.596394</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.470395</td>\n",
       "      <td>0.475284</td>\n",
       "      <td>0.470440</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.328530</td>\n",
       "      <td>0.577237</td>\n",
       "      <td>0.555739</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.293996</td>\n",
       "      <td>0.495238</td>\n",
       "      <td>0.494208</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.328421</td>\n",
       "      <td>0.476427</td>\n",
       "      <td>0.463320</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.197917</td>\n",
       "      <td>0.495935</td>\n",
       "      <td>0.498069</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.295027</td>\n",
       "      <td>0.567974</td>\n",
       "      <td>0.532828</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.345060</td>\n",
       "      <td>0.591168</td>\n",
       "      <td>0.560606</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.205003</td>\n",
       "      <td>0.618280</td>\n",
       "      <td>0.506944</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.256590</td>\n",
       "      <td>0.456570</td>\n",
       "      <td>0.473041</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.280388</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.465673</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.176213</td>\n",
       "      <td>0.270914</td>\n",
       "      <td>0.484570</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.494029</td>\n",
       "      <td>0.549096</td>\n",
       "      <td>0.591699</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.550058</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.616795</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.226873</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>0.453668</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.500885</td>\n",
       "      <td>0.556144</td>\n",
       "      <td>0.577020</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.524608</td>\n",
       "      <td>0.552608</td>\n",
       "      <td>0.573232</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.281944</td>\n",
       "      <td>0.559593</td>\n",
       "      <td>0.525884</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.561619</td>\n",
       "      <td>0.580745</td>\n",
       "      <td>0.618325</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.509990</td>\n",
       "      <td>0.515952</td>\n",
       "      <td>0.520804</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.300411</td>\n",
       "      <td>0.538313</td>\n",
       "      <td>0.525745</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.553768</td>\n",
       "      <td>0.561368</td>\n",
       "      <td>0.601351</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.546032</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.563707</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.318903</td>\n",
       "      <td>0.471429</td>\n",
       "      <td>0.456564</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.517949</td>\n",
       "      <td>0.541496</td>\n",
       "      <td>0.557449</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.588652</td>\n",
       "      <td>0.585797</td>\n",
       "      <td>0.593434</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.368840</td>\n",
       "      <td>0.599578</td>\n",
       "      <td>0.574495</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.595124</td>\n",
       "      <td>0.601814</td>\n",
       "      <td>0.645024</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.497839</td>\n",
       "      <td>0.498918</td>\n",
       "      <td>0.498786</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.330413</td>\n",
       "      <td>0.544388</td>\n",
       "      <td>0.536234</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.427278</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.515444</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.455729</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.482625</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.202899</td>\n",
       "      <td>0.420513</td>\n",
       "      <td>0.440154</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.434259</td>\n",
       "      <td>0.509579</td>\n",
       "      <td>0.512626</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.468159</td>\n",
       "      <td>0.511338</td>\n",
       "      <td>0.515783</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.286240</td>\n",
       "      <td>0.519048</td>\n",
       "      <td>0.510101</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.389206</td>\n",
       "      <td>0.513408</td>\n",
       "      <td>0.516557</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.373139</td>\n",
       "      <td>0.443142</td>\n",
       "      <td>0.417389</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.244867</td>\n",
       "      <td>0.578003</td>\n",
       "      <td>0.525052</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.513021</td>\n",
       "      <td>0.538126</td>\n",
       "      <td>0.567568</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.530130</td>\n",
       "      <td>0.533854</td>\n",
       "      <td>0.550193</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.271222</td>\n",
       "      <td>0.460317</td>\n",
       "      <td>0.451737</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.558349</td>\n",
       "      <td>0.573631</td>\n",
       "      <td>0.601010</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.609847</td>\n",
       "      <td>0.605040</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>0.593434</td>\n",
       "      <td>0.593434</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.589020</td>\n",
       "      <td>0.590489</td>\n",
       "      <td>0.623873</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.506718</td>\n",
       "      <td>0.506677</td>\n",
       "      <td>0.506848</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.354200</td>\n",
       "      <td>0.565413</td>\n",
       "      <td>0.557299</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.556322</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.562552</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.577220</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.302404</td>\n",
       "      <td>0.479389</td>\n",
       "      <td>0.472008</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.549899</td>\n",
       "      <td>0.568223</td>\n",
       "      <td>0.594066</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.627367</td>\n",
       "      <td>0.621614</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.403986</td>\n",
       "      <td>0.585135</td>\n",
       "      <td>0.579545</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.583081</td>\n",
       "      <td>0.591841</td>\n",
       "      <td>0.631241</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.492903</td>\n",
       "      <td>0.494694</td>\n",
       "      <td>0.493932</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.331985</td>\n",
       "      <td>0.591691</td>\n",
       "      <td>0.564667</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.469206</td>\n",
       "      <td>0.506410</td>\n",
       "      <td>0.511583</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.482353</td>\n",
       "      <td>0.496278</td>\n",
       "      <td>0.494208</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.302404</td>\n",
       "      <td>0.479389</td>\n",
       "      <td>0.472008</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.549899</td>\n",
       "      <td>0.568223</td>\n",
       "      <td>0.594066</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>0.613043</td>\n",
       "      <td>0.623106</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>0.593434</td>\n",
       "      <td>0.593434</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.579973</td>\n",
       "      <td>0.584036</td>\n",
       "      <td>0.616592</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.494429</td>\n",
       "      <td>0.495134</td>\n",
       "      <td>0.494712</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.370000</td>\n",
       "      <td>0.571833</td>\n",
       "      <td>0.567008</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            agg    dataset        f1      prec       rec  \\\n",
       "0     phrase_mv  Wikipedia  0.457851  0.510898  0.520270   \n",
       "1    phrase_max  Wikipedia  0.484375  0.514161  0.525097   \n",
       "2    phrase_min  Wikipedia  0.283998  0.515525  0.516409   \n",
       "3     phrase_mv   WikiNews  0.509720  0.560897  0.583965   \n",
       "4    phrase_max   WikiNews  0.567731  0.574443  0.599116   \n",
       "5    phrase_min   WikiNews  0.335613  0.553571  0.537879   \n",
       "6     phrase_mv       News  0.464065  0.530714  0.544730   \n",
       "7    phrase_max       News  0.450678  0.478701  0.469053   \n",
       "8    phrase_min       News  0.274754  0.569492  0.533114   \n",
       "9     phrase_mv  Wikipedia  0.497143  0.529915  0.554054   \n",
       "10   phrase_max  Wikipedia  0.522366  0.528571  0.543436   \n",
       "11   phrase_min  Wikipedia  0.272351  0.482639  0.480695   \n",
       "12    phrase_mv   WikiNews  0.517949  0.541496  0.557449   \n",
       "13   phrase_max   WikiNews  0.618551  0.613095  0.631944   \n",
       "14   phrase_min   WikiNews  0.403986  0.585135  0.579545   \n",
       "15    phrase_mv       News  0.598360  0.598784  0.635229   \n",
       "16   phrase_max       News  0.512847  0.512706  0.513350   \n",
       "17   phrase_min       News  0.357843  0.576892  0.566227   \n",
       "18    phrase_mv  Wikipedia  0.553768  0.561368  0.601351   \n",
       "19   phrase_max  Wikipedia  0.571096  0.566362  0.583977   \n",
       "20   phrase_min  Wikipedia  0.248447  0.425397  0.409266   \n",
       "21    phrase_mv   WikiNews  0.510603  0.530373  0.541667   \n",
       "22   phrase_max   WikiNews  0.580357  0.577489  0.586490   \n",
       "23   phrase_min   WikiNews  0.393548  0.558708  0.556818   \n",
       "24    phrase_mv       News  0.579973  0.584036  0.616592   \n",
       "25   phrase_max       News  0.479632  0.480281  0.479282   \n",
       "26   phrase_min       News  0.346176  0.561956  0.552445   \n",
       "27    phrase_mv  Wikipedia  0.505062  0.533962  0.560811   \n",
       "28   phrase_max  Wikipedia  0.530130  0.533854  0.550193   \n",
       "29   phrase_min  Wikipedia  0.258971  0.433638  0.416023   \n",
       "30    phrase_mv   WikiNews  0.534476  0.552002  0.571338   \n",
       "31   phrase_max   WikiNews  0.588652  0.585797  0.593434   \n",
       "32   phrase_min   WikiNews  0.436106  0.597367  0.600379   \n",
       "33    phrase_mv       News  0.592054  0.592702  0.626300   \n",
       "34   phrase_max       News  0.509205  0.509157  0.509275   \n",
       "35   phrase_min       News  0.361842  0.578377  0.568655   \n",
       "36    phrase_mv  Wikipedia  0.376906  0.485839  0.474903   \n",
       "37   phrase_max  Wikipedia  0.431405  0.488064  0.477799   \n",
       "38   phrase_min  Wikipedia  0.223664  0.518750  0.511583   \n",
       "39    phrase_mv   WikiNews  0.514098  0.573148  0.599747   \n",
       "40   phrase_max   WikiNews  0.597278  0.611264  0.653409   \n",
       "41   phrase_min   WikiNews  0.281944  0.559593  0.525884   \n",
       "42    phrase_mv       News  0.492667  0.577240  0.608877   \n",
       "43   phrase_max       News  0.483583  0.521814  0.532420   \n",
       "44   phrase_min       News  0.244867  0.578003  0.525052   \n",
       "45    phrase_mv  Wikipedia  0.538934  0.545537  0.572394   \n",
       "46   phrase_max  Wikipedia  0.541667  0.539683  0.548263   \n",
       "47   phrase_min  Wikipedia  0.299491  0.460535  0.443050   \n",
       "48    phrase_mv   WikiNews  0.542755  0.557407  0.578283   \n",
       "49   phrase_max   WikiNews  0.625842  0.622619  0.630051   \n",
       "50   phrase_min   WikiNews  0.401818  0.641026  0.611111   \n",
       "51    phrase_mv       News  0.550313  0.566507  0.596394   \n",
       "52   phrase_max       News  0.470395  0.475284  0.470440   \n",
       "53   phrase_min       News  0.328530  0.577237  0.555739   \n",
       "54    phrase_mv  Wikipedia  0.293996  0.495238  0.494208   \n",
       "55   phrase_max  Wikipedia  0.328421  0.476427  0.463320   \n",
       "56   phrase_min  Wikipedia  0.197917  0.495935  0.498069   \n",
       "57    phrase_mv   WikiNews  0.295027  0.567974  0.532828   \n",
       "58   phrase_max   WikiNews  0.345060  0.591168  0.560606   \n",
       "59   phrase_min   WikiNews  0.205003  0.618280  0.506944   \n",
       "60    phrase_mv       News  0.256590  0.456570  0.473041   \n",
       "61   phrase_max       News  0.280388  0.457143  0.465673   \n",
       "62   phrase_min       News  0.176213  0.270914  0.484570   \n",
       "63    phrase_mv  Wikipedia  0.494029  0.549096  0.591699   \n",
       "64   phrase_max  Wikipedia  0.550058  0.566667  0.616795   \n",
       "65   phrase_min  Wikipedia  0.226873  0.447368  0.453668   \n",
       "66    phrase_mv   WikiNews  0.500885  0.556144  0.577020   \n",
       "67   phrase_max   WikiNews  0.524608  0.552608  0.573232   \n",
       "68   phrase_min   WikiNews  0.281944  0.559593  0.525884   \n",
       "69    phrase_mv       News  0.561619  0.580745  0.618325   \n",
       "70   phrase_max       News  0.509990  0.515952  0.520804   \n",
       "71   phrase_min       News  0.300411  0.538313  0.525745   \n",
       "72    phrase_mv  Wikipedia  0.553768  0.561368  0.601351   \n",
       "73   phrase_max  Wikipedia  0.546032  0.545455  0.563707   \n",
       "74   phrase_min  Wikipedia  0.318903  0.471429  0.456564   \n",
       "75    phrase_mv   WikiNews  0.517949  0.541496  0.557449   \n",
       "76   phrase_max   WikiNews  0.588652  0.585797  0.593434   \n",
       "77   phrase_min   WikiNews  0.368840  0.599578  0.574495   \n",
       "78    phrase_mv       News  0.595124  0.601814  0.645024   \n",
       "79   phrase_max       News  0.497839  0.498918  0.498786   \n",
       "80   phrase_min       News  0.330413  0.544388  0.536234   \n",
       "81    phrase_mv  Wikipedia  0.427278  0.508333  0.515444   \n",
       "82   phrase_max  Wikipedia  0.455729  0.490196  0.482625   \n",
       "83   phrase_min  Wikipedia  0.202899  0.420513  0.440154   \n",
       "84    phrase_mv   WikiNews  0.434259  0.509579  0.512626   \n",
       "85   phrase_max   WikiNews  0.468159  0.511338  0.515783   \n",
       "86   phrase_min   WikiNews  0.286240  0.519048  0.510101   \n",
       "87    phrase_mv       News  0.389206  0.513408  0.516557   \n",
       "88   phrase_max       News  0.373139  0.443142  0.417389   \n",
       "89   phrase_min       News  0.244867  0.578003  0.525052   \n",
       "90    phrase_mv  Wikipedia  0.513021  0.538126  0.567568   \n",
       "91   phrase_max  Wikipedia  0.530130  0.533854  0.550193   \n",
       "92   phrase_min  Wikipedia  0.271222  0.460317  0.451737   \n",
       "93    phrase_mv   WikiNews  0.558349  0.573631  0.601010   \n",
       "94   phrase_max   WikiNews  0.609847  0.605040  0.625000   \n",
       "95   phrase_min   WikiNews  0.425532  0.593434  0.593434   \n",
       "96    phrase_mv       News  0.589020  0.590489  0.623873   \n",
       "97   phrase_max       News  0.506718  0.506677  0.506848   \n",
       "98   phrase_min       News  0.354200  0.565413  0.557299   \n",
       "99    phrase_mv  Wikipedia  0.545455  0.556322  0.594595   \n",
       "100  phrase_max  Wikipedia  0.562552  0.558824  0.577220   \n",
       "101  phrase_min  Wikipedia  0.302404  0.479389  0.472008   \n",
       "102   phrase_mv   WikiNews  0.549899  0.568223  0.594066   \n",
       "103  phrase_max   WikiNews  0.627367  0.621614  0.638889   \n",
       "104  phrase_min   WikiNews  0.403986  0.585135  0.579545   \n",
       "105   phrase_mv       News  0.583081  0.591841  0.631241   \n",
       "106  phrase_max       News  0.492903  0.494694  0.493932   \n",
       "107  phrase_min       News  0.331985  0.591691  0.564667   \n",
       "108   phrase_mv  Wikipedia  0.469206  0.506410  0.511583   \n",
       "109  phrase_max  Wikipedia  0.482353  0.496278  0.494208   \n",
       "110  phrase_min  Wikipedia  0.302404  0.479389  0.472008   \n",
       "111   phrase_mv   WikiNews  0.549899  0.568223  0.594066   \n",
       "112  phrase_max   WikiNews  0.617021  0.613043  0.623106   \n",
       "113  phrase_min   WikiNews  0.425532  0.593434  0.593434   \n",
       "114   phrase_mv       News  0.579973  0.584036  0.616592   \n",
       "115  phrase_max       News  0.494429  0.495134  0.494712   \n",
       "116  phrase_min       News  0.370000  0.571833  0.567008   \n",
       "\n",
       "                           zc  \n",
       "0                  linguistic  \n",
       "1                  linguistic  \n",
       "2                  linguistic  \n",
       "3                  linguistic  \n",
       "4                  linguistic  \n",
       "5                  linguistic  \n",
       "6                  linguistic  \n",
       "7                  linguistic  \n",
       "8                  linguistic  \n",
       "9                   frequency  \n",
       "10                  frequency  \n",
       "11                  frequency  \n",
       "12                  frequency  \n",
       "13                  frequency  \n",
       "14                  frequency  \n",
       "15                  frequency  \n",
       "16                  frequency  \n",
       "17                  frequency  \n",
       "18             language_model  \n",
       "19             language_model  \n",
       "20             language_model  \n",
       "21             language_model  \n",
       "22             language_model  \n",
       "23             language_model  \n",
       "24             language_model  \n",
       "25             language_model  \n",
       "26             language_model  \n",
       "27                     corpus  \n",
       "28                     corpus  \n",
       "29                     corpus  \n",
       "30                     corpus  \n",
       "31                     corpus  \n",
       "32                     corpus  \n",
       "33                     corpus  \n",
       "34                     corpus  \n",
       "35                     corpus  \n",
       "36           psycholinguistic  \n",
       "37           psycholinguistic  \n",
       "38           psycholinguistic  \n",
       "39           psycholinguistic  \n",
       "40           psycholinguistic  \n",
       "41           psycholinguistic  \n",
       "42           psycholinguistic  \n",
       "43           psycholinguistic  \n",
       "44           psycholinguistic  \n",
       "45                    wordnet  \n",
       "46                    wordnet  \n",
       "47                    wordnet  \n",
       "48                    wordnet  \n",
       "49                    wordnet  \n",
       "50                    wordnet  \n",
       "51                    wordnet  \n",
       "52                    wordnet  \n",
       "53                    wordnet  \n",
       "54                    dbpedia  \n",
       "55                    dbpedia  \n",
       "56                    dbpedia  \n",
       "57                    dbpedia  \n",
       "58                    dbpedia  \n",
       "59                    dbpedia  \n",
       "60                    dbpedia  \n",
       "61                    dbpedia  \n",
       "62                    dbpedia  \n",
       "63           brown_clustering  \n",
       "64           brown_clustering  \n",
       "65           brown_clustering  \n",
       "66           brown_clustering  \n",
       "67           brown_clustering  \n",
       "68           brown_clustering  \n",
       "69           brown_clustering  \n",
       "70           brown_clustering  \n",
       "71           brown_clustering  \n",
       "72                   semantic  \n",
       "73                   semantic  \n",
       "74                   semantic  \n",
       "75                   semantic  \n",
       "76                   semantic  \n",
       "77                   semantic  \n",
       "78                   semantic  \n",
       "79                   semantic  \n",
       "80                   semantic  \n",
       "81                 dictionary  \n",
       "82                 dictionary  \n",
       "83                 dictionary  \n",
       "84                 dictionary  \n",
       "85                 dictionary  \n",
       "86                 dictionary  \n",
       "87                 dictionary  \n",
       "88                 dictionary  \n",
       "89                 dictionary  \n",
       "90            corpus+semantic  \n",
       "91            corpus+semantic  \n",
       "92            corpus+semantic  \n",
       "93            corpus+semantic  \n",
       "94            corpus+semantic  \n",
       "95            corpus+semantic  \n",
       "96            corpus+semantic  \n",
       "97            corpus+semantic  \n",
       "98            corpus+semantic  \n",
       "99   wordnet+psycholinguistic  \n",
       "100  wordnet+psycholinguistic  \n",
       "101  wordnet+psycholinguistic  \n",
       "102  wordnet+psycholinguistic  \n",
       "103  wordnet+psycholinguistic  \n",
       "104  wordnet+psycholinguistic  \n",
       "105  wordnet+psycholinguistic  \n",
       "106  wordnet+psycholinguistic  \n",
       "107  wordnet+psycholinguistic  \n",
       "108                       all  \n",
       "109                       all  \n",
       "110                       all  \n",
       "111                       all  \n",
       "112                       all  \n",
       "113                       all  \n",
       "114                       all  \n",
       "115                       all  \n",
       "116                       all  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_eval_data_rf = create_eval_df_from_results_phrase(results_rf)\n",
    "feature_eval_data_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.598360</td>\n",
       "      <td>0.598784</td>\n",
       "      <td>0.635229</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.571096</td>\n",
       "      <td>0.566362</td>\n",
       "      <td>0.583977</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.627367</td>\n",
       "      <td>0.621614</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            agg    dataset        f1      prec       rec  \\\n",
       "15    phrase_mv       News  0.598360  0.598784  0.635229   \n",
       "19   phrase_max  Wikipedia  0.571096  0.566362  0.583977   \n",
       "103  phrase_max   WikiNews  0.627367  0.621614  0.638889   \n",
       "\n",
       "                           zc  \n",
       "15                  frequency  \n",
       "19             language_model  \n",
       "103  wordnet+psycholinguistic  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = feature_eval_data_rf.groupby(['dataset'])['f1'].transform(max) == feature_eval_data_rf['f1']\n",
    "feature_eval_data_rf[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.3 Random Forest (Extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "results_rfe = [Result(fs, fs.fc, agg, phrase_merger(fs.test, random_forest_extra(*transform_feat_to_num(\n",
    "                remove_labels_for_binary_df(fs.train), \n",
    "                remove_labels_phrase_for_binary_df(fs.test))), agg.agg)) \n",
    "          for fs in all_fc_datasets\n",
    "          for agg in phrase_aggs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.457851</td>\n",
       "      <td>0.510898</td>\n",
       "      <td>0.520270</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.484375</td>\n",
       "      <td>0.514161</td>\n",
       "      <td>0.525097</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.283998</td>\n",
       "      <td>0.515525</td>\n",
       "      <td>0.516409</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.536054</td>\n",
       "      <td>0.575283</td>\n",
       "      <td>0.604798</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.583854</td>\n",
       "      <td>0.590695</td>\n",
       "      <td>0.621843</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.335613</td>\n",
       "      <td>0.553571</td>\n",
       "      <td>0.537879</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.498557</td>\n",
       "      <td>0.545343</td>\n",
       "      <td>0.567354</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.471774</td>\n",
       "      <td>0.487513</td>\n",
       "      <td>0.482750</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.274754</td>\n",
       "      <td>0.569492</td>\n",
       "      <td>0.533114</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.465695</td>\n",
       "      <td>0.514583</td>\n",
       "      <td>0.527027</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.507136</td>\n",
       "      <td>0.518822</td>\n",
       "      <td>0.529923</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.424710</td>\n",
       "      <td>0.424710</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.493137</td>\n",
       "      <td>0.526268</td>\n",
       "      <td>0.536616</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.575940</td>\n",
       "      <td>0.576503</td>\n",
       "      <td>0.597222</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.381859</td>\n",
       "      <td>0.576023</td>\n",
       "      <td>0.565657</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.583145</td>\n",
       "      <td>0.587997</td>\n",
       "      <td>0.623093</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.492903</td>\n",
       "      <td>0.494694</td>\n",
       "      <td>0.493932</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.337523</td>\n",
       "      <td>0.568867</td>\n",
       "      <td>0.554092</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.537235</td>\n",
       "      <td>0.551500</td>\n",
       "      <td>0.587838</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.562552</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.577220</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.237750</td>\n",
       "      <td>0.416321</td>\n",
       "      <td>0.402510</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.502520</td>\n",
       "      <td>0.525183</td>\n",
       "      <td>0.534722</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.544560</td>\n",
       "      <td>0.543671</td>\n",
       "      <td>0.549874</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.372269</td>\n",
       "      <td>0.528050</td>\n",
       "      <td>0.527146</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.565108</td>\n",
       "      <td>0.578102</td>\n",
       "      <td>0.612604</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.493450</td>\n",
       "      <td>0.496272</td>\n",
       "      <td>0.495579</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.329865</td>\n",
       "      <td>0.554409</td>\n",
       "      <td>0.542736</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.497143</td>\n",
       "      <td>0.529915</td>\n",
       "      <td>0.554054</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.522366</td>\n",
       "      <td>0.528571</td>\n",
       "      <td>0.543436</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.271222</td>\n",
       "      <td>0.460317</td>\n",
       "      <td>0.451737</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.501419</td>\n",
       "      <td>0.531293</td>\n",
       "      <td>0.543561</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.582940</td>\n",
       "      <td>0.580106</td>\n",
       "      <td>0.595328</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>0.593434</td>\n",
       "      <td>0.593434</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.576974</td>\n",
       "      <td>0.581944</td>\n",
       "      <td>0.614164</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.489572</td>\n",
       "      <td>0.490863</td>\n",
       "      <td>0.489858</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.350198</td>\n",
       "      <td>0.563708</td>\n",
       "      <td>0.554872</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.493590</td>\n",
       "      <td>0.488417</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.495290</td>\n",
       "      <td>0.491313</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.225673</td>\n",
       "      <td>0.476923</td>\n",
       "      <td>0.482625</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.505090</td>\n",
       "      <td>0.568531</td>\n",
       "      <td>0.592803</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.588493</td>\n",
       "      <td>0.605791</td>\n",
       "      <td>0.646465</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.281944</td>\n",
       "      <td>0.559593</td>\n",
       "      <td>0.525884</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.499197</td>\n",
       "      <td>0.563432</td>\n",
       "      <td>0.592580</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.477924</td>\n",
       "      <td>0.507663</td>\n",
       "      <td>0.511269</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.240012</td>\n",
       "      <td>0.575173</td>\n",
       "      <td>0.522625</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>0.531034</td>\n",
       "      <td>0.552124</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.518541</td>\n",
       "      <td>0.520611</td>\n",
       "      <td>0.527992</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.327374</td>\n",
       "      <td>0.534325</td>\n",
       "      <td>0.543436</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.516176</td>\n",
       "      <td>0.547554</td>\n",
       "      <td>0.566288</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.609847</td>\n",
       "      <td>0.605040</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.357045</td>\n",
       "      <td>0.595536</td>\n",
       "      <td>0.567551</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.570693</td>\n",
       "      <td>0.586250</td>\n",
       "      <td>0.625607</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.495944</td>\n",
       "      <td>0.498304</td>\n",
       "      <td>0.498006</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.332728</td>\n",
       "      <td>0.578830</td>\n",
       "      <td>0.558166</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.180375</td>\n",
       "      <td>0.582353</td>\n",
       "      <td>0.520270</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.207621</td>\n",
       "      <td>0.584337</td>\n",
       "      <td>0.533784</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.151947</td>\n",
       "      <td>0.580460</td>\n",
       "      <td>0.506757</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.234690</td>\n",
       "      <td>0.620879</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.263133</td>\n",
       "      <td>0.623596</td>\n",
       "      <td>0.534722</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.189655</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.176101</td>\n",
       "      <td>0.106870</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.181487</td>\n",
       "      <td>0.607280</td>\n",
       "      <td>0.502427</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.176101</td>\n",
       "      <td>0.106870</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.477329</td>\n",
       "      <td>0.541860</td>\n",
       "      <td>0.578185</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.497143</td>\n",
       "      <td>0.529915</td>\n",
       "      <td>0.554054</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.238538</td>\n",
       "      <td>0.457949</td>\n",
       "      <td>0.460425</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.483093</td>\n",
       "      <td>0.546620</td>\n",
       "      <td>0.563131</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.556384</td>\n",
       "      <td>0.579091</td>\n",
       "      <td>0.609848</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.268610</td>\n",
       "      <td>0.549261</td>\n",
       "      <td>0.518939</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.575912</td>\n",
       "      <td>0.594414</td>\n",
       "      <td>0.638610</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.510779</td>\n",
       "      <td>0.515074</td>\n",
       "      <td>0.519157</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.308099</td>\n",
       "      <td>0.555212</td>\n",
       "      <td>0.537101</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.556322</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.530130</td>\n",
       "      <td>0.533854</td>\n",
       "      <td>0.550193</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.314997</td>\n",
       "      <td>0.505882</td>\n",
       "      <td>0.507722</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.507734</td>\n",
       "      <td>0.542553</td>\n",
       "      <td>0.559343</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.619008</td>\n",
       "      <td>0.614183</td>\n",
       "      <td>0.640783</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.357045</td>\n",
       "      <td>0.595536</td>\n",
       "      <td>0.567551</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.565108</td>\n",
       "      <td>0.578102</td>\n",
       "      <td>0.612604</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.477546</td>\n",
       "      <td>0.480878</td>\n",
       "      <td>0.477722</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.333392</td>\n",
       "      <td>0.567117</td>\n",
       "      <td>0.551664</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.468082</td>\n",
       "      <td>0.526357</td>\n",
       "      <td>0.549228</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.482353</td>\n",
       "      <td>0.496278</td>\n",
       "      <td>0.494208</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.202899</td>\n",
       "      <td>0.420513</td>\n",
       "      <td>0.440154</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.434259</td>\n",
       "      <td>0.509579</td>\n",
       "      <td>0.512626</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.468159</td>\n",
       "      <td>0.511338</td>\n",
       "      <td>0.515783</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.286240</td>\n",
       "      <td>0.519048</td>\n",
       "      <td>0.510101</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.389206</td>\n",
       "      <td>0.513408</td>\n",
       "      <td>0.516557</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.373139</td>\n",
       "      <td>0.443142</td>\n",
       "      <td>0.417389</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.244867</td>\n",
       "      <td>0.578003</td>\n",
       "      <td>0.525052</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.505062</td>\n",
       "      <td>0.533962</td>\n",
       "      <td>0.560811</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.530130</td>\n",
       "      <td>0.533854</td>\n",
       "      <td>0.550193</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.292164</td>\n",
       "      <td>0.473529</td>\n",
       "      <td>0.465251</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.533035</td>\n",
       "      <td>0.557727</td>\n",
       "      <td>0.580177</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.610282</td>\n",
       "      <td>0.606855</td>\n",
       "      <td>0.633838</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.436106</td>\n",
       "      <td>0.597367</td>\n",
       "      <td>0.600379</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.586171</td>\n",
       "      <td>0.590095</td>\n",
       "      <td>0.625520</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.494429</td>\n",
       "      <td>0.495134</td>\n",
       "      <td>0.494712</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.338065</td>\n",
       "      <td>0.558299</td>\n",
       "      <td>0.547590</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.529097</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>0.581081</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.546032</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.563707</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.260504</td>\n",
       "      <td>0.452775</td>\n",
       "      <td>0.444981</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.516176</td>\n",
       "      <td>0.547554</td>\n",
       "      <td>0.566288</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.592732</td>\n",
       "      <td>0.590118</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.403986</td>\n",
       "      <td>0.585135</td>\n",
       "      <td>0.579545</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.580042</td>\n",
       "      <td>0.589865</td>\n",
       "      <td>0.628814</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.514126</td>\n",
       "      <td>0.515424</td>\n",
       "      <td>0.518291</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.344569</td>\n",
       "      <td>0.595435</td>\n",
       "      <td>0.571949</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.489255</td>\n",
       "      <td>0.525967</td>\n",
       "      <td>0.547297</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.492219</td>\n",
       "      <td>0.509936</td>\n",
       "      <td>0.516409</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.302404</td>\n",
       "      <td>0.479389</td>\n",
       "      <td>0.472008</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.541463</td>\n",
       "      <td>0.562927</td>\n",
       "      <td>0.587121</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.627367</td>\n",
       "      <td>0.621614</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.392999</td>\n",
       "      <td>0.580702</td>\n",
       "      <td>0.572601</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.574116</td>\n",
       "      <td>0.581863</td>\n",
       "      <td>0.615811</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.492903</td>\n",
       "      <td>0.494694</td>\n",
       "      <td>0.493932</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.353823</td>\n",
       "      <td>0.575371</td>\n",
       "      <td>0.563800</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            agg    dataset        f1      prec       rec  \\\n",
       "0     phrase_mv  Wikipedia  0.457851  0.510898  0.520270   \n",
       "1    phrase_max  Wikipedia  0.484375  0.514161  0.525097   \n",
       "2    phrase_min  Wikipedia  0.283998  0.515525  0.516409   \n",
       "3     phrase_mv   WikiNews  0.536054  0.575283  0.604798   \n",
       "4    phrase_max   WikiNews  0.583854  0.590695  0.621843   \n",
       "5    phrase_min   WikiNews  0.335613  0.553571  0.537879   \n",
       "6     phrase_mv       News  0.498557  0.545343  0.567354   \n",
       "7    phrase_max       News  0.471774  0.487513  0.482750   \n",
       "8    phrase_min       News  0.274754  0.569492  0.533114   \n",
       "9     phrase_mv  Wikipedia  0.465695  0.514583  0.527027   \n",
       "10   phrase_max  Wikipedia  0.507136  0.518822  0.529923   \n",
       "11   phrase_min  Wikipedia  0.227273  0.424710  0.424710   \n",
       "12    phrase_mv   WikiNews  0.493137  0.526268  0.536616   \n",
       "13   phrase_max   WikiNews  0.575940  0.576503  0.597222   \n",
       "14   phrase_min   WikiNews  0.381859  0.576023  0.565657   \n",
       "15    phrase_mv       News  0.583145  0.587997  0.623093   \n",
       "16   phrase_max       News  0.492903  0.494694  0.493932   \n",
       "17   phrase_min       News  0.337523  0.568867  0.554092   \n",
       "18    phrase_mv  Wikipedia  0.537235  0.551500  0.587838   \n",
       "19   phrase_max  Wikipedia  0.562552  0.558824  0.577220   \n",
       "20   phrase_min  Wikipedia  0.237750  0.416321  0.402510   \n",
       "21    phrase_mv   WikiNews  0.502520  0.525183  0.534722   \n",
       "22   phrase_max   WikiNews  0.544560  0.543671  0.549874   \n",
       "23   phrase_min   WikiNews  0.372269  0.528050  0.527146   \n",
       "24    phrase_mv       News  0.565108  0.578102  0.612604   \n",
       "25   phrase_max       News  0.493450  0.496272  0.495579   \n",
       "26   phrase_min       News  0.329865  0.554409  0.542736   \n",
       "27    phrase_mv  Wikipedia  0.497143  0.529915  0.554054   \n",
       "28   phrase_max  Wikipedia  0.522366  0.528571  0.543436   \n",
       "29   phrase_min  Wikipedia  0.271222  0.460317  0.451737   \n",
       "30    phrase_mv   WikiNews  0.501419  0.531293  0.543561   \n",
       "31   phrase_max   WikiNews  0.582940  0.580106  0.595328   \n",
       "32   phrase_min   WikiNews  0.425532  0.593434  0.593434   \n",
       "33    phrase_mv       News  0.576974  0.581944  0.614164   \n",
       "34   phrase_max       News  0.489572  0.490863  0.489858   \n",
       "35   phrase_min       News  0.350198  0.563708  0.554872   \n",
       "36    phrase_mv  Wikipedia  0.393939  0.493590  0.488417   \n",
       "37   phrase_max  Wikipedia  0.446541  0.495290  0.491313   \n",
       "38   phrase_min  Wikipedia  0.225673  0.476923  0.482625   \n",
       "39    phrase_mv   WikiNews  0.505090  0.568531  0.592803   \n",
       "40   phrase_max   WikiNews  0.588493  0.605791  0.646465   \n",
       "41   phrase_min   WikiNews  0.281944  0.559593  0.525884   \n",
       "42    phrase_mv       News  0.499197  0.563432  0.592580   \n",
       "43   phrase_max       News  0.477924  0.507663  0.511269   \n",
       "44   phrase_min       News  0.240012  0.575173  0.522625   \n",
       "45    phrase_mv  Wikipedia  0.515152  0.531034  0.552124   \n",
       "46   phrase_max  Wikipedia  0.518541  0.520611  0.527992   \n",
       "47   phrase_min  Wikipedia  0.327374  0.534325  0.543436   \n",
       "48    phrase_mv   WikiNews  0.516176  0.547554  0.566288   \n",
       "49   phrase_max   WikiNews  0.609847  0.605040  0.625000   \n",
       "50   phrase_min   WikiNews  0.357045  0.595536  0.567551   \n",
       "51    phrase_mv       News  0.570693  0.586250  0.625607   \n",
       "52   phrase_max       News  0.495944  0.498304  0.498006   \n",
       "53   phrase_min       News  0.332728  0.578830  0.558166   \n",
       "54    phrase_mv  Wikipedia  0.180375  0.582353  0.520270   \n",
       "55   phrase_max  Wikipedia  0.207621  0.584337  0.533784   \n",
       "56   phrase_min  Wikipedia  0.151947  0.580460  0.506757   \n",
       "57    phrase_mv   WikiNews  0.234690  0.620879  0.520833   \n",
       "58   phrase_max   WikiNews  0.263133  0.623596  0.534722   \n",
       "59   phrase_min   WikiNews  0.189655  0.117021  0.500000   \n",
       "60    phrase_mv       News  0.176101  0.106870  0.500000   \n",
       "61   phrase_max       News  0.181487  0.607280  0.502427   \n",
       "62   phrase_min       News  0.176101  0.106870  0.500000   \n",
       "63    phrase_mv  Wikipedia  0.477329  0.541860  0.578185   \n",
       "64   phrase_max  Wikipedia  0.497143  0.529915  0.554054   \n",
       "65   phrase_min  Wikipedia  0.238538  0.457949  0.460425   \n",
       "66    phrase_mv   WikiNews  0.483093  0.546620  0.563131   \n",
       "67   phrase_max   WikiNews  0.556384  0.579091  0.609848   \n",
       "68   phrase_min   WikiNews  0.268610  0.549261  0.518939   \n",
       "69    phrase_mv       News  0.575912  0.594414  0.638610   \n",
       "70   phrase_max       News  0.510779  0.515074  0.519157   \n",
       "71   phrase_min       News  0.308099  0.555212  0.537101   \n",
       "72    phrase_mv  Wikipedia  0.545455  0.556322  0.594595   \n",
       "73   phrase_max  Wikipedia  0.530130  0.533854  0.550193   \n",
       "74   phrase_min  Wikipedia  0.314997  0.505882  0.507722   \n",
       "75    phrase_mv   WikiNews  0.507734  0.542553  0.559343   \n",
       "76   phrase_max   WikiNews  0.619008  0.614183  0.640783   \n",
       "77   phrase_min   WikiNews  0.357045  0.595536  0.567551   \n",
       "78    phrase_mv       News  0.565108  0.578102  0.612604   \n",
       "79   phrase_max       News  0.477546  0.480878  0.477722   \n",
       "80   phrase_min       News  0.333392  0.567117  0.551664   \n",
       "81    phrase_mv  Wikipedia  0.468082  0.526357  0.549228   \n",
       "82   phrase_max  Wikipedia  0.482353  0.496278  0.494208   \n",
       "83   phrase_min  Wikipedia  0.202899  0.420513  0.440154   \n",
       "84    phrase_mv   WikiNews  0.434259  0.509579  0.512626   \n",
       "85   phrase_max   WikiNews  0.468159  0.511338  0.515783   \n",
       "86   phrase_min   WikiNews  0.286240  0.519048  0.510101   \n",
       "87    phrase_mv       News  0.389206  0.513408  0.516557   \n",
       "88   phrase_max       News  0.373139  0.443142  0.417389   \n",
       "89   phrase_min       News  0.244867  0.578003  0.525052   \n",
       "90    phrase_mv  Wikipedia  0.505062  0.533962  0.560811   \n",
       "91   phrase_max  Wikipedia  0.530130  0.533854  0.550193   \n",
       "92   phrase_min  Wikipedia  0.292164  0.473529  0.465251   \n",
       "93    phrase_mv   WikiNews  0.533035  0.557727  0.580177   \n",
       "94   phrase_max   WikiNews  0.610282  0.606855  0.633838   \n",
       "95   phrase_min   WikiNews  0.436106  0.597367  0.600379   \n",
       "96    phrase_mv       News  0.586171  0.590095  0.625520   \n",
       "97   phrase_max       News  0.494429  0.495134  0.494712   \n",
       "98   phrase_min       News  0.338065  0.558299  0.547590   \n",
       "99    phrase_mv  Wikipedia  0.529097  0.546875  0.581081   \n",
       "100  phrase_max  Wikipedia  0.546032  0.545455  0.563707   \n",
       "101  phrase_min  Wikipedia  0.260504  0.452775  0.444981   \n",
       "102   phrase_mv   WikiNews  0.516176  0.547554  0.566288   \n",
       "103  phrase_max   WikiNews  0.592732  0.590118  0.611111   \n",
       "104  phrase_min   WikiNews  0.403986  0.585135  0.579545   \n",
       "105   phrase_mv       News  0.580042  0.589865  0.628814   \n",
       "106  phrase_max       News  0.514126  0.515424  0.518291   \n",
       "107  phrase_min       News  0.344569  0.595435  0.571949   \n",
       "108   phrase_mv  Wikipedia  0.489255  0.525967  0.547297   \n",
       "109  phrase_max  Wikipedia  0.492219  0.509936  0.516409   \n",
       "110  phrase_min  Wikipedia  0.302404  0.479389  0.472008   \n",
       "111   phrase_mv   WikiNews  0.541463  0.562927  0.587121   \n",
       "112  phrase_max   WikiNews  0.627367  0.621614  0.638889   \n",
       "113  phrase_min   WikiNews  0.392999  0.580702  0.572601   \n",
       "114   phrase_mv       News  0.574116  0.581863  0.615811   \n",
       "115  phrase_max       News  0.492903  0.494694  0.493932   \n",
       "116  phrase_min       News  0.353823  0.575371  0.563800   \n",
       "\n",
       "                           zc  \n",
       "0                  linguistic  \n",
       "1                  linguistic  \n",
       "2                  linguistic  \n",
       "3                  linguistic  \n",
       "4                  linguistic  \n",
       "5                  linguistic  \n",
       "6                  linguistic  \n",
       "7                  linguistic  \n",
       "8                  linguistic  \n",
       "9                   frequency  \n",
       "10                  frequency  \n",
       "11                  frequency  \n",
       "12                  frequency  \n",
       "13                  frequency  \n",
       "14                  frequency  \n",
       "15                  frequency  \n",
       "16                  frequency  \n",
       "17                  frequency  \n",
       "18             language_model  \n",
       "19             language_model  \n",
       "20             language_model  \n",
       "21             language_model  \n",
       "22             language_model  \n",
       "23             language_model  \n",
       "24             language_model  \n",
       "25             language_model  \n",
       "26             language_model  \n",
       "27                     corpus  \n",
       "28                     corpus  \n",
       "29                     corpus  \n",
       "30                     corpus  \n",
       "31                     corpus  \n",
       "32                     corpus  \n",
       "33                     corpus  \n",
       "34                     corpus  \n",
       "35                     corpus  \n",
       "36           psycholinguistic  \n",
       "37           psycholinguistic  \n",
       "38           psycholinguistic  \n",
       "39           psycholinguistic  \n",
       "40           psycholinguistic  \n",
       "41           psycholinguistic  \n",
       "42           psycholinguistic  \n",
       "43           psycholinguistic  \n",
       "44           psycholinguistic  \n",
       "45                    wordnet  \n",
       "46                    wordnet  \n",
       "47                    wordnet  \n",
       "48                    wordnet  \n",
       "49                    wordnet  \n",
       "50                    wordnet  \n",
       "51                    wordnet  \n",
       "52                    wordnet  \n",
       "53                    wordnet  \n",
       "54                    dbpedia  \n",
       "55                    dbpedia  \n",
       "56                    dbpedia  \n",
       "57                    dbpedia  \n",
       "58                    dbpedia  \n",
       "59                    dbpedia  \n",
       "60                    dbpedia  \n",
       "61                    dbpedia  \n",
       "62                    dbpedia  \n",
       "63           brown_clustering  \n",
       "64           brown_clustering  \n",
       "65           brown_clustering  \n",
       "66           brown_clustering  \n",
       "67           brown_clustering  \n",
       "68           brown_clustering  \n",
       "69           brown_clustering  \n",
       "70           brown_clustering  \n",
       "71           brown_clustering  \n",
       "72                   semantic  \n",
       "73                   semantic  \n",
       "74                   semantic  \n",
       "75                   semantic  \n",
       "76                   semantic  \n",
       "77                   semantic  \n",
       "78                   semantic  \n",
       "79                   semantic  \n",
       "80                   semantic  \n",
       "81                 dictionary  \n",
       "82                 dictionary  \n",
       "83                 dictionary  \n",
       "84                 dictionary  \n",
       "85                 dictionary  \n",
       "86                 dictionary  \n",
       "87                 dictionary  \n",
       "88                 dictionary  \n",
       "89                 dictionary  \n",
       "90            corpus+semantic  \n",
       "91            corpus+semantic  \n",
       "92            corpus+semantic  \n",
       "93            corpus+semantic  \n",
       "94            corpus+semantic  \n",
       "95            corpus+semantic  \n",
       "96            corpus+semantic  \n",
       "97            corpus+semantic  \n",
       "98            corpus+semantic  \n",
       "99   wordnet+psycholinguistic  \n",
       "100  wordnet+psycholinguistic  \n",
       "101  wordnet+psycholinguistic  \n",
       "102  wordnet+psycholinguistic  \n",
       "103  wordnet+psycholinguistic  \n",
       "104  wordnet+psycholinguistic  \n",
       "105  wordnet+psycholinguistic  \n",
       "106  wordnet+psycholinguistic  \n",
       "107  wordnet+psycholinguistic  \n",
       "108                       all  \n",
       "109                       all  \n",
       "110                       all  \n",
       "111                       all  \n",
       "112                       all  \n",
       "113                       all  \n",
       "114                       all  \n",
       "115                       all  \n",
       "116                       all  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_eval_data_rfe = create_eval_df_from_results_phrase(results_rfe)\n",
    "feature_eval_data_rfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.562552</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.577220</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.586171</td>\n",
       "      <td>0.590095</td>\n",
       "      <td>0.625520</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.627367</td>\n",
       "      <td>0.621614</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            agg    dataset        f1      prec       rec               zc\n",
       "19   phrase_max  Wikipedia  0.562552  0.558824  0.577220   language_model\n",
       "96    phrase_mv       News  0.586171  0.590095  0.625520  corpus+semantic\n",
       "112  phrase_max   WikiNews  0.627367  0.621614  0.638889              all"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = feature_eval_data_rfe.groupby(['dataset'])['f1'].transform(max) == feature_eval_data_rfe['f1']\n",
    "feature_eval_data_rfe[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.4 AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ada = [Result(fs, fs.fc, agg, phrase_merger(fs.test, adaboost(*transform_feat_to_num(\n",
    "                remove_labels_for_binary_df(fs.train), \n",
    "                remove_labels_phrase_for_binary_df(fs.test))), agg.agg)) \n",
    "          for fs in all_fc_datasets\n",
    "          for agg in phrase_aggs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.473539</td>\n",
       "      <td>0.518315</td>\n",
       "      <td>0.533784</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.461444</td>\n",
       "      <td>0.484220</td>\n",
       "      <td>0.473938</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.306729</td>\n",
       "      <td>0.555708</td>\n",
       "      <td>0.558880</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.579732</td>\n",
       "      <td>0.600455</td>\n",
       "      <td>0.639520</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.609847</td>\n",
       "      <td>0.605040</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.314181</td>\n",
       "      <td>0.511607</td>\n",
       "      <td>0.508207</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.475323</td>\n",
       "      <td>0.511454</td>\n",
       "      <td>0.516990</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.475234</td>\n",
       "      <td>0.487790</td>\n",
       "      <td>0.483530</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.316629</td>\n",
       "      <td>0.559489</td>\n",
       "      <td>0.541956</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.547059</td>\n",
       "      <td>0.550868</td>\n",
       "      <td>0.579151</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.598174</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.604247</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.272351</td>\n",
       "      <td>0.482639</td>\n",
       "      <td>0.480695</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.551194</td>\n",
       "      <td>0.557950</td>\n",
       "      <td>0.576389</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.608333</td>\n",
       "      <td>0.604072</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.444545</td>\n",
       "      <td>0.564935</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.573984</td>\n",
       "      <td>0.579879</td>\n",
       "      <td>0.611737</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.508523</td>\n",
       "      <td>0.509045</td>\n",
       "      <td>0.510142</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.345717</td>\n",
       "      <td>0.572210</td>\n",
       "      <td>0.558946</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.522994</td>\n",
       "      <td>0.535652</td>\n",
       "      <td>0.558880</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.510567</td>\n",
       "      <td>0.510771</td>\n",
       "      <td>0.512548</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.215808</td>\n",
       "      <td>0.412821</td>\n",
       "      <td>0.417954</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.478229</td>\n",
       "      <td>0.509977</td>\n",
       "      <td>0.513889</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.556033</td>\n",
       "      <td>0.555172</td>\n",
       "      <td>0.565657</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.347445</td>\n",
       "      <td>0.559916</td>\n",
       "      <td>0.544823</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.547359</td>\n",
       "      <td>0.564674</td>\n",
       "      <td>0.593967</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.461210</td>\n",
       "      <td>0.464903</td>\n",
       "      <td>0.459865</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.317389</td>\n",
       "      <td>0.548050</td>\n",
       "      <td>0.535454</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.530917</td>\n",
       "      <td>0.540476</td>\n",
       "      <td>0.565637</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.557991</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.561776</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.302404</td>\n",
       "      <td>0.479389</td>\n",
       "      <td>0.472008</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.526785</td>\n",
       "      <td>0.541026</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.564058</td>\n",
       "      <td>0.562229</td>\n",
       "      <td>0.572601</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.413233</td>\n",
       "      <td>0.534273</td>\n",
       "      <td>0.539141</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.592054</td>\n",
       "      <td>0.592702</td>\n",
       "      <td>0.626300</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.504242</td>\n",
       "      <td>0.504258</td>\n",
       "      <td>0.504421</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.349781</td>\n",
       "      <td>0.573812</td>\n",
       "      <td>0.561373</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.369697</td>\n",
       "      <td>0.470085</td>\n",
       "      <td>0.445946</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.392545</td>\n",
       "      <td>0.449241</td>\n",
       "      <td>0.406371</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.237750</td>\n",
       "      <td>0.487013</td>\n",
       "      <td>0.489382</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.532006</td>\n",
       "      <td>0.582418</td>\n",
       "      <td>0.613636</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.582222</td>\n",
       "      <td>0.595260</td>\n",
       "      <td>0.630682</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.295027</td>\n",
       "      <td>0.567974</td>\n",
       "      <td>0.532828</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.508892</td>\n",
       "      <td>0.576010</td>\n",
       "      <td>0.610437</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.503812</td>\n",
       "      <td>0.530828</td>\n",
       "      <td>0.545336</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.230197</td>\n",
       "      <td>0.568333</td>\n",
       "      <td>0.517770</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.499666</td>\n",
       "      <td>0.522321</td>\n",
       "      <td>0.538610</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.571096</td>\n",
       "      <td>0.566362</td>\n",
       "      <td>0.583977</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.293996</td>\n",
       "      <td>0.495238</td>\n",
       "      <td>0.494208</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.517949</td>\n",
       "      <td>0.541496</td>\n",
       "      <td>0.557449</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.580357</td>\n",
       "      <td>0.577489</td>\n",
       "      <td>0.586490</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.403175</td>\n",
       "      <td>0.610380</td>\n",
       "      <td>0.595328</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.532637</td>\n",
       "      <td>0.555739</td>\n",
       "      <td>0.581831</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.481068</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.483443</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.333392</td>\n",
       "      <td>0.567117</td>\n",
       "      <td>0.551664</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.358333</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.563707</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.416188</td>\n",
       "      <td>0.480879</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.478764</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.505090</td>\n",
       "      <td>0.568531</td>\n",
       "      <td>0.592803</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.653248</td>\n",
       "      <td>0.655903</td>\n",
       "      <td>0.650884</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.320482</td>\n",
       "      <td>0.581051</td>\n",
       "      <td>0.546717</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.455157</td>\n",
       "      <td>0.571120</td>\n",
       "      <td>0.592753</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.489511</td>\n",
       "      <td>0.522424</td>\n",
       "      <td>0.533200</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.280645</td>\n",
       "      <td>0.554596</td>\n",
       "      <td>0.529040</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.427278</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.515444</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.469206</td>\n",
       "      <td>0.506410</td>\n",
       "      <td>0.511583</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.165172</td>\n",
       "      <td>0.353616</td>\n",
       "      <td>0.419884</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.430303</td>\n",
       "      <td>0.497650</td>\n",
       "      <td>0.496843</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.526210</td>\n",
       "      <td>0.546703</td>\n",
       "      <td>0.564394</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.263133</td>\n",
       "      <td>0.623596</td>\n",
       "      <td>0.534722</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.504570</td>\n",
       "      <td>0.548566</td>\n",
       "      <td>0.572209</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.464044</td>\n",
       "      <td>0.482463</td>\n",
       "      <td>0.475468</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.257440</td>\n",
       "      <td>0.490548</td>\n",
       "      <td>0.494972</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.507379</td>\n",
       "      <td>0.526599</td>\n",
       "      <td>0.545367</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.538013</td>\n",
       "      <td>0.539465</td>\n",
       "      <td>0.556950</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.306012</td>\n",
       "      <td>0.525684</td>\n",
       "      <td>0.529923</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.606095</td>\n",
       "      <td>0.616889</td>\n",
       "      <td>0.660354</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.654623</td>\n",
       "      <td>0.650595</td>\n",
       "      <td>0.659722</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.489130</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.549947</td>\n",
       "      <td>0.571209</td>\n",
       "      <td>0.604542</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.491997</td>\n",
       "      <td>0.492977</td>\n",
       "      <td>0.492285</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.304693</td>\n",
       "      <td>0.540906</td>\n",
       "      <td>0.528173</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.410714</td>\n",
       "      <td>0.501053</td>\n",
       "      <td>0.501931</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.420215</td>\n",
       "      <td>0.462803</td>\n",
       "      <td>0.433398</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.190569</td>\n",
       "      <td>0.402954</td>\n",
       "      <td>0.433398</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.434259</td>\n",
       "      <td>0.509579</td>\n",
       "      <td>0.512626</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.468159</td>\n",
       "      <td>0.511338</td>\n",
       "      <td>0.515783</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.286240</td>\n",
       "      <td>0.519048</td>\n",
       "      <td>0.510101</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.389206</td>\n",
       "      <td>0.513408</td>\n",
       "      <td>0.516557</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.373139</td>\n",
       "      <td>0.443142</td>\n",
       "      <td>0.417389</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.240012</td>\n",
       "      <td>0.575173</td>\n",
       "      <td>0.522625</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>0.531034</td>\n",
       "      <td>0.552124</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.533800</td>\n",
       "      <td>0.532799</td>\n",
       "      <td>0.541506</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.318903</td>\n",
       "      <td>0.471429</td>\n",
       "      <td>0.456564</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.549899</td>\n",
       "      <td>0.568223</td>\n",
       "      <td>0.594066</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.564058</td>\n",
       "      <td>0.562229</td>\n",
       "      <td>0.572601</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>0.593434</td>\n",
       "      <td>0.593434</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.564639</td>\n",
       "      <td>0.582563</td>\n",
       "      <td>0.620752</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.481129</td>\n",
       "      <td>0.488268</td>\n",
       "      <td>0.485090</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.333976</td>\n",
       "      <td>0.556385</td>\n",
       "      <td>0.545163</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.556322</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.562552</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.577220</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.271222</td>\n",
       "      <td>0.460317</td>\n",
       "      <td>0.451737</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.541463</td>\n",
       "      <td>0.562927</td>\n",
       "      <td>0.587121</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.625842</td>\n",
       "      <td>0.622619</td>\n",
       "      <td>0.630051</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.414827</td>\n",
       "      <td>0.589367</td>\n",
       "      <td>0.586490</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.558930</td>\n",
       "      <td>0.567770</td>\n",
       "      <td>0.595527</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.487996</td>\n",
       "      <td>0.490625</td>\n",
       "      <td>0.489078</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.333392</td>\n",
       "      <td>0.567117</td>\n",
       "      <td>0.551664</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.461647</td>\n",
       "      <td>0.502650</td>\n",
       "      <td>0.504826</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.530130</td>\n",
       "      <td>0.533854</td>\n",
       "      <td>0.550193</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.302404</td>\n",
       "      <td>0.479389</td>\n",
       "      <td>0.472008</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.576127</td>\n",
       "      <td>0.580460</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.634810</td>\n",
       "      <td>0.632884</td>\n",
       "      <td>0.636995</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.425272</td>\n",
       "      <td>0.573214</td>\n",
       "      <td>0.577652</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.544409</td>\n",
       "      <td>0.562857</td>\n",
       "      <td>0.591540</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.514126</td>\n",
       "      <td>0.515424</td>\n",
       "      <td>0.518291</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.333392</td>\n",
       "      <td>0.567117</td>\n",
       "      <td>0.551664</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            agg    dataset        f1      prec       rec  \\\n",
       "0     phrase_mv  Wikipedia  0.473539  0.518315  0.533784   \n",
       "1    phrase_max  Wikipedia  0.461444  0.484220  0.473938   \n",
       "2    phrase_min  Wikipedia  0.306729  0.555708  0.558880   \n",
       "3     phrase_mv   WikiNews  0.579732  0.600455  0.639520   \n",
       "4    phrase_max   WikiNews  0.609847  0.605040  0.625000   \n",
       "5    phrase_min   WikiNews  0.314181  0.511607  0.508207   \n",
       "6     phrase_mv       News  0.475323  0.511454  0.516990   \n",
       "7    phrase_max       News  0.475234  0.487790  0.483530   \n",
       "8    phrase_min       News  0.316629  0.559489  0.541956   \n",
       "9     phrase_mv  Wikipedia  0.547059  0.550868  0.579151   \n",
       "10   phrase_max  Wikipedia  0.598174  0.593750  0.604247   \n",
       "11   phrase_min  Wikipedia  0.272351  0.482639  0.480695   \n",
       "12    phrase_mv   WikiNews  0.551194  0.557950  0.576389   \n",
       "13   phrase_max   WikiNews  0.608333  0.604072  0.616162   \n",
       "14   phrase_min   WikiNews  0.444545  0.564935  0.575758   \n",
       "15    phrase_mv       News  0.573984  0.579879  0.611737   \n",
       "16   phrase_max       News  0.508523  0.509045  0.510142   \n",
       "17   phrase_min       News  0.345717  0.572210  0.558946   \n",
       "18    phrase_mv  Wikipedia  0.522994  0.535652  0.558880   \n",
       "19   phrase_max  Wikipedia  0.510567  0.510771  0.512548   \n",
       "20   phrase_min  Wikipedia  0.215808  0.412821  0.417954   \n",
       "21    phrase_mv   WikiNews  0.478229  0.509977  0.513889   \n",
       "22   phrase_max   WikiNews  0.556033  0.555172  0.565657   \n",
       "23   phrase_min   WikiNews  0.347445  0.559916  0.544823   \n",
       "24    phrase_mv       News  0.547359  0.564674  0.593967   \n",
       "25   phrase_max       News  0.461210  0.464903  0.459865   \n",
       "26   phrase_min       News  0.317389  0.548050  0.535454   \n",
       "27    phrase_mv  Wikipedia  0.530917  0.540476  0.565637   \n",
       "28   phrase_max  Wikipedia  0.557991  0.555556  0.561776   \n",
       "29   phrase_min  Wikipedia  0.302404  0.479389  0.472008   \n",
       "30    phrase_mv   WikiNews  0.526785  0.541026  0.555556   \n",
       "31   phrase_max   WikiNews  0.564058  0.562229  0.572601   \n",
       "32   phrase_min   WikiNews  0.413233  0.534273  0.539141   \n",
       "33    phrase_mv       News  0.592054  0.592702  0.626300   \n",
       "34   phrase_max       News  0.504242  0.504258  0.504421   \n",
       "35   phrase_min       News  0.349781  0.573812  0.561373   \n",
       "36    phrase_mv  Wikipedia  0.369697  0.470085  0.445946   \n",
       "37   phrase_max  Wikipedia  0.392545  0.449241  0.406371   \n",
       "38   phrase_min  Wikipedia  0.237750  0.487013  0.489382   \n",
       "39    phrase_mv   WikiNews  0.532006  0.582418  0.613636   \n",
       "40   phrase_max   WikiNews  0.582222  0.595260  0.630682   \n",
       "41   phrase_min   WikiNews  0.295027  0.567974  0.532828   \n",
       "42    phrase_mv       News  0.508892  0.576010  0.610437   \n",
       "43   phrase_max       News  0.503812  0.530828  0.545336   \n",
       "44   phrase_min       News  0.230197  0.568333  0.517770   \n",
       "45    phrase_mv  Wikipedia  0.499666  0.522321  0.538610   \n",
       "46   phrase_max  Wikipedia  0.571096  0.566362  0.583977   \n",
       "47   phrase_min  Wikipedia  0.293996  0.495238  0.494208   \n",
       "48    phrase_mv   WikiNews  0.517949  0.541496  0.557449   \n",
       "49   phrase_max   WikiNews  0.580357  0.577489  0.586490   \n",
       "50   phrase_min   WikiNews  0.403175  0.610380  0.595328   \n",
       "51    phrase_mv       News  0.532637  0.555739  0.581831   \n",
       "52   phrase_max       News  0.481068  0.486592  0.483443   \n",
       "53   phrase_min       News  0.333392  0.567117  0.551664   \n",
       "54    phrase_mv  Wikipedia  0.358333  0.545455  0.563707   \n",
       "55   phrase_max  Wikipedia  0.416188  0.480879  0.464286   \n",
       "56   phrase_min  Wikipedia  0.312500  0.484848  0.478764   \n",
       "57    phrase_mv   WikiNews  0.505090  0.568531  0.592803   \n",
       "58   phrase_max   WikiNews  0.653248  0.655903  0.650884   \n",
       "59   phrase_min   WikiNews  0.320482  0.581051  0.546717   \n",
       "60    phrase_mv       News  0.455157  0.571120  0.592753   \n",
       "61   phrase_max       News  0.489511  0.522424  0.533200   \n",
       "62   phrase_min       News  0.280645  0.554596  0.529040   \n",
       "63    phrase_mv  Wikipedia  0.427278  0.508333  0.515444   \n",
       "64   phrase_max  Wikipedia  0.469206  0.506410  0.511583   \n",
       "65   phrase_min  Wikipedia  0.165172  0.353616  0.419884   \n",
       "66    phrase_mv   WikiNews  0.430303  0.497650  0.496843   \n",
       "67   phrase_max   WikiNews  0.526210  0.546703  0.564394   \n",
       "68   phrase_min   WikiNews  0.263133  0.623596  0.534722   \n",
       "69    phrase_mv       News  0.504570  0.548566  0.572209   \n",
       "70   phrase_max       News  0.464044  0.482463  0.475468   \n",
       "71   phrase_min       News  0.257440  0.490548  0.494972   \n",
       "72    phrase_mv  Wikipedia  0.507379  0.526599  0.545367   \n",
       "73   phrase_max  Wikipedia  0.538013  0.539465  0.556950   \n",
       "74   phrase_min  Wikipedia  0.306012  0.525684  0.529923   \n",
       "75    phrase_mv   WikiNews  0.606095  0.616889  0.660354   \n",
       "76   phrase_max   WikiNews  0.654623  0.650595  0.659722   \n",
       "77   phrase_min   WikiNews  0.489130  0.657143  0.666667   \n",
       "78    phrase_mv       News  0.549947  0.571209  0.604542   \n",
       "79   phrase_max       News  0.491997  0.492977  0.492285   \n",
       "80   phrase_min       News  0.304693  0.540906  0.528173   \n",
       "81    phrase_mv  Wikipedia  0.410714  0.501053  0.501931   \n",
       "82   phrase_max  Wikipedia  0.420215  0.462803  0.433398   \n",
       "83   phrase_min  Wikipedia  0.190569  0.402954  0.433398   \n",
       "84    phrase_mv   WikiNews  0.434259  0.509579  0.512626   \n",
       "85   phrase_max   WikiNews  0.468159  0.511338  0.515783   \n",
       "86   phrase_min   WikiNews  0.286240  0.519048  0.510101   \n",
       "87    phrase_mv       News  0.389206  0.513408  0.516557   \n",
       "88   phrase_max       News  0.373139  0.443142  0.417389   \n",
       "89   phrase_min       News  0.240012  0.575173  0.522625   \n",
       "90    phrase_mv  Wikipedia  0.515152  0.531034  0.552124   \n",
       "91   phrase_max  Wikipedia  0.533800  0.532799  0.541506   \n",
       "92   phrase_min  Wikipedia  0.318903  0.471429  0.456564   \n",
       "93    phrase_mv   WikiNews  0.549899  0.568223  0.594066   \n",
       "94   phrase_max   WikiNews  0.564058  0.562229  0.572601   \n",
       "95   phrase_min   WikiNews  0.425532  0.593434  0.593434   \n",
       "96    phrase_mv       News  0.564639  0.582563  0.620752   \n",
       "97   phrase_max       News  0.481129  0.488268  0.485090   \n",
       "98   phrase_min       News  0.333976  0.556385  0.545163   \n",
       "99    phrase_mv  Wikipedia  0.545455  0.556322  0.594595   \n",
       "100  phrase_max  Wikipedia  0.562552  0.558824  0.577220   \n",
       "101  phrase_min  Wikipedia  0.271222  0.460317  0.451737   \n",
       "102   phrase_mv   WikiNews  0.541463  0.562927  0.587121   \n",
       "103  phrase_max   WikiNews  0.625842  0.622619  0.630051   \n",
       "104  phrase_min   WikiNews  0.414827  0.589367  0.586490   \n",
       "105   phrase_mv       News  0.558930  0.567770  0.595527   \n",
       "106  phrase_max       News  0.487996  0.490625  0.489078   \n",
       "107  phrase_min       News  0.333392  0.567117  0.551664   \n",
       "108   phrase_mv  Wikipedia  0.461647  0.502650  0.504826   \n",
       "109  phrase_max  Wikipedia  0.530130  0.533854  0.550193   \n",
       "110  phrase_min  Wikipedia  0.302404  0.479389  0.472008   \n",
       "111   phrase_mv   WikiNews  0.576127  0.580460  0.606061   \n",
       "112  phrase_max   WikiNews  0.634810  0.632884  0.636995   \n",
       "113  phrase_min   WikiNews  0.425272  0.573214  0.577652   \n",
       "114   phrase_mv       News  0.544409  0.562857  0.591540   \n",
       "115  phrase_max       News  0.514126  0.515424  0.518291   \n",
       "116  phrase_min       News  0.333392  0.567117  0.551664   \n",
       "\n",
       "                           zc  \n",
       "0                  linguistic  \n",
       "1                  linguistic  \n",
       "2                  linguistic  \n",
       "3                  linguistic  \n",
       "4                  linguistic  \n",
       "5                  linguistic  \n",
       "6                  linguistic  \n",
       "7                  linguistic  \n",
       "8                  linguistic  \n",
       "9                   frequency  \n",
       "10                  frequency  \n",
       "11                  frequency  \n",
       "12                  frequency  \n",
       "13                  frequency  \n",
       "14                  frequency  \n",
       "15                  frequency  \n",
       "16                  frequency  \n",
       "17                  frequency  \n",
       "18             language_model  \n",
       "19             language_model  \n",
       "20             language_model  \n",
       "21             language_model  \n",
       "22             language_model  \n",
       "23             language_model  \n",
       "24             language_model  \n",
       "25             language_model  \n",
       "26             language_model  \n",
       "27                     corpus  \n",
       "28                     corpus  \n",
       "29                     corpus  \n",
       "30                     corpus  \n",
       "31                     corpus  \n",
       "32                     corpus  \n",
       "33                     corpus  \n",
       "34                     corpus  \n",
       "35                     corpus  \n",
       "36           psycholinguistic  \n",
       "37           psycholinguistic  \n",
       "38           psycholinguistic  \n",
       "39           psycholinguistic  \n",
       "40           psycholinguistic  \n",
       "41           psycholinguistic  \n",
       "42           psycholinguistic  \n",
       "43           psycholinguistic  \n",
       "44           psycholinguistic  \n",
       "45                    wordnet  \n",
       "46                    wordnet  \n",
       "47                    wordnet  \n",
       "48                    wordnet  \n",
       "49                    wordnet  \n",
       "50                    wordnet  \n",
       "51                    wordnet  \n",
       "52                    wordnet  \n",
       "53                    wordnet  \n",
       "54                    dbpedia  \n",
       "55                    dbpedia  \n",
       "56                    dbpedia  \n",
       "57                    dbpedia  \n",
       "58                    dbpedia  \n",
       "59                    dbpedia  \n",
       "60                    dbpedia  \n",
       "61                    dbpedia  \n",
       "62                    dbpedia  \n",
       "63           brown_clustering  \n",
       "64           brown_clustering  \n",
       "65           brown_clustering  \n",
       "66           brown_clustering  \n",
       "67           brown_clustering  \n",
       "68           brown_clustering  \n",
       "69           brown_clustering  \n",
       "70           brown_clustering  \n",
       "71           brown_clustering  \n",
       "72                   semantic  \n",
       "73                   semantic  \n",
       "74                   semantic  \n",
       "75                   semantic  \n",
       "76                   semantic  \n",
       "77                   semantic  \n",
       "78                   semantic  \n",
       "79                   semantic  \n",
       "80                   semantic  \n",
       "81                 dictionary  \n",
       "82                 dictionary  \n",
       "83                 dictionary  \n",
       "84                 dictionary  \n",
       "85                 dictionary  \n",
       "86                 dictionary  \n",
       "87                 dictionary  \n",
       "88                 dictionary  \n",
       "89                 dictionary  \n",
       "90            corpus+semantic  \n",
       "91            corpus+semantic  \n",
       "92            corpus+semantic  \n",
       "93            corpus+semantic  \n",
       "94            corpus+semantic  \n",
       "95            corpus+semantic  \n",
       "96            corpus+semantic  \n",
       "97            corpus+semantic  \n",
       "98            corpus+semantic  \n",
       "99   wordnet+psycholinguistic  \n",
       "100  wordnet+psycholinguistic  \n",
       "101  wordnet+psycholinguistic  \n",
       "102  wordnet+psycholinguistic  \n",
       "103  wordnet+psycholinguistic  \n",
       "104  wordnet+psycholinguistic  \n",
       "105  wordnet+psycholinguistic  \n",
       "106  wordnet+psycholinguistic  \n",
       "107  wordnet+psycholinguistic  \n",
       "108                       all  \n",
       "109                       all  \n",
       "110                       all  \n",
       "111                       all  \n",
       "112                       all  \n",
       "113                       all  \n",
       "114                       all  \n",
       "115                       all  \n",
       "116                       all  "
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_eval_data_ada = create_eval_df_from_results_phrase(results_ada)\n",
    "feature_eval_data_ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.598174</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.604247</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.592054</td>\n",
       "      <td>0.592702</td>\n",
       "      <td>0.626300</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.654623</td>\n",
       "      <td>0.650595</td>\n",
       "      <td>0.659722</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           agg    dataset        f1      prec       rec         zc\n",
       "10  phrase_max  Wikipedia  0.598174  0.593750  0.604247  frequency\n",
       "33   phrase_mv       News  0.592054  0.592702  0.626300     corpus\n",
       "76  phrase_max   WikiNews  0.654623  0.650595  0.659722   semantic"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = feature_eval_data_ada.groupby(['dataset'])['f1'].transform(max) == feature_eval_data_ada['f1']\n",
    "feature_eval_data_ada[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.5 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dt = [Result(fs, fs.fc, agg, phrase_merger(fs.test, decision_tree(*transform_feat_to_num(\n",
    "                remove_labels_for_binary_df(fs.train), \n",
    "                remove_labels_phrase_for_binary_df(fs.test))), agg.agg)) \n",
    "          for fs in all_fc_datasets\n",
    "          for agg in phrase_aggs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.448498</td>\n",
       "      <td>0.486523</td>\n",
       "      <td>0.475869</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.469570</td>\n",
       "      <td>0.480602</td>\n",
       "      <td>0.472008</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.258971</td>\n",
       "      <td>0.588608</td>\n",
       "      <td>0.560811</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.526210</td>\n",
       "      <td>0.546703</td>\n",
       "      <td>0.564394</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.591304</td>\n",
       "      <td>0.587662</td>\n",
       "      <td>0.602273</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.350402</td>\n",
       "      <td>0.514737</td>\n",
       "      <td>0.513258</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.507918</td>\n",
       "      <td>0.523875</td>\n",
       "      <td>0.533894</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.479940</td>\n",
       "      <td>0.482804</td>\n",
       "      <td>0.480149</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.342131</td>\n",
       "      <td>0.560154</td>\n",
       "      <td>0.550017</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.434039</td>\n",
       "      <td>0.479332</td>\n",
       "      <td>0.462355</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.482353</td>\n",
       "      <td>0.496278</td>\n",
       "      <td>0.494208</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.424710</td>\n",
       "      <td>0.424710</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.559375</td>\n",
       "      <td>0.568609</td>\n",
       "      <td>0.592172</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.625842</td>\n",
       "      <td>0.622619</td>\n",
       "      <td>0.630051</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.371700</td>\n",
       "      <td>0.547719</td>\n",
       "      <td>0.542929</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.559130</td>\n",
       "      <td>0.574342</td>\n",
       "      <td>0.607750</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.498661</td>\n",
       "      <td>0.502996</td>\n",
       "      <td>0.503727</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.344569</td>\n",
       "      <td>0.595435</td>\n",
       "      <td>0.571949</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>0.531034</td>\n",
       "      <td>0.552124</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.503472</td>\n",
       "      <td>0.504762</td>\n",
       "      <td>0.505792</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.258971</td>\n",
       "      <td>0.433638</td>\n",
       "      <td>0.416023</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.478831</td>\n",
       "      <td>0.503663</td>\n",
       "      <td>0.505051</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.552381</td>\n",
       "      <td>0.550905</td>\n",
       "      <td>0.556818</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.393548</td>\n",
       "      <td>0.539804</td>\n",
       "      <td>0.541035</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.553271</td>\n",
       "      <td>0.568358</td>\n",
       "      <td>0.598821</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.493661</td>\n",
       "      <td>0.497736</td>\n",
       "      <td>0.497226</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.341054</td>\n",
       "      <td>0.581869</td>\n",
       "      <td>0.563020</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.412265</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>0.442085</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.468372</td>\n",
       "      <td>0.488095</td>\n",
       "      <td>0.480695</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.283258</td>\n",
       "      <td>0.489229</td>\n",
       "      <td>0.487452</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.502517</td>\n",
       "      <td>0.519114</td>\n",
       "      <td>0.525884</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.539498</td>\n",
       "      <td>0.538690</td>\n",
       "      <td>0.541035</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>0.593434</td>\n",
       "      <td>0.593434</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.574116</td>\n",
       "      <td>0.581863</td>\n",
       "      <td>0.615811</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.514126</td>\n",
       "      <td>0.515424</td>\n",
       "      <td>0.518291</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.349287</td>\n",
       "      <td>0.584740</td>\n",
       "      <td>0.567874</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.369697</td>\n",
       "      <td>0.470085</td>\n",
       "      <td>0.445946</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.461647</td>\n",
       "      <td>0.502650</td>\n",
       "      <td>0.504826</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.202899</td>\n",
       "      <td>0.420513</td>\n",
       "      <td>0.440154</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.508768</td>\n",
       "      <td>0.581555</td>\n",
       "      <td>0.608586</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.639017</td>\n",
       "      <td>0.644444</td>\n",
       "      <td>0.696970</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.332875</td>\n",
       "      <td>0.586382</td>\n",
       "      <td>0.553662</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.516012</td>\n",
       "      <td>0.568050</td>\n",
       "      <td>0.600641</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.486275</td>\n",
       "      <td>0.512684</td>\n",
       "      <td>0.518551</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.252692</td>\n",
       "      <td>0.613360</td>\n",
       "      <td>0.536408</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.537235</td>\n",
       "      <td>0.551500</td>\n",
       "      <td>0.587838</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.580952</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>0.606178</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.341991</td>\n",
       "      <td>0.499365</td>\n",
       "      <td>0.499035</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.526210</td>\n",
       "      <td>0.546703</td>\n",
       "      <td>0.564394</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.558320</td>\n",
       "      <td>0.559476</td>\n",
       "      <td>0.574495</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.381859</td>\n",
       "      <td>0.576023</td>\n",
       "      <td>0.565657</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.559130</td>\n",
       "      <td>0.574342</td>\n",
       "      <td>0.607750</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.497677</td>\n",
       "      <td>0.505351</td>\n",
       "      <td>0.507021</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.323474</td>\n",
       "      <td>0.589009</td>\n",
       "      <td>0.559813</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.376906</td>\n",
       "      <td>0.485839</td>\n",
       "      <td>0.474903</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.556322</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.328765</td>\n",
       "      <td>0.562138</td>\n",
       "      <td>0.572394</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.423182</td>\n",
       "      <td>0.539502</td>\n",
       "      <td>0.546086</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.465149</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>0.524621</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.234690</td>\n",
       "      <td>0.620879</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.439805</td>\n",
       "      <td>0.539066</td>\n",
       "      <td>0.552965</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.498310</td>\n",
       "      <td>0.504201</td>\n",
       "      <td>0.505374</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.228114</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.524272</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.473539</td>\n",
       "      <td>0.518315</td>\n",
       "      <td>0.533784</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.492219</td>\n",
       "      <td>0.509936</td>\n",
       "      <td>0.516409</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.467181</td>\n",
       "      <td>0.467181</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.483093</td>\n",
       "      <td>0.546620</td>\n",
       "      <td>0.563131</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.559375</td>\n",
       "      <td>0.568609</td>\n",
       "      <td>0.592172</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.307869</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.539773</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.564937</td>\n",
       "      <td>0.580305</td>\n",
       "      <td>0.616678</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.513417</td>\n",
       "      <td>0.517102</td>\n",
       "      <td>0.521585</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.328530</td>\n",
       "      <td>0.577237</td>\n",
       "      <td>0.555739</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.507379</td>\n",
       "      <td>0.526599</td>\n",
       "      <td>0.545367</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.503792</td>\n",
       "      <td>0.510033</td>\n",
       "      <td>0.514479</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.325276</td>\n",
       "      <td>0.510661</td>\n",
       "      <td>0.514479</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.549899</td>\n",
       "      <td>0.568223</td>\n",
       "      <td>0.594066</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.552381</td>\n",
       "      <td>0.550905</td>\n",
       "      <td>0.556818</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.360544</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.535985</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.568105</td>\n",
       "      <td>0.580012</td>\n",
       "      <td>0.615031</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.508687</td>\n",
       "      <td>0.512124</td>\n",
       "      <td>0.515083</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.328530</td>\n",
       "      <td>0.577237</td>\n",
       "      <td>0.555739</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.435495</td>\n",
       "      <td>0.511936</td>\n",
       "      <td>0.522201</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.480460</td>\n",
       "      <td>0.467181</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.202899</td>\n",
       "      <td>0.420513</td>\n",
       "      <td>0.440154</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.434259</td>\n",
       "      <td>0.509579</td>\n",
       "      <td>0.512626</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.468159</td>\n",
       "      <td>0.511338</td>\n",
       "      <td>0.515783</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.286240</td>\n",
       "      <td>0.519048</td>\n",
       "      <td>0.510101</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.370377</td>\n",
       "      <td>0.492997</td>\n",
       "      <td>0.491418</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.347457</td>\n",
       "      <td>0.413321</td>\n",
       "      <td>0.372746</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.240012</td>\n",
       "      <td>0.575173</td>\n",
       "      <td>0.522625</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.489255</td>\n",
       "      <td>0.525967</td>\n",
       "      <td>0.547297</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.496568</td>\n",
       "      <td>0.505208</td>\n",
       "      <td>0.507722</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.325276</td>\n",
       "      <td>0.510661</td>\n",
       "      <td>0.514479</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.551052</td>\n",
       "      <td>0.562937</td>\n",
       "      <td>0.585227</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.599765</td>\n",
       "      <td>0.595633</td>\n",
       "      <td>0.609217</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.382699</td>\n",
       "      <td>0.553378</td>\n",
       "      <td>0.549874</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.559135</td>\n",
       "      <td>0.569925</td>\n",
       "      <td>0.599601</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.493557</td>\n",
       "      <td>0.499107</td>\n",
       "      <td>0.498873</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.348716</td>\n",
       "      <td>0.596622</td>\n",
       "      <td>0.574376</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.599006</td>\n",
       "      <td>0.591176</td>\n",
       "      <td>0.619691</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.517544</td>\n",
       "      <td>0.523077</td>\n",
       "      <td>0.517375</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.348148</td>\n",
       "      <td>0.541933</td>\n",
       "      <td>0.556950</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.524608</td>\n",
       "      <td>0.552608</td>\n",
       "      <td>0.573232</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.566457</td>\n",
       "      <td>0.566052</td>\n",
       "      <td>0.581439</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.534091</td>\n",
       "      <td>0.534091</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.513459</td>\n",
       "      <td>0.527489</td>\n",
       "      <td>0.538748</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.455471</td>\n",
       "      <td>0.462353</td>\n",
       "      <td>0.454230</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.357843</td>\n",
       "      <td>0.576892</td>\n",
       "      <td>0.566227</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>0.581609</td>\n",
       "      <td>0.637066</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.608392</td>\n",
       "      <td>0.599924</td>\n",
       "      <td>0.626448</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.355311</td>\n",
       "      <td>0.523438</td>\n",
       "      <td>0.534749</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.526785</td>\n",
       "      <td>0.541026</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.584596</td>\n",
       "      <td>0.584596</td>\n",
       "      <td>0.584596</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.382699</td>\n",
       "      <td>0.517262</td>\n",
       "      <td>0.518308</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.552279</td>\n",
       "      <td>0.559481</td>\n",
       "      <td>0.582524</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.483556</td>\n",
       "      <td>0.484685</td>\n",
       "      <td>0.483356</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.381643</td>\n",
       "      <td>0.576300</td>\n",
       "      <td>0.574289</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            agg    dataset        f1      prec       rec  \\\n",
       "0     phrase_mv  Wikipedia  0.448498  0.486523  0.475869   \n",
       "1    phrase_max  Wikipedia  0.469570  0.480602  0.472008   \n",
       "2    phrase_min  Wikipedia  0.258971  0.588608  0.560811   \n",
       "3     phrase_mv   WikiNews  0.526210  0.546703  0.564394   \n",
       "4    phrase_max   WikiNews  0.591304  0.587662  0.602273   \n",
       "5    phrase_min   WikiNews  0.350402  0.514737  0.513258   \n",
       "6     phrase_mv       News  0.507918  0.523875  0.533894   \n",
       "7    phrase_max       News  0.479940  0.482804  0.480149   \n",
       "8    phrase_min       News  0.342131  0.560154  0.550017   \n",
       "9     phrase_mv  Wikipedia  0.434039  0.479332  0.462355   \n",
       "10   phrase_max  Wikipedia  0.482353  0.496278  0.494208   \n",
       "11   phrase_min  Wikipedia  0.227273  0.424710  0.424710   \n",
       "12    phrase_mv   WikiNews  0.559375  0.568609  0.592172   \n",
       "13   phrase_max   WikiNews  0.625842  0.622619  0.630051   \n",
       "14   phrase_min   WikiNews  0.371700  0.547719  0.542929   \n",
       "15    phrase_mv       News  0.559130  0.574342  0.607750   \n",
       "16   phrase_max       News  0.498661  0.502996  0.503727   \n",
       "17   phrase_min       News  0.344569  0.595435  0.571949   \n",
       "18    phrase_mv  Wikipedia  0.515152  0.531034  0.552124   \n",
       "19   phrase_max  Wikipedia  0.503472  0.504762  0.505792   \n",
       "20   phrase_min  Wikipedia  0.258971  0.433638  0.416023   \n",
       "21    phrase_mv   WikiNews  0.478831  0.503663  0.505051   \n",
       "22   phrase_max   WikiNews  0.552381  0.550905  0.556818   \n",
       "23   phrase_min   WikiNews  0.393548  0.539804  0.541035   \n",
       "24    phrase_mv       News  0.553271  0.568358  0.598821   \n",
       "25   phrase_max       News  0.493661  0.497736  0.497226   \n",
       "26   phrase_min       News  0.341054  0.581869  0.563020   \n",
       "27    phrase_mv  Wikipedia  0.412265  0.468750  0.442085   \n",
       "28   phrase_max  Wikipedia  0.468372  0.488095  0.480695   \n",
       "29   phrase_min  Wikipedia  0.283258  0.489229  0.487452   \n",
       "30    phrase_mv   WikiNews  0.502517  0.519114  0.525884   \n",
       "31   phrase_max   WikiNews  0.539498  0.538690  0.541035   \n",
       "32   phrase_min   WikiNews  0.425532  0.593434  0.593434   \n",
       "33    phrase_mv       News  0.574116  0.581863  0.615811   \n",
       "34   phrase_max       News  0.514126  0.515424  0.518291   \n",
       "35   phrase_min       News  0.349287  0.584740  0.567874   \n",
       "36    phrase_mv  Wikipedia  0.369697  0.470085  0.445946   \n",
       "37   phrase_max  Wikipedia  0.461647  0.502650  0.504826   \n",
       "38   phrase_min  Wikipedia  0.202899  0.420513  0.440154   \n",
       "39    phrase_mv   WikiNews  0.508768  0.581555  0.608586   \n",
       "40   phrase_max   WikiNews  0.639017  0.644444  0.696970   \n",
       "41   phrase_min   WikiNews  0.332875  0.586382  0.553662   \n",
       "42    phrase_mv       News  0.516012  0.568050  0.600641   \n",
       "43   phrase_max       News  0.486275  0.512684  0.518551   \n",
       "44   phrase_min       News  0.252692  0.613360  0.536408   \n",
       "45    phrase_mv  Wikipedia  0.537235  0.551500  0.587838   \n",
       "46   phrase_max  Wikipedia  0.580952  0.575758  0.606178   \n",
       "47   phrase_min  Wikipedia  0.341991  0.499365  0.499035   \n",
       "48    phrase_mv   WikiNews  0.526210  0.546703  0.564394   \n",
       "49   phrase_max   WikiNews  0.558320  0.559476  0.574495   \n",
       "50   phrase_min   WikiNews  0.381859  0.576023  0.565657   \n",
       "51    phrase_mv       News  0.559130  0.574342  0.607750   \n",
       "52   phrase_max       News  0.497677  0.505351  0.507021   \n",
       "53   phrase_min       News  0.323474  0.589009  0.559813   \n",
       "54    phrase_mv  Wikipedia  0.376906  0.485839  0.474903   \n",
       "55   phrase_max  Wikipedia  0.545455  0.556322  0.594595   \n",
       "56   phrase_min  Wikipedia  0.328765  0.562138  0.572394   \n",
       "57    phrase_mv   WikiNews  0.423182  0.539502  0.546086   \n",
       "58   phrase_max   WikiNews  0.465149  0.517857  0.524621   \n",
       "59   phrase_min   WikiNews  0.234690  0.620879  0.520833   \n",
       "60    phrase_mv       News  0.439805  0.539066  0.552965   \n",
       "61   phrase_max       News  0.498310  0.504201  0.505374   \n",
       "62   phrase_min       News  0.228114  0.611111  0.524272   \n",
       "63    phrase_mv  Wikipedia  0.473539  0.518315  0.533784   \n",
       "64   phrase_max  Wikipedia  0.492219  0.509936  0.516409   \n",
       "65   phrase_min  Wikipedia  0.250000  0.467181  0.467181   \n",
       "66    phrase_mv   WikiNews  0.483093  0.546620  0.563131   \n",
       "67   phrase_max   WikiNews  0.559375  0.568609  0.592172   \n",
       "68   phrase_min   WikiNews  0.307869  0.575000  0.539773   \n",
       "69    phrase_mv       News  0.564937  0.580305  0.616678   \n",
       "70   phrase_max       News  0.513417  0.517102  0.521585   \n",
       "71   phrase_min       News  0.328530  0.577237  0.555739   \n",
       "72    phrase_mv  Wikipedia  0.507379  0.526599  0.545367   \n",
       "73   phrase_max  Wikipedia  0.503792  0.510033  0.514479   \n",
       "74   phrase_min  Wikipedia  0.325276  0.510661  0.514479   \n",
       "75    phrase_mv   WikiNews  0.549899  0.568223  0.594066   \n",
       "76   phrase_max   WikiNews  0.552381  0.550905  0.556818   \n",
       "77   phrase_min   WikiNews  0.360544  0.541667  0.535985   \n",
       "78    phrase_mv       News  0.568105  0.580012  0.615031   \n",
       "79   phrase_max       News  0.508687  0.512124  0.515083   \n",
       "80   phrase_min       News  0.328530  0.577237  0.555739   \n",
       "81    phrase_mv  Wikipedia  0.435495  0.511936  0.522201   \n",
       "82   phrase_max  Wikipedia  0.454545  0.480460  0.467181   \n",
       "83   phrase_min  Wikipedia  0.202899  0.420513  0.440154   \n",
       "84    phrase_mv   WikiNews  0.434259  0.509579  0.512626   \n",
       "85   phrase_max   WikiNews  0.468159  0.511338  0.515783   \n",
       "86   phrase_min   WikiNews  0.286240  0.519048  0.510101   \n",
       "87    phrase_mv       News  0.370377  0.492997  0.491418   \n",
       "88   phrase_max       News  0.347457  0.413321  0.372746   \n",
       "89   phrase_min       News  0.240012  0.575173  0.522625   \n",
       "90    phrase_mv  Wikipedia  0.489255  0.525967  0.547297   \n",
       "91   phrase_max  Wikipedia  0.496568  0.505208  0.507722   \n",
       "92   phrase_min  Wikipedia  0.325276  0.510661  0.514479   \n",
       "93    phrase_mv   WikiNews  0.551052  0.562937  0.585227   \n",
       "94   phrase_max   WikiNews  0.599765  0.595633  0.609217   \n",
       "95   phrase_min   WikiNews  0.382699  0.553378  0.549874   \n",
       "96    phrase_mv       News  0.559135  0.569925  0.599601   \n",
       "97   phrase_max       News  0.493557  0.499107  0.498873   \n",
       "98   phrase_min       News  0.348716  0.596622  0.574376   \n",
       "99    phrase_mv  Wikipedia  0.599006  0.591176  0.619691   \n",
       "100  phrase_max  Wikipedia  0.517544  0.523077  0.517375   \n",
       "101  phrase_min  Wikipedia  0.348148  0.541933  0.556950   \n",
       "102   phrase_mv   WikiNews  0.524608  0.552608  0.573232   \n",
       "103  phrase_max   WikiNews  0.566457  0.566052  0.581439   \n",
       "104  phrase_min   WikiNews  0.382979  0.534091  0.534091   \n",
       "105   phrase_mv       News  0.513459  0.527489  0.538748   \n",
       "106  phrase_max       News  0.455471  0.462353  0.454230   \n",
       "107  phrase_min       News  0.357843  0.576892  0.566227   \n",
       "108   phrase_mv  Wikipedia  0.575758  0.581609  0.637066   \n",
       "109  phrase_max  Wikipedia  0.608392  0.599924  0.626448   \n",
       "110  phrase_min  Wikipedia  0.355311  0.523438  0.534749   \n",
       "111   phrase_mv   WikiNews  0.526785  0.541026  0.555556   \n",
       "112  phrase_max   WikiNews  0.584596  0.584596  0.584596   \n",
       "113  phrase_min   WikiNews  0.382699  0.517262  0.518308   \n",
       "114   phrase_mv       News  0.552279  0.559481  0.582524   \n",
       "115  phrase_max       News  0.483556  0.484685  0.483356   \n",
       "116  phrase_min       News  0.381643  0.576300  0.574289   \n",
       "\n",
       "                           zc  \n",
       "0                  linguistic  \n",
       "1                  linguistic  \n",
       "2                  linguistic  \n",
       "3                  linguistic  \n",
       "4                  linguistic  \n",
       "5                  linguistic  \n",
       "6                  linguistic  \n",
       "7                  linguistic  \n",
       "8                  linguistic  \n",
       "9                   frequency  \n",
       "10                  frequency  \n",
       "11                  frequency  \n",
       "12                  frequency  \n",
       "13                  frequency  \n",
       "14                  frequency  \n",
       "15                  frequency  \n",
       "16                  frequency  \n",
       "17                  frequency  \n",
       "18             language_model  \n",
       "19             language_model  \n",
       "20             language_model  \n",
       "21             language_model  \n",
       "22             language_model  \n",
       "23             language_model  \n",
       "24             language_model  \n",
       "25             language_model  \n",
       "26             language_model  \n",
       "27                     corpus  \n",
       "28                     corpus  \n",
       "29                     corpus  \n",
       "30                     corpus  \n",
       "31                     corpus  \n",
       "32                     corpus  \n",
       "33                     corpus  \n",
       "34                     corpus  \n",
       "35                     corpus  \n",
       "36           psycholinguistic  \n",
       "37           psycholinguistic  \n",
       "38           psycholinguistic  \n",
       "39           psycholinguistic  \n",
       "40           psycholinguistic  \n",
       "41           psycholinguistic  \n",
       "42           psycholinguistic  \n",
       "43           psycholinguistic  \n",
       "44           psycholinguistic  \n",
       "45                    wordnet  \n",
       "46                    wordnet  \n",
       "47                    wordnet  \n",
       "48                    wordnet  \n",
       "49                    wordnet  \n",
       "50                    wordnet  \n",
       "51                    wordnet  \n",
       "52                    wordnet  \n",
       "53                    wordnet  \n",
       "54                    dbpedia  \n",
       "55                    dbpedia  \n",
       "56                    dbpedia  \n",
       "57                    dbpedia  \n",
       "58                    dbpedia  \n",
       "59                    dbpedia  \n",
       "60                    dbpedia  \n",
       "61                    dbpedia  \n",
       "62                    dbpedia  \n",
       "63           brown_clustering  \n",
       "64           brown_clustering  \n",
       "65           brown_clustering  \n",
       "66           brown_clustering  \n",
       "67           brown_clustering  \n",
       "68           brown_clustering  \n",
       "69           brown_clustering  \n",
       "70           brown_clustering  \n",
       "71           brown_clustering  \n",
       "72                   semantic  \n",
       "73                   semantic  \n",
       "74                   semantic  \n",
       "75                   semantic  \n",
       "76                   semantic  \n",
       "77                   semantic  \n",
       "78                   semantic  \n",
       "79                   semantic  \n",
       "80                   semantic  \n",
       "81                 dictionary  \n",
       "82                 dictionary  \n",
       "83                 dictionary  \n",
       "84                 dictionary  \n",
       "85                 dictionary  \n",
       "86                 dictionary  \n",
       "87                 dictionary  \n",
       "88                 dictionary  \n",
       "89                 dictionary  \n",
       "90            corpus+semantic  \n",
       "91            corpus+semantic  \n",
       "92            corpus+semantic  \n",
       "93            corpus+semantic  \n",
       "94            corpus+semantic  \n",
       "95            corpus+semantic  \n",
       "96            corpus+semantic  \n",
       "97            corpus+semantic  \n",
       "98            corpus+semantic  \n",
       "99   wordnet+psycholinguistic  \n",
       "100  wordnet+psycholinguistic  \n",
       "101  wordnet+psycholinguistic  \n",
       "102  wordnet+psycholinguistic  \n",
       "103  wordnet+psycholinguistic  \n",
       "104  wordnet+psycholinguistic  \n",
       "105  wordnet+psycholinguistic  \n",
       "106  wordnet+psycholinguistic  \n",
       "107  wordnet+psycholinguistic  \n",
       "108                       all  \n",
       "109                       all  \n",
       "110                       all  \n",
       "111                       all  \n",
       "112                       all  \n",
       "113                       all  \n",
       "114                       all  \n",
       "115                       all  \n",
       "116                       all  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_eval_data_dt = create_eval_df_from_results_phrase(results_dt)\n",
    "feature_eval_data_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.574116</td>\n",
       "      <td>0.581863</td>\n",
       "      <td>0.615811</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.639017</td>\n",
       "      <td>0.644444</td>\n",
       "      <td>0.696970</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.608392</td>\n",
       "      <td>0.599924</td>\n",
       "      <td>0.626448</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            agg    dataset        f1      prec       rec                zc\n",
       "33    phrase_mv       News  0.574116  0.581863  0.615811            corpus\n",
       "40   phrase_max   WikiNews  0.639017  0.644444  0.696970  psycholinguistic\n",
       "109  phrase_max  Wikipedia  0.608392  0.599924  0.626448               all"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = feature_eval_data_dt.groupby(['dataset'])['f1'].transform(max) == feature_eval_data_dt['f1']\n",
    "feature_eval_data_dt[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.6 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "results_lr = [Result(fs, fs.fc, agg, phrase_merger(fs.test, logistic_regression(*transform_feat_to_num(\n",
    "                remove_labels_for_binary_df(fs.train), \n",
    "                remove_labels_phrase_for_binary_df(fs.test))), agg.agg)) \n",
    "          for fs in all_fc_datasets\n",
    "          for agg in phrase_aggs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.442135</td>\n",
       "      <td>0.503618</td>\n",
       "      <td>0.506757</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.476780</td>\n",
       "      <td>0.510243</td>\n",
       "      <td>0.518340</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.295090</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>0.523166</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.540917</td>\n",
       "      <td>0.587095</td>\n",
       "      <td>0.620581</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.583854</td>\n",
       "      <td>0.590695</td>\n",
       "      <td>0.621843</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.335613</td>\n",
       "      <td>0.553571</td>\n",
       "      <td>0.537879</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440777</td>\n",
       "      <td>0.522488</td>\n",
       "      <td>0.531813</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.421446</td>\n",
       "      <td>0.461066</td>\n",
       "      <td>0.442354</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.274754</td>\n",
       "      <td>0.569492</td>\n",
       "      <td>0.533114</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.499666</td>\n",
       "      <td>0.522321</td>\n",
       "      <td>0.538610</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.546032</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.563707</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.328421</td>\n",
       "      <td>0.476427</td>\n",
       "      <td>0.463320</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.524539</td>\n",
       "      <td>0.529806</td>\n",
       "      <td>0.537879</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.525253</td>\n",
       "      <td>0.525253</td>\n",
       "      <td>0.525253</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.350402</td>\n",
       "      <td>0.468696</td>\n",
       "      <td>0.465909</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.481804</td>\n",
       "      <td>0.551017</td>\n",
       "      <td>0.573942</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.462855</td>\n",
       "      <td>0.491032</td>\n",
       "      <td>0.486911</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.237041</td>\n",
       "      <td>0.542645</td>\n",
       "      <td>0.513696</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.509653</td>\n",
       "      <td>0.509653</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.358333</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.563707</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.137255</td>\n",
       "      <td>0.079545</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.381859</td>\n",
       "      <td>0.502262</td>\n",
       "      <td>0.502525</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.428342</td>\n",
       "      <td>0.516890</td>\n",
       "      <td>0.521465</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.205003</td>\n",
       "      <td>0.618280</td>\n",
       "      <td>0.506944</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.314547</td>\n",
       "      <td>0.525494</td>\n",
       "      <td>0.520024</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.320452</td>\n",
       "      <td>0.465677</td>\n",
       "      <td>0.463939</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.176101</td>\n",
       "      <td>0.106870</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.507379</td>\n",
       "      <td>0.526599</td>\n",
       "      <td>0.545367</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.562552</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.577220</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.328421</td>\n",
       "      <td>0.476427</td>\n",
       "      <td>0.463320</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.524539</td>\n",
       "      <td>0.529806</td>\n",
       "      <td>0.537879</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.525253</td>\n",
       "      <td>0.525253</td>\n",
       "      <td>0.525253</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.350402</td>\n",
       "      <td>0.468696</td>\n",
       "      <td>0.465909</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.475523</td>\n",
       "      <td>0.547920</td>\n",
       "      <td>0.569088</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.460153</td>\n",
       "      <td>0.489391</td>\n",
       "      <td>0.484483</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.237041</td>\n",
       "      <td>0.542645</td>\n",
       "      <td>0.513696</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.461874</td>\n",
       "      <td>0.432432</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.434039</td>\n",
       "      <td>0.479332</td>\n",
       "      <td>0.462355</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.225673</td>\n",
       "      <td>0.476923</td>\n",
       "      <td>0.482625</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.567434</td>\n",
       "      <td>0.621031</td>\n",
       "      <td>0.666035</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.644657</td>\n",
       "      <td>0.654304</td>\n",
       "      <td>0.712753</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.226974</td>\n",
       "      <td>0.491667</td>\n",
       "      <td>0.498106</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.414110</td>\n",
       "      <td>0.559656</td>\n",
       "      <td>0.570128</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.429608</td>\n",
       "      <td>0.505201</td>\n",
       "      <td>0.507455</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.230197</td>\n",
       "      <td>0.568333</td>\n",
       "      <td>0.517770</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.328765</td>\n",
       "      <td>0.562138</td>\n",
       "      <td>0.572394</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.421596</td>\n",
       "      <td>0.548104</td>\n",
       "      <td>0.582046</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.180375</td>\n",
       "      <td>0.582353</td>\n",
       "      <td>0.520270</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.237693</td>\n",
       "      <td>0.367996</td>\n",
       "      <td>0.412247</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.287153</td>\n",
       "      <td>0.395897</td>\n",
       "      <td>0.392677</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.175149</td>\n",
       "      <td>0.201124</td>\n",
       "      <td>0.416035</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.418768</td>\n",
       "      <td>0.459467</td>\n",
       "      <td>0.439927</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.387938</td>\n",
       "      <td>0.410678</td>\n",
       "      <td>0.378901</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.281874</td>\n",
       "      <td>0.539683</td>\n",
       "      <td>0.522538</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.137255</td>\n",
       "      <td>0.079545</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.137255</td>\n",
       "      <td>0.079545</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.137255</td>\n",
       "      <td>0.079545</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.189655</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.189655</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.189655</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.176101</td>\n",
       "      <td>0.106870</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.176101</td>\n",
       "      <td>0.106870</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.176101</td>\n",
       "      <td>0.106870</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.180375</td>\n",
       "      <td>0.582353</td>\n",
       "      <td>0.520270</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.207621</td>\n",
       "      <td>0.584337</td>\n",
       "      <td>0.533784</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.151947</td>\n",
       "      <td>0.580460</td>\n",
       "      <td>0.506757</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.424490</td>\n",
       "      <td>0.555430</td>\n",
       "      <td>0.561869</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.477778</td>\n",
       "      <td>0.554598</td>\n",
       "      <td>0.571970</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.220009</td>\n",
       "      <td>0.619565</td>\n",
       "      <td>0.513889</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.268383</td>\n",
       "      <td>0.529705</td>\n",
       "      <td>0.515257</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.276254</td>\n",
       "      <td>0.453206</td>\n",
       "      <td>0.463245</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.176101</td>\n",
       "      <td>0.106870</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.492410</td>\n",
       "      <td>0.537500</td>\n",
       "      <td>0.569498</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.556322</td>\n",
       "      <td>0.594595</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.283258</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.574324</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.421438</td>\n",
       "      <td>0.492413</td>\n",
       "      <td>0.489899</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.431849</td>\n",
       "      <td>0.467347</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.238426</td>\n",
       "      <td>0.422965</td>\n",
       "      <td>0.466540</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.375232</td>\n",
       "      <td>0.505726</td>\n",
       "      <td>0.506848</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.380164</td>\n",
       "      <td>0.453403</td>\n",
       "      <td>0.432819</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.205018</td>\n",
       "      <td>0.536415</td>\n",
       "      <td>0.505635</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.394297</td>\n",
       "      <td>0.481685</td>\n",
       "      <td>0.466216</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.398437</td>\n",
       "      <td>0.442266</td>\n",
       "      <td>0.397683</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.190569</td>\n",
       "      <td>0.402954</td>\n",
       "      <td>0.433398</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.434259</td>\n",
       "      <td>0.509579</td>\n",
       "      <td>0.512626</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.468159</td>\n",
       "      <td>0.511338</td>\n",
       "      <td>0.515783</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.286240</td>\n",
       "      <td>0.519048</td>\n",
       "      <td>0.510101</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.389206</td>\n",
       "      <td>0.513408</td>\n",
       "      <td>0.516557</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.373139</td>\n",
       "      <td>0.443142</td>\n",
       "      <td>0.417389</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.240012</td>\n",
       "      <td>0.575173</td>\n",
       "      <td>0.522625</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.513021</td>\n",
       "      <td>0.538126</td>\n",
       "      <td>0.567568</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.563692</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.592664</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.318903</td>\n",
       "      <td>0.471429</td>\n",
       "      <td>0.456564</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.470558</td>\n",
       "      <td>0.492130</td>\n",
       "      <td>0.489268</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.552381</td>\n",
       "      <td>0.550905</td>\n",
       "      <td>0.556818</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.392999</td>\n",
       "      <td>0.580702</td>\n",
       "      <td>0.572601</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.518042</td>\n",
       "      <td>0.562234</td>\n",
       "      <td>0.592493</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.451383</td>\n",
       "      <td>0.467678</td>\n",
       "      <td>0.455964</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.270152</td>\n",
       "      <td>0.567045</td>\n",
       "      <td>0.530687</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.418182</td>\n",
       "      <td>0.517094</td>\n",
       "      <td>0.530888</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.507246</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.236170</td>\n",
       "      <td>0.526723</td>\n",
       "      <td>0.518340</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.490250</td>\n",
       "      <td>0.572639</td>\n",
       "      <td>0.594697</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.553526</td>\n",
       "      <td>0.585106</td>\n",
       "      <td>0.618687</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.268610</td>\n",
       "      <td>0.549261</td>\n",
       "      <td>0.518939</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.439805</td>\n",
       "      <td>0.539066</td>\n",
       "      <td>0.552965</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.455563</td>\n",
       "      <td>0.497260</td>\n",
       "      <td>0.495926</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.242967</td>\n",
       "      <td>0.612450</td>\n",
       "      <td>0.531553</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.529097</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>0.581081</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.572234</td>\n",
       "      <td>0.568896</td>\n",
       "      <td>0.599421</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.370429</td>\n",
       "      <td>0.511905</td>\n",
       "      <td>0.519305</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.461458</td>\n",
       "      <td>0.480263</td>\n",
       "      <td>0.473485</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.503546</td>\n",
       "      <td>0.504058</td>\n",
       "      <td>0.504419</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.350402</td>\n",
       "      <td>0.514737</td>\n",
       "      <td>0.513258</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.498557</td>\n",
       "      <td>0.545343</td>\n",
       "      <td>0.567354</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.443973</td>\n",
       "      <td>0.462851</td>\n",
       "      <td>0.448682</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.265175</td>\n",
       "      <td>0.511897</td>\n",
       "      <td>0.506328</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            agg    dataset        f1      prec       rec  \\\n",
       "0     phrase_mv  Wikipedia  0.442135  0.503618  0.506757   \n",
       "1    phrase_max  Wikipedia  0.476780  0.510243  0.518340   \n",
       "2    phrase_min  Wikipedia  0.295090  0.520833  0.523166   \n",
       "3     phrase_mv   WikiNews  0.540917  0.587095  0.620581   \n",
       "4    phrase_max   WikiNews  0.583854  0.590695  0.621843   \n",
       "5    phrase_min   WikiNews  0.335613  0.553571  0.537879   \n",
       "6     phrase_mv       News  0.440777  0.522488  0.531813   \n",
       "7    phrase_max       News  0.421446  0.461066  0.442354   \n",
       "8    phrase_min       News  0.274754  0.569492  0.533114   \n",
       "9     phrase_mv  Wikipedia  0.499666  0.522321  0.538610   \n",
       "10   phrase_max  Wikipedia  0.546032  0.545455  0.563707   \n",
       "11   phrase_min  Wikipedia  0.328421  0.476427  0.463320   \n",
       "12    phrase_mv   WikiNews  0.524539  0.529806  0.537879   \n",
       "13   phrase_max   WikiNews  0.525253  0.525253  0.525253   \n",
       "14   phrase_min   WikiNews  0.350402  0.468696  0.465909   \n",
       "15    phrase_mv       News  0.481804  0.551017  0.573942   \n",
       "16   phrase_max       News  0.462855  0.491032  0.486911   \n",
       "17   phrase_min       News  0.237041  0.542645  0.513696   \n",
       "18    phrase_mv  Wikipedia  0.272727  0.509653  0.509653   \n",
       "19   phrase_max  Wikipedia  0.358333  0.545455  0.563707   \n",
       "20   phrase_min  Wikipedia  0.137255  0.079545  0.500000   \n",
       "21    phrase_mv   WikiNews  0.381859  0.502262  0.502525   \n",
       "22   phrase_max   WikiNews  0.428342  0.516890  0.521465   \n",
       "23   phrase_min   WikiNews  0.205003  0.618280  0.506944   \n",
       "24    phrase_mv       News  0.314547  0.525494  0.520024   \n",
       "25   phrase_max       News  0.320452  0.465677  0.463939   \n",
       "26   phrase_min       News  0.176101  0.106870  0.500000   \n",
       "27    phrase_mv  Wikipedia  0.507379  0.526599  0.545367   \n",
       "28   phrase_max  Wikipedia  0.562552  0.558824  0.577220   \n",
       "29   phrase_min  Wikipedia  0.328421  0.476427  0.463320   \n",
       "30    phrase_mv   WikiNews  0.524539  0.529806  0.537879   \n",
       "31   phrase_max   WikiNews  0.525253  0.525253  0.525253   \n",
       "32   phrase_min   WikiNews  0.350402  0.468696  0.465909   \n",
       "33    phrase_mv       News  0.475523  0.547920  0.569088   \n",
       "34   phrase_max       News  0.460153  0.489391  0.484483   \n",
       "35   phrase_min       News  0.237041  0.542645  0.513696   \n",
       "36    phrase_mv  Wikipedia  0.352941  0.461874  0.432432   \n",
       "37   phrase_max  Wikipedia  0.434039  0.479332  0.462355   \n",
       "38   phrase_min  Wikipedia  0.225673  0.476923  0.482625   \n",
       "39    phrase_mv   WikiNews  0.567434  0.621031  0.666035   \n",
       "40   phrase_max   WikiNews  0.644657  0.654304  0.712753   \n",
       "41   phrase_min   WikiNews  0.226974  0.491667  0.498106   \n",
       "42    phrase_mv       News  0.414110  0.559656  0.570128   \n",
       "43   phrase_max       News  0.429608  0.505201  0.507455   \n",
       "44   phrase_min       News  0.230197  0.568333  0.517770   \n",
       "45    phrase_mv  Wikipedia  0.328765  0.562138  0.572394   \n",
       "46   phrase_max  Wikipedia  0.421596  0.548104  0.582046   \n",
       "47   phrase_min  Wikipedia  0.180375  0.582353  0.520270   \n",
       "48    phrase_mv   WikiNews  0.237693  0.367996  0.412247   \n",
       "49   phrase_max   WikiNews  0.287153  0.395897  0.392677   \n",
       "50   phrase_min   WikiNews  0.175149  0.201124  0.416035   \n",
       "51    phrase_mv       News  0.418768  0.459467  0.439927   \n",
       "52   phrase_max       News  0.387938  0.410678  0.378901   \n",
       "53   phrase_min       News  0.281874  0.539683  0.522538   \n",
       "54    phrase_mv  Wikipedia  0.137255  0.079545  0.500000   \n",
       "55   phrase_max  Wikipedia  0.137255  0.079545  0.500000   \n",
       "56   phrase_min  Wikipedia  0.137255  0.079545  0.500000   \n",
       "57    phrase_mv   WikiNews  0.189655  0.117021  0.500000   \n",
       "58   phrase_max   WikiNews  0.189655  0.117021  0.500000   \n",
       "59   phrase_min   WikiNews  0.189655  0.117021  0.500000   \n",
       "60    phrase_mv       News  0.176101  0.106870  0.500000   \n",
       "61   phrase_max       News  0.176101  0.106870  0.500000   \n",
       "62   phrase_min       News  0.176101  0.106870  0.500000   \n",
       "63    phrase_mv  Wikipedia  0.180375  0.582353  0.520270   \n",
       "64   phrase_max  Wikipedia  0.207621  0.584337  0.533784   \n",
       "65   phrase_min  Wikipedia  0.151947  0.580460  0.506757   \n",
       "66    phrase_mv   WikiNews  0.424490  0.555430  0.561869   \n",
       "67   phrase_max   WikiNews  0.477778  0.554598  0.571970   \n",
       "68   phrase_min   WikiNews  0.220009  0.619565  0.513889   \n",
       "69    phrase_mv       News  0.268383  0.529705  0.515257   \n",
       "70   phrase_max       News  0.276254  0.453206  0.463245   \n",
       "71   phrase_min       News  0.176101  0.106870  0.500000   \n",
       "72    phrase_mv  Wikipedia  0.492410  0.537500  0.569498   \n",
       "73   phrase_max  Wikipedia  0.545455  0.556322  0.594595   \n",
       "74   phrase_min  Wikipedia  0.283258  0.590909  0.574324   \n",
       "75    phrase_mv   WikiNews  0.421438  0.492413  0.489899   \n",
       "76   phrase_max   WikiNews  0.431849  0.467347  0.454545   \n",
       "77   phrase_min   WikiNews  0.238426  0.422965  0.466540   \n",
       "78    phrase_mv       News  0.375232  0.505726  0.506848   \n",
       "79   phrase_max       News  0.380164  0.453403  0.432819   \n",
       "80   phrase_min       News  0.205018  0.536415  0.505635   \n",
       "81    phrase_mv  Wikipedia  0.394297  0.481685  0.466216   \n",
       "82   phrase_max  Wikipedia  0.398437  0.442266  0.397683   \n",
       "83   phrase_min  Wikipedia  0.190569  0.402954  0.433398   \n",
       "84    phrase_mv   WikiNews  0.434259  0.509579  0.512626   \n",
       "85   phrase_max   WikiNews  0.468159  0.511338  0.515783   \n",
       "86   phrase_min   WikiNews  0.286240  0.519048  0.510101   \n",
       "87    phrase_mv       News  0.389206  0.513408  0.516557   \n",
       "88   phrase_max       News  0.373139  0.443142  0.417389   \n",
       "89   phrase_min       News  0.240012  0.575173  0.522625   \n",
       "90    phrase_mv  Wikipedia  0.513021  0.538126  0.567568   \n",
       "91   phrase_max  Wikipedia  0.563692  0.562500  0.592664   \n",
       "92   phrase_min  Wikipedia  0.318903  0.471429  0.456564   \n",
       "93    phrase_mv   WikiNews  0.470558  0.492130  0.489268   \n",
       "94   phrase_max   WikiNews  0.552381  0.550905  0.556818   \n",
       "95   phrase_min   WikiNews  0.392999  0.580702  0.572601   \n",
       "96    phrase_mv       News  0.518042  0.562234  0.592493   \n",
       "97   phrase_max       News  0.451383  0.467678  0.455964   \n",
       "98   phrase_min       News  0.270152  0.567045  0.530687   \n",
       "99    phrase_mv  Wikipedia  0.418182  0.517094  0.530888   \n",
       "100  phrase_max  Wikipedia  0.450000  0.507246  0.513514   \n",
       "101  phrase_min  Wikipedia  0.236170  0.526723  0.518340   \n",
       "102   phrase_mv   WikiNews  0.490250  0.572639  0.594697   \n",
       "103  phrase_max   WikiNews  0.553526  0.585106  0.618687   \n",
       "104  phrase_min   WikiNews  0.268610  0.549261  0.518939   \n",
       "105   phrase_mv       News  0.439805  0.539066  0.552965   \n",
       "106  phrase_max       News  0.455563  0.497260  0.495926   \n",
       "107  phrase_min       News  0.242967  0.612450  0.531553   \n",
       "108   phrase_mv  Wikipedia  0.529097  0.546875  0.581081   \n",
       "109  phrase_max  Wikipedia  0.572234  0.568896  0.599421   \n",
       "110  phrase_min  Wikipedia  0.370429  0.511905  0.519305   \n",
       "111   phrase_mv   WikiNews  0.461458  0.480263  0.473485   \n",
       "112  phrase_max   WikiNews  0.503546  0.504058  0.504419   \n",
       "113  phrase_min   WikiNews  0.350402  0.514737  0.513258   \n",
       "114   phrase_mv       News  0.498557  0.545343  0.567354   \n",
       "115  phrase_max       News  0.443973  0.462851  0.448682   \n",
       "116  phrase_min       News  0.265175  0.511897  0.506328   \n",
       "\n",
       "                           zc  \n",
       "0                  linguistic  \n",
       "1                  linguistic  \n",
       "2                  linguistic  \n",
       "3                  linguistic  \n",
       "4                  linguistic  \n",
       "5                  linguistic  \n",
       "6                  linguistic  \n",
       "7                  linguistic  \n",
       "8                  linguistic  \n",
       "9                   frequency  \n",
       "10                  frequency  \n",
       "11                  frequency  \n",
       "12                  frequency  \n",
       "13                  frequency  \n",
       "14                  frequency  \n",
       "15                  frequency  \n",
       "16                  frequency  \n",
       "17                  frequency  \n",
       "18             language_model  \n",
       "19             language_model  \n",
       "20             language_model  \n",
       "21             language_model  \n",
       "22             language_model  \n",
       "23             language_model  \n",
       "24             language_model  \n",
       "25             language_model  \n",
       "26             language_model  \n",
       "27                     corpus  \n",
       "28                     corpus  \n",
       "29                     corpus  \n",
       "30                     corpus  \n",
       "31                     corpus  \n",
       "32                     corpus  \n",
       "33                     corpus  \n",
       "34                     corpus  \n",
       "35                     corpus  \n",
       "36           psycholinguistic  \n",
       "37           psycholinguistic  \n",
       "38           psycholinguistic  \n",
       "39           psycholinguistic  \n",
       "40           psycholinguistic  \n",
       "41           psycholinguistic  \n",
       "42           psycholinguistic  \n",
       "43           psycholinguistic  \n",
       "44           psycholinguistic  \n",
       "45                    wordnet  \n",
       "46                    wordnet  \n",
       "47                    wordnet  \n",
       "48                    wordnet  \n",
       "49                    wordnet  \n",
       "50                    wordnet  \n",
       "51                    wordnet  \n",
       "52                    wordnet  \n",
       "53                    wordnet  \n",
       "54                    dbpedia  \n",
       "55                    dbpedia  \n",
       "56                    dbpedia  \n",
       "57                    dbpedia  \n",
       "58                    dbpedia  \n",
       "59                    dbpedia  \n",
       "60                    dbpedia  \n",
       "61                    dbpedia  \n",
       "62                    dbpedia  \n",
       "63           brown_clustering  \n",
       "64           brown_clustering  \n",
       "65           brown_clustering  \n",
       "66           brown_clustering  \n",
       "67           brown_clustering  \n",
       "68           brown_clustering  \n",
       "69           brown_clustering  \n",
       "70           brown_clustering  \n",
       "71           brown_clustering  \n",
       "72                   semantic  \n",
       "73                   semantic  \n",
       "74                   semantic  \n",
       "75                   semantic  \n",
       "76                   semantic  \n",
       "77                   semantic  \n",
       "78                   semantic  \n",
       "79                   semantic  \n",
       "80                   semantic  \n",
       "81                 dictionary  \n",
       "82                 dictionary  \n",
       "83                 dictionary  \n",
       "84                 dictionary  \n",
       "85                 dictionary  \n",
       "86                 dictionary  \n",
       "87                 dictionary  \n",
       "88                 dictionary  \n",
       "89                 dictionary  \n",
       "90            corpus+semantic  \n",
       "91            corpus+semantic  \n",
       "92            corpus+semantic  \n",
       "93            corpus+semantic  \n",
       "94            corpus+semantic  \n",
       "95            corpus+semantic  \n",
       "96            corpus+semantic  \n",
       "97            corpus+semantic  \n",
       "98            corpus+semantic  \n",
       "99   wordnet+psycholinguistic  \n",
       "100  wordnet+psycholinguistic  \n",
       "101  wordnet+psycholinguistic  \n",
       "102  wordnet+psycholinguistic  \n",
       "103  wordnet+psycholinguistic  \n",
       "104  wordnet+psycholinguistic  \n",
       "105  wordnet+psycholinguistic  \n",
       "106  wordnet+psycholinguistic  \n",
       "107  wordnet+psycholinguistic  \n",
       "108                       all  \n",
       "109                       all  \n",
       "110                       all  \n",
       "111                       all  \n",
       "112                       all  \n",
       "113                       all  \n",
       "114                       all  \n",
       "115                       all  \n",
       "116                       all  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_eval_data_lr = create_eval_df_from_results_phrase(results_lr)\n",
    "feature_eval_data_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.644657</td>\n",
       "      <td>0.654304</td>\n",
       "      <td>0.712753</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.518042</td>\n",
       "      <td>0.562234</td>\n",
       "      <td>0.592493</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.572234</td>\n",
       "      <td>0.568896</td>\n",
       "      <td>0.599421</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            agg    dataset        f1      prec       rec                zc\n",
       "40   phrase_max   WikiNews  0.644657  0.654304  0.712753  psycholinguistic\n",
       "96    phrase_mv       News  0.518042  0.562234  0.592493   corpus+semantic\n",
       "109  phrase_max  Wikipedia  0.572234  0.568896  0.599421               all"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = feature_eval_data_lr.groupby(['dataset'])['f1'].transform(max) == feature_eval_data_lr['f1']\n",
    "feature_eval_data_lr[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.7 SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "results_svm = [Result(fs, fs.fc, agg, phrase_merger(fs.test, svm(*transform_feat_to_num(\n",
    "                remove_labels_for_binary_df(fs.train), \n",
    "                remove_labels_phrase_for_binary_df(fs.test))), agg.agg)) \n",
    "          for fs in all_fc_datasets\n",
    "          for agg in phrase_aggs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.418391</td>\n",
       "      <td>0.492754</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.455729</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.482625</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.248447</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.525097</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.514098</td>\n",
       "      <td>0.573148</td>\n",
       "      <td>0.599747</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.517949</td>\n",
       "      <td>0.541496</td>\n",
       "      <td>0.557449</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.311355</td>\n",
       "      <td>0.538618</td>\n",
       "      <td>0.523990</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.425024</td>\n",
       "      <td>0.506434</td>\n",
       "      <td>0.509102</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.426789</td>\n",
       "      <td>0.464261</td>\n",
       "      <td>0.447209</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.276125</td>\n",
       "      <td>0.551814</td>\n",
       "      <td>0.526612</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.365079</td>\n",
       "      <td>0.527302</td>\n",
       "      <td>0.541506</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.421596</td>\n",
       "      <td>0.548104</td>\n",
       "      <td>0.582046</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.166316</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.411631</td>\n",
       "      <td>0.519629</td>\n",
       "      <td>0.523359</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.437644</td>\n",
       "      <td>0.522059</td>\n",
       "      <td>0.528409</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.263133</td>\n",
       "      <td>0.623596</td>\n",
       "      <td>0.534722</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.566198</td>\n",
       "      <td>0.611033</td>\n",
       "      <td>0.664702</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.556148</td>\n",
       "      <td>0.572490</td>\n",
       "      <td>0.605322</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.291609</td>\n",
       "      <td>0.596034</td>\n",
       "      <td>0.549324</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.137255</td>\n",
       "      <td>0.079545</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.137255</td>\n",
       "      <td>0.079545</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.137255</td>\n",
       "      <td>0.079545</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.189655</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.189655</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.189655</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.176101</td>\n",
       "      <td>0.106870</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.176101</td>\n",
       "      <td>0.106870</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.176101</td>\n",
       "      <td>0.106870</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.365079</td>\n",
       "      <td>0.527302</td>\n",
       "      <td>0.541506</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.421596</td>\n",
       "      <td>0.548104</td>\n",
       "      <td>0.582046</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.166316</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.411631</td>\n",
       "      <td>0.519629</td>\n",
       "      <td>0.523359</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.437644</td>\n",
       "      <td>0.522059</td>\n",
       "      <td>0.528409</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.263133</td>\n",
       "      <td>0.623596</td>\n",
       "      <td>0.534722</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.566198</td>\n",
       "      <td>0.611033</td>\n",
       "      <td>0.664702</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.556148</td>\n",
       "      <td>0.572490</td>\n",
       "      <td>0.605322</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.291609</td>\n",
       "      <td>0.596034</td>\n",
       "      <td>0.549324</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.318903</td>\n",
       "      <td>0.471429</td>\n",
       "      <td>0.456564</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.377963</td>\n",
       "      <td>0.474033</td>\n",
       "      <td>0.452703</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.148090</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.442085</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.480902</td>\n",
       "      <td>0.568137</td>\n",
       "      <td>0.587753</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.540917</td>\n",
       "      <td>0.587095</td>\n",
       "      <td>0.620581</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.268610</td>\n",
       "      <td>0.549261</td>\n",
       "      <td>0.518939</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.516878</td>\n",
       "      <td>0.583826</td>\n",
       "      <td>0.621793</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.506526</td>\n",
       "      <td>0.529934</td>\n",
       "      <td>0.543689</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.235122</td>\n",
       "      <td>0.571980</td>\n",
       "      <td>0.520198</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.368278</td>\n",
       "      <td>0.481818</td>\n",
       "      <td>0.468147</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.402377</td>\n",
       "      <td>0.485417</td>\n",
       "      <td>0.472973</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.207621</td>\n",
       "      <td>0.584337</td>\n",
       "      <td>0.533784</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.560284</td>\n",
       "      <td>0.558551</td>\n",
       "      <td>0.563763</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.519006</td>\n",
       "      <td>0.530357</td>\n",
       "      <td>0.521465</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.437644</td>\n",
       "      <td>0.522059</td>\n",
       "      <td>0.528409</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.532097</td>\n",
       "      <td>0.532253</td>\n",
       "      <td>0.538575</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.493806</td>\n",
       "      <td>0.494060</td>\n",
       "      <td>0.494626</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.362363</td>\n",
       "      <td>0.559660</td>\n",
       "      <td>0.555652</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.292164</td>\n",
       "      <td>0.473529</td>\n",
       "      <td>0.465251</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.328421</td>\n",
       "      <td>0.476427</td>\n",
       "      <td>0.463320</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.171115</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.484556</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.424490</td>\n",
       "      <td>0.555430</td>\n",
       "      <td>0.561869</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.459273</td>\n",
       "      <td>0.545098</td>\n",
       "      <td>0.558081</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.307869</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.539773</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.467598</td>\n",
       "      <td>0.571139</td>\n",
       "      <td>0.595960</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.472331</td>\n",
       "      <td>0.528052</td>\n",
       "      <td>0.541436</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.279326</td>\n",
       "      <td>0.571779</td>\n",
       "      <td>0.535541</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.385458</td>\n",
       "      <td>0.489757</td>\n",
       "      <td>0.481660</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.507246</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.194139</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.527027</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.434570</td>\n",
       "      <td>0.560254</td>\n",
       "      <td>0.568813</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.468559</td>\n",
       "      <td>0.549879</td>\n",
       "      <td>0.565025</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.249060</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.570657</td>\n",
       "      <td>0.599907</td>\n",
       "      <td>0.648405</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.528176</td>\n",
       "      <td>0.539289</td>\n",
       "      <td>0.554958</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.318220</td>\n",
       "      <td>0.602919</td>\n",
       "      <td>0.563887</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.260504</td>\n",
       "      <td>0.452775</td>\n",
       "      <td>0.444981</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.478764</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.151947</td>\n",
       "      <td>0.580460</td>\n",
       "      <td>0.506757</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.368840</td>\n",
       "      <td>0.599578</td>\n",
       "      <td>0.574495</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.382699</td>\n",
       "      <td>0.553378</td>\n",
       "      <td>0.549874</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.263133</td>\n",
       "      <td>0.623596</td>\n",
       "      <td>0.534722</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.558438</td>\n",
       "      <td>0.611124</td>\n",
       "      <td>0.663922</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.546959</td>\n",
       "      <td>0.569437</td>\n",
       "      <td>0.602115</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.263946</td>\n",
       "      <td>0.586853</td>\n",
       "      <td>0.534761</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.419020</td>\n",
       "      <td>0.504710</td>\n",
       "      <td>0.508687</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.440803</td>\n",
       "      <td>0.473214</td>\n",
       "      <td>0.453668</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.202899</td>\n",
       "      <td>0.420513</td>\n",
       "      <td>0.440154</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.425752</td>\n",
       "      <td>0.486111</td>\n",
       "      <td>0.481061</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.476513</td>\n",
       "      <td>0.516304</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.268315</td>\n",
       "      <td>0.443089</td>\n",
       "      <td>0.464646</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.361842</td>\n",
       "      <td>0.509137</td>\n",
       "      <td>0.510142</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.360205</td>\n",
       "      <td>0.438048</td>\n",
       "      <td>0.411755</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.235122</td>\n",
       "      <td>0.571980</td>\n",
       "      <td>0.520198</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.283998</td>\n",
       "      <td>0.515525</td>\n",
       "      <td>0.516409</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.337831</td>\n",
       "      <td>0.538235</td>\n",
       "      <td>0.550193</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.151947</td>\n",
       "      <td>0.580460</td>\n",
       "      <td>0.506757</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.368840</td>\n",
       "      <td>0.599578</td>\n",
       "      <td>0.574495</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.382699</td>\n",
       "      <td>0.553378</td>\n",
       "      <td>0.549874</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.263133</td>\n",
       "      <td>0.623596</td>\n",
       "      <td>0.534722</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.556915</td>\n",
       "      <td>0.614431</td>\n",
       "      <td>0.667996</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.546565</td>\n",
       "      <td>0.571890</td>\n",
       "      <td>0.606189</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.263946</td>\n",
       "      <td>0.586853</td>\n",
       "      <td>0.534761</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.365385</td>\n",
       "      <td>0.494253</td>\n",
       "      <td>0.490347</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.402356</td>\n",
       "      <td>0.497350</td>\n",
       "      <td>0.495174</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.207621</td>\n",
       "      <td>0.584337</td>\n",
       "      <td>0.533784</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.401818</td>\n",
       "      <td>0.514069</td>\n",
       "      <td>0.516414</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.434259</td>\n",
       "      <td>0.509579</td>\n",
       "      <td>0.512626</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.303704</td>\n",
       "      <td>0.627907</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.512847</td>\n",
       "      <td>0.512706</td>\n",
       "      <td>0.513350</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.483161</td>\n",
       "      <td>0.483434</td>\n",
       "      <td>0.486477</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.388422</td>\n",
       "      <td>0.541589</td>\n",
       "      <td>0.546637</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.283998</td>\n",
       "      <td>0.515525</td>\n",
       "      <td>0.516409</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.327374</td>\n",
       "      <td>0.534325</td>\n",
       "      <td>0.543436</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.151947</td>\n",
       "      <td>0.580460</td>\n",
       "      <td>0.506757</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.347445</td>\n",
       "      <td>0.559916</td>\n",
       "      <td>0.544823</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.361702</td>\n",
       "      <td>0.504419</td>\n",
       "      <td>0.504419</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.249060</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.524673</td>\n",
       "      <td>0.600266</td>\n",
       "      <td>0.643724</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.520147</td>\n",
       "      <td>0.554038</td>\n",
       "      <td>0.580270</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.240012</td>\n",
       "      <td>0.575173</td>\n",
       "      <td>0.522625</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            agg    dataset        f1      prec       rec  \\\n",
       "0     phrase_mv  Wikipedia  0.418391  0.492754  0.486486   \n",
       "1    phrase_max  Wikipedia  0.455729  0.490196  0.482625   \n",
       "2    phrase_min  Wikipedia  0.248447  0.533333  0.525097   \n",
       "3     phrase_mv   WikiNews  0.514098  0.573148  0.599747   \n",
       "4    phrase_max   WikiNews  0.517949  0.541496  0.557449   \n",
       "5    phrase_min   WikiNews  0.311355  0.538618  0.523990   \n",
       "6     phrase_mv       News  0.425024  0.506434  0.509102   \n",
       "7    phrase_max       News  0.426789  0.464261  0.447209   \n",
       "8    phrase_min       News  0.276125  0.551814  0.526612   \n",
       "9     phrase_mv  Wikipedia  0.365079  0.527302  0.541506   \n",
       "10   phrase_max  Wikipedia  0.421596  0.548104  0.582046   \n",
       "11   phrase_min  Wikipedia  0.166316  0.581395  0.513514   \n",
       "12    phrase_mv   WikiNews  0.411631  0.519629  0.523359   \n",
       "13   phrase_max   WikiNews  0.437644  0.522059  0.528409   \n",
       "14   phrase_min   WikiNews  0.263133  0.623596  0.534722   \n",
       "15    phrase_mv       News  0.566198  0.611033  0.664702   \n",
       "16   phrase_max       News  0.556148  0.572490  0.605322   \n",
       "17   phrase_min       News  0.291609  0.596034  0.549324   \n",
       "18    phrase_mv  Wikipedia  0.137255  0.079545  0.500000   \n",
       "19   phrase_max  Wikipedia  0.137255  0.079545  0.500000   \n",
       "20   phrase_min  Wikipedia  0.137255  0.079545  0.500000   \n",
       "21    phrase_mv   WikiNews  0.189655  0.117021  0.500000   \n",
       "22   phrase_max   WikiNews  0.189655  0.117021  0.500000   \n",
       "23   phrase_min   WikiNews  0.189655  0.117021  0.500000   \n",
       "24    phrase_mv       News  0.176101  0.106870  0.500000   \n",
       "25   phrase_max       News  0.176101  0.106870  0.500000   \n",
       "26   phrase_min       News  0.176101  0.106870  0.500000   \n",
       "27    phrase_mv  Wikipedia  0.365079  0.527302  0.541506   \n",
       "28   phrase_max  Wikipedia  0.421596  0.548104  0.582046   \n",
       "29   phrase_min  Wikipedia  0.166316  0.581395  0.513514   \n",
       "30    phrase_mv   WikiNews  0.411631  0.519629  0.523359   \n",
       "31   phrase_max   WikiNews  0.437644  0.522059  0.528409   \n",
       "32   phrase_min   WikiNews  0.263133  0.623596  0.534722   \n",
       "33    phrase_mv       News  0.566198  0.611033  0.664702   \n",
       "34   phrase_max       News  0.556148  0.572490  0.605322   \n",
       "35   phrase_min       News  0.291609  0.596034  0.549324   \n",
       "36    phrase_mv  Wikipedia  0.318903  0.471429  0.456564   \n",
       "37   phrase_max  Wikipedia  0.377963  0.474033  0.452703   \n",
       "38   phrase_min  Wikipedia  0.148090  0.321429  0.442085   \n",
       "39    phrase_mv   WikiNews  0.480902  0.568137  0.587753   \n",
       "40   phrase_max   WikiNews  0.540917  0.587095  0.620581   \n",
       "41   phrase_min   WikiNews  0.268610  0.549261  0.518939   \n",
       "42    phrase_mv       News  0.516878  0.583826  0.621793   \n",
       "43   phrase_max       News  0.506526  0.529934  0.543689   \n",
       "44   phrase_min       News  0.235122  0.571980  0.520198   \n",
       "45    phrase_mv  Wikipedia  0.368278  0.481818  0.468147   \n",
       "46   phrase_max  Wikipedia  0.402377  0.485417  0.472973   \n",
       "47   phrase_min  Wikipedia  0.207621  0.584337  0.533784   \n",
       "48    phrase_mv   WikiNews  0.560284  0.558551  0.563763   \n",
       "49   phrase_max   WikiNews  0.519006  0.530357  0.521465   \n",
       "50   phrase_min   WikiNews  0.437644  0.522059  0.528409   \n",
       "51    phrase_mv       News  0.532097  0.532253  0.538575   \n",
       "52   phrase_max       News  0.493806  0.494060  0.494626   \n",
       "53   phrase_min       News  0.362363  0.559660  0.555652   \n",
       "54    phrase_mv  Wikipedia  0.292164  0.473529  0.465251   \n",
       "55   phrase_max  Wikipedia  0.328421  0.476427  0.463320   \n",
       "56   phrase_min  Wikipedia  0.171115  0.452381  0.484556   \n",
       "57    phrase_mv   WikiNews  0.424490  0.555430  0.561869   \n",
       "58   phrase_max   WikiNews  0.459273  0.545098  0.558081   \n",
       "59   phrase_min   WikiNews  0.307869  0.575000  0.539773   \n",
       "60    phrase_mv       News  0.467598  0.571139  0.595960   \n",
       "61   phrase_max       News  0.472331  0.528052  0.541436   \n",
       "62   phrase_min       News  0.279326  0.571779  0.535541   \n",
       "63    phrase_mv  Wikipedia  0.385458  0.489757  0.481660   \n",
       "64   phrase_max  Wikipedia  0.450000  0.507246  0.513514   \n",
       "65   phrase_min  Wikipedia  0.194139  0.583333  0.527027   \n",
       "66    phrase_mv   WikiNews  0.434570  0.560254  0.568813   \n",
       "67   phrase_max   WikiNews  0.468559  0.549879  0.565025   \n",
       "68   phrase_min   WikiNews  0.249060  0.622222  0.527778   \n",
       "69    phrase_mv       News  0.570657  0.599907  0.648405   \n",
       "70   phrase_max       News  0.528176  0.539289  0.554958   \n",
       "71   phrase_min       News  0.318220  0.602919  0.563887   \n",
       "72    phrase_mv  Wikipedia  0.260504  0.452775  0.444981   \n",
       "73   phrase_max  Wikipedia  0.312500  0.484848  0.478764   \n",
       "74   phrase_min  Wikipedia  0.151947  0.580460  0.506757   \n",
       "75    phrase_mv   WikiNews  0.368840  0.599578  0.574495   \n",
       "76   phrase_max   WikiNews  0.382699  0.553378  0.549874   \n",
       "77   phrase_min   WikiNews  0.263133  0.623596  0.534722   \n",
       "78    phrase_mv       News  0.558438  0.611124  0.663922   \n",
       "79   phrase_max       News  0.546959  0.569437  0.602115   \n",
       "80   phrase_min       News  0.263946  0.586853  0.534761   \n",
       "81    phrase_mv  Wikipedia  0.419020  0.504710  0.508687   \n",
       "82   phrase_max  Wikipedia  0.440803  0.473214  0.453668   \n",
       "83   phrase_min  Wikipedia  0.202899  0.420513  0.440154   \n",
       "84    phrase_mv   WikiNews  0.425752  0.486111  0.481061   \n",
       "85   phrase_max   WikiNews  0.476513  0.516304  0.522727   \n",
       "86   phrase_min   WikiNews  0.268315  0.443089  0.464646   \n",
       "87    phrase_mv       News  0.361842  0.509137  0.510142   \n",
       "88   phrase_max       News  0.360205  0.438048  0.411755   \n",
       "89   phrase_min       News  0.235122  0.571980  0.520198   \n",
       "90    phrase_mv  Wikipedia  0.283998  0.515525  0.516409   \n",
       "91   phrase_max  Wikipedia  0.337831  0.538235  0.550193   \n",
       "92   phrase_min  Wikipedia  0.151947  0.580460  0.506757   \n",
       "93    phrase_mv   WikiNews  0.368840  0.599578  0.574495   \n",
       "94   phrase_max   WikiNews  0.382699  0.553378  0.549874   \n",
       "95   phrase_min   WikiNews  0.263133  0.623596  0.534722   \n",
       "96    phrase_mv       News  0.556915  0.614431  0.667996   \n",
       "97   phrase_max       News  0.546565  0.571890  0.606189   \n",
       "98   phrase_min       News  0.263946  0.586853  0.534761   \n",
       "99    phrase_mv  Wikipedia  0.365385  0.494253  0.490347   \n",
       "100  phrase_max  Wikipedia  0.402356  0.497350  0.495174   \n",
       "101  phrase_min  Wikipedia  0.207621  0.584337  0.533784   \n",
       "102   phrase_mv   WikiNews  0.401818  0.514069  0.516414   \n",
       "103  phrase_max   WikiNews  0.434259  0.509579  0.512626   \n",
       "104  phrase_min   WikiNews  0.303704  0.627907  0.555556   \n",
       "105   phrase_mv       News  0.512847  0.512706  0.513350   \n",
       "106  phrase_max       News  0.483161  0.483434  0.486477   \n",
       "107  phrase_min       News  0.388422  0.541589  0.546637   \n",
       "108   phrase_mv  Wikipedia  0.283998  0.515525  0.516409   \n",
       "109  phrase_max  Wikipedia  0.327374  0.534325  0.543436   \n",
       "110  phrase_min  Wikipedia  0.151947  0.580460  0.506757   \n",
       "111   phrase_mv   WikiNews  0.347445  0.559916  0.544823   \n",
       "112  phrase_max   WikiNews  0.361702  0.504419  0.504419   \n",
       "113  phrase_min   WikiNews  0.249060  0.622222  0.527778   \n",
       "114   phrase_mv       News  0.524673  0.600266  0.643724   \n",
       "115  phrase_max       News  0.520147  0.554038  0.580270   \n",
       "116  phrase_min       News  0.240012  0.575173  0.522625   \n",
       "\n",
       "                           zc  \n",
       "0                  linguistic  \n",
       "1                  linguistic  \n",
       "2                  linguistic  \n",
       "3                  linguistic  \n",
       "4                  linguistic  \n",
       "5                  linguistic  \n",
       "6                  linguistic  \n",
       "7                  linguistic  \n",
       "8                  linguistic  \n",
       "9                   frequency  \n",
       "10                  frequency  \n",
       "11                  frequency  \n",
       "12                  frequency  \n",
       "13                  frequency  \n",
       "14                  frequency  \n",
       "15                  frequency  \n",
       "16                  frequency  \n",
       "17                  frequency  \n",
       "18             language_model  \n",
       "19             language_model  \n",
       "20             language_model  \n",
       "21             language_model  \n",
       "22             language_model  \n",
       "23             language_model  \n",
       "24             language_model  \n",
       "25             language_model  \n",
       "26             language_model  \n",
       "27                     corpus  \n",
       "28                     corpus  \n",
       "29                     corpus  \n",
       "30                     corpus  \n",
       "31                     corpus  \n",
       "32                     corpus  \n",
       "33                     corpus  \n",
       "34                     corpus  \n",
       "35                     corpus  \n",
       "36           psycholinguistic  \n",
       "37           psycholinguistic  \n",
       "38           psycholinguistic  \n",
       "39           psycholinguistic  \n",
       "40           psycholinguistic  \n",
       "41           psycholinguistic  \n",
       "42           psycholinguistic  \n",
       "43           psycholinguistic  \n",
       "44           psycholinguistic  \n",
       "45                    wordnet  \n",
       "46                    wordnet  \n",
       "47                    wordnet  \n",
       "48                    wordnet  \n",
       "49                    wordnet  \n",
       "50                    wordnet  \n",
       "51                    wordnet  \n",
       "52                    wordnet  \n",
       "53                    wordnet  \n",
       "54                    dbpedia  \n",
       "55                    dbpedia  \n",
       "56                    dbpedia  \n",
       "57                    dbpedia  \n",
       "58                    dbpedia  \n",
       "59                    dbpedia  \n",
       "60                    dbpedia  \n",
       "61                    dbpedia  \n",
       "62                    dbpedia  \n",
       "63           brown_clustering  \n",
       "64           brown_clustering  \n",
       "65           brown_clustering  \n",
       "66           brown_clustering  \n",
       "67           brown_clustering  \n",
       "68           brown_clustering  \n",
       "69           brown_clustering  \n",
       "70           brown_clustering  \n",
       "71           brown_clustering  \n",
       "72                   semantic  \n",
       "73                   semantic  \n",
       "74                   semantic  \n",
       "75                   semantic  \n",
       "76                   semantic  \n",
       "77                   semantic  \n",
       "78                   semantic  \n",
       "79                   semantic  \n",
       "80                   semantic  \n",
       "81                 dictionary  \n",
       "82                 dictionary  \n",
       "83                 dictionary  \n",
       "84                 dictionary  \n",
       "85                 dictionary  \n",
       "86                 dictionary  \n",
       "87                 dictionary  \n",
       "88                 dictionary  \n",
       "89                 dictionary  \n",
       "90            corpus+semantic  \n",
       "91            corpus+semantic  \n",
       "92            corpus+semantic  \n",
       "93            corpus+semantic  \n",
       "94            corpus+semantic  \n",
       "95            corpus+semantic  \n",
       "96            corpus+semantic  \n",
       "97            corpus+semantic  \n",
       "98            corpus+semantic  \n",
       "99   wordnet+psycholinguistic  \n",
       "100  wordnet+psycholinguistic  \n",
       "101  wordnet+psycholinguistic  \n",
       "102  wordnet+psycholinguistic  \n",
       "103  wordnet+psycholinguistic  \n",
       "104  wordnet+psycholinguistic  \n",
       "105  wordnet+psycholinguistic  \n",
       "106  wordnet+psycholinguistic  \n",
       "107  wordnet+psycholinguistic  \n",
       "108                       all  \n",
       "109                       all  \n",
       "110                       all  \n",
       "111                       all  \n",
       "112                       all  \n",
       "113                       all  \n",
       "114                       all  \n",
       "115                       all  \n",
       "116                       all  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_eval_data_svm = create_eval_df_from_results_phrase(results_svm)\n",
    "feature_eval_data_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.455729</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.482625</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.560284</td>\n",
       "      <td>0.558551</td>\n",
       "      <td>0.563763</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.570657</td>\n",
       "      <td>0.599907</td>\n",
       "      <td>0.648405</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           agg    dataset        f1      prec       rec                zc\n",
       "1   phrase_max  Wikipedia  0.455729  0.490196  0.482625        linguistic\n",
       "48   phrase_mv   WikiNews  0.560284  0.558551  0.563763           wordnet\n",
       "69   phrase_mv       News  0.570657  0.599907  0.648405  brown_clustering"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = feature_eval_data_svm.groupby(['dataset'])['f1'].transform(max) == feature_eval_data_svm['f1']\n",
    "feature_eval_data_svm[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.8 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "results_nb = [Result(fs, fs.fc, agg, phrase_merger(fs.test, naive_bayes(*transform_feat_to_num(\n",
    "                remove_labels_for_binary_df(fs.train), \n",
    "                remove_labels_phrase_for_binary_df(fs.test))), agg.agg)) \n",
    "          for fs in all_fc_datasets\n",
    "          for agg in phrase_aggs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.557086</td>\n",
       "      <td>0.564935</td>\n",
       "      <td>0.553089</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.555784</td>\n",
       "      <td>0.616340</td>\n",
       "      <td>0.556187</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.519429</td>\n",
       "      <td>0.712106</td>\n",
       "      <td>0.537361</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.454167</td>\n",
       "      <td>0.518411</td>\n",
       "      <td>0.501647</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.570339</td>\n",
       "      <td>0.580811</td>\n",
       "      <td>0.565967</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.593846</td>\n",
       "      <td>0.600877</td>\n",
       "      <td>0.588803</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.576767</td>\n",
       "      <td>0.762745</td>\n",
       "      <td>0.564672</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.431405</td>\n",
       "      <td>0.488064</td>\n",
       "      <td>0.477799</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.512082</td>\n",
       "      <td>0.519409</td>\n",
       "      <td>0.514520</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.502646</td>\n",
       "      <td>0.587640</td>\n",
       "      <td>0.524621</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.494434</td>\n",
       "      <td>0.520064</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.641913</td>\n",
       "      <td>0.709412</td>\n",
       "      <td>0.622660</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.464320</td>\n",
       "      <td>0.504392</td>\n",
       "      <td>0.500867</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.535461</td>\n",
       "      <td>0.574709</td>\n",
       "      <td>0.611130</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.548718</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.546332</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.439490</td>\n",
       "      <td>0.415663</td>\n",
       "      <td>0.466216</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.409558</td>\n",
       "      <td>0.513477</td>\n",
       "      <td>0.524131</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.484388</td>\n",
       "      <td>0.508721</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.469448</td>\n",
       "      <td>0.551282</td>\n",
       "      <td>0.508838</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.493137</td>\n",
       "      <td>0.526268</td>\n",
       "      <td>0.536616</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.592170</td>\n",
       "      <td>0.603732</td>\n",
       "      <td>0.586252</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.436455</td>\n",
       "      <td>0.419461</td>\n",
       "      <td>0.472521</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.486626</td>\n",
       "      <td>0.557324</td>\n",
       "      <td>0.582871</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.593846</td>\n",
       "      <td>0.600877</td>\n",
       "      <td>0.588803</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.576767</td>\n",
       "      <td>0.762745</td>\n",
       "      <td>0.564672</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.431405</td>\n",
       "      <td>0.488064</td>\n",
       "      <td>0.477799</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.512082</td>\n",
       "      <td>0.519409</td>\n",
       "      <td>0.514520</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.502646</td>\n",
       "      <td>0.587640</td>\n",
       "      <td>0.524621</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.494434</td>\n",
       "      <td>0.520064</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.641913</td>\n",
       "      <td>0.709412</td>\n",
       "      <td>0.622660</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.464320</td>\n",
       "      <td>0.504392</td>\n",
       "      <td>0.500867</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.535461</td>\n",
       "      <td>0.574709</td>\n",
       "      <td>0.611130</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.503792</td>\n",
       "      <td>0.510033</td>\n",
       "      <td>0.514479</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.458462</td>\n",
       "      <td>0.456140</td>\n",
       "      <td>0.461390</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.402356</td>\n",
       "      <td>0.497350</td>\n",
       "      <td>0.495174</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.415158</td>\n",
       "      <td>0.426042</td>\n",
       "      <td>0.410354</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.412029</td>\n",
       "      <td>0.410633</td>\n",
       "      <td>0.413510</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.342054</td>\n",
       "      <td>0.423497</td>\n",
       "      <td>0.402778</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.559945</td>\n",
       "      <td>0.558946</td>\n",
       "      <td>0.561200</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.452387</td>\n",
       "      <td>0.448186</td>\n",
       "      <td>0.473388</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.464406</td>\n",
       "      <td>0.546569</td>\n",
       "      <td>0.565881</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.566807</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.557915</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.526294</td>\n",
       "      <td>0.925287</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.530917</td>\n",
       "      <td>0.540476</td>\n",
       "      <td>0.565637</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.430303</td>\n",
       "      <td>0.381720</td>\n",
       "      <td>0.493056</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.547164</td>\n",
       "      <td>0.546540</td>\n",
       "      <td>0.547980</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.499447</td>\n",
       "      <td>0.619455</td>\n",
       "      <td>0.523578</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.454167</td>\n",
       "      <td>0.518411</td>\n",
       "      <td>0.501647</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.601218</td>\n",
       "      <td>0.599548</td>\n",
       "      <td>0.633582</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.532468</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.530888</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.498491</td>\n",
       "      <td>0.521687</td>\n",
       "      <td>0.508687</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.540641</td>\n",
       "      <td>0.542051</td>\n",
       "      <td>0.539575</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.553656</td>\n",
       "      <td>0.769444</td>\n",
       "      <td>0.561237</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.553656</td>\n",
       "      <td>0.769444</td>\n",
       "      <td>0.561237</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.548077</td>\n",
       "      <td>0.592857</td>\n",
       "      <td>0.549242</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.454167</td>\n",
       "      <td>0.518411</td>\n",
       "      <td>0.501647</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.454167</td>\n",
       "      <td>0.518411</td>\n",
       "      <td>0.501647</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.549312</td>\n",
       "      <td>0.574049</td>\n",
       "      <td>0.547243</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.293996</td>\n",
       "      <td>0.495238</td>\n",
       "      <td>0.494208</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.361056</td>\n",
       "      <td>0.507893</td>\n",
       "      <td>0.512548</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.151947</td>\n",
       "      <td>0.580460</td>\n",
       "      <td>0.506757</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.327886</td>\n",
       "      <td>0.499236</td>\n",
       "      <td>0.499369</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.372269</td>\n",
       "      <td>0.511023</td>\n",
       "      <td>0.511364</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.205003</td>\n",
       "      <td>0.618280</td>\n",
       "      <td>0.506944</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.422636</td>\n",
       "      <td>0.545879</td>\n",
       "      <td>0.557906</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.397095</td>\n",
       "      <td>0.466589</td>\n",
       "      <td>0.451456</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.217370</td>\n",
       "      <td>0.516661</td>\n",
       "      <td>0.503988</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.498491</td>\n",
       "      <td>0.521687</td>\n",
       "      <td>0.508687</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.546032</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.563707</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.430303</td>\n",
       "      <td>0.381720</td>\n",
       "      <td>0.493056</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.562791</td>\n",
       "      <td>0.563927</td>\n",
       "      <td>0.561869</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.467755</td>\n",
       "      <td>0.536975</td>\n",
       "      <td>0.505721</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.435345</td>\n",
       "      <td>0.391473</td>\n",
       "      <td>0.490291</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.627028</td>\n",
       "      <td>0.620523</td>\n",
       "      <td>0.639130</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.489644</td>\n",
       "      <td>0.494118</td>\n",
       "      <td>0.492278</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.474851</td>\n",
       "      <td>0.473277</td>\n",
       "      <td>0.481660</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.424837</td>\n",
       "      <td>0.533769</td>\n",
       "      <td>0.559846</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.493628</td>\n",
       "      <td>0.507587</td>\n",
       "      <td>0.510101</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.495581</td>\n",
       "      <td>0.495581</td>\n",
       "      <td>0.495581</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.319149</td>\n",
       "      <td>0.445076</td>\n",
       "      <td>0.445076</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.424573</td>\n",
       "      <td>0.476541</td>\n",
       "      <td>0.465153</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.404227</td>\n",
       "      <td>0.438095</td>\n",
       "      <td>0.409847</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.252900</td>\n",
       "      <td>0.485485</td>\n",
       "      <td>0.492545</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.628138</td>\n",
       "      <td>0.632308</td>\n",
       "      <td>0.624517</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.566807</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.557915</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.465695</td>\n",
       "      <td>0.514583</td>\n",
       "      <td>0.527027</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.519006</td>\n",
       "      <td>0.530357</td>\n",
       "      <td>0.521465</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.502646</td>\n",
       "      <td>0.587640</td>\n",
       "      <td>0.524621</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.502517</td>\n",
       "      <td>0.519114</td>\n",
       "      <td>0.525884</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.627623</td>\n",
       "      <td>0.713502</td>\n",
       "      <td>0.609657</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.451194</td>\n",
       "      <td>0.475911</td>\n",
       "      <td>0.496793</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.537598</td>\n",
       "      <td>0.579116</td>\n",
       "      <td>0.617632</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.554205</td>\n",
       "      <td>0.551883</td>\n",
       "      <td>0.570463</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.474851</td>\n",
       "      <td>0.473277</td>\n",
       "      <td>0.481660</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.435268</td>\n",
       "      <td>0.524211</td>\n",
       "      <td>0.544402</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.484388</td>\n",
       "      <td>0.508721</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.423313</td>\n",
       "      <td>0.379121</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.447405</td>\n",
       "      <td>0.477200</td>\n",
       "      <td>0.468434</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.549577</td>\n",
       "      <td>0.554952</td>\n",
       "      <td>0.547330</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.430569</td>\n",
       "      <td>0.411364</td>\n",
       "      <td>0.462812</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.498642</td>\n",
       "      <td>0.555673</td>\n",
       "      <td>0.582004</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.628138</td>\n",
       "      <td>0.632308</td>\n",
       "      <td>0.624517</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.566807</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.557915</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.465695</td>\n",
       "      <td>0.514583</td>\n",
       "      <td>0.527027</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.519006</td>\n",
       "      <td>0.530357</td>\n",
       "      <td>0.521465</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.502646</td>\n",
       "      <td>0.587640</td>\n",
       "      <td>0.524621</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.502517</td>\n",
       "      <td>0.519114</td>\n",
       "      <td>0.525884</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.627623</td>\n",
       "      <td>0.713502</td>\n",
       "      <td>0.609657</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.451194</td>\n",
       "      <td>0.475911</td>\n",
       "      <td>0.496793</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.537598</td>\n",
       "      <td>0.579116</td>\n",
       "      <td>0.617632</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            agg    dataset        f1      prec       rec  \\\n",
       "0     phrase_mv  Wikipedia  0.453416  0.419540  0.493243   \n",
       "1    phrase_max  Wikipedia  0.456790  0.420455  0.500000   \n",
       "2    phrase_min  Wikipedia  0.557086  0.564935  0.553089   \n",
       "3     phrase_mv   WikiNews  0.433735  0.382979  0.500000   \n",
       "4    phrase_max   WikiNews  0.433735  0.382979  0.500000   \n",
       "5    phrase_min   WikiNews  0.555784  0.616340  0.556187   \n",
       "6     phrase_mv       News  0.519429  0.712106  0.537361   \n",
       "7    phrase_max       News  0.454167  0.518411  0.501647   \n",
       "8    phrase_min       News  0.570339  0.580811  0.565967   \n",
       "9     phrase_mv  Wikipedia  0.593846  0.600877  0.588803   \n",
       "10   phrase_max  Wikipedia  0.576767  0.762745  0.564672   \n",
       "11   phrase_min  Wikipedia  0.431405  0.488064  0.477799   \n",
       "12    phrase_mv   WikiNews  0.512082  0.519409  0.514520   \n",
       "13   phrase_max   WikiNews  0.502646  0.587640  0.524621   \n",
       "14   phrase_min   WikiNews  0.494434  0.520064  0.527778   \n",
       "15    phrase_mv       News  0.641913  0.709412  0.622660   \n",
       "16   phrase_max       News  0.464320  0.504392  0.500867   \n",
       "17   phrase_min       News  0.535461  0.574709  0.611130   \n",
       "18    phrase_mv  Wikipedia  0.548718  0.552632  0.546332   \n",
       "19   phrase_max  Wikipedia  0.439490  0.415663  0.466216   \n",
       "20   phrase_min  Wikipedia  0.409558  0.513477  0.524131   \n",
       "21    phrase_mv   WikiNews  0.484388  0.508721  0.503788   \n",
       "22   phrase_max   WikiNews  0.469448  0.551282  0.508838   \n",
       "23   phrase_min   WikiNews  0.493137  0.526268  0.536616   \n",
       "24    phrase_mv       News  0.592170  0.603732  0.586252   \n",
       "25   phrase_max       News  0.436455  0.419461  0.472521   \n",
       "26   phrase_min       News  0.486626  0.557324  0.582871   \n",
       "27    phrase_mv  Wikipedia  0.593846  0.600877  0.588803   \n",
       "28   phrase_max  Wikipedia  0.576767  0.762745  0.564672   \n",
       "29   phrase_min  Wikipedia  0.431405  0.488064  0.477799   \n",
       "30    phrase_mv   WikiNews  0.512082  0.519409  0.514520   \n",
       "31   phrase_max   WikiNews  0.502646  0.587640  0.524621   \n",
       "32   phrase_min   WikiNews  0.494434  0.520064  0.527778   \n",
       "33    phrase_mv       News  0.641913  0.709412  0.622660   \n",
       "34   phrase_max       News  0.464320  0.504392  0.500867   \n",
       "35   phrase_min       News  0.535461  0.574709  0.611130   \n",
       "36    phrase_mv  Wikipedia  0.503792  0.510033  0.514479   \n",
       "37   phrase_max  Wikipedia  0.458462  0.456140  0.461390   \n",
       "38   phrase_min  Wikipedia  0.402356  0.497350  0.495174   \n",
       "39    phrase_mv   WikiNews  0.415158  0.426042  0.410354   \n",
       "40   phrase_max   WikiNews  0.412029  0.410633  0.413510   \n",
       "41   phrase_min   WikiNews  0.342054  0.423497  0.402778   \n",
       "42    phrase_mv       News  0.559945  0.558946  0.561200   \n",
       "43   phrase_max       News  0.452387  0.448186  0.473388   \n",
       "44   phrase_min       News  0.464406  0.546569  0.565881   \n",
       "45    phrase_mv  Wikipedia  0.566807  0.678571  0.557915   \n",
       "46   phrase_max  Wikipedia  0.526294  0.925287  0.535714   \n",
       "47   phrase_min  Wikipedia  0.530917  0.540476  0.565637   \n",
       "48    phrase_mv   WikiNews  0.430303  0.381720  0.493056   \n",
       "49   phrase_max   WikiNews  0.433735  0.382979  0.500000   \n",
       "50   phrase_min   WikiNews  0.547164  0.546540  0.547980   \n",
       "51    phrase_mv       News  0.499447  0.619455  0.523578   \n",
       "52   phrase_max       News  0.454167  0.518411  0.501647   \n",
       "53   phrase_min       News  0.601218  0.599548  0.633582   \n",
       "54    phrase_mv  Wikipedia  0.532468  0.550000  0.530888   \n",
       "55   phrase_max  Wikipedia  0.498491  0.521687  0.508687   \n",
       "56   phrase_min  Wikipedia  0.540641  0.542051  0.539575   \n",
       "57    phrase_mv   WikiNews  0.553656  0.769444  0.561237   \n",
       "58   phrase_max   WikiNews  0.553656  0.769444  0.561237   \n",
       "59   phrase_min   WikiNews  0.548077  0.592857  0.549242   \n",
       "60    phrase_mv       News  0.454167  0.518411  0.501647   \n",
       "61   phrase_max       News  0.454167  0.518411  0.501647   \n",
       "62   phrase_min       News  0.549312  0.574049  0.547243   \n",
       "63    phrase_mv  Wikipedia  0.293996  0.495238  0.494208   \n",
       "64   phrase_max  Wikipedia  0.361056  0.507893  0.512548   \n",
       "65   phrase_min  Wikipedia  0.151947  0.580460  0.506757   \n",
       "66    phrase_mv   WikiNews  0.327886  0.499236  0.499369   \n",
       "67   phrase_max   WikiNews  0.372269  0.511023  0.511364   \n",
       "68   phrase_min   WikiNews  0.205003  0.618280  0.506944   \n",
       "69    phrase_mv       News  0.422636  0.545879  0.557906   \n",
       "70   phrase_max       News  0.397095  0.466589  0.451456   \n",
       "71   phrase_min       News  0.217370  0.516661  0.503988   \n",
       "72    phrase_mv  Wikipedia  0.498491  0.521687  0.508687   \n",
       "73   phrase_max  Wikipedia  0.453416  0.419540  0.493243   \n",
       "74   phrase_min  Wikipedia  0.546032  0.545455  0.563707   \n",
       "75    phrase_mv   WikiNews  0.430303  0.381720  0.493056   \n",
       "76   phrase_max   WikiNews  0.433735  0.382979  0.500000   \n",
       "77   phrase_min   WikiNews  0.562791  0.563927  0.561869   \n",
       "78    phrase_mv       News  0.467755  0.536975  0.505721   \n",
       "79   phrase_max       News  0.435345  0.391473  0.490291   \n",
       "80   phrase_min       News  0.627028  0.620523  0.639130   \n",
       "81    phrase_mv  Wikipedia  0.489644  0.494118  0.492278   \n",
       "82   phrase_max  Wikipedia  0.474851  0.473277  0.481660   \n",
       "83   phrase_min  Wikipedia  0.424837  0.533769  0.559846   \n",
       "84    phrase_mv   WikiNews  0.493628  0.507587  0.510101   \n",
       "85   phrase_max   WikiNews  0.495581  0.495581  0.495581   \n",
       "86   phrase_min   WikiNews  0.319149  0.445076  0.445076   \n",
       "87    phrase_mv       News  0.424573  0.476541  0.465153   \n",
       "88   phrase_max       News  0.404227  0.438095  0.409847   \n",
       "89   phrase_min       News  0.252900  0.485485  0.492545   \n",
       "90    phrase_mv  Wikipedia  0.628138  0.632308  0.624517   \n",
       "91   phrase_max  Wikipedia  0.566807  0.678571  0.557915   \n",
       "92   phrase_min  Wikipedia  0.465695  0.514583  0.527027   \n",
       "93    phrase_mv   WikiNews  0.519006  0.530357  0.521465   \n",
       "94   phrase_max   WikiNews  0.502646  0.587640  0.524621   \n",
       "95   phrase_min   WikiNews  0.502517  0.519114  0.525884   \n",
       "96    phrase_mv       News  0.627623  0.713502  0.609657   \n",
       "97   phrase_max       News  0.451194  0.475911  0.496793   \n",
       "98   phrase_min       News  0.537598  0.579116  0.617632   \n",
       "99    phrase_mv  Wikipedia  0.554205  0.551883  0.570463   \n",
       "100  phrase_max  Wikipedia  0.474851  0.473277  0.481660   \n",
       "101  phrase_min  Wikipedia  0.435268  0.524211  0.544402   \n",
       "102   phrase_mv   WikiNews  0.484388  0.508721  0.503788   \n",
       "103  phrase_max   WikiNews  0.423313  0.379121  0.479167   \n",
       "104  phrase_min   WikiNews  0.447405  0.477200  0.468434   \n",
       "105   phrase_mv       News  0.549577  0.554952  0.547330   \n",
       "106  phrase_max       News  0.430569  0.411364  0.462812   \n",
       "107  phrase_min       News  0.498642  0.555673  0.582004   \n",
       "108   phrase_mv  Wikipedia  0.628138  0.632308  0.624517   \n",
       "109  phrase_max  Wikipedia  0.566807  0.678571  0.557915   \n",
       "110  phrase_min  Wikipedia  0.465695  0.514583  0.527027   \n",
       "111   phrase_mv   WikiNews  0.519006  0.530357  0.521465   \n",
       "112  phrase_max   WikiNews  0.502646  0.587640  0.524621   \n",
       "113  phrase_min   WikiNews  0.502517  0.519114  0.525884   \n",
       "114   phrase_mv       News  0.627623  0.713502  0.609657   \n",
       "115  phrase_max       News  0.451194  0.475911  0.496793   \n",
       "116  phrase_min       News  0.537598  0.579116  0.617632   \n",
       "\n",
       "                           zc  \n",
       "0                  linguistic  \n",
       "1                  linguistic  \n",
       "2                  linguistic  \n",
       "3                  linguistic  \n",
       "4                  linguistic  \n",
       "5                  linguistic  \n",
       "6                  linguistic  \n",
       "7                  linguistic  \n",
       "8                  linguistic  \n",
       "9                   frequency  \n",
       "10                  frequency  \n",
       "11                  frequency  \n",
       "12                  frequency  \n",
       "13                  frequency  \n",
       "14                  frequency  \n",
       "15                  frequency  \n",
       "16                  frequency  \n",
       "17                  frequency  \n",
       "18             language_model  \n",
       "19             language_model  \n",
       "20             language_model  \n",
       "21             language_model  \n",
       "22             language_model  \n",
       "23             language_model  \n",
       "24             language_model  \n",
       "25             language_model  \n",
       "26             language_model  \n",
       "27                     corpus  \n",
       "28                     corpus  \n",
       "29                     corpus  \n",
       "30                     corpus  \n",
       "31                     corpus  \n",
       "32                     corpus  \n",
       "33                     corpus  \n",
       "34                     corpus  \n",
       "35                     corpus  \n",
       "36           psycholinguistic  \n",
       "37           psycholinguistic  \n",
       "38           psycholinguistic  \n",
       "39           psycholinguistic  \n",
       "40           psycholinguistic  \n",
       "41           psycholinguistic  \n",
       "42           psycholinguistic  \n",
       "43           psycholinguistic  \n",
       "44           psycholinguistic  \n",
       "45                    wordnet  \n",
       "46                    wordnet  \n",
       "47                    wordnet  \n",
       "48                    wordnet  \n",
       "49                    wordnet  \n",
       "50                    wordnet  \n",
       "51                    wordnet  \n",
       "52                    wordnet  \n",
       "53                    wordnet  \n",
       "54                    dbpedia  \n",
       "55                    dbpedia  \n",
       "56                    dbpedia  \n",
       "57                    dbpedia  \n",
       "58                    dbpedia  \n",
       "59                    dbpedia  \n",
       "60                    dbpedia  \n",
       "61                    dbpedia  \n",
       "62                    dbpedia  \n",
       "63           brown_clustering  \n",
       "64           brown_clustering  \n",
       "65           brown_clustering  \n",
       "66           brown_clustering  \n",
       "67           brown_clustering  \n",
       "68           brown_clustering  \n",
       "69           brown_clustering  \n",
       "70           brown_clustering  \n",
       "71           brown_clustering  \n",
       "72                   semantic  \n",
       "73                   semantic  \n",
       "74                   semantic  \n",
       "75                   semantic  \n",
       "76                   semantic  \n",
       "77                   semantic  \n",
       "78                   semantic  \n",
       "79                   semantic  \n",
       "80                   semantic  \n",
       "81                 dictionary  \n",
       "82                 dictionary  \n",
       "83                 dictionary  \n",
       "84                 dictionary  \n",
       "85                 dictionary  \n",
       "86                 dictionary  \n",
       "87                 dictionary  \n",
       "88                 dictionary  \n",
       "89                 dictionary  \n",
       "90            corpus+semantic  \n",
       "91            corpus+semantic  \n",
       "92            corpus+semantic  \n",
       "93            corpus+semantic  \n",
       "94            corpus+semantic  \n",
       "95            corpus+semantic  \n",
       "96            corpus+semantic  \n",
       "97            corpus+semantic  \n",
       "98            corpus+semantic  \n",
       "99   wordnet+psycholinguistic  \n",
       "100  wordnet+psycholinguistic  \n",
       "101  wordnet+psycholinguistic  \n",
       "102  wordnet+psycholinguistic  \n",
       "103  wordnet+psycholinguistic  \n",
       "104  wordnet+psycholinguistic  \n",
       "105  wordnet+psycholinguistic  \n",
       "106  wordnet+psycholinguistic  \n",
       "107  wordnet+psycholinguistic  \n",
       "108                       all  \n",
       "109                       all  \n",
       "110                       all  \n",
       "111                       all  \n",
       "112                       all  \n",
       "113                       all  \n",
       "114                       all  \n",
       "115                       all  \n",
       "116                       all  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_eval_data_nb = create_eval_df_from_results_phrase(results_nb)\n",
    "feature_eval_data_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.641913</td>\n",
       "      <td>0.709412</td>\n",
       "      <td>0.622660</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.641913</td>\n",
       "      <td>0.709412</td>\n",
       "      <td>0.622660</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.562791</td>\n",
       "      <td>0.563927</td>\n",
       "      <td>0.561869</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.628138</td>\n",
       "      <td>0.632308</td>\n",
       "      <td>0.624517</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.628138</td>\n",
       "      <td>0.632308</td>\n",
       "      <td>0.624517</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            agg    dataset        f1      prec       rec               zc\n",
       "15    phrase_mv       News  0.641913  0.709412  0.622660        frequency\n",
       "33    phrase_mv       News  0.641913  0.709412  0.622660           corpus\n",
       "77   phrase_min   WikiNews  0.562791  0.563927  0.561869         semantic\n",
       "90    phrase_mv  Wikipedia  0.628138  0.632308  0.624517  corpus+semantic\n",
       "108   phrase_mv  Wikipedia  0.628138  0.632308  0.624517              all"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = feature_eval_data_nb.groupby(['dataset'])['f1'].transform(max) == feature_eval_data_nb['f1']\n",
    "feature_eval_data_nb[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.9 kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_knn = [Result(fs, fs.fc, agg, phrase_merger(fs.test, knn(*transform_feat_to_num(\n",
    "                remove_labels_for_binary_df(fs.train), \n",
    "                remove_labels_phrase_for_binary_df(fs.test))), agg.agg)) \n",
    "          for fs in all_fc_datasets\n",
    "          for agg in phrase_aggs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.435495</td>\n",
       "      <td>0.511936</td>\n",
       "      <td>0.522201</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.513021</td>\n",
       "      <td>0.538126</td>\n",
       "      <td>0.567568</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.202899</td>\n",
       "      <td>0.420513</td>\n",
       "      <td>0.440154</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.439642</td>\n",
       "      <td>0.472273</td>\n",
       "      <td>0.461490</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.498838</td>\n",
       "      <td>0.506458</td>\n",
       "      <td>0.508207</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.286240</td>\n",
       "      <td>0.519048</td>\n",
       "      <td>0.510101</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.459133</td>\n",
       "      <td>0.524529</td>\n",
       "      <td>0.535801</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.403717</td>\n",
       "      <td>0.442763</td>\n",
       "      <td>0.415569</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.269678</td>\n",
       "      <td>0.515918</td>\n",
       "      <td>0.508755</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.473539</td>\n",
       "      <td>0.518315</td>\n",
       "      <td>0.533784</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.522366</td>\n",
       "      <td>0.528571</td>\n",
       "      <td>0.543436</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.248447</td>\n",
       "      <td>0.425397</td>\n",
       "      <td>0.409266</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.530630</td>\n",
       "      <td>0.563830</td>\n",
       "      <td>0.589015</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.609750</td>\n",
       "      <td>0.609443</td>\n",
       "      <td>0.642677</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.403986</td>\n",
       "      <td>0.585135</td>\n",
       "      <td>0.579545</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.520319</td>\n",
       "      <td>0.536215</td>\n",
       "      <td>0.551751</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.466172</td>\n",
       "      <td>0.473853</td>\n",
       "      <td>0.467233</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.341632</td>\n",
       "      <td>0.570563</td>\n",
       "      <td>0.556519</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.434039</td>\n",
       "      <td>0.479332</td>\n",
       "      <td>0.462355</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.507136</td>\n",
       "      <td>0.518822</td>\n",
       "      <td>0.529923</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.467181</td>\n",
       "      <td>0.467181</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.502517</td>\n",
       "      <td>0.519114</td>\n",
       "      <td>0.525884</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.513906</td>\n",
       "      <td>0.517921</td>\n",
       "      <td>0.522096</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.446558</td>\n",
       "      <td>0.601190</td>\n",
       "      <td>0.607323</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.577119</td>\n",
       "      <td>0.583882</td>\n",
       "      <td>0.618239</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.529792</td>\n",
       "      <td>0.529613</td>\n",
       "      <td>0.534501</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.325059</td>\n",
       "      <td>0.563440</td>\n",
       "      <td>0.546810</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.481390</td>\n",
       "      <td>0.522105</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.530130</td>\n",
       "      <td>0.533854</td>\n",
       "      <td>0.550193</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.248447</td>\n",
       "      <td>0.425397</td>\n",
       "      <td>0.409266</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.530630</td>\n",
       "      <td>0.563830</td>\n",
       "      <td>0.589015</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.601640</td>\n",
       "      <td>0.599851</td>\n",
       "      <td>0.626894</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.414827</td>\n",
       "      <td>0.589367</td>\n",
       "      <td>0.586490</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.520319</td>\n",
       "      <td>0.536215</td>\n",
       "      <td>0.551751</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.470992</td>\n",
       "      <td>0.477396</td>\n",
       "      <td>0.472087</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.341632</td>\n",
       "      <td>0.570563</td>\n",
       "      <td>0.556519</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.475339</td>\n",
       "      <td>0.492107</td>\n",
       "      <td>0.487452</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.517808</td>\n",
       "      <td>0.517361</td>\n",
       "      <td>0.519305</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.332890</td>\n",
       "      <td>0.464348</td>\n",
       "      <td>0.441120</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.483862</td>\n",
       "      <td>0.495642</td>\n",
       "      <td>0.494318</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.472334</td>\n",
       "      <td>0.471950</td>\n",
       "      <td>0.472854</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.366533</td>\n",
       "      <td>0.458013</td>\n",
       "      <td>0.448232</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.484936</td>\n",
       "      <td>0.552562</td>\n",
       "      <td>0.576370</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.475140</td>\n",
       "      <td>0.506003</td>\n",
       "      <td>0.508842</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.232177</td>\n",
       "      <td>0.537442</td>\n",
       "      <td>0.511269</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.470234</td>\n",
       "      <td>0.497768</td>\n",
       "      <td>0.496139</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.441270</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.436293</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.293996</td>\n",
       "      <td>0.495238</td>\n",
       "      <td>0.494208</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.526210</td>\n",
       "      <td>0.546703</td>\n",
       "      <td>0.564394</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.564058</td>\n",
       "      <td>0.562229</td>\n",
       "      <td>0.572601</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.359091</td>\n",
       "      <td>0.565705</td>\n",
       "      <td>0.551768</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.544425</td>\n",
       "      <td>0.560502</td>\n",
       "      <td>0.587465</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.485551</td>\n",
       "      <td>0.488644</td>\n",
       "      <td>0.486650</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.317389</td>\n",
       "      <td>0.548050</td>\n",
       "      <td>0.535454</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.443678</td>\n",
       "      <td>0.515528</td>\n",
       "      <td>0.528958</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.406431</td>\n",
       "      <td>0.456015</td>\n",
       "      <td>0.419884</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.237750</td>\n",
       "      <td>0.416321</td>\n",
       "      <td>0.402510</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.428342</td>\n",
       "      <td>0.516890</td>\n",
       "      <td>0.521465</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.459770</td>\n",
       "      <td>0.506364</td>\n",
       "      <td>0.508838</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.295027</td>\n",
       "      <td>0.567974</td>\n",
       "      <td>0.532828</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.481960</td>\n",
       "      <td>0.482383</td>\n",
       "      <td>0.481709</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.470707</td>\n",
       "      <td>0.469523</td>\n",
       "      <td>0.475121</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.433776</td>\n",
       "      <td>0.556408</td>\n",
       "      <td>0.571689</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.537235</td>\n",
       "      <td>0.551500</td>\n",
       "      <td>0.587838</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.496503</td>\n",
       "      <td>0.499237</td>\n",
       "      <td>0.499035</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.328765</td>\n",
       "      <td>0.562138</td>\n",
       "      <td>0.572394</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.504789</td>\n",
       "      <td>0.549091</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.559375</td>\n",
       "      <td>0.568609</td>\n",
       "      <td>0.592172</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.357045</td>\n",
       "      <td>0.595536</td>\n",
       "      <td>0.567551</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.552938</td>\n",
       "      <td>0.572996</td>\n",
       "      <td>0.606969</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.508687</td>\n",
       "      <td>0.512124</td>\n",
       "      <td>0.515083</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.320856</td>\n",
       "      <td>0.561502</td>\n",
       "      <td>0.544383</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.462972</td>\n",
       "      <td>0.493939</td>\n",
       "      <td>0.489382</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.475339</td>\n",
       "      <td>0.492107</td>\n",
       "      <td>0.487452</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.279532</td>\n",
       "      <td>0.448117</td>\n",
       "      <td>0.429537</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.559375</td>\n",
       "      <td>0.568609</td>\n",
       "      <td>0.592172</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.580357</td>\n",
       "      <td>0.577489</td>\n",
       "      <td>0.586490</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.370560</td>\n",
       "      <td>0.571047</td>\n",
       "      <td>0.558712</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.552578</td>\n",
       "      <td>0.575388</td>\n",
       "      <td>0.611044</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.476150</td>\n",
       "      <td>0.482915</td>\n",
       "      <td>0.478589</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.304693</td>\n",
       "      <td>0.540906</td>\n",
       "      <td>0.528173</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.538013</td>\n",
       "      <td>0.539465</td>\n",
       "      <td>0.556950</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.524865</td>\n",
       "      <td>0.535162</td>\n",
       "      <td>0.524131</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.341991</td>\n",
       "      <td>0.499365</td>\n",
       "      <td>0.499035</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.416015</td>\n",
       "      <td>0.499020</td>\n",
       "      <td>0.498737</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.465149</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>0.524621</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.286240</td>\n",
       "      <td>0.519048</td>\n",
       "      <td>0.510101</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.358180</td>\n",
       "      <td>0.507023</td>\n",
       "      <td>0.507715</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.355958</td>\n",
       "      <td>0.444721</td>\n",
       "      <td>0.423977</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.220238</td>\n",
       "      <td>0.559127</td>\n",
       "      <td>0.512916</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.497143</td>\n",
       "      <td>0.529915</td>\n",
       "      <td>0.554054</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.572234</td>\n",
       "      <td>0.568896</td>\n",
       "      <td>0.599421</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.322459</td>\n",
       "      <td>0.489967</td>\n",
       "      <td>0.485521</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.533035</td>\n",
       "      <td>0.557727</td>\n",
       "      <td>0.580177</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.599765</td>\n",
       "      <td>0.595633</td>\n",
       "      <td>0.609217</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.371700</td>\n",
       "      <td>0.547719</td>\n",
       "      <td>0.542929</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.541140</td>\n",
       "      <td>0.553980</td>\n",
       "      <td>0.576890</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.475158</td>\n",
       "      <td>0.478984</td>\n",
       "      <td>0.475295</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.324307</td>\n",
       "      <td>0.575590</td>\n",
       "      <td>0.553311</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.475339</td>\n",
       "      <td>0.492107</td>\n",
       "      <td>0.487452</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.477626</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>0.476834</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.351579</td>\n",
       "      <td>0.503722</td>\n",
       "      <td>0.505792</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.567731</td>\n",
       "      <td>0.574443</td>\n",
       "      <td>0.599116</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>0.613043</td>\n",
       "      <td>0.623106</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.370560</td>\n",
       "      <td>0.571047</td>\n",
       "      <td>0.558712</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.531966</td>\n",
       "      <td>0.538301</td>\n",
       "      <td>0.551664</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.476470</td>\n",
       "      <td>0.478664</td>\n",
       "      <td>0.476075</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.358442</td>\n",
       "      <td>0.557925</td>\n",
       "      <td>0.553225</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.505062</td>\n",
       "      <td>0.533962</td>\n",
       "      <td>0.560811</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.572234</td>\n",
       "      <td>0.568896</td>\n",
       "      <td>0.599421</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.478764</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.541463</td>\n",
       "      <td>0.562927</td>\n",
       "      <td>0.587121</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.627367</td>\n",
       "      <td>0.621614</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.381859</td>\n",
       "      <td>0.576023</td>\n",
       "      <td>0.565657</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.538482</td>\n",
       "      <td>0.554473</td>\n",
       "      <td>0.578537</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.481068</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.483443</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.341632</td>\n",
       "      <td>0.570563</td>\n",
       "      <td>0.556519</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            agg    dataset        f1      prec       rec  \\\n",
       "0     phrase_mv  Wikipedia  0.435495  0.511936  0.522201   \n",
       "1    phrase_max  Wikipedia  0.513021  0.538126  0.567568   \n",
       "2    phrase_min  Wikipedia  0.202899  0.420513  0.440154   \n",
       "3     phrase_mv   WikiNews  0.439642  0.472273  0.461490   \n",
       "4    phrase_max   WikiNews  0.498838  0.506458  0.508207   \n",
       "5    phrase_min   WikiNews  0.286240  0.519048  0.510101   \n",
       "6     phrase_mv       News  0.459133  0.524529  0.535801   \n",
       "7    phrase_max       News  0.403717  0.442763  0.415569   \n",
       "8    phrase_min       News  0.269678  0.515918  0.508755   \n",
       "9     phrase_mv  Wikipedia  0.473539  0.518315  0.533784   \n",
       "10   phrase_max  Wikipedia  0.522366  0.528571  0.543436   \n",
       "11   phrase_min  Wikipedia  0.248447  0.425397  0.409266   \n",
       "12    phrase_mv   WikiNews  0.530630  0.563830  0.589015   \n",
       "13   phrase_max   WikiNews  0.609750  0.609443  0.642677   \n",
       "14   phrase_min   WikiNews  0.403986  0.585135  0.579545   \n",
       "15    phrase_mv       News  0.520319  0.536215  0.551751   \n",
       "16   phrase_max       News  0.466172  0.473853  0.467233   \n",
       "17   phrase_min       News  0.341632  0.570563  0.556519   \n",
       "18    phrase_mv  Wikipedia  0.434039  0.479332  0.462355   \n",
       "19   phrase_max  Wikipedia  0.507136  0.518822  0.529923   \n",
       "20   phrase_min  Wikipedia  0.250000  0.467181  0.467181   \n",
       "21    phrase_mv   WikiNews  0.502517  0.519114  0.525884   \n",
       "22   phrase_max   WikiNews  0.513906  0.517921  0.522096   \n",
       "23   phrase_min   WikiNews  0.446558  0.601190  0.607323   \n",
       "24    phrase_mv       News  0.577119  0.583882  0.618239   \n",
       "25   phrase_max       News  0.529792  0.529613  0.534501   \n",
       "26   phrase_min       News  0.325059  0.563440  0.546810   \n",
       "27    phrase_mv  Wikipedia  0.481390  0.522105  0.540541   \n",
       "28   phrase_max  Wikipedia  0.530130  0.533854  0.550193   \n",
       "29   phrase_min  Wikipedia  0.248447  0.425397  0.409266   \n",
       "30    phrase_mv   WikiNews  0.530630  0.563830  0.589015   \n",
       "31   phrase_max   WikiNews  0.601640  0.599851  0.626894   \n",
       "32   phrase_min   WikiNews  0.414827  0.589367  0.586490   \n",
       "33    phrase_mv       News  0.520319  0.536215  0.551751   \n",
       "34   phrase_max       News  0.470992  0.477396  0.472087   \n",
       "35   phrase_min       News  0.341632  0.570563  0.556519   \n",
       "36    phrase_mv  Wikipedia  0.475339  0.492107  0.487452   \n",
       "37   phrase_max  Wikipedia  0.517808  0.517361  0.519305   \n",
       "38   phrase_min  Wikipedia  0.332890  0.464348  0.441120   \n",
       "39    phrase_mv   WikiNews  0.483862  0.495642  0.494318   \n",
       "40   phrase_max   WikiNews  0.472334  0.471950  0.472854   \n",
       "41   phrase_min   WikiNews  0.366533  0.458013  0.448232   \n",
       "42    phrase_mv       News  0.484936  0.552562  0.576370   \n",
       "43   phrase_max       News  0.475140  0.506003  0.508842   \n",
       "44   phrase_min       News  0.232177  0.537442  0.511269   \n",
       "45    phrase_mv  Wikipedia  0.470234  0.497768  0.496139   \n",
       "46   phrase_max  Wikipedia  0.441270  0.454545  0.436293   \n",
       "47   phrase_min  Wikipedia  0.293996  0.495238  0.494208   \n",
       "48    phrase_mv   WikiNews  0.526210  0.546703  0.564394   \n",
       "49   phrase_max   WikiNews  0.564058  0.562229  0.572601   \n",
       "50   phrase_min   WikiNews  0.359091  0.565705  0.551768   \n",
       "51    phrase_mv       News  0.544425  0.560502  0.587465   \n",
       "52   phrase_max       News  0.485551  0.488644  0.486650   \n",
       "53   phrase_min       News  0.317389  0.548050  0.535454   \n",
       "54    phrase_mv  Wikipedia  0.443678  0.515528  0.528958   \n",
       "55   phrase_max  Wikipedia  0.406431  0.456015  0.419884   \n",
       "56   phrase_min  Wikipedia  0.237750  0.416321  0.402510   \n",
       "57    phrase_mv   WikiNews  0.428342  0.516890  0.521465   \n",
       "58   phrase_max   WikiNews  0.459770  0.506364  0.508838   \n",
       "59   phrase_min   WikiNews  0.295027  0.567974  0.532828   \n",
       "60    phrase_mv       News  0.481960  0.482383  0.481709   \n",
       "61   phrase_max       News  0.470707  0.469523  0.475121   \n",
       "62   phrase_min       News  0.433776  0.556408  0.571689   \n",
       "63    phrase_mv  Wikipedia  0.537235  0.551500  0.587838   \n",
       "64   phrase_max  Wikipedia  0.496503  0.499237  0.499035   \n",
       "65   phrase_min  Wikipedia  0.328765  0.562138  0.572394   \n",
       "66    phrase_mv   WikiNews  0.504789  0.549091  0.568182   \n",
       "67   phrase_max   WikiNews  0.559375  0.568609  0.592172   \n",
       "68   phrase_min   WikiNews  0.357045  0.595536  0.567551   \n",
       "69    phrase_mv       News  0.552938  0.572996  0.606969   \n",
       "70   phrase_max       News  0.508687  0.512124  0.515083   \n",
       "71   phrase_min       News  0.320856  0.561502  0.544383   \n",
       "72    phrase_mv  Wikipedia  0.462972  0.493939  0.489382   \n",
       "73   phrase_max  Wikipedia  0.475339  0.492107  0.487452   \n",
       "74   phrase_min  Wikipedia  0.279532  0.448117  0.429537   \n",
       "75    phrase_mv   WikiNews  0.559375  0.568609  0.592172   \n",
       "76   phrase_max   WikiNews  0.580357  0.577489  0.586490   \n",
       "77   phrase_min   WikiNews  0.370560  0.571047  0.558712   \n",
       "78    phrase_mv       News  0.552578  0.575388  0.611044   \n",
       "79   phrase_max       News  0.476150  0.482915  0.478589   \n",
       "80   phrase_min       News  0.304693  0.540906  0.528173   \n",
       "81    phrase_mv  Wikipedia  0.538013  0.539465  0.556950   \n",
       "82   phrase_max  Wikipedia  0.524865  0.535162  0.524131   \n",
       "83   phrase_min  Wikipedia  0.341991  0.499365  0.499035   \n",
       "84    phrase_mv   WikiNews  0.416015  0.499020  0.498737   \n",
       "85   phrase_max   WikiNews  0.465149  0.517857  0.524621   \n",
       "86   phrase_min   WikiNews  0.286240  0.519048  0.510101   \n",
       "87    phrase_mv       News  0.358180  0.507023  0.507715   \n",
       "88   phrase_max       News  0.355958  0.444721  0.423977   \n",
       "89   phrase_min       News  0.220238  0.559127  0.512916   \n",
       "90    phrase_mv  Wikipedia  0.497143  0.529915  0.554054   \n",
       "91   phrase_max  Wikipedia  0.572234  0.568896  0.599421   \n",
       "92   phrase_min  Wikipedia  0.322459  0.489967  0.485521   \n",
       "93    phrase_mv   WikiNews  0.533035  0.557727  0.580177   \n",
       "94   phrase_max   WikiNews  0.599765  0.595633  0.609217   \n",
       "95   phrase_min   WikiNews  0.371700  0.547719  0.542929   \n",
       "96    phrase_mv       News  0.541140  0.553980  0.576890   \n",
       "97   phrase_max       News  0.475158  0.478984  0.475295   \n",
       "98   phrase_min       News  0.324307  0.575590  0.553311   \n",
       "99    phrase_mv  Wikipedia  0.475339  0.492107  0.487452   \n",
       "100  phrase_max  Wikipedia  0.477626  0.479167  0.476834   \n",
       "101  phrase_min  Wikipedia  0.351579  0.503722  0.505792   \n",
       "102   phrase_mv   WikiNews  0.567731  0.574443  0.599116   \n",
       "103  phrase_max   WikiNews  0.617021  0.613043  0.623106   \n",
       "104  phrase_min   WikiNews  0.370560  0.571047  0.558712   \n",
       "105   phrase_mv       News  0.531966  0.538301  0.551664   \n",
       "106  phrase_max       News  0.476470  0.478664  0.476075   \n",
       "107  phrase_min       News  0.358442  0.557925  0.553225   \n",
       "108   phrase_mv  Wikipedia  0.505062  0.533962  0.560811   \n",
       "109  phrase_max  Wikipedia  0.572234  0.568896  0.599421   \n",
       "110  phrase_min  Wikipedia  0.312500  0.484848  0.478764   \n",
       "111   phrase_mv   WikiNews  0.541463  0.562927  0.587121   \n",
       "112  phrase_max   WikiNews  0.627367  0.621614  0.638889   \n",
       "113  phrase_min   WikiNews  0.381859  0.576023  0.565657   \n",
       "114   phrase_mv       News  0.538482  0.554473  0.578537   \n",
       "115  phrase_max       News  0.481068  0.486592  0.483443   \n",
       "116  phrase_min       News  0.341632  0.570563  0.556519   \n",
       "\n",
       "                           zc  \n",
       "0                  linguistic  \n",
       "1                  linguistic  \n",
       "2                  linguistic  \n",
       "3                  linguistic  \n",
       "4                  linguistic  \n",
       "5                  linguistic  \n",
       "6                  linguistic  \n",
       "7                  linguistic  \n",
       "8                  linguistic  \n",
       "9                   frequency  \n",
       "10                  frequency  \n",
       "11                  frequency  \n",
       "12                  frequency  \n",
       "13                  frequency  \n",
       "14                  frequency  \n",
       "15                  frequency  \n",
       "16                  frequency  \n",
       "17                  frequency  \n",
       "18             language_model  \n",
       "19             language_model  \n",
       "20             language_model  \n",
       "21             language_model  \n",
       "22             language_model  \n",
       "23             language_model  \n",
       "24             language_model  \n",
       "25             language_model  \n",
       "26             language_model  \n",
       "27                     corpus  \n",
       "28                     corpus  \n",
       "29                     corpus  \n",
       "30                     corpus  \n",
       "31                     corpus  \n",
       "32                     corpus  \n",
       "33                     corpus  \n",
       "34                     corpus  \n",
       "35                     corpus  \n",
       "36           psycholinguistic  \n",
       "37           psycholinguistic  \n",
       "38           psycholinguistic  \n",
       "39           psycholinguistic  \n",
       "40           psycholinguistic  \n",
       "41           psycholinguistic  \n",
       "42           psycholinguistic  \n",
       "43           psycholinguistic  \n",
       "44           psycholinguistic  \n",
       "45                    wordnet  \n",
       "46                    wordnet  \n",
       "47                    wordnet  \n",
       "48                    wordnet  \n",
       "49                    wordnet  \n",
       "50                    wordnet  \n",
       "51                    wordnet  \n",
       "52                    wordnet  \n",
       "53                    wordnet  \n",
       "54                    dbpedia  \n",
       "55                    dbpedia  \n",
       "56                    dbpedia  \n",
       "57                    dbpedia  \n",
       "58                    dbpedia  \n",
       "59                    dbpedia  \n",
       "60                    dbpedia  \n",
       "61                    dbpedia  \n",
       "62                    dbpedia  \n",
       "63           brown_clustering  \n",
       "64           brown_clustering  \n",
       "65           brown_clustering  \n",
       "66           brown_clustering  \n",
       "67           brown_clustering  \n",
       "68           brown_clustering  \n",
       "69           brown_clustering  \n",
       "70           brown_clustering  \n",
       "71           brown_clustering  \n",
       "72                   semantic  \n",
       "73                   semantic  \n",
       "74                   semantic  \n",
       "75                   semantic  \n",
       "76                   semantic  \n",
       "77                   semantic  \n",
       "78                   semantic  \n",
       "79                   semantic  \n",
       "80                   semantic  \n",
       "81                 dictionary  \n",
       "82                 dictionary  \n",
       "83                 dictionary  \n",
       "84                 dictionary  \n",
       "85                 dictionary  \n",
       "86                 dictionary  \n",
       "87                 dictionary  \n",
       "88                 dictionary  \n",
       "89                 dictionary  \n",
       "90            corpus+semantic  \n",
       "91            corpus+semantic  \n",
       "92            corpus+semantic  \n",
       "93            corpus+semantic  \n",
       "94            corpus+semantic  \n",
       "95            corpus+semantic  \n",
       "96            corpus+semantic  \n",
       "97            corpus+semantic  \n",
       "98            corpus+semantic  \n",
       "99   wordnet+psycholinguistic  \n",
       "100  wordnet+psycholinguistic  \n",
       "101  wordnet+psycholinguistic  \n",
       "102  wordnet+psycholinguistic  \n",
       "103  wordnet+psycholinguistic  \n",
       "104  wordnet+psycholinguistic  \n",
       "105  wordnet+psycholinguistic  \n",
       "106  wordnet+psycholinguistic  \n",
       "107  wordnet+psycholinguistic  \n",
       "108                       all  \n",
       "109                       all  \n",
       "110                       all  \n",
       "111                       all  \n",
       "112                       all  \n",
       "113                       all  \n",
       "114                       all  \n",
       "115                       all  \n",
       "116                       all  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_eval_data_knn = create_eval_df_from_results_phrase(results_knn)\n",
    "feature_eval_data_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.577119</td>\n",
       "      <td>0.583882</td>\n",
       "      <td>0.618239</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.572234</td>\n",
       "      <td>0.568896</td>\n",
       "      <td>0.599421</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.572234</td>\n",
       "      <td>0.568896</td>\n",
       "      <td>0.599421</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.627367</td>\n",
       "      <td>0.621614</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            agg    dataset        f1      prec       rec               zc\n",
       "24    phrase_mv       News  0.577119  0.583882  0.618239   language_model\n",
       "91   phrase_max  Wikipedia  0.572234  0.568896  0.599421  corpus+semantic\n",
       "109  phrase_max  Wikipedia  0.572234  0.568896  0.599421              all\n",
       "112  phrase_max   WikiNews  0.627367  0.621614  0.638889              all"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = feature_eval_data_knn.groupby(['dataset'])['f1'].transform(max) == feature_eval_data_knn['f1']\n",
    "feature_eval_data_knn[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.10 MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "results_mlp = [Result(fs, fs.fc, agg, phrase_merger(fs.test, mlp(*transform_feat_to_num(\n",
    "                remove_labels_for_binary_df(fs.train), \n",
    "                remove_labels_phrase_for_binary_df(fs.test))), agg.agg)) \n",
    "          for fs in all_fc_datasets\n",
    "          for agg in phrase_aggs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.442135</td>\n",
       "      <td>0.503618</td>\n",
       "      <td>0.506757</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.461647</td>\n",
       "      <td>0.502650</td>\n",
       "      <td>0.504826</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.283998</td>\n",
       "      <td>0.548205</td>\n",
       "      <td>0.545367</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.508768</td>\n",
       "      <td>0.581555</td>\n",
       "      <td>0.608586</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.539216</td>\n",
       "      <td>0.568841</td>\n",
       "      <td>0.595960</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.286240</td>\n",
       "      <td>0.519048</td>\n",
       "      <td>0.510101</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.443737</td>\n",
       "      <td>0.516414</td>\n",
       "      <td>0.523665</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.426148</td>\n",
       "      <td>0.461332</td>\n",
       "      <td>0.443135</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.260854</td>\n",
       "      <td>0.561570</td>\n",
       "      <td>0.525832</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.407008</td>\n",
       "      <td>0.526786</td>\n",
       "      <td>0.546332</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.435495</td>\n",
       "      <td>0.511936</td>\n",
       "      <td>0.522201</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.248447</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.525097</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.549899</td>\n",
       "      <td>0.568223</td>\n",
       "      <td>0.594066</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.645797</td>\n",
       "      <td>0.638528</td>\n",
       "      <td>0.661616</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.424490</td>\n",
       "      <td>0.644737</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.633384</td>\n",
       "      <td>0.639811</td>\n",
       "      <td>0.628467</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.508443</td>\n",
       "      <td>0.530802</td>\n",
       "      <td>0.516383</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.457557</td>\n",
       "      <td>0.566810</td>\n",
       "      <td>0.588679</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.356297</td>\n",
       "      <td>0.490064</td>\n",
       "      <td>0.483591</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.451833</td>\n",
       "      <td>0.519121</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.166316</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.446867</td>\n",
       "      <td>0.527119</td>\n",
       "      <td>0.535354</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.487435</td>\n",
       "      <td>0.539377</td>\n",
       "      <td>0.554293</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.234690</td>\n",
       "      <td>0.620879</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.454191</td>\n",
       "      <td>0.565353</td>\n",
       "      <td>0.586252</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.473896</td>\n",
       "      <td>0.522409</td>\n",
       "      <td>0.533287</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.259226</td>\n",
       "      <td>0.584927</td>\n",
       "      <td>0.532334</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.661196</td>\n",
       "      <td>0.645700</td>\n",
       "      <td>0.697876</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.593846</td>\n",
       "      <td>0.600877</td>\n",
       "      <td>0.588803</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.384290</td>\n",
       "      <td>0.534608</td>\n",
       "      <td>0.555019</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.461996</td>\n",
       "      <td>0.558972</td>\n",
       "      <td>0.573864</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.544794</td>\n",
       "      <td>0.580163</td>\n",
       "      <td>0.611742</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.226974</td>\n",
       "      <td>0.491667</td>\n",
       "      <td>0.498106</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.485087</td>\n",
       "      <td>0.531671</td>\n",
       "      <td>0.547070</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.404577</td>\n",
       "      <td>0.428522</td>\n",
       "      <td>0.400832</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.312376</td>\n",
       "      <td>0.557395</td>\n",
       "      <td>0.539528</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.503472</td>\n",
       "      <td>0.504762</td>\n",
       "      <td>0.505792</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.414634</td>\n",
       "      <td>0.459459</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.454094</td>\n",
       "      <td>0.498947</td>\n",
       "      <td>0.498069</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.549082</td>\n",
       "      <td>0.612354</td>\n",
       "      <td>0.652146</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.623854</td>\n",
       "      <td>0.628671</td>\n",
       "      <td>0.674242</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.281944</td>\n",
       "      <td>0.559593</td>\n",
       "      <td>0.525884</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.212928</td>\n",
       "      <td>0.609804</td>\n",
       "      <td>0.516990</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.208672</td>\n",
       "      <td>0.424251</td>\n",
       "      <td>0.477202</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.176101</td>\n",
       "      <td>0.106870</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.368394</td>\n",
       "      <td>0.548829</td>\n",
       "      <td>0.570463</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.417064</td>\n",
       "      <td>0.564286</td>\n",
       "      <td>0.604247</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.143158</td>\n",
       "      <td>0.325581</td>\n",
       "      <td>0.471042</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.311355</td>\n",
       "      <td>0.538618</td>\n",
       "      <td>0.523990</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.393548</td>\n",
       "      <td>0.539804</td>\n",
       "      <td>0.541035</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.189655</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.483832</td>\n",
       "      <td>0.583169</td>\n",
       "      <td>0.514650</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.451194</td>\n",
       "      <td>0.475911</td>\n",
       "      <td>0.496793</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.592486</td>\n",
       "      <td>0.588095</td>\n",
       "      <td>0.602635</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.233788</td>\n",
       "      <td>0.586420</td>\n",
       "      <td>0.547297</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.223664</td>\n",
       "      <td>0.518750</td>\n",
       "      <td>0.511583</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.166316</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.241137</td>\n",
       "      <td>0.517978</td>\n",
       "      <td>0.505051</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.307869</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.539773</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.205003</td>\n",
       "      <td>0.618280</td>\n",
       "      <td>0.506944</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.373717</td>\n",
       "      <td>0.534355</td>\n",
       "      <td>0.536928</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.392664</td>\n",
       "      <td>0.515259</td>\n",
       "      <td>0.518984</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.251429</td>\n",
       "      <td>0.555100</td>\n",
       "      <td>0.520978</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.474851</td>\n",
       "      <td>0.473277</td>\n",
       "      <td>0.481660</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.414634</td>\n",
       "      <td>0.459459</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.385549</td>\n",
       "      <td>0.445833</td>\n",
       "      <td>0.399614</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.643038</td>\n",
       "      <td>0.781977</td>\n",
       "      <td>0.622475</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.612665</td>\n",
       "      <td>0.759442</td>\n",
       "      <td>0.599747</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.518329</td>\n",
       "      <td>0.529872</td>\n",
       "      <td>0.539773</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.207789</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>0.514563</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.225236</td>\n",
       "      <td>0.564107</td>\n",
       "      <td>0.515343</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.176101</td>\n",
       "      <td>0.106870</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.360664</td>\n",
       "      <td>0.570588</td>\n",
       "      <td>0.592664</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.439794</td>\n",
       "      <td>0.554545</td>\n",
       "      <td>0.595560</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.151947</td>\n",
       "      <td>0.580460</td>\n",
       "      <td>0.506757</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.548077</td>\n",
       "      <td>0.592857</td>\n",
       "      <td>0.549242</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.571893</td>\n",
       "      <td>0.682266</td>\n",
       "      <td>0.570076</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.547164</td>\n",
       "      <td>0.546540</td>\n",
       "      <td>0.547980</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.484689</td>\n",
       "      <td>0.496208</td>\n",
       "      <td>0.494886</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.446589</td>\n",
       "      <td>0.449941</td>\n",
       "      <td>0.444435</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.374046</td>\n",
       "      <td>0.556432</td>\n",
       "      <td>0.556432</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.427278</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.515444</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.433944</td>\n",
       "      <td>0.469697</td>\n",
       "      <td>0.446911</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.202899</td>\n",
       "      <td>0.420513</td>\n",
       "      <td>0.440154</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.425752</td>\n",
       "      <td>0.486111</td>\n",
       "      <td>0.481061</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.453753</td>\n",
       "      <td>0.495018</td>\n",
       "      <td>0.493056</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.268315</td>\n",
       "      <td>0.443089</td>\n",
       "      <td>0.464646</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.393412</td>\n",
       "      <td>0.520798</td>\n",
       "      <td>0.525485</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.357843</td>\n",
       "      <td>0.425202</td>\n",
       "      <td>0.390603</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.240012</td>\n",
       "      <td>0.575173</td>\n",
       "      <td>0.522625</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.454094</td>\n",
       "      <td>0.498947</td>\n",
       "      <td>0.498069</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.461444</td>\n",
       "      <td>0.484220</td>\n",
       "      <td>0.473938</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.322459</td>\n",
       "      <td>0.489967</td>\n",
       "      <td>0.485521</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.548077</td>\n",
       "      <td>0.548438</td>\n",
       "      <td>0.558712</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.540117</td>\n",
       "      <td>0.541892</td>\n",
       "      <td>0.539141</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.455907</td>\n",
       "      <td>0.586235</td>\n",
       "      <td>0.598485</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.492348</td>\n",
       "      <td>0.521422</td>\n",
       "      <td>0.531553</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.420610</td>\n",
       "      <td>0.431415</td>\n",
       "      <td>0.415309</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.297010</td>\n",
       "      <td>0.524418</td>\n",
       "      <td>0.516817</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.452179</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>0.557915</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.529097</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>0.581081</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.248447</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.525097</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.423313</td>\n",
       "      <td>0.379121</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.593074</td>\n",
       "      <td>0.593137</td>\n",
       "      <td>0.619949</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.260642</td>\n",
       "      <td>0.507595</td>\n",
       "      <td>0.503901</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.276254</td>\n",
       "      <td>0.385775</td>\n",
       "      <td>0.365725</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.181487</td>\n",
       "      <td>0.607280</td>\n",
       "      <td>0.502427</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.554205</td>\n",
       "      <td>0.551883</td>\n",
       "      <td>0.570463</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.575290</td>\n",
       "      <td>0.575290</td>\n",
       "      <td>0.575290</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.402356</td>\n",
       "      <td>0.497350</td>\n",
       "      <td>0.495174</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.470098</td>\n",
       "      <td>0.504982</td>\n",
       "      <td>0.506944</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.556033</td>\n",
       "      <td>0.555172</td>\n",
       "      <td>0.565657</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.307869</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.539773</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.598417</td>\n",
       "      <td>0.669283</td>\n",
       "      <td>0.586945</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.479952</td>\n",
       "      <td>0.544841</td>\n",
       "      <td>0.509795</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>phrase_min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.590106</td>\n",
       "      <td>0.608046</td>\n",
       "      <td>0.658894</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            agg    dataset        f1      prec       rec  \\\n",
       "0     phrase_mv  Wikipedia  0.442135  0.503618  0.506757   \n",
       "1    phrase_max  Wikipedia  0.461647  0.502650  0.504826   \n",
       "2    phrase_min  Wikipedia  0.283998  0.548205  0.545367   \n",
       "3     phrase_mv   WikiNews  0.508768  0.581555  0.608586   \n",
       "4    phrase_max   WikiNews  0.539216  0.568841  0.595960   \n",
       "5    phrase_min   WikiNews  0.286240  0.519048  0.510101   \n",
       "6     phrase_mv       News  0.443737  0.516414  0.523665   \n",
       "7    phrase_max       News  0.426148  0.461332  0.443135   \n",
       "8    phrase_min       News  0.260854  0.561570  0.525832   \n",
       "9     phrase_mv  Wikipedia  0.407008  0.526786  0.546332   \n",
       "10   phrase_max  Wikipedia  0.435495  0.511936  0.522201   \n",
       "11   phrase_min  Wikipedia  0.248447  0.533333  0.525097   \n",
       "12    phrase_mv   WikiNews  0.549899  0.568223  0.594066   \n",
       "13   phrase_max   WikiNews  0.645797  0.638528  0.661616   \n",
       "14   phrase_min   WikiNews  0.424490  0.644737  0.625000   \n",
       "15    phrase_mv       News  0.633384  0.639811  0.628467   \n",
       "16   phrase_max       News  0.508443  0.530802  0.516383   \n",
       "17   phrase_min       News  0.457557  0.566810  0.588679   \n",
       "18    phrase_mv  Wikipedia  0.356297  0.490064  0.483591   \n",
       "19   phrase_max  Wikipedia  0.451833  0.519121  0.535714   \n",
       "20   phrase_min  Wikipedia  0.166316  0.581395  0.513514   \n",
       "21    phrase_mv   WikiNews  0.446867  0.527119  0.535354   \n",
       "22   phrase_max   WikiNews  0.487435  0.539377  0.554293   \n",
       "23   phrase_min   WikiNews  0.234690  0.620879  0.520833   \n",
       "24    phrase_mv       News  0.454191  0.565353  0.586252   \n",
       "25   phrase_max       News  0.473896  0.522409  0.533287   \n",
       "26   phrase_min       News  0.259226  0.584927  0.532334   \n",
       "27    phrase_mv  Wikipedia  0.661196  0.645700  0.697876   \n",
       "28   phrase_max  Wikipedia  0.593846  0.600877  0.588803   \n",
       "29   phrase_min  Wikipedia  0.384290  0.534608  0.555019   \n",
       "30    phrase_mv   WikiNews  0.461996  0.558972  0.573864   \n",
       "31   phrase_max   WikiNews  0.544794  0.580163  0.611742   \n",
       "32   phrase_min   WikiNews  0.226974  0.491667  0.498106   \n",
       "33    phrase_mv       News  0.485087  0.531671  0.547070   \n",
       "34   phrase_max       News  0.404577  0.428522  0.400832   \n",
       "35   phrase_min       News  0.312376  0.557395  0.539528   \n",
       "36    phrase_mv  Wikipedia  0.503472  0.504762  0.505792   \n",
       "37   phrase_max  Wikipedia  0.435897  0.414634  0.459459   \n",
       "38   phrase_min  Wikipedia  0.454094  0.498947  0.498069   \n",
       "39    phrase_mv   WikiNews  0.549082  0.612354  0.652146   \n",
       "40   phrase_max   WikiNews  0.623854  0.628671  0.674242   \n",
       "41   phrase_min   WikiNews  0.281944  0.559593  0.525884   \n",
       "42    phrase_mv       News  0.212928  0.609804  0.516990   \n",
       "43   phrase_max       News  0.208672  0.424251  0.477202   \n",
       "44   phrase_min       News  0.176101  0.106870  0.500000   \n",
       "45    phrase_mv  Wikipedia  0.368394  0.548829  0.570463   \n",
       "46   phrase_max  Wikipedia  0.417064  0.564286  0.604247   \n",
       "47   phrase_min  Wikipedia  0.143158  0.325581  0.471042   \n",
       "48    phrase_mv   WikiNews  0.311355  0.538618  0.523990   \n",
       "49   phrase_max   WikiNews  0.393548  0.539804  0.541035   \n",
       "50   phrase_min   WikiNews  0.189655  0.117021  0.500000   \n",
       "51    phrase_mv       News  0.483832  0.583169  0.514650   \n",
       "52   phrase_max       News  0.451194  0.475911  0.496793   \n",
       "53   phrase_min       News  0.592486  0.588095  0.602635   \n",
       "54    phrase_mv  Wikipedia  0.233788  0.586420  0.547297   \n",
       "55   phrase_max  Wikipedia  0.223664  0.518750  0.511583   \n",
       "56   phrase_min  Wikipedia  0.166316  0.581395  0.513514   \n",
       "57    phrase_mv   WikiNews  0.241137  0.517978  0.505051   \n",
       "58   phrase_max   WikiNews  0.307869  0.575000  0.539773   \n",
       "59   phrase_min   WikiNews  0.205003  0.618280  0.506944   \n",
       "60    phrase_mv       News  0.373717  0.534355  0.536928   \n",
       "61   phrase_max       News  0.392664  0.515259  0.518984   \n",
       "62   phrase_min       News  0.251429  0.555100  0.520978   \n",
       "63    phrase_mv  Wikipedia  0.474851  0.473277  0.481660   \n",
       "64   phrase_max  Wikipedia  0.435897  0.414634  0.459459   \n",
       "65   phrase_min  Wikipedia  0.385549  0.445833  0.399614   \n",
       "66    phrase_mv   WikiNews  0.643038  0.781977  0.622475   \n",
       "67   phrase_max   WikiNews  0.612665  0.759442  0.599747   \n",
       "68   phrase_min   WikiNews  0.518329  0.529872  0.539773   \n",
       "69    phrase_mv       News  0.207789  0.609375  0.514563   \n",
       "70   phrase_max       News  0.225236  0.564107  0.515343   \n",
       "71   phrase_min       News  0.176101  0.106870  0.500000   \n",
       "72    phrase_mv  Wikipedia  0.360664  0.570588  0.592664   \n",
       "73   phrase_max  Wikipedia  0.439794  0.554545  0.595560   \n",
       "74   phrase_min  Wikipedia  0.151947  0.580460  0.506757   \n",
       "75    phrase_mv   WikiNews  0.548077  0.592857  0.549242   \n",
       "76   phrase_max   WikiNews  0.571893  0.682266  0.570076   \n",
       "77   phrase_min   WikiNews  0.547164  0.546540  0.547980   \n",
       "78    phrase_mv       News  0.484689  0.496208  0.494886   \n",
       "79   phrase_max       News  0.446589  0.449941  0.444435   \n",
       "80   phrase_min       News  0.374046  0.556432  0.556432   \n",
       "81    phrase_mv  Wikipedia  0.427278  0.508333  0.515444   \n",
       "82   phrase_max  Wikipedia  0.433944  0.469697  0.446911   \n",
       "83   phrase_min  Wikipedia  0.202899  0.420513  0.440154   \n",
       "84    phrase_mv   WikiNews  0.425752  0.486111  0.481061   \n",
       "85   phrase_max   WikiNews  0.453753  0.495018  0.493056   \n",
       "86   phrase_min   WikiNews  0.268315  0.443089  0.464646   \n",
       "87    phrase_mv       News  0.393412  0.520798  0.525485   \n",
       "88   phrase_max       News  0.357843  0.425202  0.390603   \n",
       "89   phrase_min       News  0.240012  0.575173  0.522625   \n",
       "90    phrase_mv  Wikipedia  0.454094  0.498947  0.498069   \n",
       "91   phrase_max  Wikipedia  0.461444  0.484220  0.473938   \n",
       "92   phrase_min  Wikipedia  0.322459  0.489967  0.485521   \n",
       "93    phrase_mv   WikiNews  0.548077  0.548438  0.558712   \n",
       "94   phrase_max   WikiNews  0.540117  0.541892  0.539141   \n",
       "95   phrase_min   WikiNews  0.455907  0.586235  0.598485   \n",
       "96    phrase_mv       News  0.492348  0.521422  0.531553   \n",
       "97   phrase_max       News  0.420610  0.431415  0.415309   \n",
       "98   phrase_min       News  0.297010  0.524418  0.516817   \n",
       "99    phrase_mv  Wikipedia  0.452179  0.531250  0.557915   \n",
       "100  phrase_max  Wikipedia  0.529097  0.546875  0.581081   \n",
       "101  phrase_min  Wikipedia  0.248447  0.533333  0.525097   \n",
       "102   phrase_mv   WikiNews  0.423313  0.379121  0.479167   \n",
       "103  phrase_max   WikiNews  0.433735  0.382979  0.500000   \n",
       "104  phrase_min   WikiNews  0.593074  0.593137  0.619949   \n",
       "105   phrase_mv       News  0.260642  0.507595  0.503901   \n",
       "106  phrase_max       News  0.276254  0.385775  0.365725   \n",
       "107  phrase_min       News  0.181487  0.607280  0.502427   \n",
       "108   phrase_mv  Wikipedia  0.554205  0.551883  0.570463   \n",
       "109  phrase_max  Wikipedia  0.575290  0.575290  0.575290   \n",
       "110  phrase_min  Wikipedia  0.402356  0.497350  0.495174   \n",
       "111   phrase_mv   WikiNews  0.470098  0.504982  0.506944   \n",
       "112  phrase_max   WikiNews  0.556033  0.555172  0.565657   \n",
       "113  phrase_min   WikiNews  0.307869  0.575000  0.539773   \n",
       "114   phrase_mv       News  0.598417  0.669283  0.586945   \n",
       "115  phrase_max       News  0.479952  0.544841  0.509795   \n",
       "116  phrase_min       News  0.590106  0.608046  0.658894   \n",
       "\n",
       "                           zc  \n",
       "0                  linguistic  \n",
       "1                  linguistic  \n",
       "2                  linguistic  \n",
       "3                  linguistic  \n",
       "4                  linguistic  \n",
       "5                  linguistic  \n",
       "6                  linguistic  \n",
       "7                  linguistic  \n",
       "8                  linguistic  \n",
       "9                   frequency  \n",
       "10                  frequency  \n",
       "11                  frequency  \n",
       "12                  frequency  \n",
       "13                  frequency  \n",
       "14                  frequency  \n",
       "15                  frequency  \n",
       "16                  frequency  \n",
       "17                  frequency  \n",
       "18             language_model  \n",
       "19             language_model  \n",
       "20             language_model  \n",
       "21             language_model  \n",
       "22             language_model  \n",
       "23             language_model  \n",
       "24             language_model  \n",
       "25             language_model  \n",
       "26             language_model  \n",
       "27                     corpus  \n",
       "28                     corpus  \n",
       "29                     corpus  \n",
       "30                     corpus  \n",
       "31                     corpus  \n",
       "32                     corpus  \n",
       "33                     corpus  \n",
       "34                     corpus  \n",
       "35                     corpus  \n",
       "36           psycholinguistic  \n",
       "37           psycholinguistic  \n",
       "38           psycholinguistic  \n",
       "39           psycholinguistic  \n",
       "40           psycholinguistic  \n",
       "41           psycholinguistic  \n",
       "42           psycholinguistic  \n",
       "43           psycholinguistic  \n",
       "44           psycholinguistic  \n",
       "45                    wordnet  \n",
       "46                    wordnet  \n",
       "47                    wordnet  \n",
       "48                    wordnet  \n",
       "49                    wordnet  \n",
       "50                    wordnet  \n",
       "51                    wordnet  \n",
       "52                    wordnet  \n",
       "53                    wordnet  \n",
       "54                    dbpedia  \n",
       "55                    dbpedia  \n",
       "56                    dbpedia  \n",
       "57                    dbpedia  \n",
       "58                    dbpedia  \n",
       "59                    dbpedia  \n",
       "60                    dbpedia  \n",
       "61                    dbpedia  \n",
       "62                    dbpedia  \n",
       "63           brown_clustering  \n",
       "64           brown_clustering  \n",
       "65           brown_clustering  \n",
       "66           brown_clustering  \n",
       "67           brown_clustering  \n",
       "68           brown_clustering  \n",
       "69           brown_clustering  \n",
       "70           brown_clustering  \n",
       "71           brown_clustering  \n",
       "72                   semantic  \n",
       "73                   semantic  \n",
       "74                   semantic  \n",
       "75                   semantic  \n",
       "76                   semantic  \n",
       "77                   semantic  \n",
       "78                   semantic  \n",
       "79                   semantic  \n",
       "80                   semantic  \n",
       "81                 dictionary  \n",
       "82                 dictionary  \n",
       "83                 dictionary  \n",
       "84                 dictionary  \n",
       "85                 dictionary  \n",
       "86                 dictionary  \n",
       "87                 dictionary  \n",
       "88                 dictionary  \n",
       "89                 dictionary  \n",
       "90            corpus+semantic  \n",
       "91            corpus+semantic  \n",
       "92            corpus+semantic  \n",
       "93            corpus+semantic  \n",
       "94            corpus+semantic  \n",
       "95            corpus+semantic  \n",
       "96            corpus+semantic  \n",
       "97            corpus+semantic  \n",
       "98            corpus+semantic  \n",
       "99   wordnet+psycholinguistic  \n",
       "100  wordnet+psycholinguistic  \n",
       "101  wordnet+psycholinguistic  \n",
       "102  wordnet+psycholinguistic  \n",
       "103  wordnet+psycholinguistic  \n",
       "104  wordnet+psycholinguistic  \n",
       "105  wordnet+psycholinguistic  \n",
       "106  wordnet+psycholinguistic  \n",
       "107  wordnet+psycholinguistic  \n",
       "108                       all  \n",
       "109                       all  \n",
       "110                       all  \n",
       "111                       all  \n",
       "112                       all  \n",
       "113                       all  \n",
       "114                       all  \n",
       "115                       all  \n",
       "116                       all  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_eval_data_mlp = create_eval_df_from_results_phrase(results_mlp)\n",
    "feature_eval_data_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>phrase_max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.645797</td>\n",
       "      <td>0.638528</td>\n",
       "      <td>0.661616</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>News</td>\n",
       "      <td>0.633384</td>\n",
       "      <td>0.639811</td>\n",
       "      <td>0.628467</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>phrase_mv</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.661196</td>\n",
       "      <td>0.645700</td>\n",
       "      <td>0.697876</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           agg    dataset        f1      prec       rec         zc\n",
       "13  phrase_max   WikiNews  0.645797  0.638528  0.661616  frequency\n",
       "15   phrase_mv       News  0.633384  0.639811  0.628467  frequency\n",
       "27   phrase_mv  Wikipedia  0.661196  0.645700  0.697876     corpus"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = feature_eval_data_mlp.groupby(['dataset'])['f1'].transform(max) == feature_eval_data_mlp['f1']\n",
    "feature_eval_data_mlp[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.2 A2 Feature Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test both training set inputs (DS-P and DS-WP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Studio\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\core\\fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\Studio\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: Mean of empty slice\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: RuntimeWarning: Mean of empty slice\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:166: RuntimeWarning: invalid value encountered in double_scalars\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:202: RuntimeWarning: invalid value encountered in long_scalars\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:226: RuntimeWarning: divide by zero encountered in long_scalars\n",
      "C:\\Users\\Studio\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\core\\_methods.py:26: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_maximum(a, axis, None, out, keepdims)\n",
      "C:\\Users\\Studio\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\core\\_methods.py:29: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_minimum(a, axis, None, out, keepdims)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:202: RuntimeWarning: invalid value encountered in double_scalars\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:226: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    }
   ],
   "source": [
    "datasets = load_datasets(['Wikipedia', 'WikiNews', 'News'], 'Train', 'Dev', type_train='phrase', type_test='phrase')\n",
    "datasets = preprocess_datasets(datasets)\n",
    "# 1. Linguistic Features\n",
    "datasets_fc_linguistic = compute_features_linguistic(datasets, aggs=aggs_all)\n",
    "# 2. Corpus Features\n",
    "datasets_fc_frequency = compute_features_frequency(datasets, aggs=aggs_all)\n",
    "datasets_fc_language_model = compute_features_language_model(datasets, aggs=aggs_all)\n",
    "datasets_fc_corpus = compute_features_corpus([datasets_fc_frequency, datasets_fc_language_model])\n",
    "# 3. Psycholinguistic\n",
    "datasets_fc_psycholinguistic = compute_features_psycholinguistic(datasets, aggs=aggs_all)\n",
    "# 4. Semantic Features\n",
    "datasets_fc_wordnet = compute_features_wordnet(datasets, aggs=aggs_all)\n",
    "datasets_fc_dbpedia = compute_features_dbpedia(datasets, aggs=aggs_all)\n",
    "datasets_fc_brown_clustering = compute_features_brown_clustering(datasets, aggs=aggs_all)\n",
    "datasets_fc_semantic = compute_features_semantic([datasets_fc_wordnet, datasets_fc_dbpedia, datasets_fc_brown_clustering])\n",
    "# 5. Dictionary Features\n",
    "datasets_fc_dictionary = compute_features_dictionary(datasets, aggs=aggs_all)\n",
    "# 6. Concatentation of feature categories\n",
    "# (1) Corpus + Semantic\n",
    "datasets_fc_corpus_semantic = concat_feature_datasets(datasets_fc_corpus, datasets_fc_semantic, name='corpus+semantic')\n",
    "# (2) WordNet + Psycholinguistic\n",
    "datasets_fc_wordnet_psycholinguistic = concat_feature_datasets(datasets_fc_wordnet, \\\n",
    "                            datasets_fc_psycholinguistic, name='wordnet+psycholinguistic')\n",
    "#(3) All categories\n",
    "datasets_fc_all = concat_feature_datasets(datasets_fc_linguistic, datasets_fc_psycholinguistic, \\\n",
    "                            datasets_fc_semantic, datasets_fc_corpus, datasets_fc_dictionary, name='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fc_datasets = []\n",
    "all_fc_datasets.extend(datasets_fc_linguistic)\n",
    "all_fc_datasets.extend(datasets_fc_frequency)\n",
    "all_fc_datasets.extend(datasets_fc_language_model)\n",
    "all_fc_datasets.extend(datasets_fc_corpus)\n",
    "all_fc_datasets.extend(datasets_fc_psycholinguistic)\n",
    "all_fc_datasets.extend(datasets_fc_wordnet)\n",
    "all_fc_datasets.extend(datasets_fc_dbpedia)\n",
    "all_fc_datasets.extend(datasets_fc_brown_clustering)\n",
    "all_fc_datasets.extend(datasets_fc_semantic)\n",
    "all_fc_datasets.extend(datasets_fc_dictionary)\n",
    "all_fc_datasets.extend(datasets_fc_corpus_semantic)\n",
    "all_fc_datasets.extend(datasets_fc_wordnet_psycholinguistic)\n",
    "all_fc_datasets.extend(datasets_fc_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_datasets(['Wikipedia', 'WikiNews', 'News'], 'Train', 'Dev', type_train='both', type_test='phrase')\n",
    "datasets = preprocess_datasets(datasets)\n",
    "# 1. Linguistic Features\n",
    "datasets_fc_linguistic = compute_features_linguistic(datasets, aggs=aggs_all)\n",
    "# 2. Corpus Features\n",
    "datasets_fc_frequency = compute_features_frequency(datasets, aggs=aggs_all)\n",
    "datasets_fc_language_model = compute_features_language_model(datasets, aggs=aggs_all)\n",
    "datasets_fc_corpus = compute_features_corpus([datasets_fc_frequency, datasets_fc_language_model])\n",
    "# 3. Psycholinguistic\n",
    "datasets_fc_psycholinguistic = compute_features_psycholinguistic(datasets, aggs=aggs_all)\n",
    "# 4. Semantic Features\n",
    "datasets_fc_wordnet = compute_features_wordnet(datasets, aggs=aggs_all)\n",
    "datasets_fc_dbpedia = compute_features_dbpedia(datasets, aggs=aggs_all)\n",
    "datasets_fc_brown_clustering = compute_features_brown_clustering(datasets, aggs=aggs_all)\n",
    "datasets_fc_semantic = compute_features_semantic([datasets_fc_wordnet, datasets_fc_dbpedia, datasets_fc_brown_clustering])\n",
    "# 5. Dictionary Features\n",
    "datasets_fc_dictionary = compute_features_dictionary(datasets, aggs=aggs_all)\n",
    "# 6. Concatentation of feature categories\n",
    "# (1) Corpus + Semantic\n",
    "datasets_fc_corpus_semantic = concat_feature_datasets(datasets_fc_corpus, datasets_fc_semantic, name='corpus+semantic')\n",
    "# (2) WordNet + Psycholinguistic\n",
    "datasets_fc_wordnet_psycholinguistic = concat_feature_datasets(datasets_fc_wordnet, \\\n",
    "                            datasets_fc_psycholinguistic, name='wordnet+psycholinguistic')\n",
    "#(3) All categories\n",
    "datasets_fc_all = concat_feature_datasets(datasets_fc_linguistic, datasets_fc_psycholinguistic, \\\n",
    "                            datasets_fc_semantic, datasets_fc_corpus, datasets_fc_dictionary, name='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fc_datasets_dswp = []\n",
    "all_fc_datasets_dswp.extend(datasets_fc_linguistic)\n",
    "all_fc_datasets_dswp.extend(datasets_fc_frequency)\n",
    "all_fc_datasets_dswp.extend(datasets_fc_language_model)\n",
    "all_fc_datasets_dswp.extend(datasets_fc_corpus)\n",
    "all_fc_datasets_dswp.extend(datasets_fc_psycholinguistic)\n",
    "all_fc_datasets_dswp.extend(datasets_fc_wordnet)\n",
    "all_fc_datasets_dswp.extend(datasets_fc_dbpedia)\n",
    "all_fc_datasets_dswp.extend(datasets_fc_brown_clustering)\n",
    "all_fc_datasets_dswp.extend(datasets_fc_semantic)\n",
    "all_fc_datasets_dswp.extend(datasets_fc_dictionary)\n",
    "all_fc_datasets_dswp.extend(datasets_fc_corpus_semantic)\n",
    "all_fc_datasets_dswp.extend(datasets_fc_wordnet_psycholinguistic)\n",
    "all_fc_datasets_dswp.extend(datasets_fc_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.2.0 Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_fc_baseline_1 = compute_features_baseline_1(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [Result(fs, fs.fc, fs.agg,\n",
    "        precision_recall_fscore_support(*nn(*transform_feat_to_num(remove_labels_for_binary_df(fs.train), \n",
    "        remove_labels_for_binary_df(fs.test))), average='macro')) for fs in datasets_fc_baseline_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.337831</td>\n",
       "      <td>0.538235</td>\n",
       "      <td>0.550193</td>\n",
       "      <td>baseline_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.287153</td>\n",
       "      <td>0.395897</td>\n",
       "      <td>0.392677</td>\n",
       "      <td>baseline_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.251908</td>\n",
       "      <td>0.374740</td>\n",
       "      <td>0.374740</td>\n",
       "      <td>baseline_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    agg    dataset        f1      prec       rec          zc\n",
       "0  mean  Wikipedia  0.337831  0.538235  0.550193  baseline_1\n",
       "1  mean   WikiNews  0.287153  0.395897  0.392677  baseline_1\n",
       "2  mean       News  0.251908  0.374740  0.374740  baseline_1"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_eval_data_nn = create_eval_df_from_results_macro(results, False)\n",
    "feature_eval_data_nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.2.1 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-error:0.261364\ttrain-error:0.100279\n",
      "[1]\teval-error:0.25\ttrain-error:0.052925\n",
      "[2]\teval-error:0.238636\ttrain-error:0.019499\n",
      "[3]\teval-error:0.261364\ttrain-error:0.004178\n",
      "[4]\teval-error:0.227273\ttrain-error:0\n",
      "[5]\teval-error:0.227273\ttrain-error:0\n",
      "[6]\teval-error:0.215909\ttrain-error:0\n",
      "[7]\teval-error:0.238636\ttrain-error:0\n",
      "[8]\teval-error:0.238636\ttrain-error:0\n",
      "[9]\teval-error:0.227273\ttrain-error:0\n",
      "[10]\teval-error:0.215909\ttrain-error:0\n",
      "[11]\teval-error:0.204545\ttrain-error:0\n",
      "[12]\teval-error:0.204545\ttrain-error:0\n",
      "[13]\teval-error:0.204545\ttrain-error:0\n",
      "[14]\teval-error:0.204545\ttrain-error:0\n",
      "[15]\teval-error:0.204545\ttrain-error:0\n",
      "[16]\teval-error:0.204545\ttrain-error:0\n",
      "[17]\teval-error:0.215909\ttrain-error:0\n",
      "[18]\teval-error:0.215909\ttrain-error:0\n",
      "[19]\teval-error:0.227273\ttrain-error:0\n",
      "[20]\teval-error:0.215909\ttrain-error:0\n",
      "[21]\teval-error:0.227273\ttrain-error:0\n",
      "[22]\teval-error:0.227273\ttrain-error:0\n",
      "[23]\teval-error:0.227273\ttrain-error:0\n",
      "[24]\teval-error:0.227273\ttrain-error:0\n",
      "[25]\teval-error:0.227273\ttrain-error:0\n",
      "[26]\teval-error:0.227273\ttrain-error:0\n",
      "[27]\teval-error:0.215909\ttrain-error:0\n",
      "[28]\teval-error:0.215909\ttrain-error:0\n",
      "[29]\teval-error:0.227273\ttrain-error:0\n",
      "[30]\teval-error:0.227273\ttrain-error:0\n",
      "[31]\teval-error:0.238636\ttrain-error:0\n",
      "[32]\teval-error:0.238636\ttrain-error:0\n",
      "[33]\teval-error:0.238636\ttrain-error:0\n",
      "[34]\teval-error:0.238636\ttrain-error:0\n",
      "[35]\teval-error:0.238636\ttrain-error:0\n",
      "[36]\teval-error:0.238636\ttrain-error:0\n",
      "[37]\teval-error:0.238636\ttrain-error:0\n",
      "[38]\teval-error:0.238636\ttrain-error:0\n",
      "[39]\teval-error:0.227273\ttrain-error:0\n",
      "[40]\teval-error:0.227273\ttrain-error:0\n",
      "[41]\teval-error:0.238636\ttrain-error:0\n",
      "[42]\teval-error:0.238636\ttrain-error:0\n",
      "[43]\teval-error:0.238636\ttrain-error:0\n",
      "[44]\teval-error:0.238636\ttrain-error:0\n",
      "[45]\teval-error:0.238636\ttrain-error:0\n",
      "[46]\teval-error:0.238636\ttrain-error:0\n",
      "[47]\teval-error:0.238636\ttrain-error:0\n",
      "[48]\teval-error:0.238636\ttrain-error:0\n",
      "[49]\teval-error:0.238636\ttrain-error:0\n",
      "[50]\teval-error:0.238636\ttrain-error:0\n",
      "[51]\teval-error:0.25\ttrain-error:0\n",
      "[52]\teval-error:0.25\ttrain-error:0\n",
      "[53]\teval-error:0.238636\ttrain-error:0\n",
      "[54]\teval-error:0.238636\ttrain-error:0\n",
      "[55]\teval-error:0.238636\ttrain-error:0\n",
      "[56]\teval-error:0.238636\ttrain-error:0\n",
      "[57]\teval-error:0.238636\ttrain-error:0\n",
      "[58]\teval-error:0.238636\ttrain-error:0\n",
      "[59]\teval-error:0.25\ttrain-error:0\n",
      "[60]\teval-error:0.261364\ttrain-error:0\n",
      "[61]\teval-error:0.25\ttrain-error:0\n",
      "[62]\teval-error:0.238636\ttrain-error:0\n",
      "[63]\teval-error:0.238636\ttrain-error:0\n",
      "[64]\teval-error:0.25\ttrain-error:0\n",
      "[65]\teval-error:0.25\ttrain-error:0\n",
      "[66]\teval-error:0.25\ttrain-error:0\n",
      "[67]\teval-error:0.25\ttrain-error:0\n",
      "[68]\teval-error:0.238636\ttrain-error:0\n",
      "[69]\teval-error:0.238636\ttrain-error:0\n",
      "[0]\teval-error:0.25\ttrain-error:0.119777\n",
      "[1]\teval-error:0.261364\ttrain-error:0.059889\n",
      "[2]\teval-error:0.215909\ttrain-error:0.020891\n",
      "[3]\teval-error:0.238636\ttrain-error:0.006964\n",
      "[4]\teval-error:0.238636\ttrain-error:0\n",
      "[5]\teval-error:0.25\ttrain-error:0\n",
      "[6]\teval-error:0.227273\ttrain-error:0\n",
      "[7]\teval-error:0.227273\ttrain-error:0\n",
      "[8]\teval-error:0.204545\ttrain-error:0\n",
      "[9]\teval-error:0.215909\ttrain-error:0\n",
      "[10]\teval-error:0.204545\ttrain-error:0\n",
      "[11]\teval-error:0.204545\ttrain-error:0\n",
      "[12]\teval-error:0.215909\ttrain-error:0\n",
      "[13]\teval-error:0.204545\ttrain-error:0\n",
      "[14]\teval-error:0.215909\ttrain-error:0\n",
      "[15]\teval-error:0.227273\ttrain-error:0\n",
      "[16]\teval-error:0.215909\ttrain-error:0\n",
      "[17]\teval-error:0.215909\ttrain-error:0\n",
      "[18]\teval-error:0.215909\ttrain-error:0\n",
      "[19]\teval-error:0.215909\ttrain-error:0\n",
      "[20]\teval-error:0.204545\ttrain-error:0\n",
      "[21]\teval-error:0.204545\ttrain-error:0\n",
      "[22]\teval-error:0.204545\ttrain-error:0\n",
      "[23]\teval-error:0.204545\ttrain-error:0\n",
      "[24]\teval-error:0.204545\ttrain-error:0\n",
      "[25]\teval-error:0.204545\ttrain-error:0\n",
      "[26]\teval-error:0.204545\ttrain-error:0\n",
      "[27]\teval-error:0.204545\ttrain-error:0\n",
      "[28]\teval-error:0.204545\ttrain-error:0\n",
      "[29]\teval-error:0.204545\ttrain-error:0\n",
      "[30]\teval-error:0.215909\ttrain-error:0\n",
      "[31]\teval-error:0.215909\ttrain-error:0\n",
      "[32]\teval-error:0.204545\ttrain-error:0\n",
      "[33]\teval-error:0.215909\ttrain-error:0\n",
      "[34]\teval-error:0.193182\ttrain-error:0\n",
      "[35]\teval-error:0.204545\ttrain-error:0\n",
      "[36]\teval-error:0.181818\ttrain-error:0\n",
      "[37]\teval-error:0.193182\ttrain-error:0\n",
      "[38]\teval-error:0.181818\ttrain-error:0\n",
      "[39]\teval-error:0.193182\ttrain-error:0\n",
      "[40]\teval-error:0.193182\ttrain-error:0\n",
      "[41]\teval-error:0.204545\ttrain-error:0\n",
      "[42]\teval-error:0.193182\ttrain-error:0\n",
      "[43]\teval-error:0.204545\ttrain-error:0\n",
      "[44]\teval-error:0.204545\ttrain-error:0\n",
      "[45]\teval-error:0.204545\ttrain-error:0\n",
      "[46]\teval-error:0.204545\ttrain-error:0\n",
      "[47]\teval-error:0.204545\ttrain-error:0\n",
      "[48]\teval-error:0.204545\ttrain-error:0\n",
      "[49]\teval-error:0.193182\ttrain-error:0\n",
      "[50]\teval-error:0.193182\ttrain-error:0\n",
      "[51]\teval-error:0.204545\ttrain-error:0\n",
      "[52]\teval-error:0.204545\ttrain-error:0\n",
      "[53]\teval-error:0.204545\ttrain-error:0\n",
      "[54]\teval-error:0.204545\ttrain-error:0\n",
      "[55]\teval-error:0.204545\ttrain-error:0\n",
      "[56]\teval-error:0.204545\ttrain-error:0\n",
      "[57]\teval-error:0.204545\ttrain-error:0\n",
      "[58]\teval-error:0.193182\ttrain-error:0\n",
      "[59]\teval-error:0.193182\ttrain-error:0\n",
      "[60]\teval-error:0.204545\ttrain-error:0\n",
      "[61]\teval-error:0.193182\ttrain-error:0\n",
      "[62]\teval-error:0.193182\ttrain-error:0\n",
      "[63]\teval-error:0.193182\ttrain-error:0\n",
      "[64]\teval-error:0.193182\ttrain-error:0\n",
      "[65]\teval-error:0.193182\ttrain-error:0\n",
      "[66]\teval-error:0.193182\ttrain-error:0\n",
      "[67]\teval-error:0.193182\ttrain-error:0\n",
      "[68]\teval-error:0.193182\ttrain-error:0\n",
      "[69]\teval-error:0.193182\ttrain-error:0\n",
      "[0]\teval-error:0.204545\ttrain-error:0.100279\n",
      "[1]\teval-error:0.238636\ttrain-error:0.062674\n",
      "[2]\teval-error:0.238636\ttrain-error:0.032033\n",
      "[3]\teval-error:0.238636\ttrain-error:0.013928\n",
      "[4]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[5]\teval-error:0.238636\ttrain-error:0.001393\n",
      "[6]\teval-error:0.227273\ttrain-error:0.001393\n",
      "[7]\teval-error:0.215909\ttrain-error:0.001393\n",
      "[8]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[9]\teval-error:0.204545\ttrain-error:0\n",
      "[10]\teval-error:0.227273\ttrain-error:0\n",
      "[11]\teval-error:0.227273\ttrain-error:0\n",
      "[12]\teval-error:0.215909\ttrain-error:0\n",
      "[13]\teval-error:0.215909\ttrain-error:0\n",
      "[14]\teval-error:0.204545\ttrain-error:0\n",
      "[15]\teval-error:0.204545\ttrain-error:0\n",
      "[16]\teval-error:0.215909\ttrain-error:0\n",
      "[17]\teval-error:0.215909\ttrain-error:0\n",
      "[18]\teval-error:0.204545\ttrain-error:0\n",
      "[19]\teval-error:0.204545\ttrain-error:0\n",
      "[20]\teval-error:0.204545\ttrain-error:0\n",
      "[21]\teval-error:0.204545\ttrain-error:0\n",
      "[22]\teval-error:0.204545\ttrain-error:0\n",
      "[23]\teval-error:0.204545\ttrain-error:0\n",
      "[24]\teval-error:0.204545\ttrain-error:0\n",
      "[25]\teval-error:0.204545\ttrain-error:0\n",
      "[26]\teval-error:0.204545\ttrain-error:0\n",
      "[27]\teval-error:0.204545\ttrain-error:0\n",
      "[28]\teval-error:0.204545\ttrain-error:0\n",
      "[29]\teval-error:0.215909\ttrain-error:0\n",
      "[30]\teval-error:0.204545\ttrain-error:0\n",
      "[31]\teval-error:0.204545\ttrain-error:0\n",
      "[32]\teval-error:0.204545\ttrain-error:0\n",
      "[33]\teval-error:0.215909\ttrain-error:0\n",
      "[34]\teval-error:0.215909\ttrain-error:0\n",
      "[35]\teval-error:0.204545\ttrain-error:0\n",
      "[36]\teval-error:0.204545\ttrain-error:0\n",
      "[37]\teval-error:0.204545\ttrain-error:0\n",
      "[38]\teval-error:0.204545\ttrain-error:0\n",
      "[39]\teval-error:0.204545\ttrain-error:0\n",
      "[40]\teval-error:0.204545\ttrain-error:0\n",
      "[41]\teval-error:0.204545\ttrain-error:0\n",
      "[42]\teval-error:0.204545\ttrain-error:0\n",
      "[43]\teval-error:0.204545\ttrain-error:0\n",
      "[44]\teval-error:0.204545\ttrain-error:0\n",
      "[45]\teval-error:0.204545\ttrain-error:0\n",
      "[46]\teval-error:0.204545\ttrain-error:0\n",
      "[47]\teval-error:0.204545\ttrain-error:0\n",
      "[48]\teval-error:0.204545\ttrain-error:0\n",
      "[49]\teval-error:0.204545\ttrain-error:0\n",
      "[50]\teval-error:0.204545\ttrain-error:0\n",
      "[51]\teval-error:0.204545\ttrain-error:0\n",
      "[52]\teval-error:0.215909\ttrain-error:0\n",
      "[53]\teval-error:0.215909\ttrain-error:0\n",
      "[54]\teval-error:0.215909\ttrain-error:0\n",
      "[55]\teval-error:0.227273\ttrain-error:0\n",
      "[56]\teval-error:0.227273\ttrain-error:0\n",
      "[57]\teval-error:0.227273\ttrain-error:0\n",
      "[58]\teval-error:0.227273\ttrain-error:0\n",
      "[59]\teval-error:0.227273\ttrain-error:0\n",
      "[60]\teval-error:0.227273\ttrain-error:0\n",
      "[61]\teval-error:0.238636\ttrain-error:0\n",
      "[62]\teval-error:0.238636\ttrain-error:0\n",
      "[63]\teval-error:0.238636\ttrain-error:0\n",
      "[64]\teval-error:0.238636\ttrain-error:0\n",
      "[65]\teval-error:0.238636\ttrain-error:0\n",
      "[66]\teval-error:0.215909\ttrain-error:0\n",
      "[67]\teval-error:0.227273\ttrain-error:0\n",
      "[68]\teval-error:0.227273\ttrain-error:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69]\teval-error:0.215909\ttrain-error:0\n",
      "[0]\teval-error:0.261364\ttrain-error:0.108635\n",
      "[1]\teval-error:0.261364\ttrain-error:0.044568\n",
      "[2]\teval-error:0.215909\ttrain-error:0.01532\n",
      "[3]\teval-error:0.25\ttrain-error:0.001393\n",
      "[4]\teval-error:0.215909\ttrain-error:0\n",
      "[5]\teval-error:0.227273\ttrain-error:0\n",
      "[6]\teval-error:0.215909\ttrain-error:0\n",
      "[7]\teval-error:0.193182\ttrain-error:0\n",
      "[8]\teval-error:0.204545\ttrain-error:0\n",
      "[9]\teval-error:0.204545\ttrain-error:0\n",
      "[10]\teval-error:0.204545\ttrain-error:0\n",
      "[11]\teval-error:0.204545\ttrain-error:0\n",
      "[12]\teval-error:0.204545\ttrain-error:0\n",
      "[13]\teval-error:0.204545\ttrain-error:0\n",
      "[14]\teval-error:0.204545\ttrain-error:0\n",
      "[15]\teval-error:0.204545\ttrain-error:0\n",
      "[16]\teval-error:0.204545\ttrain-error:0\n",
      "[17]\teval-error:0.204545\ttrain-error:0\n",
      "[18]\teval-error:0.204545\ttrain-error:0\n",
      "[19]\teval-error:0.204545\ttrain-error:0\n",
      "[20]\teval-error:0.204545\ttrain-error:0\n",
      "[21]\teval-error:0.215909\ttrain-error:0\n",
      "[22]\teval-error:0.215909\ttrain-error:0\n",
      "[23]\teval-error:0.215909\ttrain-error:0\n",
      "[24]\teval-error:0.215909\ttrain-error:0\n",
      "[25]\teval-error:0.215909\ttrain-error:0\n",
      "[26]\teval-error:0.204545\ttrain-error:0\n",
      "[27]\teval-error:0.215909\ttrain-error:0\n",
      "[28]\teval-error:0.204545\ttrain-error:0\n",
      "[29]\teval-error:0.204545\ttrain-error:0\n",
      "[30]\teval-error:0.204545\ttrain-error:0\n",
      "[31]\teval-error:0.204545\ttrain-error:0\n",
      "[32]\teval-error:0.215909\ttrain-error:0\n",
      "[33]\teval-error:0.204545\ttrain-error:0\n",
      "[34]\teval-error:0.204545\ttrain-error:0\n",
      "[35]\teval-error:0.193182\ttrain-error:0\n",
      "[36]\teval-error:0.193182\ttrain-error:0\n",
      "[37]\teval-error:0.193182\ttrain-error:0\n",
      "[38]\teval-error:0.193182\ttrain-error:0\n",
      "[39]\teval-error:0.193182\ttrain-error:0\n",
      "[40]\teval-error:0.204545\ttrain-error:0\n",
      "[41]\teval-error:0.193182\ttrain-error:0\n",
      "[42]\teval-error:0.193182\ttrain-error:0\n",
      "[43]\teval-error:0.193182\ttrain-error:0\n",
      "[44]\teval-error:0.193182\ttrain-error:0\n",
      "[45]\teval-error:0.193182\ttrain-error:0\n",
      "[46]\teval-error:0.193182\ttrain-error:0\n",
      "[47]\teval-error:0.193182\ttrain-error:0\n",
      "[48]\teval-error:0.181818\ttrain-error:0\n",
      "[49]\teval-error:0.193182\ttrain-error:0\n",
      "[50]\teval-error:0.193182\ttrain-error:0\n",
      "[51]\teval-error:0.193182\ttrain-error:0\n",
      "[52]\teval-error:0.181818\ttrain-error:0\n",
      "[53]\teval-error:0.193182\ttrain-error:0\n",
      "[54]\teval-error:0.181818\ttrain-error:0\n",
      "[55]\teval-error:0.193182\ttrain-error:0\n",
      "[56]\teval-error:0.193182\ttrain-error:0\n",
      "[57]\teval-error:0.204545\ttrain-error:0\n",
      "[58]\teval-error:0.204545\ttrain-error:0\n",
      "[59]\teval-error:0.204545\ttrain-error:0\n",
      "[60]\teval-error:0.204545\ttrain-error:0\n",
      "[61]\teval-error:0.204545\ttrain-error:0\n",
      "[62]\teval-error:0.204545\ttrain-error:0\n",
      "[63]\teval-error:0.204545\ttrain-error:0\n",
      "[64]\teval-error:0.193182\ttrain-error:0\n",
      "[65]\teval-error:0.204545\ttrain-error:0\n",
      "[66]\teval-error:0.204545\ttrain-error:0\n",
      "[67]\teval-error:0.204545\ttrain-error:0\n",
      "[68]\teval-error:0.204545\ttrain-error:0\n",
      "[69]\teval-error:0.193182\ttrain-error:0\n",
      "[0]\teval-error:0.287234\ttrain-error:0.11272\n",
      "[1]\teval-error:0.265957\ttrain-error:0.059979\n",
      "[2]\teval-error:0.265957\ttrain-error:0.023785\n",
      "[3]\teval-error:0.244681\ttrain-error:0.009307\n",
      "[4]\teval-error:0.244681\ttrain-error:0.004137\n",
      "[5]\teval-error:0.244681\ttrain-error:0.003102\n",
      "[6]\teval-error:0.255319\ttrain-error:0.003102\n",
      "[7]\teval-error:0.244681\ttrain-error:0.002068\n",
      "[8]\teval-error:0.255319\ttrain-error:0.002068\n",
      "[9]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[10]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[11]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[12]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[13]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[14]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[15]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[16]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[17]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[18]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[19]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[20]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[21]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[22]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[23]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[24]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[25]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[26]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[27]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[28]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[29]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[30]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[31]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[32]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[33]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[34]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[35]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[36]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[37]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[38]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[39]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[40]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[41]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[42]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[43]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[44]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[45]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[46]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[47]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[48]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[49]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[50]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[51]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[52]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[53]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[54]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[55]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[56]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[57]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[58]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[59]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[60]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[61]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[62]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[63]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[64]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[65]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[66]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[67]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[68]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[69]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[0]\teval-error:0.361702\ttrain-error:0.126163\n",
      "[1]\teval-error:0.276596\ttrain-error:0.055843\n",
      "[2]\teval-error:0.276596\ttrain-error:0.031024\n",
      "[3]\teval-error:0.255319\ttrain-error:0.005171\n",
      "[4]\teval-error:0.234043\ttrain-error:0.004137\n",
      "[5]\teval-error:0.234043\ttrain-error:0.003102\n",
      "[6]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[7]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[8]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[9]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[10]\teval-error:0.265957\ttrain-error:0.002068\n",
      "[11]\teval-error:0.244681\ttrain-error:0.002068\n",
      "[12]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[13]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[14]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[15]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[16]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[17]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[18]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[19]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[20]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[21]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[22]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[23]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[24]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[25]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[26]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[27]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[28]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[29]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[30]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[31]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[32]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[33]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[34]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[35]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[36]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[37]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[38]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[39]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[40]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[41]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[42]\teval-error:0.212766\ttrain-error:0.001034\n",
      "[43]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[44]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[45]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[46]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[47]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[48]\teval-error:0.223404\ttrain-error:0.001034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[50]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[51]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[52]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[53]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[54]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[55]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[56]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[57]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[58]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[59]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[60]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[61]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[62]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[63]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[64]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[65]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[66]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[67]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[68]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[69]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[0]\teval-error:0.351064\ttrain-error:0.110652\n",
      "[1]\teval-error:0.308511\ttrain-error:0.061013\n",
      "[2]\teval-error:0.287234\ttrain-error:0.022751\n",
      "[3]\teval-error:0.297872\ttrain-error:0.010341\n",
      "[4]\teval-error:0.308511\ttrain-error:0.007239\n",
      "[5]\teval-error:0.276596\ttrain-error:0.003102\n",
      "[6]\teval-error:0.265957\ttrain-error:0.003102\n",
      "[7]\teval-error:0.265957\ttrain-error:0.003102\n",
      "[8]\teval-error:0.265957\ttrain-error:0.002068\n",
      "[9]\teval-error:0.265957\ttrain-error:0.002068\n",
      "[10]\teval-error:0.255319\ttrain-error:0.002068\n",
      "[11]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[12]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[13]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[14]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[15]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[16]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[17]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[18]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[19]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[20]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[21]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[22]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[23]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[24]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[25]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[26]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[27]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[28]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[29]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[30]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[31]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[32]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[33]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[34]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[35]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[36]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[37]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[38]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[39]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[40]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[41]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[42]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[43]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[44]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[45]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[46]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[47]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[48]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[49]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[50]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[51]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[52]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[53]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[54]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[55]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[56]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[57]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[58]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[59]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[60]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[61]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[62]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[63]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[64]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[65]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[66]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[67]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[68]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[69]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[0]\teval-error:0.244681\ttrain-error:0.104447\n",
      "[1]\teval-error:0.223404\ttrain-error:0.04757\n",
      "[2]\teval-error:0.223404\ttrain-error:0.01758\n",
      "[3]\teval-error:0.191489\ttrain-error:0.007239\n",
      "[4]\teval-error:0.244681\ttrain-error:0.004137\n",
      "[5]\teval-error:0.255319\ttrain-error:0.002068\n",
      "[6]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[7]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[8]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[9]\teval-error:0.212766\ttrain-error:0.001034\n",
      "[10]\teval-error:0.202128\ttrain-error:0.001034\n",
      "[11]\teval-error:0.202128\ttrain-error:0.001034\n",
      "[12]\teval-error:0.202128\ttrain-error:0.001034\n",
      "[13]\teval-error:0.212766\ttrain-error:0.001034\n",
      "[14]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[15]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[16]\teval-error:0.212766\ttrain-error:0.001034\n",
      "[17]\teval-error:0.191489\ttrain-error:0.001034\n",
      "[18]\teval-error:0.191489\ttrain-error:0.001034\n",
      "[19]\teval-error:0.202128\ttrain-error:0.001034\n",
      "[20]\teval-error:0.212766\ttrain-error:0.001034\n",
      "[21]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[22]\teval-error:0.212766\ttrain-error:0.001034\n",
      "[23]\teval-error:0.202128\ttrain-error:0.001034\n",
      "[24]\teval-error:0.212766\ttrain-error:0.001034\n",
      "[25]\teval-error:0.212766\ttrain-error:0.001034\n",
      "[26]\teval-error:0.212766\ttrain-error:0.001034\n",
      "[27]\teval-error:0.212766\ttrain-error:0.001034\n",
      "[28]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[29]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[30]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[31]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[32]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[33]\teval-error:0.212766\ttrain-error:0.001034\n",
      "[34]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[35]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[36]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[37]\teval-error:0.212766\ttrain-error:0.001034\n",
      "[38]\teval-error:0.202128\ttrain-error:0.001034\n",
      "[39]\teval-error:0.212766\ttrain-error:0.001034\n",
      "[40]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[41]\teval-error:0.212766\ttrain-error:0.001034\n",
      "[42]\teval-error:0.212766\ttrain-error:0.001034\n",
      "[43]\teval-error:0.202128\ttrain-error:0.001034\n",
      "[44]\teval-error:0.202128\ttrain-error:0.001034\n",
      "[45]\teval-error:0.202128\ttrain-error:0.001034\n",
      "[46]\teval-error:0.202128\ttrain-error:0.001034\n",
      "[47]\teval-error:0.202128\ttrain-error:0.001034\n",
      "[48]\teval-error:0.202128\ttrain-error:0.001034\n",
      "[49]\teval-error:0.212766\ttrain-error:0.001034\n",
      "[50]\teval-error:0.212766\ttrain-error:0.001034\n",
      "[51]\teval-error:0.212766\ttrain-error:0.001034\n",
      "[52]\teval-error:0.212766\ttrain-error:0.001034\n",
      "[53]\teval-error:0.202128\ttrain-error:0.001034\n",
      "[54]\teval-error:0.202128\ttrain-error:0.001034\n",
      "[55]\teval-error:0.212766\ttrain-error:0.001034\n",
      "[56]\teval-error:0.212766\ttrain-error:0.001034\n",
      "[57]\teval-error:0.212766\ttrain-error:0.001034\n",
      "[58]\teval-error:0.212766\ttrain-error:0.001034\n",
      "[59]\teval-error:0.202128\ttrain-error:0.001034\n",
      "[60]\teval-error:0.202128\ttrain-error:0.001034\n",
      "[61]\teval-error:0.202128\ttrain-error:0.001034\n",
      "[62]\teval-error:0.202128\ttrain-error:0.001034\n",
      "[63]\teval-error:0.202128\ttrain-error:0.001034\n",
      "[64]\teval-error:0.212766\ttrain-error:0.001034\n",
      "[65]\teval-error:0.212766\ttrain-error:0.001034\n",
      "[66]\teval-error:0.202128\ttrain-error:0.001034\n",
      "[67]\teval-error:0.202128\ttrain-error:0.001034\n",
      "[68]\teval-error:0.202128\ttrain-error:0.001034\n",
      "[69]\teval-error:0.202128\ttrain-error:0.001034\n",
      "[0]\teval-error:0.251908\ttrain-error:0.100097\n",
      "[1]\teval-error:0.232824\ttrain-error:0.051506\n",
      "[2]\teval-error:0.217557\ttrain-error:0.022838\n",
      "[3]\teval-error:0.209924\ttrain-error:0.015063\n",
      "[4]\teval-error:0.240458\ttrain-error:0.009718\n",
      "[5]\teval-error:0.240458\ttrain-error:0.007289\n",
      "[6]\teval-error:0.244275\ttrain-error:0.006317\n",
      "[7]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[8]\teval-error:0.221374\ttrain-error:0.006317\n",
      "[9]\teval-error:0.236641\ttrain-error:0.005831\n",
      "[10]\teval-error:0.232824\ttrain-error:0.005831\n",
      "[11]\teval-error:0.240458\ttrain-error:0.005831\n",
      "[12]\teval-error:0.240458\ttrain-error:0.005831\n",
      "[13]\teval-error:0.248092\ttrain-error:0.005831\n",
      "[14]\teval-error:0.248092\ttrain-error:0.005831\n",
      "[15]\teval-error:0.236641\ttrain-error:0.005831\n",
      "[16]\teval-error:0.240458\ttrain-error:0.005831\n",
      "[17]\teval-error:0.244275\ttrain-error:0.005831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18]\teval-error:0.236641\ttrain-error:0.005831\n",
      "[19]\teval-error:0.232824\ttrain-error:0.005831\n",
      "[20]\teval-error:0.225191\ttrain-error:0.005831\n",
      "[21]\teval-error:0.232824\ttrain-error:0.005831\n",
      "[22]\teval-error:0.240458\ttrain-error:0.005831\n",
      "[23]\teval-error:0.232824\ttrain-error:0.005831\n",
      "[24]\teval-error:0.225191\ttrain-error:0.005831\n",
      "[25]\teval-error:0.225191\ttrain-error:0.005831\n",
      "[26]\teval-error:0.232824\ttrain-error:0.005831\n",
      "[27]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[28]\teval-error:0.225191\ttrain-error:0.005831\n",
      "[29]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[30]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[31]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[32]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[33]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[34]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[35]\teval-error:0.232824\ttrain-error:0.005831\n",
      "[36]\teval-error:0.236641\ttrain-error:0.005831\n",
      "[37]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[38]\teval-error:0.225191\ttrain-error:0.005831\n",
      "[39]\teval-error:0.225191\ttrain-error:0.005831\n",
      "[40]\teval-error:0.225191\ttrain-error:0.005831\n",
      "[41]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[42]\teval-error:0.225191\ttrain-error:0.005831\n",
      "[43]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[44]\teval-error:0.225191\ttrain-error:0.005831\n",
      "[45]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[46]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[47]\teval-error:0.232824\ttrain-error:0.005831\n",
      "[48]\teval-error:0.232824\ttrain-error:0.005831\n",
      "[49]\teval-error:0.232824\ttrain-error:0.005831\n",
      "[50]\teval-error:0.232824\ttrain-error:0.005831\n",
      "[51]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[52]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[53]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[54]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[55]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[56]\teval-error:0.225191\ttrain-error:0.005831\n",
      "[57]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[58]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[59]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[60]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[61]\teval-error:0.232824\ttrain-error:0.005831\n",
      "[62]\teval-error:0.232824\ttrain-error:0.005831\n",
      "[63]\teval-error:0.232824\ttrain-error:0.005831\n",
      "[64]\teval-error:0.232824\ttrain-error:0.005831\n",
      "[65]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[66]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[67]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[68]\teval-error:0.225191\ttrain-error:0.005831\n",
      "[69]\teval-error:0.225191\ttrain-error:0.005831\n",
      "[0]\teval-error:0.229008\ttrain-error:0.102041\n",
      "[1]\teval-error:0.225191\ttrain-error:0.060253\n",
      "[2]\teval-error:0.209924\ttrain-error:0.030612\n",
      "[3]\teval-error:0.232824\ttrain-error:0.015063\n",
      "[4]\teval-error:0.229008\ttrain-error:0.010204\n",
      "[5]\teval-error:0.236641\ttrain-error:0.008746\n",
      "[6]\teval-error:0.221374\ttrain-error:0.007289\n",
      "[7]\teval-error:0.236641\ttrain-error:0.006803\n",
      "[8]\teval-error:0.221374\ttrain-error:0.006803\n",
      "[9]\teval-error:0.217557\ttrain-error:0.006803\n",
      "[10]\teval-error:0.206107\ttrain-error:0.006803\n",
      "[11]\teval-error:0.21374\ttrain-error:0.006803\n",
      "[12]\teval-error:0.209924\ttrain-error:0.006317\n",
      "[13]\teval-error:0.20229\ttrain-error:0.005831\n",
      "[14]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[15]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[16]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[17]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[18]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[19]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[20]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[21]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[22]\teval-error:0.20229\ttrain-error:0.005831\n",
      "[23]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[24]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[25]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[26]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[27]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[28]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[29]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[30]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[31]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[32]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[33]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[34]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[35]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[36]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[37]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[38]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[39]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[40]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[41]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[42]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[43]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[44]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[45]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[46]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[47]\teval-error:0.198473\ttrain-error:0.005831\n",
      "[48]\teval-error:0.20229\ttrain-error:0.005831\n",
      "[49]\teval-error:0.198473\ttrain-error:0.005831\n",
      "[50]\teval-error:0.198473\ttrain-error:0.005831\n",
      "[51]\teval-error:0.194656\ttrain-error:0.005831\n",
      "[52]\teval-error:0.20229\ttrain-error:0.005831\n",
      "[53]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[54]\teval-error:0.20229\ttrain-error:0.005831\n",
      "[55]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[56]\teval-error:0.20229\ttrain-error:0.005831\n",
      "[57]\teval-error:0.20229\ttrain-error:0.005831\n",
      "[58]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[59]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[60]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[61]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[62]\teval-error:0.20229\ttrain-error:0.005831\n",
      "[63]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[64]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[65]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[66]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[67]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[68]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[69]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[0]\teval-error:0.255725\ttrain-error:0.109329\n",
      "[1]\teval-error:0.248092\ttrain-error:0.071914\n",
      "[2]\teval-error:0.255725\ttrain-error:0.0345\n",
      "[3]\teval-error:0.248092\ttrain-error:0.017979\n",
      "[4]\teval-error:0.244275\ttrain-error:0.01069\n",
      "[5]\teval-error:0.244275\ttrain-error:0.007289\n",
      "[6]\teval-error:0.240458\ttrain-error:0.006317\n",
      "[7]\teval-error:0.225191\ttrain-error:0.006317\n",
      "[8]\teval-error:0.232824\ttrain-error:0.006317\n",
      "[9]\teval-error:0.240458\ttrain-error:0.005831\n",
      "[10]\teval-error:0.240458\ttrain-error:0.005831\n",
      "[11]\teval-error:0.244275\ttrain-error:0.005831\n",
      "[12]\teval-error:0.236641\ttrain-error:0.005831\n",
      "[13]\teval-error:0.240458\ttrain-error:0.005831\n",
      "[14]\teval-error:0.232824\ttrain-error:0.005831\n",
      "[15]\teval-error:0.232824\ttrain-error:0.005831\n",
      "[16]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[17]\teval-error:0.232824\ttrain-error:0.005831\n",
      "[18]\teval-error:0.236641\ttrain-error:0.005831\n",
      "[19]\teval-error:0.232824\ttrain-error:0.005831\n",
      "[20]\teval-error:0.232824\ttrain-error:0.005831\n",
      "[21]\teval-error:0.240458\ttrain-error:0.005831\n",
      "[22]\teval-error:0.236641\ttrain-error:0.005831\n",
      "[23]\teval-error:0.232824\ttrain-error:0.005831\n",
      "[24]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[25]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[26]\teval-error:0.232824\ttrain-error:0.005831\n",
      "[27]\teval-error:0.236641\ttrain-error:0.005831\n",
      "[28]\teval-error:0.240458\ttrain-error:0.005831\n",
      "[29]\teval-error:0.236641\ttrain-error:0.005831\n",
      "[30]\teval-error:0.240458\ttrain-error:0.005831\n",
      "[31]\teval-error:0.236641\ttrain-error:0.005831\n",
      "[32]\teval-error:0.240458\ttrain-error:0.005831\n",
      "[33]\teval-error:0.240458\ttrain-error:0.005831\n",
      "[34]\teval-error:0.236641\ttrain-error:0.005831\n",
      "[35]\teval-error:0.232824\ttrain-error:0.005831\n",
      "[36]\teval-error:0.232824\ttrain-error:0.005831\n",
      "[37]\teval-error:0.232824\ttrain-error:0.005831\n",
      "[38]\teval-error:0.236641\ttrain-error:0.005831\n",
      "[39]\teval-error:0.236641\ttrain-error:0.005831\n",
      "[40]\teval-error:0.240458\ttrain-error:0.005831\n",
      "[41]\teval-error:0.232824\ttrain-error:0.005831\n",
      "[42]\teval-error:0.236641\ttrain-error:0.005831\n",
      "[43]\teval-error:0.232824\ttrain-error:0.005831\n",
      "[44]\teval-error:0.232824\ttrain-error:0.005831\n",
      "[45]\teval-error:0.232824\ttrain-error:0.005831\n",
      "[46]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[47]\teval-error:0.232824\ttrain-error:0.005831\n",
      "[48]\teval-error:0.232824\ttrain-error:0.005831\n",
      "[49]\teval-error:0.232824\ttrain-error:0.005831\n",
      "[50]\teval-error:0.236641\ttrain-error:0.005831\n",
      "[51]\teval-error:0.236641\ttrain-error:0.005831\n",
      "[52]\teval-error:0.232824\ttrain-error:0.005831\n",
      "[53]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[54]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[55]\teval-error:0.232824\ttrain-error:0.005831\n",
      "[56]\teval-error:0.229008\ttrain-error:0.005831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[58]\teval-error:0.225191\ttrain-error:0.005831\n",
      "[59]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[60]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[61]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[62]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[63]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[64]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[65]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[66]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[67]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[68]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[69]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[0]\teval-error:0.236641\ttrain-error:0.099611\n",
      "[1]\teval-error:0.21374\ttrain-error:0.052478\n",
      "[2]\teval-error:0.217557\ttrain-error:0.02138\n",
      "[3]\teval-error:0.194656\ttrain-error:0.011176\n",
      "[4]\teval-error:0.183206\ttrain-error:0.009232\n",
      "[5]\teval-error:0.187023\ttrain-error:0.006803\n",
      "[6]\teval-error:0.20229\ttrain-error:0.007289\n",
      "[7]\teval-error:0.19084\ttrain-error:0.006317\n",
      "[8]\teval-error:0.198473\ttrain-error:0.006317\n",
      "[9]\teval-error:0.187023\ttrain-error:0.005831\n",
      "[10]\teval-error:0.179389\ttrain-error:0.005831\n",
      "[11]\teval-error:0.19084\ttrain-error:0.005831\n",
      "[12]\teval-error:0.183206\ttrain-error:0.005831\n",
      "[13]\teval-error:0.175573\ttrain-error:0.005831\n",
      "[14]\teval-error:0.179389\ttrain-error:0.005831\n",
      "[15]\teval-error:0.183206\ttrain-error:0.005831\n",
      "[16]\teval-error:0.179389\ttrain-error:0.005831\n",
      "[17]\teval-error:0.175573\ttrain-error:0.005831\n",
      "[18]\teval-error:0.175573\ttrain-error:0.005831\n",
      "[19]\teval-error:0.183206\ttrain-error:0.005831\n",
      "[20]\teval-error:0.183206\ttrain-error:0.005831\n",
      "[21]\teval-error:0.187023\ttrain-error:0.005831\n",
      "[22]\teval-error:0.187023\ttrain-error:0.005831\n",
      "[23]\teval-error:0.183206\ttrain-error:0.005831\n",
      "[24]\teval-error:0.179389\ttrain-error:0.005831\n",
      "[25]\teval-error:0.187023\ttrain-error:0.005831\n",
      "[26]\teval-error:0.183206\ttrain-error:0.005831\n",
      "[27]\teval-error:0.179389\ttrain-error:0.005831\n",
      "[28]\teval-error:0.183206\ttrain-error:0.005831\n",
      "[29]\teval-error:0.179389\ttrain-error:0.005831\n",
      "[30]\teval-error:0.175573\ttrain-error:0.005831\n",
      "[31]\teval-error:0.179389\ttrain-error:0.005831\n",
      "[32]\teval-error:0.183206\ttrain-error:0.005831\n",
      "[33]\teval-error:0.183206\ttrain-error:0.005831\n",
      "[34]\teval-error:0.183206\ttrain-error:0.005831\n",
      "[35]\teval-error:0.183206\ttrain-error:0.005831\n",
      "[36]\teval-error:0.183206\ttrain-error:0.005831\n",
      "[37]\teval-error:0.183206\ttrain-error:0.005831\n",
      "[38]\teval-error:0.187023\ttrain-error:0.005831\n",
      "[39]\teval-error:0.187023\ttrain-error:0.005831\n",
      "[40]\teval-error:0.179389\ttrain-error:0.005831\n",
      "[41]\teval-error:0.179389\ttrain-error:0.005831\n",
      "[42]\teval-error:0.179389\ttrain-error:0.005831\n",
      "[43]\teval-error:0.179389\ttrain-error:0.005831\n",
      "[44]\teval-error:0.179389\ttrain-error:0.005831\n",
      "[45]\teval-error:0.183206\ttrain-error:0.005831\n",
      "[46]\teval-error:0.179389\ttrain-error:0.005831\n",
      "[47]\teval-error:0.179389\ttrain-error:0.005831\n",
      "[48]\teval-error:0.175573\ttrain-error:0.005831\n",
      "[49]\teval-error:0.179389\ttrain-error:0.005831\n",
      "[50]\teval-error:0.183206\ttrain-error:0.005831\n",
      "[51]\teval-error:0.179389\ttrain-error:0.005831\n",
      "[52]\teval-error:0.183206\ttrain-error:0.005831\n",
      "[53]\teval-error:0.183206\ttrain-error:0.005831\n",
      "[54]\teval-error:0.183206\ttrain-error:0.005831\n",
      "[55]\teval-error:0.175573\ttrain-error:0.005831\n",
      "[56]\teval-error:0.179389\ttrain-error:0.005831\n",
      "[57]\teval-error:0.175573\ttrain-error:0.005831\n",
      "[58]\teval-error:0.175573\ttrain-error:0.005831\n",
      "[59]\teval-error:0.175573\ttrain-error:0.005831\n",
      "[60]\teval-error:0.179389\ttrain-error:0.005831\n",
      "[61]\teval-error:0.175573\ttrain-error:0.005831\n",
      "[62]\teval-error:0.175573\ttrain-error:0.005831\n",
      "[63]\teval-error:0.175573\ttrain-error:0.005831\n",
      "[64]\teval-error:0.175573\ttrain-error:0.005831\n",
      "[65]\teval-error:0.179389\ttrain-error:0.005831\n",
      "[66]\teval-error:0.179389\ttrain-error:0.005831\n",
      "[67]\teval-error:0.179389\ttrain-error:0.005831\n",
      "[68]\teval-error:0.179389\ttrain-error:0.005831\n",
      "[69]\teval-error:0.179389\ttrain-error:0.005831\n",
      "[0]\teval-error:0.306818\ttrain-error:0.093315\n",
      "[1]\teval-error:0.227273\ttrain-error:0.052925\n",
      "[2]\teval-error:0.204545\ttrain-error:0.027855\n",
      "[3]\teval-error:0.238636\ttrain-error:0.01532\n",
      "[4]\teval-error:0.227273\ttrain-error:0.008357\n",
      "[5]\teval-error:0.227273\ttrain-error:0.006964\n",
      "[6]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[7]\teval-error:0.227273\ttrain-error:0.005571\n",
      "[8]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[9]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[10]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[11]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[12]\teval-error:0.227273\ttrain-error:0.005571\n",
      "[13]\teval-error:0.227273\ttrain-error:0.005571\n",
      "[14]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[15]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[16]\teval-error:0.227273\ttrain-error:0.005571\n",
      "[17]\teval-error:0.227273\ttrain-error:0.005571\n",
      "[18]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[19]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[20]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[21]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[22]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[23]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[24]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[25]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[26]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[27]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[28]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[29]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[30]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[31]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[32]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[33]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[34]\teval-error:0.227273\ttrain-error:0.005571\n",
      "[35]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[36]\teval-error:0.227273\ttrain-error:0.005571\n",
      "[37]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[38]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[39]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[40]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[41]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[42]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[43]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[44]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[45]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[46]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[47]\teval-error:0.227273\ttrain-error:0.005571\n",
      "[48]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[49]\teval-error:0.227273\ttrain-error:0.005571\n",
      "[50]\teval-error:0.227273\ttrain-error:0.005571\n",
      "[51]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[52]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[53]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[54]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[55]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[56]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[57]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[58]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[59]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[60]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[61]\teval-error:0.227273\ttrain-error:0.005571\n",
      "[62]\teval-error:0.227273\ttrain-error:0.005571\n",
      "[63]\teval-error:0.227273\ttrain-error:0.005571\n",
      "[64]\teval-error:0.227273\ttrain-error:0.005571\n",
      "[65]\teval-error:0.227273\ttrain-error:0.005571\n",
      "[66]\teval-error:0.227273\ttrain-error:0.005571\n",
      "[67]\teval-error:0.215909\ttrain-error:0.005571\n",
      "[68]\teval-error:0.215909\ttrain-error:0.005571\n",
      "[69]\teval-error:0.215909\ttrain-error:0.005571\n",
      "[0]\teval-error:0.272727\ttrain-error:0.097493\n",
      "[1]\teval-error:0.261364\ttrain-error:0.059889\n",
      "[2]\teval-error:0.25\ttrain-error:0.033426\n",
      "[3]\teval-error:0.261364\ttrain-error:0.016713\n",
      "[4]\teval-error:0.25\ttrain-error:0.011142\n",
      "[5]\teval-error:0.272727\ttrain-error:0.006964\n",
      "[6]\teval-error:0.284091\ttrain-error:0.006964\n",
      "[7]\teval-error:0.261364\ttrain-error:0.006964\n",
      "[8]\teval-error:0.261364\ttrain-error:0.006964\n",
      "[9]\teval-error:0.25\ttrain-error:0.006964\n",
      "[10]\teval-error:0.272727\ttrain-error:0.006964\n",
      "[11]\teval-error:0.272727\ttrain-error:0.006964\n",
      "[12]\teval-error:0.261364\ttrain-error:0.006964\n",
      "[13]\teval-error:0.261364\ttrain-error:0.006964\n",
      "[14]\teval-error:0.261364\ttrain-error:0.006964\n",
      "[15]\teval-error:0.272727\ttrain-error:0.006964\n",
      "[16]\teval-error:0.272727\ttrain-error:0.006964\n",
      "[17]\teval-error:0.261364\ttrain-error:0.006964\n",
      "[18]\teval-error:0.272727\ttrain-error:0.006964\n",
      "[19]\teval-error:0.272727\ttrain-error:0.006964\n",
      "[20]\teval-error:0.261364\ttrain-error:0.006964\n",
      "[21]\teval-error:0.261364\ttrain-error:0.006964\n",
      "[22]\teval-error:0.261364\ttrain-error:0.006964\n",
      "[23]\teval-error:0.261364\ttrain-error:0.006964\n",
      "[24]\teval-error:0.272727\ttrain-error:0.006964\n",
      "[25]\teval-error:0.272727\ttrain-error:0.006964\n",
      "[26]\teval-error:0.272727\ttrain-error:0.006964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27]\teval-error:0.272727\ttrain-error:0.006964\n",
      "[28]\teval-error:0.272727\ttrain-error:0.006964\n",
      "[29]\teval-error:0.261364\ttrain-error:0.006964\n",
      "[30]\teval-error:0.261364\ttrain-error:0.006964\n",
      "[31]\teval-error:0.261364\ttrain-error:0.006964\n",
      "[32]\teval-error:0.25\ttrain-error:0.006964\n",
      "[33]\teval-error:0.261364\ttrain-error:0.006964\n",
      "[34]\teval-error:0.261364\ttrain-error:0.006964\n",
      "[35]\teval-error:0.261364\ttrain-error:0.006964\n",
      "[36]\teval-error:0.261364\ttrain-error:0.006964\n",
      "[37]\teval-error:0.261364\ttrain-error:0.006964\n",
      "[38]\teval-error:0.261364\ttrain-error:0.006964\n",
      "[39]\teval-error:0.261364\ttrain-error:0.006964\n",
      "[40]\teval-error:0.25\ttrain-error:0.006964\n",
      "[41]\teval-error:0.25\ttrain-error:0.006964\n",
      "[42]\teval-error:0.25\ttrain-error:0.006964\n",
      "[43]\teval-error:0.25\ttrain-error:0.006964\n",
      "[44]\teval-error:0.25\ttrain-error:0.006964\n",
      "[45]\teval-error:0.25\ttrain-error:0.006964\n",
      "[46]\teval-error:0.25\ttrain-error:0.006964\n",
      "[47]\teval-error:0.25\ttrain-error:0.006964\n",
      "[48]\teval-error:0.238636\ttrain-error:0.006964\n",
      "[49]\teval-error:0.25\ttrain-error:0.006964\n",
      "[50]\teval-error:0.25\ttrain-error:0.006964\n",
      "[51]\teval-error:0.25\ttrain-error:0.006964\n",
      "[52]\teval-error:0.25\ttrain-error:0.006964\n",
      "[53]\teval-error:0.25\ttrain-error:0.006964\n",
      "[54]\teval-error:0.238636\ttrain-error:0.006964\n",
      "[55]\teval-error:0.238636\ttrain-error:0.006964\n",
      "[56]\teval-error:0.25\ttrain-error:0.006964\n",
      "[57]\teval-error:0.25\ttrain-error:0.006964\n",
      "[58]\teval-error:0.261364\ttrain-error:0.006964\n",
      "[59]\teval-error:0.261364\ttrain-error:0.006964\n",
      "[60]\teval-error:0.25\ttrain-error:0.006964\n",
      "[61]\teval-error:0.261364\ttrain-error:0.006964\n",
      "[62]\teval-error:0.25\ttrain-error:0.006964\n",
      "[63]\teval-error:0.25\ttrain-error:0.006964\n",
      "[64]\teval-error:0.25\ttrain-error:0.006964\n",
      "[65]\teval-error:0.25\ttrain-error:0.006964\n",
      "[66]\teval-error:0.25\ttrain-error:0.006964\n",
      "[67]\teval-error:0.25\ttrain-error:0.006964\n",
      "[68]\teval-error:0.25\ttrain-error:0.006964\n",
      "[69]\teval-error:0.261364\ttrain-error:0.006964\n",
      "[0]\teval-error:0.238636\ttrain-error:0.114206\n",
      "[1]\teval-error:0.215909\ttrain-error:0.082173\n",
      "[2]\teval-error:0.25\ttrain-error:0.058496\n",
      "[3]\teval-error:0.238636\ttrain-error:0.030641\n",
      "[4]\teval-error:0.227273\ttrain-error:0.016713\n",
      "[5]\teval-error:0.238636\ttrain-error:0.01532\n",
      "[6]\teval-error:0.227273\ttrain-error:0.011142\n",
      "[7]\teval-error:0.204545\ttrain-error:0.009749\n",
      "[8]\teval-error:0.227273\ttrain-error:0.008357\n",
      "[9]\teval-error:0.215909\ttrain-error:0.008357\n",
      "[10]\teval-error:0.204545\ttrain-error:0.008357\n",
      "[11]\teval-error:0.238636\ttrain-error:0.008357\n",
      "[12]\teval-error:0.204545\ttrain-error:0.008357\n",
      "[13]\teval-error:0.227273\ttrain-error:0.008357\n",
      "[14]\teval-error:0.215909\ttrain-error:0.008357\n",
      "[15]\teval-error:0.227273\ttrain-error:0.008357\n",
      "[16]\teval-error:0.238636\ttrain-error:0.008357\n",
      "[17]\teval-error:0.227273\ttrain-error:0.008357\n",
      "[18]\teval-error:0.238636\ttrain-error:0.008357\n",
      "[19]\teval-error:0.227273\ttrain-error:0.008357\n",
      "[20]\teval-error:0.227273\ttrain-error:0.008357\n",
      "[21]\teval-error:0.227273\ttrain-error:0.008357\n",
      "[22]\teval-error:0.227273\ttrain-error:0.008357\n",
      "[23]\teval-error:0.215909\ttrain-error:0.008357\n",
      "[24]\teval-error:0.204545\ttrain-error:0.008357\n",
      "[25]\teval-error:0.215909\ttrain-error:0.008357\n",
      "[26]\teval-error:0.227273\ttrain-error:0.008357\n",
      "[27]\teval-error:0.238636\ttrain-error:0.008357\n",
      "[28]\teval-error:0.238636\ttrain-error:0.008357\n",
      "[29]\teval-error:0.238636\ttrain-error:0.008357\n",
      "[30]\teval-error:0.227273\ttrain-error:0.008357\n",
      "[31]\teval-error:0.238636\ttrain-error:0.008357\n",
      "[32]\teval-error:0.238636\ttrain-error:0.008357\n",
      "[33]\teval-error:0.227273\ttrain-error:0.008357\n",
      "[34]\teval-error:0.227273\ttrain-error:0.008357\n",
      "[35]\teval-error:0.238636\ttrain-error:0.008357\n",
      "[36]\teval-error:0.238636\ttrain-error:0.008357\n",
      "[37]\teval-error:0.238636\ttrain-error:0.008357\n",
      "[38]\teval-error:0.238636\ttrain-error:0.008357\n",
      "[39]\teval-error:0.238636\ttrain-error:0.008357\n",
      "[40]\teval-error:0.238636\ttrain-error:0.008357\n",
      "[41]\teval-error:0.238636\ttrain-error:0.008357\n",
      "[42]\teval-error:0.227273\ttrain-error:0.008357\n",
      "[43]\teval-error:0.238636\ttrain-error:0.008357\n",
      "[44]\teval-error:0.238636\ttrain-error:0.008357\n",
      "[45]\teval-error:0.227273\ttrain-error:0.008357\n",
      "[46]\teval-error:0.227273\ttrain-error:0.008357\n",
      "[47]\teval-error:0.227273\ttrain-error:0.008357\n",
      "[48]\teval-error:0.238636\ttrain-error:0.008357\n",
      "[49]\teval-error:0.227273\ttrain-error:0.008357\n",
      "[50]\teval-error:0.238636\ttrain-error:0.008357\n",
      "[51]\teval-error:0.238636\ttrain-error:0.008357\n",
      "[52]\teval-error:0.238636\ttrain-error:0.008357\n",
      "[53]\teval-error:0.238636\ttrain-error:0.008357\n",
      "[54]\teval-error:0.238636\ttrain-error:0.008357\n",
      "[55]\teval-error:0.238636\ttrain-error:0.008357\n",
      "[56]\teval-error:0.238636\ttrain-error:0.008357\n",
      "[57]\teval-error:0.238636\ttrain-error:0.008357\n",
      "[58]\teval-error:0.25\ttrain-error:0.008357\n",
      "[59]\teval-error:0.25\ttrain-error:0.008357\n",
      "[60]\teval-error:0.25\ttrain-error:0.008357\n",
      "[61]\teval-error:0.25\ttrain-error:0.008357\n",
      "[62]\teval-error:0.25\ttrain-error:0.008357\n",
      "[63]\teval-error:0.25\ttrain-error:0.008357\n",
      "[64]\teval-error:0.25\ttrain-error:0.008357\n",
      "[65]\teval-error:0.25\ttrain-error:0.008357\n",
      "[66]\teval-error:0.25\ttrain-error:0.008357\n",
      "[67]\teval-error:0.25\ttrain-error:0.008357\n",
      "[68]\teval-error:0.25\ttrain-error:0.008357\n",
      "[69]\teval-error:0.25\ttrain-error:0.008357\n",
      "[0]\teval-error:0.329545\ttrain-error:0.110028\n",
      "[1]\teval-error:0.25\ttrain-error:0.064067\n",
      "[2]\teval-error:0.25\ttrain-error:0.029248\n",
      "[3]\teval-error:0.25\ttrain-error:0.01532\n",
      "[4]\teval-error:0.215909\ttrain-error:0.008357\n",
      "[5]\teval-error:0.227273\ttrain-error:0.005571\n",
      "[6]\teval-error:0.227273\ttrain-error:0.005571\n",
      "[7]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[8]\teval-error:0.25\ttrain-error:0.005571\n",
      "[9]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[10]\teval-error:0.25\ttrain-error:0.005571\n",
      "[11]\teval-error:0.261364\ttrain-error:0.005571\n",
      "[12]\teval-error:0.227273\ttrain-error:0.005571\n",
      "[13]\teval-error:0.25\ttrain-error:0.005571\n",
      "[14]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[15]\teval-error:0.261364\ttrain-error:0.005571\n",
      "[16]\teval-error:0.25\ttrain-error:0.005571\n",
      "[17]\teval-error:0.25\ttrain-error:0.005571\n",
      "[18]\teval-error:0.25\ttrain-error:0.005571\n",
      "[19]\teval-error:0.261364\ttrain-error:0.005571\n",
      "[20]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[21]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[22]\teval-error:0.261364\ttrain-error:0.005571\n",
      "[23]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[24]\teval-error:0.25\ttrain-error:0.005571\n",
      "[25]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[26]\teval-error:0.25\ttrain-error:0.005571\n",
      "[27]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[28]\teval-error:0.25\ttrain-error:0.005571\n",
      "[29]\teval-error:0.261364\ttrain-error:0.005571\n",
      "[30]\teval-error:0.261364\ttrain-error:0.005571\n",
      "[31]\teval-error:0.261364\ttrain-error:0.005571\n",
      "[32]\teval-error:0.261364\ttrain-error:0.005571\n",
      "[33]\teval-error:0.261364\ttrain-error:0.005571\n",
      "[34]\teval-error:0.272727\ttrain-error:0.005571\n",
      "[35]\teval-error:0.272727\ttrain-error:0.005571\n",
      "[36]\teval-error:0.261364\ttrain-error:0.005571\n",
      "[37]\teval-error:0.261364\ttrain-error:0.005571\n",
      "[38]\teval-error:0.261364\ttrain-error:0.005571\n",
      "[39]\teval-error:0.261364\ttrain-error:0.005571\n",
      "[40]\teval-error:0.261364\ttrain-error:0.005571\n",
      "[41]\teval-error:0.272727\ttrain-error:0.005571\n",
      "[42]\teval-error:0.261364\ttrain-error:0.005571\n",
      "[43]\teval-error:0.25\ttrain-error:0.005571\n",
      "[44]\teval-error:0.25\ttrain-error:0.005571\n",
      "[45]\teval-error:0.261364\ttrain-error:0.005571\n",
      "[46]\teval-error:0.25\ttrain-error:0.005571\n",
      "[47]\teval-error:0.25\ttrain-error:0.005571\n",
      "[48]\teval-error:0.25\ttrain-error:0.005571\n",
      "[49]\teval-error:0.25\ttrain-error:0.005571\n",
      "[50]\teval-error:0.25\ttrain-error:0.005571\n",
      "[51]\teval-error:0.25\ttrain-error:0.005571\n",
      "[52]\teval-error:0.25\ttrain-error:0.005571\n",
      "[53]\teval-error:0.25\ttrain-error:0.005571\n",
      "[54]\teval-error:0.25\ttrain-error:0.005571\n",
      "[55]\teval-error:0.25\ttrain-error:0.005571\n",
      "[56]\teval-error:0.261364\ttrain-error:0.005571\n",
      "[57]\teval-error:0.261364\ttrain-error:0.005571\n",
      "[58]\teval-error:0.25\ttrain-error:0.005571\n",
      "[59]\teval-error:0.25\ttrain-error:0.005571\n",
      "[60]\teval-error:0.25\ttrain-error:0.005571\n",
      "[61]\teval-error:0.25\ttrain-error:0.005571\n",
      "[62]\teval-error:0.25\ttrain-error:0.005571\n",
      "[63]\teval-error:0.25\ttrain-error:0.005571\n",
      "[64]\teval-error:0.25\ttrain-error:0.005571\n",
      "[65]\teval-error:0.25\ttrain-error:0.005571\n",
      "[66]\teval-error:0.25\ttrain-error:0.005571\n",
      "[67]\teval-error:0.25\ttrain-error:0.005571\n",
      "[68]\teval-error:0.25\ttrain-error:0.005571\n",
      "[69]\teval-error:0.25\ttrain-error:0.005571\n",
      "[0]\teval-error:0.351064\ttrain-error:0.124095\n",
      "[1]\teval-error:0.308511\ttrain-error:0.079628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\teval-error:0.276596\ttrain-error:0.045502\n",
      "[3]\teval-error:0.244681\ttrain-error:0.02999\n",
      "[4]\teval-error:0.244681\ttrain-error:0.025853\n",
      "[5]\teval-error:0.244681\ttrain-error:0.021717\n",
      "[6]\teval-error:0.244681\ttrain-error:0.020683\n",
      "[7]\teval-error:0.255319\ttrain-error:0.020683\n",
      "[8]\teval-error:0.234043\ttrain-error:0.020683\n",
      "[9]\teval-error:0.244681\ttrain-error:0.018614\n",
      "[10]\teval-error:0.234043\ttrain-error:0.020683\n",
      "[11]\teval-error:0.244681\ttrain-error:0.018614\n",
      "[12]\teval-error:0.244681\ttrain-error:0.018614\n",
      "[13]\teval-error:0.244681\ttrain-error:0.018614\n",
      "[14]\teval-error:0.244681\ttrain-error:0.018614\n",
      "[15]\teval-error:0.234043\ttrain-error:0.018614\n",
      "[16]\teval-error:0.234043\ttrain-error:0.018614\n",
      "[17]\teval-error:0.234043\ttrain-error:0.018614\n",
      "[18]\teval-error:0.223404\ttrain-error:0.018614\n",
      "[19]\teval-error:0.223404\ttrain-error:0.018614\n",
      "[20]\teval-error:0.234043\ttrain-error:0.018614\n",
      "[21]\teval-error:0.234043\ttrain-error:0.018614\n",
      "[22]\teval-error:0.234043\ttrain-error:0.018614\n",
      "[23]\teval-error:0.244681\ttrain-error:0.019648\n",
      "[24]\teval-error:0.244681\ttrain-error:0.018614\n",
      "[25]\teval-error:0.234043\ttrain-error:0.018614\n",
      "[26]\teval-error:0.223404\ttrain-error:0.019648\n",
      "[27]\teval-error:0.234043\ttrain-error:0.018614\n",
      "[28]\teval-error:0.244681\ttrain-error:0.019648\n",
      "[29]\teval-error:0.234043\ttrain-error:0.018614\n",
      "[30]\teval-error:0.244681\ttrain-error:0.019648\n",
      "[31]\teval-error:0.244681\ttrain-error:0.018614\n",
      "[32]\teval-error:0.244681\ttrain-error:0.019648\n",
      "[33]\teval-error:0.244681\ttrain-error:0.019648\n",
      "[34]\teval-error:0.244681\ttrain-error:0.018614\n",
      "[35]\teval-error:0.244681\ttrain-error:0.019648\n",
      "[36]\teval-error:0.244681\ttrain-error:0.019648\n",
      "[37]\teval-error:0.244681\ttrain-error:0.019648\n",
      "[38]\teval-error:0.244681\ttrain-error:0.018614\n",
      "[39]\teval-error:0.244681\ttrain-error:0.019648\n",
      "[40]\teval-error:0.244681\ttrain-error:0.018614\n",
      "[41]\teval-error:0.244681\ttrain-error:0.018614\n",
      "[42]\teval-error:0.244681\ttrain-error:0.018614\n",
      "[43]\teval-error:0.244681\ttrain-error:0.018614\n",
      "[44]\teval-error:0.234043\ttrain-error:0.019648\n",
      "[45]\teval-error:0.234043\ttrain-error:0.019648\n",
      "[46]\teval-error:0.244681\ttrain-error:0.019648\n",
      "[47]\teval-error:0.255319\ttrain-error:0.018614\n",
      "[48]\teval-error:0.244681\ttrain-error:0.019648\n",
      "[49]\teval-error:0.255319\ttrain-error:0.019648\n",
      "[50]\teval-error:0.255319\ttrain-error:0.019648\n",
      "[51]\teval-error:0.255319\ttrain-error:0.019648\n",
      "[52]\teval-error:0.244681\ttrain-error:0.019648\n",
      "[53]\teval-error:0.255319\ttrain-error:0.019648\n",
      "[54]\teval-error:0.244681\ttrain-error:0.018614\n",
      "[55]\teval-error:0.244681\ttrain-error:0.019648\n",
      "[56]\teval-error:0.244681\ttrain-error:0.019648\n",
      "[57]\teval-error:0.244681\ttrain-error:0.019648\n",
      "[58]\teval-error:0.234043\ttrain-error:0.018614\n",
      "[59]\teval-error:0.234043\ttrain-error:0.019648\n",
      "[60]\teval-error:0.234043\ttrain-error:0.019648\n",
      "[61]\teval-error:0.223404\ttrain-error:0.019648\n",
      "[62]\teval-error:0.234043\ttrain-error:0.019648\n",
      "[63]\teval-error:0.255319\ttrain-error:0.019648\n",
      "[64]\teval-error:0.234043\ttrain-error:0.019648\n",
      "[65]\teval-error:0.244681\ttrain-error:0.019648\n",
      "[66]\teval-error:0.234043\ttrain-error:0.019648\n",
      "[67]\teval-error:0.234043\ttrain-error:0.019648\n",
      "[68]\teval-error:0.234043\ttrain-error:0.019648\n",
      "[69]\teval-error:0.234043\ttrain-error:0.019648\n",
      "[0]\teval-error:0.43617\ttrain-error:0.1303\n",
      "[1]\teval-error:0.340426\ttrain-error:0.081696\n",
      "[2]\teval-error:0.287234\ttrain-error:0.054809\n",
      "[3]\teval-error:0.287234\ttrain-error:0.033092\n",
      "[4]\teval-error:0.265957\ttrain-error:0.022751\n",
      "[5]\teval-error:0.265957\ttrain-error:0.021717\n",
      "[6]\teval-error:0.265957\ttrain-error:0.020683\n",
      "[7]\teval-error:0.297872\ttrain-error:0.020683\n",
      "[8]\teval-error:0.276596\ttrain-error:0.019648\n",
      "[9]\teval-error:0.297872\ttrain-error:0.019648\n",
      "[10]\teval-error:0.297872\ttrain-error:0.019648\n",
      "[11]\teval-error:0.276596\ttrain-error:0.019648\n",
      "[12]\teval-error:0.287234\ttrain-error:0.019648\n",
      "[13]\teval-error:0.276596\ttrain-error:0.019648\n",
      "[14]\teval-error:0.276596\ttrain-error:0.019648\n",
      "[15]\teval-error:0.276596\ttrain-error:0.019648\n",
      "[16]\teval-error:0.276596\ttrain-error:0.019648\n",
      "[17]\teval-error:0.297872\ttrain-error:0.019648\n",
      "[18]\teval-error:0.297872\ttrain-error:0.019648\n",
      "[19]\teval-error:0.276596\ttrain-error:0.019648\n",
      "[20]\teval-error:0.276596\ttrain-error:0.019648\n",
      "[21]\teval-error:0.297872\ttrain-error:0.019648\n",
      "[22]\teval-error:0.297872\ttrain-error:0.019648\n",
      "[23]\teval-error:0.297872\ttrain-error:0.019648\n",
      "[24]\teval-error:0.276596\ttrain-error:0.019648\n",
      "[25]\teval-error:0.276596\ttrain-error:0.019648\n",
      "[26]\teval-error:0.297872\ttrain-error:0.019648\n",
      "[27]\teval-error:0.287234\ttrain-error:0.019648\n",
      "[28]\teval-error:0.287234\ttrain-error:0.019648\n",
      "[29]\teval-error:0.308511\ttrain-error:0.019648\n",
      "[30]\teval-error:0.308511\ttrain-error:0.019648\n",
      "[31]\teval-error:0.308511\ttrain-error:0.019648\n",
      "[32]\teval-error:0.308511\ttrain-error:0.019648\n",
      "[33]\teval-error:0.308511\ttrain-error:0.019648\n",
      "[34]\teval-error:0.308511\ttrain-error:0.019648\n",
      "[35]\teval-error:0.308511\ttrain-error:0.019648\n",
      "[36]\teval-error:0.308511\ttrain-error:0.019648\n",
      "[37]\teval-error:0.308511\ttrain-error:0.019648\n",
      "[38]\teval-error:0.297872\ttrain-error:0.019648\n",
      "[39]\teval-error:0.297872\ttrain-error:0.019648\n",
      "[40]\teval-error:0.297872\ttrain-error:0.019648\n",
      "[41]\teval-error:0.297872\ttrain-error:0.019648\n",
      "[42]\teval-error:0.308511\ttrain-error:0.019648\n",
      "[43]\teval-error:0.308511\ttrain-error:0.019648\n",
      "[44]\teval-error:0.297872\ttrain-error:0.019648\n",
      "[45]\teval-error:0.308511\ttrain-error:0.019648\n",
      "[46]\teval-error:0.308511\ttrain-error:0.019648\n",
      "[47]\teval-error:0.308511\ttrain-error:0.019648\n",
      "[48]\teval-error:0.308511\ttrain-error:0.019648\n",
      "[49]\teval-error:0.308511\ttrain-error:0.019648\n",
      "[50]\teval-error:0.308511\ttrain-error:0.019648\n",
      "[51]\teval-error:0.308511\ttrain-error:0.019648\n",
      "[52]\teval-error:0.308511\ttrain-error:0.019648\n",
      "[53]\teval-error:0.308511\ttrain-error:0.019648\n",
      "[54]\teval-error:0.308511\ttrain-error:0.019648\n",
      "[55]\teval-error:0.308511\ttrain-error:0.019648\n",
      "[56]\teval-error:0.308511\ttrain-error:0.019648\n",
      "[57]\teval-error:0.308511\ttrain-error:0.019648\n",
      "[58]\teval-error:0.308511\ttrain-error:0.019648\n",
      "[59]\teval-error:0.308511\ttrain-error:0.019648\n",
      "[60]\teval-error:0.308511\ttrain-error:0.019648\n",
      "[61]\teval-error:0.308511\ttrain-error:0.019648\n",
      "[62]\teval-error:0.308511\ttrain-error:0.019648\n",
      "[63]\teval-error:0.297872\ttrain-error:0.019648\n",
      "[64]\teval-error:0.276596\ttrain-error:0.019648\n",
      "[65]\teval-error:0.265957\ttrain-error:0.019648\n",
      "[66]\teval-error:0.265957\ttrain-error:0.019648\n",
      "[67]\teval-error:0.265957\ttrain-error:0.019648\n",
      "[68]\teval-error:0.265957\ttrain-error:0.019648\n",
      "[69]\teval-error:0.265957\ttrain-error:0.019648\n",
      "[0]\teval-error:0.361702\ttrain-error:0.136505\n",
      "[1]\teval-error:0.37234\ttrain-error:0.086867\n",
      "[2]\teval-error:0.361702\ttrain-error:0.063082\n",
      "[3]\teval-error:0.361702\ttrain-error:0.046536\n",
      "[4]\teval-error:0.361702\ttrain-error:0.038263\n",
      "[5]\teval-error:0.37234\ttrain-error:0.036194\n",
      "[6]\teval-error:0.37234\ttrain-error:0.037229\n",
      "[7]\teval-error:0.361702\ttrain-error:0.037229\n",
      "[8]\teval-error:0.351064\ttrain-error:0.036194\n",
      "[9]\teval-error:0.351064\ttrain-error:0.036194\n",
      "[10]\teval-error:0.351064\ttrain-error:0.036194\n",
      "[11]\teval-error:0.351064\ttrain-error:0.03516\n",
      "[12]\teval-error:0.351064\ttrain-error:0.03516\n",
      "[13]\teval-error:0.361702\ttrain-error:0.03516\n",
      "[14]\teval-error:0.361702\ttrain-error:0.03516\n",
      "[15]\teval-error:0.37234\ttrain-error:0.03516\n",
      "[16]\teval-error:0.37234\ttrain-error:0.03516\n",
      "[17]\teval-error:0.361702\ttrain-error:0.03516\n",
      "[18]\teval-error:0.351064\ttrain-error:0.03516\n",
      "[19]\teval-error:0.361702\ttrain-error:0.03516\n",
      "[20]\teval-error:0.340426\ttrain-error:0.03516\n",
      "[21]\teval-error:0.361702\ttrain-error:0.03516\n",
      "[22]\teval-error:0.382979\ttrain-error:0.03516\n",
      "[23]\teval-error:0.340426\ttrain-error:0.03516\n",
      "[24]\teval-error:0.340426\ttrain-error:0.03516\n",
      "[25]\teval-error:0.340426\ttrain-error:0.03516\n",
      "[26]\teval-error:0.340426\ttrain-error:0.03516\n",
      "[27]\teval-error:0.361702\ttrain-error:0.03516\n",
      "[28]\teval-error:0.351064\ttrain-error:0.03516\n",
      "[29]\teval-error:0.351064\ttrain-error:0.03516\n",
      "[30]\teval-error:0.329787\ttrain-error:0.03516\n",
      "[31]\teval-error:0.329787\ttrain-error:0.03516\n",
      "[32]\teval-error:0.329787\ttrain-error:0.03516\n",
      "[33]\teval-error:0.329787\ttrain-error:0.03516\n",
      "[34]\teval-error:0.329787\ttrain-error:0.03516\n",
      "[35]\teval-error:0.329787\ttrain-error:0.03516\n",
      "[36]\teval-error:0.329787\ttrain-error:0.03516\n",
      "[37]\teval-error:0.329787\ttrain-error:0.03516\n",
      "[38]\teval-error:0.329787\ttrain-error:0.03516\n",
      "[39]\teval-error:0.329787\ttrain-error:0.03516\n",
      "[40]\teval-error:0.329787\ttrain-error:0.03516\n",
      "[41]\teval-error:0.340426\ttrain-error:0.03516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42]\teval-error:0.329787\ttrain-error:0.03516\n",
      "[43]\teval-error:0.340426\ttrain-error:0.03516\n",
      "[44]\teval-error:0.340426\ttrain-error:0.03516\n",
      "[45]\teval-error:0.340426\ttrain-error:0.03516\n",
      "[46]\teval-error:0.340426\ttrain-error:0.03516\n",
      "[47]\teval-error:0.340426\ttrain-error:0.03516\n",
      "[48]\teval-error:0.340426\ttrain-error:0.03516\n",
      "[49]\teval-error:0.340426\ttrain-error:0.03516\n",
      "[50]\teval-error:0.351064\ttrain-error:0.03516\n",
      "[51]\teval-error:0.340426\ttrain-error:0.03516\n",
      "[52]\teval-error:0.340426\ttrain-error:0.03516\n",
      "[53]\teval-error:0.340426\ttrain-error:0.03516\n",
      "[54]\teval-error:0.340426\ttrain-error:0.03516\n",
      "[55]\teval-error:0.340426\ttrain-error:0.03516\n",
      "[56]\teval-error:0.340426\ttrain-error:0.03516\n",
      "[57]\teval-error:0.340426\ttrain-error:0.03516\n",
      "[58]\teval-error:0.329787\ttrain-error:0.03516\n",
      "[59]\teval-error:0.329787\ttrain-error:0.03516\n",
      "[60]\teval-error:0.329787\ttrain-error:0.03516\n",
      "[61]\teval-error:0.329787\ttrain-error:0.03516\n",
      "[62]\teval-error:0.329787\ttrain-error:0.03516\n",
      "[63]\teval-error:0.340426\ttrain-error:0.03516\n",
      "[64]\teval-error:0.340426\ttrain-error:0.03516\n",
      "[65]\teval-error:0.351064\ttrain-error:0.03516\n",
      "[66]\teval-error:0.351064\ttrain-error:0.03516\n",
      "[67]\teval-error:0.340426\ttrain-error:0.03516\n",
      "[68]\teval-error:0.340426\ttrain-error:0.03516\n",
      "[69]\teval-error:0.340426\ttrain-error:0.03516\n",
      "[0]\teval-error:0.329787\ttrain-error:0.120993\n",
      "[1]\teval-error:0.287234\ttrain-error:0.066184\n",
      "[2]\teval-error:0.287234\ttrain-error:0.033092\n",
      "[3]\teval-error:0.255319\ttrain-error:0.026887\n",
      "[4]\teval-error:0.255319\ttrain-error:0.020683\n",
      "[5]\teval-error:0.234043\ttrain-error:0.019648\n",
      "[6]\teval-error:0.255319\ttrain-error:0.019648\n",
      "[7]\teval-error:0.255319\ttrain-error:0.019648\n",
      "[8]\teval-error:0.244681\ttrain-error:0.019648\n",
      "[9]\teval-error:0.244681\ttrain-error:0.019648\n",
      "[10]\teval-error:0.244681\ttrain-error:0.019648\n",
      "[11]\teval-error:0.234043\ttrain-error:0.019648\n",
      "[12]\teval-error:0.244681\ttrain-error:0.019648\n",
      "[13]\teval-error:0.234043\ttrain-error:0.019648\n",
      "[14]\teval-error:0.234043\ttrain-error:0.019648\n",
      "[15]\teval-error:0.244681\ttrain-error:0.019648\n",
      "[16]\teval-error:0.244681\ttrain-error:0.019648\n",
      "[17]\teval-error:0.234043\ttrain-error:0.019648\n",
      "[18]\teval-error:0.234043\ttrain-error:0.019648\n",
      "[19]\teval-error:0.234043\ttrain-error:0.019648\n",
      "[20]\teval-error:0.234043\ttrain-error:0.019648\n",
      "[21]\teval-error:0.244681\ttrain-error:0.019648\n",
      "[22]\teval-error:0.234043\ttrain-error:0.019648\n",
      "[23]\teval-error:0.244681\ttrain-error:0.019648\n",
      "[24]\teval-error:0.244681\ttrain-error:0.019648\n",
      "[25]\teval-error:0.223404\ttrain-error:0.019648\n",
      "[26]\teval-error:0.223404\ttrain-error:0.019648\n",
      "[27]\teval-error:0.223404\ttrain-error:0.019648\n",
      "[28]\teval-error:0.223404\ttrain-error:0.019648\n",
      "[29]\teval-error:0.234043\ttrain-error:0.019648\n",
      "[30]\teval-error:0.234043\ttrain-error:0.019648\n",
      "[31]\teval-error:0.223404\ttrain-error:0.019648\n",
      "[32]\teval-error:0.223404\ttrain-error:0.019648\n",
      "[33]\teval-error:0.223404\ttrain-error:0.019648\n",
      "[34]\teval-error:0.234043\ttrain-error:0.019648\n",
      "[35]\teval-error:0.223404\ttrain-error:0.019648\n",
      "[36]\teval-error:0.234043\ttrain-error:0.019648\n",
      "[37]\teval-error:0.234043\ttrain-error:0.019648\n",
      "[38]\teval-error:0.234043\ttrain-error:0.019648\n",
      "[39]\teval-error:0.234043\ttrain-error:0.019648\n",
      "[40]\teval-error:0.234043\ttrain-error:0.019648\n",
      "[41]\teval-error:0.234043\ttrain-error:0.019648\n",
      "[42]\teval-error:0.234043\ttrain-error:0.019648\n",
      "[43]\teval-error:0.234043\ttrain-error:0.019648\n",
      "[44]\teval-error:0.223404\ttrain-error:0.019648\n",
      "[45]\teval-error:0.223404\ttrain-error:0.019648\n",
      "[46]\teval-error:0.234043\ttrain-error:0.019648\n",
      "[47]\teval-error:0.223404\ttrain-error:0.019648\n",
      "[48]\teval-error:0.234043\ttrain-error:0.019648\n",
      "[49]\teval-error:0.234043\ttrain-error:0.019648\n",
      "[50]\teval-error:0.212766\ttrain-error:0.019648\n",
      "[51]\teval-error:0.223404\ttrain-error:0.019648\n",
      "[52]\teval-error:0.223404\ttrain-error:0.019648\n",
      "[53]\teval-error:0.212766\ttrain-error:0.019648\n",
      "[54]\teval-error:0.212766\ttrain-error:0.019648\n",
      "[55]\teval-error:0.212766\ttrain-error:0.019648\n",
      "[56]\teval-error:0.212766\ttrain-error:0.019648\n",
      "[57]\teval-error:0.212766\ttrain-error:0.019648\n",
      "[58]\teval-error:0.212766\ttrain-error:0.019648\n",
      "[59]\teval-error:0.212766\ttrain-error:0.019648\n",
      "[60]\teval-error:0.212766\ttrain-error:0.019648\n",
      "[61]\teval-error:0.212766\ttrain-error:0.019648\n",
      "[62]\teval-error:0.212766\ttrain-error:0.019648\n",
      "[63]\teval-error:0.212766\ttrain-error:0.019648\n",
      "[64]\teval-error:0.212766\ttrain-error:0.019648\n",
      "[65]\teval-error:0.212766\ttrain-error:0.019648\n",
      "[66]\teval-error:0.212766\ttrain-error:0.019648\n",
      "[67]\teval-error:0.212766\ttrain-error:0.019648\n",
      "[68]\teval-error:0.212766\ttrain-error:0.019648\n",
      "[69]\teval-error:0.212766\ttrain-error:0.019648\n",
      "[0]\teval-error:0.251908\ttrain-error:0.103984\n",
      "[1]\teval-error:0.236641\ttrain-error:0.073372\n",
      "[2]\teval-error:0.229008\ttrain-error:0.046161\n",
      "[3]\teval-error:0.21374\ttrain-error:0.032556\n",
      "[4]\teval-error:0.21374\ttrain-error:0.028669\n",
      "[5]\teval-error:0.221374\ttrain-error:0.027697\n",
      "[6]\teval-error:0.225191\ttrain-error:0.027697\n",
      "[7]\teval-error:0.225191\ttrain-error:0.027697\n",
      "[8]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[9]\teval-error:0.225191\ttrain-error:0.027697\n",
      "[10]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[11]\teval-error:0.221374\ttrain-error:0.027697\n",
      "[12]\teval-error:0.221374\ttrain-error:0.027697\n",
      "[13]\teval-error:0.225191\ttrain-error:0.027697\n",
      "[14]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[15]\teval-error:0.221374\ttrain-error:0.027697\n",
      "[16]\teval-error:0.221374\ttrain-error:0.027697\n",
      "[17]\teval-error:0.232824\ttrain-error:0.027697\n",
      "[18]\teval-error:0.236641\ttrain-error:0.027697\n",
      "[19]\teval-error:0.232824\ttrain-error:0.027697\n",
      "[20]\teval-error:0.236641\ttrain-error:0.027697\n",
      "[21]\teval-error:0.221374\ttrain-error:0.027697\n",
      "[22]\teval-error:0.225191\ttrain-error:0.027697\n",
      "[23]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[24]\teval-error:0.221374\ttrain-error:0.027697\n",
      "[25]\teval-error:0.221374\ttrain-error:0.027697\n",
      "[26]\teval-error:0.229008\ttrain-error:0.027697\n",
      "[27]\teval-error:0.221374\ttrain-error:0.027697\n",
      "[28]\teval-error:0.221374\ttrain-error:0.027697\n",
      "[29]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[30]\teval-error:0.221374\ttrain-error:0.027697\n",
      "[31]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[32]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[33]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[34]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[35]\teval-error:0.209924\ttrain-error:0.027697\n",
      "[36]\teval-error:0.209924\ttrain-error:0.027697\n",
      "[37]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[38]\teval-error:0.209924\ttrain-error:0.027697\n",
      "[39]\teval-error:0.221374\ttrain-error:0.027697\n",
      "[40]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[41]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[42]\teval-error:0.221374\ttrain-error:0.027697\n",
      "[43]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[44]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[45]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[46]\teval-error:0.221374\ttrain-error:0.027697\n",
      "[47]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[48]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[49]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[50]\teval-error:0.221374\ttrain-error:0.027697\n",
      "[51]\teval-error:0.221374\ttrain-error:0.027697\n",
      "[52]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[53]\teval-error:0.209924\ttrain-error:0.027697\n",
      "[54]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[55]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[56]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[57]\teval-error:0.209924\ttrain-error:0.027697\n",
      "[58]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[59]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[60]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[61]\teval-error:0.206107\ttrain-error:0.027697\n",
      "[62]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[63]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[64]\teval-error:0.209924\ttrain-error:0.027697\n",
      "[65]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[66]\teval-error:0.209924\ttrain-error:0.027697\n",
      "[67]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[68]\teval-error:0.221374\ttrain-error:0.027697\n",
      "[69]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[0]\teval-error:0.251908\ttrain-error:0.111759\n",
      "[1]\teval-error:0.232824\ttrain-error:0.080175\n",
      "[2]\teval-error:0.225191\ttrain-error:0.054908\n",
      "[3]\teval-error:0.240458\ttrain-error:0.038873\n",
      "[4]\teval-error:0.244275\ttrain-error:0.034985\n",
      "[5]\teval-error:0.225191\ttrain-error:0.030126\n",
      "[6]\teval-error:0.225191\ttrain-error:0.02964\n",
      "[7]\teval-error:0.225191\ttrain-error:0.029155\n",
      "[8]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[9]\teval-error:0.225191\ttrain-error:0.028183\n",
      "[10]\teval-error:0.225191\ttrain-error:0.028183\n",
      "[11]\teval-error:0.232824\ttrain-error:0.028183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[13]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[14]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[15]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[16]\teval-error:0.225191\ttrain-error:0.028183\n",
      "[17]\teval-error:0.225191\ttrain-error:0.028183\n",
      "[18]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[19]\teval-error:0.225191\ttrain-error:0.028183\n",
      "[20]\teval-error:0.232824\ttrain-error:0.028183\n",
      "[21]\teval-error:0.225191\ttrain-error:0.028183\n",
      "[22]\teval-error:0.236641\ttrain-error:0.028183\n",
      "[23]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[24]\teval-error:0.232824\ttrain-error:0.028183\n",
      "[25]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[26]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[27]\teval-error:0.236641\ttrain-error:0.028183\n",
      "[28]\teval-error:0.232824\ttrain-error:0.028183\n",
      "[29]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[30]\teval-error:0.225191\ttrain-error:0.028183\n",
      "[31]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[32]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[33]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[34]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[35]\teval-error:0.225191\ttrain-error:0.028183\n",
      "[36]\teval-error:0.232824\ttrain-error:0.028183\n",
      "[37]\teval-error:0.232824\ttrain-error:0.028183\n",
      "[38]\teval-error:0.225191\ttrain-error:0.028183\n",
      "[39]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[40]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[41]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[42]\teval-error:0.225191\ttrain-error:0.028183\n",
      "[43]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[44]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[45]\teval-error:0.225191\ttrain-error:0.028183\n",
      "[46]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[47]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[48]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[49]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[50]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[51]\teval-error:0.225191\ttrain-error:0.028183\n",
      "[52]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[53]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[54]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[55]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[56]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[57]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[58]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[59]\teval-error:0.225191\ttrain-error:0.028183\n",
      "[60]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[61]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[62]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[63]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[64]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[65]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[66]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[67]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[68]\teval-error:0.225191\ttrain-error:0.028183\n",
      "[69]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[0]\teval-error:0.278626\ttrain-error:0.108844\n",
      "[1]\teval-error:0.259542\ttrain-error:0.07483\n",
      "[2]\teval-error:0.251908\ttrain-error:0.055879\n",
      "[3]\teval-error:0.232824\ttrain-error:0.04276\n",
      "[4]\teval-error:0.240458\ttrain-error:0.04033\n",
      "[5]\teval-error:0.244275\ttrain-error:0.035957\n",
      "[6]\teval-error:0.240458\ttrain-error:0.034985\n",
      "[7]\teval-error:0.225191\ttrain-error:0.0345\n",
      "[8]\teval-error:0.244275\ttrain-error:0.034014\n",
      "[9]\teval-error:0.240458\ttrain-error:0.033528\n",
      "[10]\teval-error:0.232824\ttrain-error:0.033528\n",
      "[11]\teval-error:0.244275\ttrain-error:0.033528\n",
      "[12]\teval-error:0.240458\ttrain-error:0.033528\n",
      "[13]\teval-error:0.244275\ttrain-error:0.033528\n",
      "[14]\teval-error:0.248092\ttrain-error:0.033528\n",
      "[15]\teval-error:0.248092\ttrain-error:0.033042\n",
      "[16]\teval-error:0.251908\ttrain-error:0.033042\n",
      "[17]\teval-error:0.259542\ttrain-error:0.033042\n",
      "[18]\teval-error:0.244275\ttrain-error:0.033042\n",
      "[19]\teval-error:0.240458\ttrain-error:0.033042\n",
      "[20]\teval-error:0.251908\ttrain-error:0.033042\n",
      "[21]\teval-error:0.244275\ttrain-error:0.033042\n",
      "[22]\teval-error:0.244275\ttrain-error:0.033042\n",
      "[23]\teval-error:0.248092\ttrain-error:0.033042\n",
      "[24]\teval-error:0.240458\ttrain-error:0.033042\n",
      "[25]\teval-error:0.244275\ttrain-error:0.033042\n",
      "[26]\teval-error:0.248092\ttrain-error:0.033042\n",
      "[27]\teval-error:0.236641\ttrain-error:0.033042\n",
      "[28]\teval-error:0.244275\ttrain-error:0.033042\n",
      "[29]\teval-error:0.248092\ttrain-error:0.033042\n",
      "[30]\teval-error:0.244275\ttrain-error:0.033042\n",
      "[31]\teval-error:0.248092\ttrain-error:0.033042\n",
      "[32]\teval-error:0.244275\ttrain-error:0.033042\n",
      "[33]\teval-error:0.240458\ttrain-error:0.033042\n",
      "[34]\teval-error:0.244275\ttrain-error:0.033042\n",
      "[35]\teval-error:0.244275\ttrain-error:0.033042\n",
      "[36]\teval-error:0.244275\ttrain-error:0.033042\n",
      "[37]\teval-error:0.248092\ttrain-error:0.033042\n",
      "[38]\teval-error:0.248092\ttrain-error:0.033042\n",
      "[39]\teval-error:0.240458\ttrain-error:0.033042\n",
      "[40]\teval-error:0.236641\ttrain-error:0.033042\n",
      "[41]\teval-error:0.240458\ttrain-error:0.033042\n",
      "[42]\teval-error:0.240458\ttrain-error:0.033042\n",
      "[43]\teval-error:0.240458\ttrain-error:0.033042\n",
      "[44]\teval-error:0.244275\ttrain-error:0.033042\n",
      "[45]\teval-error:0.244275\ttrain-error:0.033042\n",
      "[46]\teval-error:0.248092\ttrain-error:0.033042\n",
      "[47]\teval-error:0.248092\ttrain-error:0.033042\n",
      "[48]\teval-error:0.240458\ttrain-error:0.033042\n",
      "[49]\teval-error:0.251908\ttrain-error:0.033042\n",
      "[50]\teval-error:0.248092\ttrain-error:0.033042\n",
      "[51]\teval-error:0.248092\ttrain-error:0.033042\n",
      "[52]\teval-error:0.251908\ttrain-error:0.033042\n",
      "[53]\teval-error:0.251908\ttrain-error:0.033042\n",
      "[54]\teval-error:0.240458\ttrain-error:0.033042\n",
      "[55]\teval-error:0.248092\ttrain-error:0.033042\n",
      "[56]\teval-error:0.248092\ttrain-error:0.033042\n",
      "[57]\teval-error:0.236641\ttrain-error:0.033042\n",
      "[58]\teval-error:0.251908\ttrain-error:0.033042\n",
      "[59]\teval-error:0.248092\ttrain-error:0.033042\n",
      "[60]\teval-error:0.240458\ttrain-error:0.033042\n",
      "[61]\teval-error:0.232824\ttrain-error:0.033042\n",
      "[62]\teval-error:0.240458\ttrain-error:0.033042\n",
      "[63]\teval-error:0.240458\ttrain-error:0.033042\n",
      "[64]\teval-error:0.240458\ttrain-error:0.033042\n",
      "[65]\teval-error:0.236641\ttrain-error:0.033042\n",
      "[66]\teval-error:0.236641\ttrain-error:0.033042\n",
      "[67]\teval-error:0.244275\ttrain-error:0.033042\n",
      "[68]\teval-error:0.244275\ttrain-error:0.033042\n",
      "[69]\teval-error:0.236641\ttrain-error:0.033042\n",
      "[0]\teval-error:0.263359\ttrain-error:0.103499\n",
      "[1]\teval-error:0.217557\ttrain-error:0.062196\n",
      "[2]\teval-error:0.217557\ttrain-error:0.040816\n",
      "[3]\teval-error:0.20229\ttrain-error:0.031584\n",
      "[4]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[5]\teval-error:0.21374\ttrain-error:0.027211\n",
      "[6]\teval-error:0.209924\ttrain-error:0.027211\n",
      "[7]\teval-error:0.206107\ttrain-error:0.027211\n",
      "[8]\teval-error:0.209924\ttrain-error:0.027211\n",
      "[9]\teval-error:0.21374\ttrain-error:0.027211\n",
      "[10]\teval-error:0.206107\ttrain-error:0.027211\n",
      "[11]\teval-error:0.194656\ttrain-error:0.027211\n",
      "[12]\teval-error:0.198473\ttrain-error:0.027211\n",
      "[13]\teval-error:0.194656\ttrain-error:0.027211\n",
      "[14]\teval-error:0.198473\ttrain-error:0.027211\n",
      "[15]\teval-error:0.20229\ttrain-error:0.027211\n",
      "[16]\teval-error:0.198473\ttrain-error:0.027211\n",
      "[17]\teval-error:0.209924\ttrain-error:0.027211\n",
      "[18]\teval-error:0.20229\ttrain-error:0.027211\n",
      "[19]\teval-error:0.206107\ttrain-error:0.027211\n",
      "[20]\teval-error:0.194656\ttrain-error:0.027211\n",
      "[21]\teval-error:0.194656\ttrain-error:0.027211\n",
      "[22]\teval-error:0.20229\ttrain-error:0.027211\n",
      "[23]\teval-error:0.194656\ttrain-error:0.027211\n",
      "[24]\teval-error:0.194656\ttrain-error:0.027211\n",
      "[25]\teval-error:0.194656\ttrain-error:0.027211\n",
      "[26]\teval-error:0.198473\ttrain-error:0.027211\n",
      "[27]\teval-error:0.194656\ttrain-error:0.027211\n",
      "[28]\teval-error:0.198473\ttrain-error:0.027211\n",
      "[29]\teval-error:0.194656\ttrain-error:0.027211\n",
      "[30]\teval-error:0.194656\ttrain-error:0.027211\n",
      "[31]\teval-error:0.19084\ttrain-error:0.027211\n",
      "[32]\teval-error:0.198473\ttrain-error:0.027211\n",
      "[33]\teval-error:0.194656\ttrain-error:0.027211\n",
      "[34]\teval-error:0.198473\ttrain-error:0.027211\n",
      "[35]\teval-error:0.198473\ttrain-error:0.027211\n",
      "[36]\teval-error:0.198473\ttrain-error:0.027211\n",
      "[37]\teval-error:0.198473\ttrain-error:0.027211\n",
      "[38]\teval-error:0.20229\ttrain-error:0.027211\n",
      "[39]\teval-error:0.194656\ttrain-error:0.027211\n",
      "[40]\teval-error:0.194656\ttrain-error:0.027211\n",
      "[41]\teval-error:0.198473\ttrain-error:0.027211\n",
      "[42]\teval-error:0.20229\ttrain-error:0.027211\n",
      "[43]\teval-error:0.20229\ttrain-error:0.027211\n",
      "[44]\teval-error:0.194656\ttrain-error:0.027211\n",
      "[45]\teval-error:0.209924\ttrain-error:0.027211\n",
      "[46]\teval-error:0.198473\ttrain-error:0.027211\n",
      "[47]\teval-error:0.198473\ttrain-error:0.027211\n",
      "[48]\teval-error:0.19084\ttrain-error:0.027211\n",
      "[49]\teval-error:0.198473\ttrain-error:0.027211\n",
      "[50]\teval-error:0.198473\ttrain-error:0.027211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51]\teval-error:0.194656\ttrain-error:0.027211\n",
      "[52]\teval-error:0.198473\ttrain-error:0.027211\n",
      "[53]\teval-error:0.198473\ttrain-error:0.027211\n",
      "[54]\teval-error:0.20229\ttrain-error:0.027211\n",
      "[55]\teval-error:0.19084\ttrain-error:0.027211\n",
      "[56]\teval-error:0.194656\ttrain-error:0.027211\n",
      "[57]\teval-error:0.198473\ttrain-error:0.027211\n",
      "[58]\teval-error:0.194656\ttrain-error:0.027211\n",
      "[59]\teval-error:0.194656\ttrain-error:0.027211\n",
      "[60]\teval-error:0.19084\ttrain-error:0.027211\n",
      "[61]\teval-error:0.198473\ttrain-error:0.027211\n",
      "[62]\teval-error:0.19084\ttrain-error:0.027211\n",
      "[63]\teval-error:0.198473\ttrain-error:0.027211\n",
      "[64]\teval-error:0.194656\ttrain-error:0.027211\n",
      "[65]\teval-error:0.194656\ttrain-error:0.027211\n",
      "[66]\teval-error:0.198473\ttrain-error:0.027211\n",
      "[67]\teval-error:0.19084\ttrain-error:0.027211\n",
      "[68]\teval-error:0.19084\ttrain-error:0.027211\n",
      "[69]\teval-error:0.187023\ttrain-error:0.027211\n",
      "[0]\teval-error:0.295455\ttrain-error:0.118384\n",
      "[1]\teval-error:0.238636\ttrain-error:0.084958\n",
      "[2]\teval-error:0.193182\ttrain-error:0.038997\n",
      "[3]\teval-error:0.238636\ttrain-error:0.01532\n",
      "[4]\teval-error:0.261364\ttrain-error:0.006964\n",
      "[5]\teval-error:0.272727\ttrain-error:0.005571\n",
      "[6]\teval-error:0.261364\ttrain-error:0.005571\n",
      "[7]\teval-error:0.25\ttrain-error:0.004178\n",
      "[8]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[9]\teval-error:0.25\ttrain-error:0.004178\n",
      "[10]\teval-error:0.25\ttrain-error:0.004178\n",
      "[11]\teval-error:0.25\ttrain-error:0.004178\n",
      "[12]\teval-error:0.25\ttrain-error:0.004178\n",
      "[13]\teval-error:0.25\ttrain-error:0.004178\n",
      "[14]\teval-error:0.261364\ttrain-error:0.004178\n",
      "[15]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[16]\teval-error:0.25\ttrain-error:0.004178\n",
      "[17]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[18]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[19]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[20]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[21]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[22]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[23]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[24]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[25]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[26]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[27]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[28]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[29]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[30]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[31]\teval-error:0.25\ttrain-error:0.004178\n",
      "[32]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[33]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[34]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[35]\teval-error:0.25\ttrain-error:0.004178\n",
      "[36]\teval-error:0.25\ttrain-error:0.004178\n",
      "[37]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[38]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[39]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[40]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[41]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[42]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[43]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[44]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[45]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[46]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[47]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[48]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[49]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[50]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[51]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[52]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[53]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[54]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[55]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[56]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[57]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[58]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[59]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[60]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[61]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[62]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[63]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[64]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[65]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[66]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[67]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[68]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[69]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[0]\teval-error:0.295455\ttrain-error:0.110028\n",
      "[1]\teval-error:0.25\ttrain-error:0.069638\n",
      "[2]\teval-error:0.272727\ttrain-error:0.037604\n",
      "[3]\teval-error:0.272727\ttrain-error:0.02507\n",
      "[4]\teval-error:0.272727\ttrain-error:0.016713\n",
      "[5]\teval-error:0.284091\ttrain-error:0.005571\n",
      "[6]\teval-error:0.284091\ttrain-error:0.005571\n",
      "[7]\teval-error:0.272727\ttrain-error:0.006964\n",
      "[8]\teval-error:0.261364\ttrain-error:0.006964\n",
      "[9]\teval-error:0.261364\ttrain-error:0.005571\n",
      "[10]\teval-error:0.272727\ttrain-error:0.005571\n",
      "[11]\teval-error:0.261364\ttrain-error:0.005571\n",
      "[12]\teval-error:0.272727\ttrain-error:0.005571\n",
      "[13]\teval-error:0.295455\ttrain-error:0.005571\n",
      "[14]\teval-error:0.284091\ttrain-error:0.005571\n",
      "[15]\teval-error:0.284091\ttrain-error:0.005571\n",
      "[16]\teval-error:0.284091\ttrain-error:0.005571\n",
      "[17]\teval-error:0.272727\ttrain-error:0.005571\n",
      "[18]\teval-error:0.272727\ttrain-error:0.005571\n",
      "[19]\teval-error:0.25\ttrain-error:0.005571\n",
      "[20]\teval-error:0.261364\ttrain-error:0.005571\n",
      "[21]\teval-error:0.261364\ttrain-error:0.005571\n",
      "[22]\teval-error:0.25\ttrain-error:0.005571\n",
      "[23]\teval-error:0.25\ttrain-error:0.005571\n",
      "[24]\teval-error:0.25\ttrain-error:0.005571\n",
      "[25]\teval-error:0.25\ttrain-error:0.005571\n",
      "[26]\teval-error:0.25\ttrain-error:0.005571\n",
      "[27]\teval-error:0.261364\ttrain-error:0.005571\n",
      "[28]\teval-error:0.25\ttrain-error:0.005571\n",
      "[29]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[30]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[31]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[32]\teval-error:0.25\ttrain-error:0.005571\n",
      "[33]\teval-error:0.25\ttrain-error:0.005571\n",
      "[34]\teval-error:0.25\ttrain-error:0.005571\n",
      "[35]\teval-error:0.25\ttrain-error:0.005571\n",
      "[36]\teval-error:0.25\ttrain-error:0.005571\n",
      "[37]\teval-error:0.25\ttrain-error:0.005571\n",
      "[38]\teval-error:0.261364\ttrain-error:0.005571\n",
      "[39]\teval-error:0.261364\ttrain-error:0.005571\n",
      "[40]\teval-error:0.261364\ttrain-error:0.005571\n",
      "[41]\teval-error:0.272727\ttrain-error:0.005571\n",
      "[42]\teval-error:0.25\ttrain-error:0.005571\n",
      "[43]\teval-error:0.25\ttrain-error:0.005571\n",
      "[44]\teval-error:0.25\ttrain-error:0.005571\n",
      "[45]\teval-error:0.25\ttrain-error:0.005571\n",
      "[46]\teval-error:0.25\ttrain-error:0.005571\n",
      "[47]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[48]\teval-error:0.25\ttrain-error:0.005571\n",
      "[49]\teval-error:0.25\ttrain-error:0.005571\n",
      "[50]\teval-error:0.25\ttrain-error:0.005571\n",
      "[51]\teval-error:0.25\ttrain-error:0.005571\n",
      "[52]\teval-error:0.25\ttrain-error:0.005571\n",
      "[53]\teval-error:0.25\ttrain-error:0.005571\n",
      "[54]\teval-error:0.25\ttrain-error:0.005571\n",
      "[55]\teval-error:0.25\ttrain-error:0.005571\n",
      "[56]\teval-error:0.25\ttrain-error:0.005571\n",
      "[57]\teval-error:0.25\ttrain-error:0.005571\n",
      "[58]\teval-error:0.25\ttrain-error:0.005571\n",
      "[59]\teval-error:0.25\ttrain-error:0.005571\n",
      "[60]\teval-error:0.25\ttrain-error:0.005571\n",
      "[61]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[62]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[63]\teval-error:0.25\ttrain-error:0.005571\n",
      "[64]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[65]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[66]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[67]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[68]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[69]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[0]\teval-error:0.215909\ttrain-error:0.12117\n",
      "[1]\teval-error:0.215909\ttrain-error:0.079387\n",
      "[2]\teval-error:0.238636\ttrain-error:0.045961\n",
      "[3]\teval-error:0.227273\ttrain-error:0.029248\n",
      "[4]\teval-error:0.215909\ttrain-error:0.011142\n",
      "[5]\teval-error:0.215909\ttrain-error:0.008357\n",
      "[6]\teval-error:0.204545\ttrain-error:0.008357\n",
      "[7]\teval-error:0.215909\ttrain-error:0.006964\n",
      "[8]\teval-error:0.215909\ttrain-error:0.005571\n",
      "[9]\teval-error:0.215909\ttrain-error:0.005571\n",
      "[10]\teval-error:0.215909\ttrain-error:0.005571\n",
      "[11]\teval-error:0.215909\ttrain-error:0.005571\n",
      "[12]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[13]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[14]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[15]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[16]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[17]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[18]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[19]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[20]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[21]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[22]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[23]\teval-error:0.227273\ttrain-error:0.004178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[25]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[26]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[27]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[28]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[29]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[30]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[31]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[32]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[33]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[34]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[35]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[36]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[37]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[38]\teval-error:0.25\ttrain-error:0.004178\n",
      "[39]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[40]\teval-error:0.25\ttrain-error:0.004178\n",
      "[41]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[42]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[43]\teval-error:0.25\ttrain-error:0.004178\n",
      "[44]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[45]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[46]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[47]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[48]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[49]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[50]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[51]\teval-error:0.25\ttrain-error:0.004178\n",
      "[52]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[53]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[54]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[55]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[56]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[57]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[58]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[59]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[60]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[61]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[62]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[63]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[64]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[65]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[66]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[67]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[68]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[69]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[0]\teval-error:0.284091\ttrain-error:0.115599\n",
      "[1]\teval-error:0.295455\ttrain-error:0.076602\n",
      "[2]\teval-error:0.284091\ttrain-error:0.041783\n",
      "[3]\teval-error:0.295455\ttrain-error:0.019499\n",
      "[4]\teval-error:0.272727\ttrain-error:0.009749\n",
      "[5]\teval-error:0.284091\ttrain-error:0.006964\n",
      "[6]\teval-error:0.261364\ttrain-error:0.004178\n",
      "[7]\teval-error:0.272727\ttrain-error:0.005571\n",
      "[8]\teval-error:0.272727\ttrain-error:0.004178\n",
      "[9]\teval-error:0.284091\ttrain-error:0.004178\n",
      "[10]\teval-error:0.272727\ttrain-error:0.004178\n",
      "[11]\teval-error:0.272727\ttrain-error:0.004178\n",
      "[12]\teval-error:0.272727\ttrain-error:0.004178\n",
      "[13]\teval-error:0.272727\ttrain-error:0.004178\n",
      "[14]\teval-error:0.272727\ttrain-error:0.004178\n",
      "[15]\teval-error:0.272727\ttrain-error:0.004178\n",
      "[16]\teval-error:0.272727\ttrain-error:0.004178\n",
      "[17]\teval-error:0.272727\ttrain-error:0.004178\n",
      "[18]\teval-error:0.272727\ttrain-error:0.004178\n",
      "[19]\teval-error:0.272727\ttrain-error:0.004178\n",
      "[20]\teval-error:0.261364\ttrain-error:0.004178\n",
      "[21]\teval-error:0.261364\ttrain-error:0.004178\n",
      "[22]\teval-error:0.261364\ttrain-error:0.004178\n",
      "[23]\teval-error:0.261364\ttrain-error:0.004178\n",
      "[24]\teval-error:0.261364\ttrain-error:0.004178\n",
      "[25]\teval-error:0.272727\ttrain-error:0.004178\n",
      "[26]\teval-error:0.261364\ttrain-error:0.004178\n",
      "[27]\teval-error:0.272727\ttrain-error:0.004178\n",
      "[28]\teval-error:0.261364\ttrain-error:0.004178\n",
      "[29]\teval-error:0.261364\ttrain-error:0.004178\n",
      "[30]\teval-error:0.272727\ttrain-error:0.004178\n",
      "[31]\teval-error:0.261364\ttrain-error:0.004178\n",
      "[32]\teval-error:0.272727\ttrain-error:0.004178\n",
      "[33]\teval-error:0.261364\ttrain-error:0.004178\n",
      "[34]\teval-error:0.272727\ttrain-error:0.004178\n",
      "[35]\teval-error:0.272727\ttrain-error:0.004178\n",
      "[36]\teval-error:0.272727\ttrain-error:0.004178\n",
      "[37]\teval-error:0.272727\ttrain-error:0.004178\n",
      "[38]\teval-error:0.272727\ttrain-error:0.004178\n",
      "[39]\teval-error:0.272727\ttrain-error:0.004178\n",
      "[40]\teval-error:0.272727\ttrain-error:0.004178\n",
      "[41]\teval-error:0.272727\ttrain-error:0.004178\n",
      "[42]\teval-error:0.272727\ttrain-error:0.004178\n",
      "[43]\teval-error:0.284091\ttrain-error:0.004178\n",
      "[44]\teval-error:0.284091\ttrain-error:0.004178\n",
      "[45]\teval-error:0.272727\ttrain-error:0.004178\n",
      "[46]\teval-error:0.261364\ttrain-error:0.004178\n",
      "[47]\teval-error:0.272727\ttrain-error:0.004178\n",
      "[48]\teval-error:0.272727\ttrain-error:0.004178\n",
      "[49]\teval-error:0.272727\ttrain-error:0.004178\n",
      "[50]\teval-error:0.272727\ttrain-error:0.004178\n",
      "[51]\teval-error:0.295455\ttrain-error:0.004178\n",
      "[52]\teval-error:0.272727\ttrain-error:0.004178\n",
      "[53]\teval-error:0.261364\ttrain-error:0.004178\n",
      "[54]\teval-error:0.272727\ttrain-error:0.004178\n",
      "[55]\teval-error:0.272727\ttrain-error:0.004178\n",
      "[56]\teval-error:0.295455\ttrain-error:0.004178\n",
      "[57]\teval-error:0.261364\ttrain-error:0.004178\n",
      "[58]\teval-error:0.284091\ttrain-error:0.004178\n",
      "[59]\teval-error:0.295455\ttrain-error:0.004178\n",
      "[60]\teval-error:0.306818\ttrain-error:0.004178\n",
      "[61]\teval-error:0.295455\ttrain-error:0.004178\n",
      "[62]\teval-error:0.272727\ttrain-error:0.004178\n",
      "[63]\teval-error:0.295455\ttrain-error:0.004178\n",
      "[64]\teval-error:0.295455\ttrain-error:0.004178\n",
      "[65]\teval-error:0.284091\ttrain-error:0.004178\n",
      "[66]\teval-error:0.272727\ttrain-error:0.004178\n",
      "[67]\teval-error:0.272727\ttrain-error:0.004178\n",
      "[68]\teval-error:0.284091\ttrain-error:0.004178\n",
      "[69]\teval-error:0.284091\ttrain-error:0.004178\n",
      "[0]\teval-error:0.319149\ttrain-error:0.1303\n",
      "[1]\teval-error:0.255319\ttrain-error:0.080662\n",
      "[2]\teval-error:0.265957\ttrain-error:0.046536\n",
      "[3]\teval-error:0.329787\ttrain-error:0.02999\n",
      "[4]\teval-error:0.319149\ttrain-error:0.021717\n",
      "[5]\teval-error:0.297872\ttrain-error:0.019648\n",
      "[6]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[7]\teval-error:0.255319\ttrain-error:0.018614\n",
      "[8]\teval-error:0.244681\ttrain-error:0.018614\n",
      "[9]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[10]\teval-error:0.255319\ttrain-error:0.018614\n",
      "[11]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[12]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[13]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[14]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[15]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[16]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[17]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[18]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[19]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[20]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[21]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[22]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[23]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[24]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[25]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[26]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[27]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[28]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[29]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[30]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[31]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[32]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[33]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[34]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[35]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[36]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[37]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[38]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[39]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[40]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[41]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[42]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[43]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[44]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[45]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[46]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[47]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[48]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[49]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[50]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[51]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[52]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[53]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[54]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[55]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[56]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[57]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[58]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[59]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[60]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[61]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[62]\teval-error:0.287234\ttrain-error:0.018614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[63]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[64]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[65]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[66]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[67]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[68]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[69]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[0]\teval-error:0.308511\ttrain-error:0.127198\n",
      "[1]\teval-error:0.329787\ttrain-error:0.085832\n",
      "[2]\teval-error:0.308511\ttrain-error:0.05274\n",
      "[3]\teval-error:0.319149\ttrain-error:0.036194\n",
      "[4]\teval-error:0.297872\ttrain-error:0.027921\n",
      "[5]\teval-error:0.329787\ttrain-error:0.025853\n",
      "[6]\teval-error:0.297872\ttrain-error:0.025853\n",
      "[7]\teval-error:0.308511\ttrain-error:0.022751\n",
      "[8]\teval-error:0.308511\ttrain-error:0.021717\n",
      "[9]\teval-error:0.287234\ttrain-error:0.020683\n",
      "[10]\teval-error:0.308511\ttrain-error:0.019648\n",
      "[11]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[12]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[13]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[14]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[15]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[16]\teval-error:0.329787\ttrain-error:0.018614\n",
      "[17]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[18]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[19]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[20]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[21]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[22]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[23]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[24]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[25]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[26]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[27]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[28]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[29]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[30]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[31]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[32]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[33]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[34]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[35]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[36]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[37]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[38]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[39]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[40]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[41]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[42]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[43]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[44]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[45]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[46]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[47]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[48]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[49]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[50]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[51]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[52]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[53]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[54]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[55]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[56]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[57]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[58]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[59]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[60]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[61]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[62]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[63]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[64]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[65]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[66]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[67]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[68]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[69]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[0]\teval-error:0.340426\ttrain-error:0.1303\n",
      "[1]\teval-error:0.308511\ttrain-error:0.074457\n",
      "[2]\teval-error:0.340426\ttrain-error:0.044467\n",
      "[3]\teval-error:0.308511\ttrain-error:0.028956\n",
      "[4]\teval-error:0.297872\ttrain-error:0.025853\n",
      "[5]\teval-error:0.287234\ttrain-error:0.022751\n",
      "[6]\teval-error:0.297872\ttrain-error:0.021717\n",
      "[7]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[8]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[9]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[10]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[11]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[12]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[13]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[14]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[15]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[16]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[17]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[18]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[19]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[20]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[21]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[22]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[23]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[24]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[25]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[26]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[27]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[28]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[29]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[30]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[31]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[32]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[33]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[34]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[35]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[36]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[37]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[38]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[39]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[40]\teval-error:0.255319\ttrain-error:0.018614\n",
      "[41]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[42]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[43]\teval-error:0.255319\ttrain-error:0.018614\n",
      "[44]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[45]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[46]\teval-error:0.255319\ttrain-error:0.018614\n",
      "[47]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[48]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[49]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[50]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[51]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[52]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[53]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[54]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[55]\teval-error:0.255319\ttrain-error:0.018614\n",
      "[56]\teval-error:0.255319\ttrain-error:0.018614\n",
      "[57]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[58]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[59]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[60]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[61]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[62]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[63]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[64]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[65]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[66]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[67]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[68]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[69]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[0]\teval-error:0.340426\ttrain-error:0.135471\n",
      "[1]\teval-error:0.351064\ttrain-error:0.075491\n",
      "[2]\teval-error:0.319149\ttrain-error:0.037229\n",
      "[3]\teval-error:0.340426\ttrain-error:0.025853\n",
      "[4]\teval-error:0.340426\ttrain-error:0.021717\n",
      "[5]\teval-error:0.351064\ttrain-error:0.019648\n",
      "[6]\teval-error:0.361702\ttrain-error:0.019648\n",
      "[7]\teval-error:0.340426\ttrain-error:0.019648\n",
      "[8]\teval-error:0.361702\ttrain-error:0.019648\n",
      "[9]\teval-error:0.351064\ttrain-error:0.018614\n",
      "[10]\teval-error:0.361702\ttrain-error:0.018614\n",
      "[11]\teval-error:0.351064\ttrain-error:0.018614\n",
      "[12]\teval-error:0.340426\ttrain-error:0.018614\n",
      "[13]\teval-error:0.361702\ttrain-error:0.018614\n",
      "[14]\teval-error:0.340426\ttrain-error:0.018614\n",
      "[15]\teval-error:0.340426\ttrain-error:0.018614\n",
      "[16]\teval-error:0.340426\ttrain-error:0.018614\n",
      "[17]\teval-error:0.351064\ttrain-error:0.018614\n",
      "[18]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[19]\teval-error:0.340426\ttrain-error:0.018614\n",
      "[20]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[21]\teval-error:0.329787\ttrain-error:0.018614\n",
      "[22]\teval-error:0.329787\ttrain-error:0.018614\n",
      "[23]\teval-error:0.329787\ttrain-error:0.018614\n",
      "[24]\teval-error:0.340426\ttrain-error:0.018614\n",
      "[25]\teval-error:0.329787\ttrain-error:0.018614\n",
      "[26]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[27]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[28]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[29]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[30]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[31]\teval-error:0.308511\ttrain-error:0.018614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[33]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[34]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[35]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[36]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[37]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[38]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[39]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[40]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[41]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[42]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[43]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[44]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[45]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[46]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[47]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[48]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[49]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[50]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[51]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[52]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[53]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[54]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[55]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[56]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[57]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[58]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[59]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[60]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[61]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[62]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[63]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[64]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[65]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[66]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[67]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[68]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[69]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[0]\teval-error:0.232824\ttrain-error:0.104956\n",
      "[1]\teval-error:0.206107\ttrain-error:0.077259\n",
      "[2]\teval-error:0.20229\ttrain-error:0.052964\n",
      "[3]\teval-error:0.209924\ttrain-error:0.037901\n",
      "[4]\teval-error:0.217557\ttrain-error:0.030612\n",
      "[5]\teval-error:0.21374\ttrain-error:0.029155\n",
      "[6]\teval-error:0.209924\ttrain-error:0.028183\n",
      "[7]\teval-error:0.198473\ttrain-error:0.028183\n",
      "[8]\teval-error:0.209924\ttrain-error:0.028183\n",
      "[9]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[10]\teval-error:0.206107\ttrain-error:0.028183\n",
      "[11]\teval-error:0.209924\ttrain-error:0.028183\n",
      "[12]\teval-error:0.20229\ttrain-error:0.028183\n",
      "[13]\teval-error:0.21374\ttrain-error:0.028183\n",
      "[14]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[15]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[16]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[17]\teval-error:0.21374\ttrain-error:0.028183\n",
      "[18]\teval-error:0.206107\ttrain-error:0.028183\n",
      "[19]\teval-error:0.21374\ttrain-error:0.028183\n",
      "[20]\teval-error:0.206107\ttrain-error:0.028183\n",
      "[21]\teval-error:0.209924\ttrain-error:0.028183\n",
      "[22]\teval-error:0.21374\ttrain-error:0.028183\n",
      "[23]\teval-error:0.209924\ttrain-error:0.028183\n",
      "[24]\teval-error:0.209924\ttrain-error:0.028183\n",
      "[25]\teval-error:0.209924\ttrain-error:0.028183\n",
      "[26]\teval-error:0.206107\ttrain-error:0.028183\n",
      "[27]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[28]\teval-error:0.206107\ttrain-error:0.028183\n",
      "[29]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[30]\teval-error:0.21374\ttrain-error:0.028183\n",
      "[31]\teval-error:0.209924\ttrain-error:0.028183\n",
      "[32]\teval-error:0.206107\ttrain-error:0.028183\n",
      "[33]\teval-error:0.209924\ttrain-error:0.028183\n",
      "[34]\teval-error:0.20229\ttrain-error:0.028183\n",
      "[35]\teval-error:0.20229\ttrain-error:0.028183\n",
      "[36]\teval-error:0.21374\ttrain-error:0.028183\n",
      "[37]\teval-error:0.209924\ttrain-error:0.028183\n",
      "[38]\teval-error:0.20229\ttrain-error:0.028183\n",
      "[39]\teval-error:0.206107\ttrain-error:0.028183\n",
      "[40]\teval-error:0.20229\ttrain-error:0.028183\n",
      "[41]\teval-error:0.209924\ttrain-error:0.028183\n",
      "[42]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[43]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[44]\teval-error:0.206107\ttrain-error:0.028183\n",
      "[45]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[46]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[47]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[48]\teval-error:0.21374\ttrain-error:0.028183\n",
      "[49]\teval-error:0.209924\ttrain-error:0.028183\n",
      "[50]\teval-error:0.21374\ttrain-error:0.028183\n",
      "[51]\teval-error:0.20229\ttrain-error:0.028183\n",
      "[52]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[53]\teval-error:0.21374\ttrain-error:0.028183\n",
      "[54]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[55]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[56]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[57]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[58]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[59]\teval-error:0.206107\ttrain-error:0.028183\n",
      "[60]\teval-error:0.206107\ttrain-error:0.028183\n",
      "[61]\teval-error:0.21374\ttrain-error:0.028183\n",
      "[62]\teval-error:0.21374\ttrain-error:0.028183\n",
      "[63]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[64]\teval-error:0.21374\ttrain-error:0.028183\n",
      "[65]\teval-error:0.209924\ttrain-error:0.028183\n",
      "[66]\teval-error:0.20229\ttrain-error:0.028183\n",
      "[67]\teval-error:0.209924\ttrain-error:0.028183\n",
      "[68]\teval-error:0.21374\ttrain-error:0.028183\n",
      "[69]\teval-error:0.21374\ttrain-error:0.028183\n",
      "[0]\teval-error:0.251908\ttrain-error:0.122935\n",
      "[1]\teval-error:0.232824\ttrain-error:0.089893\n",
      "[2]\teval-error:0.229008\ttrain-error:0.067541\n",
      "[3]\teval-error:0.229008\ttrain-error:0.049077\n",
      "[4]\teval-error:0.255725\ttrain-error:0.041302\n",
      "[5]\teval-error:0.259542\ttrain-error:0.036443\n",
      "[6]\teval-error:0.251908\ttrain-error:0.031584\n",
      "[7]\teval-error:0.255725\ttrain-error:0.030126\n",
      "[8]\teval-error:0.248092\ttrain-error:0.02964\n",
      "[9]\teval-error:0.248092\ttrain-error:0.029155\n",
      "[10]\teval-error:0.251908\ttrain-error:0.028669\n",
      "[11]\teval-error:0.240458\ttrain-error:0.028669\n",
      "[12]\teval-error:0.236641\ttrain-error:0.028183\n",
      "[13]\teval-error:0.229008\ttrain-error:0.028669\n",
      "[14]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[15]\teval-error:0.232824\ttrain-error:0.028183\n",
      "[16]\teval-error:0.236641\ttrain-error:0.028183\n",
      "[17]\teval-error:0.225191\ttrain-error:0.028183\n",
      "[18]\teval-error:0.232824\ttrain-error:0.028183\n",
      "[19]\teval-error:0.244275\ttrain-error:0.028183\n",
      "[20]\teval-error:0.232824\ttrain-error:0.028183\n",
      "[21]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[22]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[23]\teval-error:0.236641\ttrain-error:0.028183\n",
      "[24]\teval-error:0.244275\ttrain-error:0.028183\n",
      "[25]\teval-error:0.232824\ttrain-error:0.028183\n",
      "[26]\teval-error:0.236641\ttrain-error:0.028183\n",
      "[27]\teval-error:0.232824\ttrain-error:0.028183\n",
      "[28]\teval-error:0.236641\ttrain-error:0.028183\n",
      "[29]\teval-error:0.232824\ttrain-error:0.028183\n",
      "[30]\teval-error:0.232824\ttrain-error:0.028183\n",
      "[31]\teval-error:0.248092\ttrain-error:0.028183\n",
      "[32]\teval-error:0.248092\ttrain-error:0.028183\n",
      "[33]\teval-error:0.232824\ttrain-error:0.028183\n",
      "[34]\teval-error:0.240458\ttrain-error:0.028183\n",
      "[35]\teval-error:0.232824\ttrain-error:0.028183\n",
      "[36]\teval-error:0.232824\ttrain-error:0.028183\n",
      "[37]\teval-error:0.232824\ttrain-error:0.028183\n",
      "[38]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[39]\teval-error:0.225191\ttrain-error:0.028183\n",
      "[40]\teval-error:0.232824\ttrain-error:0.028183\n",
      "[41]\teval-error:0.225191\ttrain-error:0.028183\n",
      "[42]\teval-error:0.232824\ttrain-error:0.028183\n",
      "[43]\teval-error:0.225191\ttrain-error:0.028183\n",
      "[44]\teval-error:0.232824\ttrain-error:0.028183\n",
      "[45]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[46]\teval-error:0.232824\ttrain-error:0.028183\n",
      "[47]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[48]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[49]\teval-error:0.225191\ttrain-error:0.028183\n",
      "[50]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[51]\teval-error:0.240458\ttrain-error:0.028183\n",
      "[52]\teval-error:0.236641\ttrain-error:0.028183\n",
      "[53]\teval-error:0.232824\ttrain-error:0.028183\n",
      "[54]\teval-error:0.225191\ttrain-error:0.028183\n",
      "[55]\teval-error:0.232824\ttrain-error:0.028183\n",
      "[56]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[57]\teval-error:0.240458\ttrain-error:0.028183\n",
      "[58]\teval-error:0.240458\ttrain-error:0.028183\n",
      "[59]\teval-error:0.236641\ttrain-error:0.028183\n",
      "[60]\teval-error:0.236641\ttrain-error:0.028183\n",
      "[61]\teval-error:0.240458\ttrain-error:0.028183\n",
      "[62]\teval-error:0.232824\ttrain-error:0.028183\n",
      "[63]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[64]\teval-error:0.225191\ttrain-error:0.028183\n",
      "[65]\teval-error:0.232824\ttrain-error:0.028183\n",
      "[66]\teval-error:0.236641\ttrain-error:0.028183\n",
      "[67]\teval-error:0.244275\ttrain-error:0.028183\n",
      "[68]\teval-error:0.251908\ttrain-error:0.028183\n",
      "[69]\teval-error:0.240458\ttrain-error:0.028183\n",
      "[0]\teval-error:0.240458\ttrain-error:0.116132\n",
      "[1]\teval-error:0.229008\ttrain-error:0.086978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\teval-error:0.21374\ttrain-error:0.052478\n",
      "[3]\teval-error:0.217557\ttrain-error:0.038873\n",
      "[4]\teval-error:0.229008\ttrain-error:0.032556\n",
      "[5]\teval-error:0.229008\ttrain-error:0.02964\n",
      "[6]\teval-error:0.240458\ttrain-error:0.028183\n",
      "[7]\teval-error:0.225191\ttrain-error:0.028183\n",
      "[8]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[9]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[10]\teval-error:0.209924\ttrain-error:0.028183\n",
      "[11]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[12]\teval-error:0.21374\ttrain-error:0.028183\n",
      "[13]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[14]\teval-error:0.21374\ttrain-error:0.028183\n",
      "[15]\teval-error:0.206107\ttrain-error:0.028183\n",
      "[16]\teval-error:0.206107\ttrain-error:0.028183\n",
      "[17]\teval-error:0.21374\ttrain-error:0.028183\n",
      "[18]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[19]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[20]\teval-error:0.225191\ttrain-error:0.028183\n",
      "[21]\teval-error:0.225191\ttrain-error:0.028183\n",
      "[22]\teval-error:0.232824\ttrain-error:0.028183\n",
      "[23]\teval-error:0.225191\ttrain-error:0.028183\n",
      "[24]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[25]\teval-error:0.232824\ttrain-error:0.028183\n",
      "[26]\teval-error:0.225191\ttrain-error:0.028183\n",
      "[27]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[28]\teval-error:0.232824\ttrain-error:0.028183\n",
      "[29]\teval-error:0.225191\ttrain-error:0.028183\n",
      "[30]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[31]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[32]\teval-error:0.232824\ttrain-error:0.028183\n",
      "[33]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[34]\teval-error:0.225191\ttrain-error:0.028183\n",
      "[35]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[36]\teval-error:0.225191\ttrain-error:0.028183\n",
      "[37]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[38]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[39]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[40]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[41]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[42]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[43]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[44]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[45]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[46]\teval-error:0.21374\ttrain-error:0.028183\n",
      "[47]\teval-error:0.232824\ttrain-error:0.028183\n",
      "[48]\teval-error:0.232824\ttrain-error:0.028183\n",
      "[49]\teval-error:0.232824\ttrain-error:0.028183\n",
      "[50]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[51]\teval-error:0.225191\ttrain-error:0.028183\n",
      "[52]\teval-error:0.232824\ttrain-error:0.028183\n",
      "[53]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[54]\teval-error:0.225191\ttrain-error:0.028183\n",
      "[55]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[56]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[57]\teval-error:0.21374\ttrain-error:0.028183\n",
      "[58]\teval-error:0.232824\ttrain-error:0.028183\n",
      "[59]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[60]\teval-error:0.225191\ttrain-error:0.028183\n",
      "[61]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[62]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[63]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[64]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[65]\teval-error:0.225191\ttrain-error:0.028183\n",
      "[66]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[67]\teval-error:0.225191\ttrain-error:0.028183\n",
      "[68]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[69]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[0]\teval-error:0.259542\ttrain-error:0.132653\n",
      "[1]\teval-error:0.225191\ttrain-error:0.094752\n",
      "[2]\teval-error:0.236641\ttrain-error:0.055879\n",
      "[3]\teval-error:0.244275\ttrain-error:0.036929\n",
      "[4]\teval-error:0.236641\ttrain-error:0.033528\n",
      "[5]\teval-error:0.229008\ttrain-error:0.02964\n",
      "[6]\teval-error:0.240458\ttrain-error:0.028183\n",
      "[7]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[8]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[9]\teval-error:0.236641\ttrain-error:0.028183\n",
      "[10]\teval-error:0.21374\ttrain-error:0.028183\n",
      "[11]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[12]\teval-error:0.20229\ttrain-error:0.028183\n",
      "[13]\teval-error:0.198473\ttrain-error:0.028183\n",
      "[14]\teval-error:0.206107\ttrain-error:0.028183\n",
      "[15]\teval-error:0.209924\ttrain-error:0.028183\n",
      "[16]\teval-error:0.209924\ttrain-error:0.028183\n",
      "[17]\teval-error:0.206107\ttrain-error:0.028183\n",
      "[18]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[19]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[20]\teval-error:0.20229\ttrain-error:0.028183\n",
      "[21]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[22]\teval-error:0.20229\ttrain-error:0.028183\n",
      "[23]\teval-error:0.209924\ttrain-error:0.028183\n",
      "[24]\teval-error:0.209924\ttrain-error:0.028183\n",
      "[25]\teval-error:0.206107\ttrain-error:0.028183\n",
      "[26]\teval-error:0.21374\ttrain-error:0.028183\n",
      "[27]\teval-error:0.209924\ttrain-error:0.028183\n",
      "[28]\teval-error:0.209924\ttrain-error:0.028183\n",
      "[29]\teval-error:0.206107\ttrain-error:0.028183\n",
      "[30]\teval-error:0.206107\ttrain-error:0.028183\n",
      "[31]\teval-error:0.209924\ttrain-error:0.028183\n",
      "[32]\teval-error:0.206107\ttrain-error:0.028183\n",
      "[33]\teval-error:0.206107\ttrain-error:0.028183\n",
      "[34]\teval-error:0.198473\ttrain-error:0.028183\n",
      "[35]\teval-error:0.20229\ttrain-error:0.028183\n",
      "[36]\teval-error:0.206107\ttrain-error:0.028183\n",
      "[37]\teval-error:0.206107\ttrain-error:0.028183\n",
      "[38]\teval-error:0.206107\ttrain-error:0.028183\n",
      "[39]\teval-error:0.206107\ttrain-error:0.028183\n",
      "[40]\teval-error:0.206107\ttrain-error:0.028183\n",
      "[41]\teval-error:0.209924\ttrain-error:0.028183\n",
      "[42]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[43]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[44]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[45]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[46]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[47]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[48]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[49]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[50]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[51]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[52]\teval-error:0.209924\ttrain-error:0.028183\n",
      "[53]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[54]\teval-error:0.21374\ttrain-error:0.028183\n",
      "[55]\teval-error:0.209924\ttrain-error:0.028183\n",
      "[56]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[57]\teval-error:0.206107\ttrain-error:0.028183\n",
      "[58]\teval-error:0.209924\ttrain-error:0.028183\n",
      "[59]\teval-error:0.209924\ttrain-error:0.028183\n",
      "[60]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[61]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[62]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[63]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[64]\teval-error:0.225191\ttrain-error:0.028183\n",
      "[65]\teval-error:0.209924\ttrain-error:0.028183\n",
      "[66]\teval-error:0.209924\ttrain-error:0.028183\n",
      "[67]\teval-error:0.21374\ttrain-error:0.028183\n",
      "[68]\teval-error:0.209924\ttrain-error:0.028183\n",
      "[69]\teval-error:0.217557\ttrain-error:0.028183\n",
      "[0]\teval-error:0.295455\ttrain-error:0.107242\n",
      "[1]\teval-error:0.25\ttrain-error:0.045961\n",
      "[2]\teval-error:0.272727\ttrain-error:0.018106\n",
      "[3]\teval-error:0.261364\ttrain-error:0.009749\n",
      "[4]\teval-error:0.272727\ttrain-error:0.004178\n",
      "[5]\teval-error:0.25\ttrain-error:0.004178\n",
      "[6]\teval-error:0.25\ttrain-error:0.004178\n",
      "[7]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[8]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[9]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[10]\teval-error:0.25\ttrain-error:0.004178\n",
      "[11]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[12]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[13]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[14]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[15]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[16]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[17]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[18]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[19]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[20]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[21]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[22]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[23]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[24]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[25]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[26]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[27]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[28]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[29]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[30]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[31]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[32]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[33]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[34]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[35]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[36]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[37]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[38]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[39]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[40]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[41]\teval-error:0.204545\ttrain-error:0.004178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[43]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[44]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[45]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[46]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[47]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[48]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[49]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[50]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[51]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[52]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[53]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[54]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[55]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[56]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[57]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[58]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[59]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[60]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[61]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[62]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[63]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[64]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[65]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[66]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[67]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[68]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[69]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[0]\teval-error:0.295455\ttrain-error:0.103064\n",
      "[1]\teval-error:0.25\ttrain-error:0.041783\n",
      "[2]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[3]\teval-error:0.25\ttrain-error:0.008357\n",
      "[4]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[5]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[6]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[7]\teval-error:0.25\ttrain-error:0.004178\n",
      "[8]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[9]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[10]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[11]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[12]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[13]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[14]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[15]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[16]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[17]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[18]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[19]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[20]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[21]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[22]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[23]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[24]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[25]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[26]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[27]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[28]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[29]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[30]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[31]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[32]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[33]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[34]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[35]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[36]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[37]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[38]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[39]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[40]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[41]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[42]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[43]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[44]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[45]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[46]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[47]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[48]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[49]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[50]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[51]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[52]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[53]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[54]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[55]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[56]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[57]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[58]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[59]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[60]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[61]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[62]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[63]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[64]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[65]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[66]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[67]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[68]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[69]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[0]\teval-error:0.352273\ttrain-error:0.093315\n",
      "[1]\teval-error:0.272727\ttrain-error:0.058496\n",
      "[2]\teval-error:0.261364\ttrain-error:0.029248\n",
      "[3]\teval-error:0.238636\ttrain-error:0.013928\n",
      "[4]\teval-error:0.261364\ttrain-error:0.008357\n",
      "[5]\teval-error:0.272727\ttrain-error:0.006964\n",
      "[6]\teval-error:0.25\ttrain-error:0.004178\n",
      "[7]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[8]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[9]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[10]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[11]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[12]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[13]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[14]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[15]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[16]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[17]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[18]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[19]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[20]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[21]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[22]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[23]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[24]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[25]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[26]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[27]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[28]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[29]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[30]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[31]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[32]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[33]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[34]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[35]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[36]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[37]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[38]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[39]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[40]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[41]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[42]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[43]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[44]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[45]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[46]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[47]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[48]\teval-error:0.25\ttrain-error:0.004178\n",
      "[49]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[50]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[51]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[52]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[53]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[54]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[55]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[56]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[57]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[58]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[59]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[60]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[61]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[62]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[63]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[64]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[65]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[66]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[67]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[68]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[69]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[0]\teval-error:0.375\ttrain-error:0.097493\n",
      "[1]\teval-error:0.284091\ttrain-error:0.045961\n",
      "[2]\teval-error:0.238636\ttrain-error:0.018106\n",
      "[3]\teval-error:0.25\ttrain-error:0.008357\n",
      "[4]\teval-error:0.25\ttrain-error:0.006964\n",
      "[5]\teval-error:0.25\ttrain-error:0.005571\n",
      "[6]\teval-error:0.261364\ttrain-error:0.004178\n",
      "[7]\teval-error:0.261364\ttrain-error:0.004178\n",
      "[8]\teval-error:0.261364\ttrain-error:0.004178\n",
      "[9]\teval-error:0.261364\ttrain-error:0.004178\n",
      "[10]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[11]\teval-error:0.25\ttrain-error:0.004178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12]\teval-error:0.261364\ttrain-error:0.004178\n",
      "[13]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[14]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[15]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[16]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[17]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[18]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[19]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[20]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[21]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[22]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[23]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[24]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[25]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[26]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[27]\teval-error:0.25\ttrain-error:0.004178\n",
      "[28]\teval-error:0.25\ttrain-error:0.004178\n",
      "[29]\teval-error:0.25\ttrain-error:0.004178\n",
      "[30]\teval-error:0.25\ttrain-error:0.004178\n",
      "[31]\teval-error:0.25\ttrain-error:0.004178\n",
      "[32]\teval-error:0.25\ttrain-error:0.004178\n",
      "[33]\teval-error:0.25\ttrain-error:0.004178\n",
      "[34]\teval-error:0.25\ttrain-error:0.004178\n",
      "[35]\teval-error:0.25\ttrain-error:0.004178\n",
      "[36]\teval-error:0.25\ttrain-error:0.004178\n",
      "[37]\teval-error:0.25\ttrain-error:0.004178\n",
      "[38]\teval-error:0.25\ttrain-error:0.004178\n",
      "[39]\teval-error:0.25\ttrain-error:0.004178\n",
      "[40]\teval-error:0.25\ttrain-error:0.004178\n",
      "[41]\teval-error:0.25\ttrain-error:0.004178\n",
      "[42]\teval-error:0.25\ttrain-error:0.004178\n",
      "[43]\teval-error:0.25\ttrain-error:0.004178\n",
      "[44]\teval-error:0.261364\ttrain-error:0.004178\n",
      "[45]\teval-error:0.261364\ttrain-error:0.004178\n",
      "[46]\teval-error:0.261364\ttrain-error:0.004178\n",
      "[47]\teval-error:0.25\ttrain-error:0.004178\n",
      "[48]\teval-error:0.261364\ttrain-error:0.004178\n",
      "[49]\teval-error:0.25\ttrain-error:0.004178\n",
      "[50]\teval-error:0.261364\ttrain-error:0.004178\n",
      "[51]\teval-error:0.25\ttrain-error:0.004178\n",
      "[52]\teval-error:0.25\ttrain-error:0.004178\n",
      "[53]\teval-error:0.25\ttrain-error:0.004178\n",
      "[54]\teval-error:0.25\ttrain-error:0.004178\n",
      "[55]\teval-error:0.25\ttrain-error:0.004178\n",
      "[56]\teval-error:0.25\ttrain-error:0.004178\n",
      "[57]\teval-error:0.261364\ttrain-error:0.004178\n",
      "[58]\teval-error:0.261364\ttrain-error:0.004178\n",
      "[59]\teval-error:0.261364\ttrain-error:0.004178\n",
      "[60]\teval-error:0.25\ttrain-error:0.004178\n",
      "[61]\teval-error:0.261364\ttrain-error:0.004178\n",
      "[62]\teval-error:0.25\ttrain-error:0.004178\n",
      "[63]\teval-error:0.25\ttrain-error:0.004178\n",
      "[64]\teval-error:0.25\ttrain-error:0.004178\n",
      "[65]\teval-error:0.25\ttrain-error:0.004178\n",
      "[66]\teval-error:0.25\ttrain-error:0.004178\n",
      "[67]\teval-error:0.25\ttrain-error:0.004178\n",
      "[68]\teval-error:0.261364\ttrain-error:0.004178\n",
      "[69]\teval-error:0.261364\ttrain-error:0.004178\n",
      "[0]\teval-error:0.329787\ttrain-error:0.11789\n",
      "[1]\teval-error:0.276596\ttrain-error:0.054809\n",
      "[2]\teval-error:0.297872\ttrain-error:0.034126\n",
      "[3]\teval-error:0.308511\ttrain-error:0.023785\n",
      "[4]\teval-error:0.255319\ttrain-error:0.020683\n",
      "[5]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[6]\teval-error:0.244681\ttrain-error:0.018614\n",
      "[7]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[8]\teval-error:0.255319\ttrain-error:0.018614\n",
      "[9]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[10]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[11]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[12]\teval-error:0.255319\ttrain-error:0.018614\n",
      "[13]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[14]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[15]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[16]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[17]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[18]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[19]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[20]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[21]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[22]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[23]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[24]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[25]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[26]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[27]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[28]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[29]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[30]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[31]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[32]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[33]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[34]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[35]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[36]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[37]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[38]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[39]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[40]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[41]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[42]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[43]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[44]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[45]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[46]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[47]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[48]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[49]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[50]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[51]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[52]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[53]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[54]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[55]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[56]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[57]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[58]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[59]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[60]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[61]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[62]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[63]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[64]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[65]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[66]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[67]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[68]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[69]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[0]\teval-error:0.340426\ttrain-error:0.126163\n",
      "[1]\teval-error:0.308511\ttrain-error:0.062048\n",
      "[2]\teval-error:0.319149\ttrain-error:0.041365\n",
      "[3]\teval-error:0.329787\ttrain-error:0.027921\n",
      "[4]\teval-error:0.308511\ttrain-error:0.021717\n",
      "[5]\teval-error:0.319149\ttrain-error:0.019648\n",
      "[6]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[7]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[8]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[9]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[10]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[11]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[12]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[13]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[14]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[15]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[16]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[17]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[18]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[19]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[20]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[21]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[22]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[23]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[24]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[25]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[26]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[27]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[28]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[29]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[30]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[31]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[32]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[33]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[34]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[35]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[36]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[37]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[38]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[39]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[40]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[41]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[42]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[43]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[44]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[45]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[46]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[47]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[48]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[49]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[50]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[51]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[52]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[53]\teval-error:0.287234\ttrain-error:0.018614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[54]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[55]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[56]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[57]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[58]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[59]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[60]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[61]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[62]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[63]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[64]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[65]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[66]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[67]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[68]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[69]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[0]\teval-error:0.351064\ttrain-error:0.113754\n",
      "[1]\teval-error:0.319149\ttrain-error:0.066184\n",
      "[2]\teval-error:0.265957\ttrain-error:0.031024\n",
      "[3]\teval-error:0.244681\ttrain-error:0.025853\n",
      "[4]\teval-error:0.255319\ttrain-error:0.021717\n",
      "[5]\teval-error:0.276596\ttrain-error:0.020683\n",
      "[6]\teval-error:0.255319\ttrain-error:0.020683\n",
      "[7]\teval-error:0.287234\ttrain-error:0.020683\n",
      "[8]\teval-error:0.297872\ttrain-error:0.020683\n",
      "[9]\teval-error:0.308511\ttrain-error:0.020683\n",
      "[10]\teval-error:0.297872\ttrain-error:0.019648\n",
      "[11]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[12]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[13]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[14]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[15]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[16]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[17]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[18]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[19]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[20]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[21]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[22]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[23]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[24]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[25]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[26]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[27]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[28]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[29]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[30]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[31]\teval-error:0.308511\ttrain-error:0.018614\n",
      "[32]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[33]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[34]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[35]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[36]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[37]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[38]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[39]\teval-error:0.329787\ttrain-error:0.018614\n",
      "[40]\teval-error:0.329787\ttrain-error:0.018614\n",
      "[41]\teval-error:0.329787\ttrain-error:0.018614\n",
      "[42]\teval-error:0.329787\ttrain-error:0.018614\n",
      "[43]\teval-error:0.329787\ttrain-error:0.018614\n",
      "[44]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[45]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[46]\teval-error:0.329787\ttrain-error:0.018614\n",
      "[47]\teval-error:0.329787\ttrain-error:0.018614\n",
      "[48]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[49]\teval-error:0.329787\ttrain-error:0.018614\n",
      "[50]\teval-error:0.329787\ttrain-error:0.018614\n",
      "[51]\teval-error:0.329787\ttrain-error:0.018614\n",
      "[52]\teval-error:0.329787\ttrain-error:0.018614\n",
      "[53]\teval-error:0.329787\ttrain-error:0.018614\n",
      "[54]\teval-error:0.329787\ttrain-error:0.018614\n",
      "[55]\teval-error:0.329787\ttrain-error:0.018614\n",
      "[56]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[57]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[58]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[59]\teval-error:0.329787\ttrain-error:0.018614\n",
      "[60]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[61]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[62]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[63]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[64]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[65]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[66]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[67]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[68]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[69]\teval-error:0.319149\ttrain-error:0.018614\n",
      "[0]\teval-error:0.340426\ttrain-error:0.104447\n",
      "[1]\teval-error:0.255319\ttrain-error:0.039297\n",
      "[2]\teval-error:0.297872\ttrain-error:0.024819\n",
      "[3]\teval-error:0.308511\ttrain-error:0.019648\n",
      "[4]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[5]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[6]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[7]\teval-error:0.255319\ttrain-error:0.018614\n",
      "[8]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[9]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[10]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[11]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[12]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[13]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[14]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[15]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[16]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[17]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[18]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[19]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[20]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[21]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[22]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[23]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[24]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[25]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[26]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[27]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[28]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[29]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[30]\teval-error:0.297872\ttrain-error:0.018614\n",
      "[31]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[32]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[33]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[34]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[35]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[36]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[37]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[38]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[39]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[40]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[41]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[42]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[43]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[44]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[45]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[46]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[47]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[48]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[49]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[50]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[51]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[52]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[53]\teval-error:0.287234\ttrain-error:0.018614\n",
      "[54]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[55]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[56]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[57]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[58]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[59]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[60]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[61]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[62]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[63]\teval-error:0.265957\ttrain-error:0.018614\n",
      "[64]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[65]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[66]\teval-error:0.244681\ttrain-error:0.018614\n",
      "[67]\teval-error:0.255319\ttrain-error:0.018614\n",
      "[68]\teval-error:0.244681\ttrain-error:0.018614\n",
      "[69]\teval-error:0.244681\ttrain-error:0.018614\n",
      "[0]\teval-error:0.244275\ttrain-error:0.096696\n",
      "[1]\teval-error:0.244275\ttrain-error:0.059281\n",
      "[2]\teval-error:0.255725\ttrain-error:0.034985\n",
      "[3]\teval-error:0.221374\ttrain-error:0.02964\n",
      "[4]\teval-error:0.206107\ttrain-error:0.028183\n",
      "[5]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[6]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[7]\teval-error:0.206107\ttrain-error:0.027697\n",
      "[8]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[9]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[10]\teval-error:0.206107\ttrain-error:0.027697\n",
      "[11]\teval-error:0.20229\ttrain-error:0.027697\n",
      "[12]\teval-error:0.209924\ttrain-error:0.027697\n",
      "[13]\teval-error:0.221374\ttrain-error:0.027697\n",
      "[14]\teval-error:0.209924\ttrain-error:0.027697\n",
      "[15]\teval-error:0.209924\ttrain-error:0.027697\n",
      "[16]\teval-error:0.209924\ttrain-error:0.027697\n",
      "[17]\teval-error:0.209924\ttrain-error:0.027697\n",
      "[18]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[19]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[20]\teval-error:0.209924\ttrain-error:0.027697\n",
      "[21]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[22]\teval-error:0.217557\ttrain-error:0.027697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[24]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[25]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[26]\teval-error:0.221374\ttrain-error:0.027697\n",
      "[27]\teval-error:0.229008\ttrain-error:0.027697\n",
      "[28]\teval-error:0.221374\ttrain-error:0.027697\n",
      "[29]\teval-error:0.229008\ttrain-error:0.027697\n",
      "[30]\teval-error:0.225191\ttrain-error:0.027697\n",
      "[31]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[32]\teval-error:0.221374\ttrain-error:0.027697\n",
      "[33]\teval-error:0.221374\ttrain-error:0.027697\n",
      "[34]\teval-error:0.221374\ttrain-error:0.027697\n",
      "[35]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[36]\teval-error:0.225191\ttrain-error:0.027697\n",
      "[37]\teval-error:0.209924\ttrain-error:0.027697\n",
      "[38]\teval-error:0.209924\ttrain-error:0.027697\n",
      "[39]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[40]\teval-error:0.209924\ttrain-error:0.027697\n",
      "[41]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[42]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[43]\teval-error:0.206107\ttrain-error:0.027697\n",
      "[44]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[45]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[46]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[47]\teval-error:0.209924\ttrain-error:0.027697\n",
      "[48]\teval-error:0.209924\ttrain-error:0.027697\n",
      "[49]\teval-error:0.206107\ttrain-error:0.027697\n",
      "[50]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[51]\teval-error:0.221374\ttrain-error:0.027697\n",
      "[52]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[53]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[54]\teval-error:0.221374\ttrain-error:0.027697\n",
      "[55]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[56]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[57]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[58]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[59]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[60]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[61]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[62]\teval-error:0.209924\ttrain-error:0.027697\n",
      "[63]\teval-error:0.209924\ttrain-error:0.027697\n",
      "[64]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[65]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[66]\teval-error:0.221374\ttrain-error:0.027697\n",
      "[67]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[68]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[69]\teval-error:0.209924\ttrain-error:0.027697\n",
      "[0]\teval-error:0.274809\ttrain-error:0.094266\n",
      "[1]\teval-error:0.244275\ttrain-error:0.062682\n",
      "[2]\teval-error:0.267176\ttrain-error:0.04033\n",
      "[3]\teval-error:0.259542\ttrain-error:0.031584\n",
      "[4]\teval-error:0.259542\ttrain-error:0.028669\n",
      "[5]\teval-error:0.251908\ttrain-error:0.028183\n",
      "[6]\teval-error:0.248092\ttrain-error:0.028183\n",
      "[7]\teval-error:0.255725\ttrain-error:0.028183\n",
      "[8]\teval-error:0.251908\ttrain-error:0.028183\n",
      "[9]\teval-error:0.255725\ttrain-error:0.027697\n",
      "[10]\teval-error:0.267176\ttrain-error:0.027697\n",
      "[11]\teval-error:0.259542\ttrain-error:0.027697\n",
      "[12]\teval-error:0.259542\ttrain-error:0.027697\n",
      "[13]\teval-error:0.259542\ttrain-error:0.027697\n",
      "[14]\teval-error:0.270992\ttrain-error:0.027697\n",
      "[15]\teval-error:0.267176\ttrain-error:0.027697\n",
      "[16]\teval-error:0.263359\ttrain-error:0.027697\n",
      "[17]\teval-error:0.263359\ttrain-error:0.027697\n",
      "[18]\teval-error:0.267176\ttrain-error:0.027697\n",
      "[19]\teval-error:0.263359\ttrain-error:0.027697\n",
      "[20]\teval-error:0.263359\ttrain-error:0.027697\n",
      "[21]\teval-error:0.255725\ttrain-error:0.027697\n",
      "[22]\teval-error:0.248092\ttrain-error:0.027697\n",
      "[23]\teval-error:0.244275\ttrain-error:0.027697\n",
      "[24]\teval-error:0.240458\ttrain-error:0.027697\n",
      "[25]\teval-error:0.251908\ttrain-error:0.027697\n",
      "[26]\teval-error:0.259542\ttrain-error:0.027697\n",
      "[27]\teval-error:0.244275\ttrain-error:0.027697\n",
      "[28]\teval-error:0.251908\ttrain-error:0.027697\n",
      "[29]\teval-error:0.240458\ttrain-error:0.027697\n",
      "[30]\teval-error:0.248092\ttrain-error:0.027697\n",
      "[31]\teval-error:0.240458\ttrain-error:0.027697\n",
      "[32]\teval-error:0.240458\ttrain-error:0.027697\n",
      "[33]\teval-error:0.240458\ttrain-error:0.027697\n",
      "[34]\teval-error:0.232824\ttrain-error:0.027697\n",
      "[35]\teval-error:0.240458\ttrain-error:0.027697\n",
      "[36]\teval-error:0.244275\ttrain-error:0.027697\n",
      "[37]\teval-error:0.240458\ttrain-error:0.027697\n",
      "[38]\teval-error:0.240458\ttrain-error:0.027697\n",
      "[39]\teval-error:0.236641\ttrain-error:0.027697\n",
      "[40]\teval-error:0.240458\ttrain-error:0.027697\n",
      "[41]\teval-error:0.244275\ttrain-error:0.027697\n",
      "[42]\teval-error:0.248092\ttrain-error:0.027697\n",
      "[43]\teval-error:0.248092\ttrain-error:0.027697\n",
      "[44]\teval-error:0.240458\ttrain-error:0.027697\n",
      "[45]\teval-error:0.248092\ttrain-error:0.027697\n",
      "[46]\teval-error:0.240458\ttrain-error:0.027697\n",
      "[47]\teval-error:0.244275\ttrain-error:0.027697\n",
      "[48]\teval-error:0.255725\ttrain-error:0.027697\n",
      "[49]\teval-error:0.248092\ttrain-error:0.027697\n",
      "[50]\teval-error:0.251908\ttrain-error:0.027697\n",
      "[51]\teval-error:0.255725\ttrain-error:0.027697\n",
      "[52]\teval-error:0.259542\ttrain-error:0.027697\n",
      "[53]\teval-error:0.255725\ttrain-error:0.027697\n",
      "[54]\teval-error:0.248092\ttrain-error:0.027697\n",
      "[55]\teval-error:0.255725\ttrain-error:0.027697\n",
      "[56]\teval-error:0.255725\ttrain-error:0.027697\n",
      "[57]\teval-error:0.259542\ttrain-error:0.027697\n",
      "[58]\teval-error:0.248092\ttrain-error:0.027697\n",
      "[59]\teval-error:0.251908\ttrain-error:0.027697\n",
      "[60]\teval-error:0.255725\ttrain-error:0.027697\n",
      "[61]\teval-error:0.259542\ttrain-error:0.027697\n",
      "[62]\teval-error:0.259542\ttrain-error:0.027697\n",
      "[63]\teval-error:0.259542\ttrain-error:0.027697\n",
      "[64]\teval-error:0.255725\ttrain-error:0.027697\n",
      "[65]\teval-error:0.255725\ttrain-error:0.027697\n",
      "[66]\teval-error:0.251908\ttrain-error:0.027697\n",
      "[67]\teval-error:0.259542\ttrain-error:0.027697\n",
      "[68]\teval-error:0.255725\ttrain-error:0.027697\n",
      "[69]\teval-error:0.248092\ttrain-error:0.027697\n",
      "[0]\teval-error:0.248092\ttrain-error:0.100583\n",
      "[1]\teval-error:0.206107\ttrain-error:0.066569\n",
      "[2]\teval-error:0.194656\ttrain-error:0.040816\n",
      "[3]\teval-error:0.225191\ttrain-error:0.033528\n",
      "[4]\teval-error:0.225191\ttrain-error:0.030126\n",
      "[5]\teval-error:0.221374\ttrain-error:0.029155\n",
      "[6]\teval-error:0.21374\ttrain-error:0.028183\n",
      "[7]\teval-error:0.217557\ttrain-error:0.028669\n",
      "[8]\teval-error:0.21374\ttrain-error:0.028183\n",
      "[9]\teval-error:0.21374\ttrain-error:0.028183\n",
      "[10]\teval-error:0.221374\ttrain-error:0.028183\n",
      "[11]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[12]\teval-error:0.221374\ttrain-error:0.027697\n",
      "[13]\teval-error:0.206107\ttrain-error:0.027697\n",
      "[14]\teval-error:0.206107\ttrain-error:0.027697\n",
      "[15]\teval-error:0.209924\ttrain-error:0.027697\n",
      "[16]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[17]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[18]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[19]\teval-error:0.225191\ttrain-error:0.027697\n",
      "[20]\teval-error:0.221374\ttrain-error:0.027697\n",
      "[21]\teval-error:0.221374\ttrain-error:0.027697\n",
      "[22]\teval-error:0.225191\ttrain-error:0.027697\n",
      "[23]\teval-error:0.229008\ttrain-error:0.027697\n",
      "[24]\teval-error:0.225191\ttrain-error:0.027697\n",
      "[25]\teval-error:0.221374\ttrain-error:0.027697\n",
      "[26]\teval-error:0.232824\ttrain-error:0.027697\n",
      "[27]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[28]\teval-error:0.206107\ttrain-error:0.027697\n",
      "[29]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[30]\teval-error:0.206107\ttrain-error:0.027697\n",
      "[31]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[32]\teval-error:0.209924\ttrain-error:0.027697\n",
      "[33]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[34]\teval-error:0.206107\ttrain-error:0.027697\n",
      "[35]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[36]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[37]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[38]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[39]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[40]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[41]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[42]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[43]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[44]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[45]\teval-error:0.221374\ttrain-error:0.027697\n",
      "[46]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[47]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[48]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[49]\teval-error:0.209924\ttrain-error:0.027697\n",
      "[50]\teval-error:0.225191\ttrain-error:0.027697\n",
      "[51]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[52]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[53]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[54]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[55]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[56]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[57]\teval-error:0.209924\ttrain-error:0.027697\n",
      "[58]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[59]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[60]\teval-error:0.209924\ttrain-error:0.027697\n",
      "[61]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[62]\teval-error:0.217557\ttrain-error:0.027697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[63]\teval-error:0.209924\ttrain-error:0.027697\n",
      "[64]\teval-error:0.209924\ttrain-error:0.027697\n",
      "[65]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[66]\teval-error:0.21374\ttrain-error:0.027697\n",
      "[67]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[68]\teval-error:0.225191\ttrain-error:0.027697\n",
      "[69]\teval-error:0.217557\ttrain-error:0.027697\n",
      "[0]\teval-error:0.282443\ttrain-error:0.098154\n",
      "[1]\teval-error:0.248092\ttrain-error:0.063654\n",
      "[2]\teval-error:0.240458\ttrain-error:0.04033\n",
      "[3]\teval-error:0.263359\ttrain-error:0.031098\n",
      "[4]\teval-error:0.244275\ttrain-error:0.02964\n",
      "[5]\teval-error:0.236641\ttrain-error:0.027211\n",
      "[6]\teval-error:0.240458\ttrain-error:0.027211\n",
      "[7]\teval-error:0.225191\ttrain-error:0.027211\n",
      "[8]\teval-error:0.225191\ttrain-error:0.027211\n",
      "[9]\teval-error:0.221374\ttrain-error:0.027211\n",
      "[10]\teval-error:0.225191\ttrain-error:0.027211\n",
      "[11]\teval-error:0.229008\ttrain-error:0.027211\n",
      "[12]\teval-error:0.209924\ttrain-error:0.027211\n",
      "[13]\teval-error:0.221374\ttrain-error:0.027211\n",
      "[14]\teval-error:0.217557\ttrain-error:0.027211\n",
      "[15]\teval-error:0.229008\ttrain-error:0.027211\n",
      "[16]\teval-error:0.232824\ttrain-error:0.027211\n",
      "[17]\teval-error:0.217557\ttrain-error:0.027211\n",
      "[18]\teval-error:0.21374\ttrain-error:0.027211\n",
      "[19]\teval-error:0.209924\ttrain-error:0.027211\n",
      "[20]\teval-error:0.209924\ttrain-error:0.027211\n",
      "[21]\teval-error:0.209924\ttrain-error:0.027211\n",
      "[22]\teval-error:0.21374\ttrain-error:0.027211\n",
      "[23]\teval-error:0.209924\ttrain-error:0.027211\n",
      "[24]\teval-error:0.206107\ttrain-error:0.027211\n",
      "[25]\teval-error:0.209924\ttrain-error:0.027211\n",
      "[26]\teval-error:0.209924\ttrain-error:0.027211\n",
      "[27]\teval-error:0.217557\ttrain-error:0.027211\n",
      "[28]\teval-error:0.21374\ttrain-error:0.027211\n",
      "[29]\teval-error:0.209924\ttrain-error:0.027211\n",
      "[30]\teval-error:0.206107\ttrain-error:0.027211\n",
      "[31]\teval-error:0.206107\ttrain-error:0.027211\n",
      "[32]\teval-error:0.206107\ttrain-error:0.027211\n",
      "[33]\teval-error:0.209924\ttrain-error:0.027211\n",
      "[34]\teval-error:0.206107\ttrain-error:0.027211\n",
      "[35]\teval-error:0.217557\ttrain-error:0.027211\n",
      "[36]\teval-error:0.209924\ttrain-error:0.027211\n",
      "[37]\teval-error:0.209924\ttrain-error:0.027211\n",
      "[38]\teval-error:0.221374\ttrain-error:0.027211\n",
      "[39]\teval-error:0.206107\ttrain-error:0.027211\n",
      "[40]\teval-error:0.209924\ttrain-error:0.027211\n",
      "[41]\teval-error:0.206107\ttrain-error:0.027211\n",
      "[42]\teval-error:0.209924\ttrain-error:0.027211\n",
      "[43]\teval-error:0.20229\ttrain-error:0.027211\n",
      "[44]\teval-error:0.206107\ttrain-error:0.027211\n",
      "[45]\teval-error:0.209924\ttrain-error:0.027211\n",
      "[46]\teval-error:0.20229\ttrain-error:0.027211\n",
      "[47]\teval-error:0.206107\ttrain-error:0.027211\n",
      "[48]\teval-error:0.20229\ttrain-error:0.027211\n",
      "[49]\teval-error:0.206107\ttrain-error:0.027211\n",
      "[50]\teval-error:0.206107\ttrain-error:0.027211\n",
      "[51]\teval-error:0.209924\ttrain-error:0.027211\n",
      "[52]\teval-error:0.206107\ttrain-error:0.027211\n",
      "[53]\teval-error:0.206107\ttrain-error:0.027211\n",
      "[54]\teval-error:0.209924\ttrain-error:0.027211\n",
      "[55]\teval-error:0.21374\ttrain-error:0.027211\n",
      "[56]\teval-error:0.198473\ttrain-error:0.027211\n",
      "[57]\teval-error:0.20229\ttrain-error:0.027211\n",
      "[58]\teval-error:0.209924\ttrain-error:0.027211\n",
      "[59]\teval-error:0.206107\ttrain-error:0.027211\n",
      "[60]\teval-error:0.206107\ttrain-error:0.027211\n",
      "[61]\teval-error:0.206107\ttrain-error:0.027211\n",
      "[62]\teval-error:0.209924\ttrain-error:0.027211\n",
      "[63]\teval-error:0.198473\ttrain-error:0.027211\n",
      "[64]\teval-error:0.20229\ttrain-error:0.027211\n",
      "[65]\teval-error:0.20229\ttrain-error:0.027211\n",
      "[66]\teval-error:0.206107\ttrain-error:0.027211\n",
      "[67]\teval-error:0.206107\ttrain-error:0.027211\n",
      "[68]\teval-error:0.198473\ttrain-error:0.027211\n",
      "[69]\teval-error:0.20229\ttrain-error:0.027211\n",
      "[0]\teval-error:0.340909\ttrain-error:0.119777\n",
      "[1]\teval-error:0.295455\ttrain-error:0.077994\n",
      "[2]\teval-error:0.318182\ttrain-error:0.061281\n",
      "[3]\teval-error:0.284091\ttrain-error:0.038997\n",
      "[4]\teval-error:0.261364\ttrain-error:0.026462\n",
      "[5]\teval-error:0.227273\ttrain-error:0.02507\n",
      "[6]\teval-error:0.25\ttrain-error:0.023677\n",
      "[7]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[8]\teval-error:0.261364\ttrain-error:0.023677\n",
      "[9]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[10]\teval-error:0.227273\ttrain-error:0.023677\n",
      "[11]\teval-error:0.25\ttrain-error:0.023677\n",
      "[12]\teval-error:0.261364\ttrain-error:0.023677\n",
      "[13]\teval-error:0.261364\ttrain-error:0.023677\n",
      "[14]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[15]\teval-error:0.227273\ttrain-error:0.023677\n",
      "[16]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[17]\teval-error:0.261364\ttrain-error:0.023677\n",
      "[18]\teval-error:0.227273\ttrain-error:0.023677\n",
      "[19]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[20]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[21]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[22]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[23]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[24]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[25]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[26]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[27]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[28]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[29]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[30]\teval-error:0.261364\ttrain-error:0.023677\n",
      "[31]\teval-error:0.261364\ttrain-error:0.023677\n",
      "[32]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[33]\teval-error:0.261364\ttrain-error:0.023677\n",
      "[34]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[35]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[36]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[37]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[38]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[39]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[40]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[41]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[42]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[43]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[44]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[45]\teval-error:0.227273\ttrain-error:0.023677\n",
      "[46]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[47]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[48]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[49]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[50]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[51]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[52]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[53]\teval-error:0.227273\ttrain-error:0.023677\n",
      "[54]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[55]\teval-error:0.227273\ttrain-error:0.023677\n",
      "[56]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[57]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[58]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[59]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[60]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[61]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[62]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[63]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[64]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[65]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[66]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[67]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[68]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[69]\teval-error:0.238636\ttrain-error:0.023677\n",
      "[0]\teval-error:0.318182\ttrain-error:0.133705\n",
      "[1]\teval-error:0.284091\ttrain-error:0.100279\n",
      "[2]\teval-error:0.238636\ttrain-error:0.069638\n",
      "[3]\teval-error:0.272727\ttrain-error:0.058496\n",
      "[4]\teval-error:0.261364\ttrain-error:0.050139\n",
      "[5]\teval-error:0.272727\ttrain-error:0.044568\n",
      "[6]\teval-error:0.284091\ttrain-error:0.043175\n",
      "[7]\teval-error:0.238636\ttrain-error:0.038997\n",
      "[8]\teval-error:0.238636\ttrain-error:0.037604\n",
      "[9]\teval-error:0.238636\ttrain-error:0.037604\n",
      "[10]\teval-error:0.238636\ttrain-error:0.037604\n",
      "[11]\teval-error:0.238636\ttrain-error:0.036212\n",
      "[12]\teval-error:0.25\ttrain-error:0.036212\n",
      "[13]\teval-error:0.25\ttrain-error:0.036212\n",
      "[14]\teval-error:0.238636\ttrain-error:0.036212\n",
      "[15]\teval-error:0.227273\ttrain-error:0.036212\n",
      "[16]\teval-error:0.238636\ttrain-error:0.036212\n",
      "[17]\teval-error:0.261364\ttrain-error:0.036212\n",
      "[18]\teval-error:0.25\ttrain-error:0.036212\n",
      "[19]\teval-error:0.25\ttrain-error:0.036212\n",
      "[20]\teval-error:0.25\ttrain-error:0.036212\n",
      "[21]\teval-error:0.25\ttrain-error:0.036212\n",
      "[22]\teval-error:0.25\ttrain-error:0.036212\n",
      "[23]\teval-error:0.25\ttrain-error:0.036212\n",
      "[24]\teval-error:0.261364\ttrain-error:0.036212\n",
      "[25]\teval-error:0.261364\ttrain-error:0.036212\n",
      "[26]\teval-error:0.25\ttrain-error:0.036212\n",
      "[27]\teval-error:0.25\ttrain-error:0.036212\n",
      "[28]\teval-error:0.261364\ttrain-error:0.036212\n",
      "[29]\teval-error:0.25\ttrain-error:0.036212\n",
      "[30]\teval-error:0.261364\ttrain-error:0.036212\n",
      "[31]\teval-error:0.25\ttrain-error:0.036212\n",
      "[32]\teval-error:0.25\ttrain-error:0.036212\n",
      "[33]\teval-error:0.25\ttrain-error:0.036212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34]\teval-error:0.25\ttrain-error:0.036212\n",
      "[35]\teval-error:0.25\ttrain-error:0.036212\n",
      "[36]\teval-error:0.25\ttrain-error:0.036212\n",
      "[37]\teval-error:0.261364\ttrain-error:0.036212\n",
      "[38]\teval-error:0.25\ttrain-error:0.036212\n",
      "[39]\teval-error:0.25\ttrain-error:0.036212\n",
      "[40]\teval-error:0.261364\ttrain-error:0.036212\n",
      "[41]\teval-error:0.261364\ttrain-error:0.036212\n",
      "[42]\teval-error:0.261364\ttrain-error:0.036212\n",
      "[43]\teval-error:0.261364\ttrain-error:0.036212\n",
      "[44]\teval-error:0.261364\ttrain-error:0.036212\n",
      "[45]\teval-error:0.261364\ttrain-error:0.036212\n",
      "[46]\teval-error:0.261364\ttrain-error:0.036212\n",
      "[47]\teval-error:0.261364\ttrain-error:0.036212\n",
      "[48]\teval-error:0.261364\ttrain-error:0.036212\n",
      "[49]\teval-error:0.261364\ttrain-error:0.036212\n",
      "[50]\teval-error:0.261364\ttrain-error:0.036212\n",
      "[51]\teval-error:0.261364\ttrain-error:0.036212\n",
      "[52]\teval-error:0.261364\ttrain-error:0.036212\n",
      "[53]\teval-error:0.261364\ttrain-error:0.036212\n",
      "[54]\teval-error:0.261364\ttrain-error:0.036212\n",
      "[55]\teval-error:0.261364\ttrain-error:0.036212\n",
      "[56]\teval-error:0.261364\ttrain-error:0.036212\n",
      "[57]\teval-error:0.261364\ttrain-error:0.036212\n",
      "[58]\teval-error:0.261364\ttrain-error:0.036212\n",
      "[59]\teval-error:0.261364\ttrain-error:0.036212\n",
      "[60]\teval-error:0.261364\ttrain-error:0.036212\n",
      "[61]\teval-error:0.261364\ttrain-error:0.036212\n",
      "[62]\teval-error:0.261364\ttrain-error:0.036212\n",
      "[63]\teval-error:0.261364\ttrain-error:0.036212\n",
      "[64]\teval-error:0.261364\ttrain-error:0.036212\n",
      "[65]\teval-error:0.261364\ttrain-error:0.036212\n",
      "[66]\teval-error:0.261364\ttrain-error:0.036212\n",
      "[67]\teval-error:0.261364\ttrain-error:0.036212\n",
      "[68]\teval-error:0.261364\ttrain-error:0.036212\n",
      "[69]\teval-error:0.261364\ttrain-error:0.036212\n",
      "[0]\teval-error:0.272727\ttrain-error:0.149025\n",
      "[1]\teval-error:0.204545\ttrain-error:0.133705\n",
      "[2]\teval-error:0.215909\ttrain-error:0.101671\n",
      "[3]\teval-error:0.181818\ttrain-error:0.079387\n",
      "[4]\teval-error:0.193182\ttrain-error:0.073816\n",
      "[5]\teval-error:0.181818\ttrain-error:0.066852\n",
      "[6]\teval-error:0.204545\ttrain-error:0.064067\n",
      "[7]\teval-error:0.215909\ttrain-error:0.059889\n",
      "[8]\teval-error:0.215909\ttrain-error:0.059889\n",
      "[9]\teval-error:0.215909\ttrain-error:0.058496\n",
      "[10]\teval-error:0.215909\ttrain-error:0.059889\n",
      "[11]\teval-error:0.227273\ttrain-error:0.058496\n",
      "[12]\teval-error:0.227273\ttrain-error:0.058496\n",
      "[13]\teval-error:0.227273\ttrain-error:0.057103\n",
      "[14]\teval-error:0.227273\ttrain-error:0.057103\n",
      "[15]\teval-error:0.215909\ttrain-error:0.057103\n",
      "[16]\teval-error:0.215909\ttrain-error:0.057103\n",
      "[17]\teval-error:0.227273\ttrain-error:0.057103\n",
      "[18]\teval-error:0.215909\ttrain-error:0.057103\n",
      "[19]\teval-error:0.227273\ttrain-error:0.057103\n",
      "[20]\teval-error:0.215909\ttrain-error:0.057103\n",
      "[21]\teval-error:0.215909\ttrain-error:0.057103\n",
      "[22]\teval-error:0.215909\ttrain-error:0.05571\n",
      "[23]\teval-error:0.215909\ttrain-error:0.05571\n",
      "[24]\teval-error:0.215909\ttrain-error:0.05571\n",
      "[25]\teval-error:0.215909\ttrain-error:0.054318\n",
      "[26]\teval-error:0.204545\ttrain-error:0.05571\n",
      "[27]\teval-error:0.204545\ttrain-error:0.054318\n",
      "[28]\teval-error:0.204545\ttrain-error:0.054318\n",
      "[29]\teval-error:0.215909\ttrain-error:0.052925\n",
      "[30]\teval-error:0.215909\ttrain-error:0.052925\n",
      "[31]\teval-error:0.215909\ttrain-error:0.052925\n",
      "[32]\teval-error:0.215909\ttrain-error:0.052925\n",
      "[33]\teval-error:0.215909\ttrain-error:0.052925\n",
      "[34]\teval-error:0.215909\ttrain-error:0.052925\n",
      "[35]\teval-error:0.215909\ttrain-error:0.052925\n",
      "[36]\teval-error:0.215909\ttrain-error:0.052925\n",
      "[37]\teval-error:0.215909\ttrain-error:0.052925\n",
      "[38]\teval-error:0.215909\ttrain-error:0.052925\n",
      "[39]\teval-error:0.215909\ttrain-error:0.052925\n",
      "[40]\teval-error:0.215909\ttrain-error:0.052925\n",
      "[41]\teval-error:0.215909\ttrain-error:0.052925\n",
      "[42]\teval-error:0.215909\ttrain-error:0.052925\n",
      "[43]\teval-error:0.215909\ttrain-error:0.052925\n",
      "[44]\teval-error:0.204545\ttrain-error:0.052925\n",
      "[45]\teval-error:0.204545\ttrain-error:0.052925\n",
      "[46]\teval-error:0.204545\ttrain-error:0.052925\n",
      "[47]\teval-error:0.204545\ttrain-error:0.052925\n",
      "[48]\teval-error:0.204545\ttrain-error:0.052925\n",
      "[49]\teval-error:0.204545\ttrain-error:0.052925\n",
      "[50]\teval-error:0.204545\ttrain-error:0.052925\n",
      "[51]\teval-error:0.204545\ttrain-error:0.052925\n",
      "[52]\teval-error:0.204545\ttrain-error:0.052925\n",
      "[53]\teval-error:0.204545\ttrain-error:0.052925\n",
      "[54]\teval-error:0.204545\ttrain-error:0.052925\n",
      "[55]\teval-error:0.204545\ttrain-error:0.052925\n",
      "[56]\teval-error:0.204545\ttrain-error:0.052925\n",
      "[57]\teval-error:0.204545\ttrain-error:0.052925\n",
      "[58]\teval-error:0.204545\ttrain-error:0.052925\n",
      "[59]\teval-error:0.204545\ttrain-error:0.052925\n",
      "[60]\teval-error:0.204545\ttrain-error:0.052925\n",
      "[61]\teval-error:0.204545\ttrain-error:0.052925\n",
      "[62]\teval-error:0.204545\ttrain-error:0.052925\n",
      "[63]\teval-error:0.204545\ttrain-error:0.052925\n",
      "[64]\teval-error:0.204545\ttrain-error:0.052925\n",
      "[65]\teval-error:0.204545\ttrain-error:0.052925\n",
      "[66]\teval-error:0.204545\ttrain-error:0.052925\n",
      "[67]\teval-error:0.204545\ttrain-error:0.052925\n",
      "[68]\teval-error:0.204545\ttrain-error:0.052925\n",
      "[69]\teval-error:0.204545\ttrain-error:0.052925\n",
      "[0]\teval-error:0.215909\ttrain-error:0.114206\n",
      "[1]\teval-error:0.215909\ttrain-error:0.090529\n",
      "[2]\teval-error:0.238636\ttrain-error:0.051532\n",
      "[3]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[4]\teval-error:0.204545\ttrain-error:0.027855\n",
      "[5]\teval-error:0.159091\ttrain-error:0.02507\n",
      "[6]\teval-error:0.159091\ttrain-error:0.019499\n",
      "[7]\teval-error:0.159091\ttrain-error:0.020891\n",
      "[8]\teval-error:0.159091\ttrain-error:0.019499\n",
      "[9]\teval-error:0.159091\ttrain-error:0.019499\n",
      "[10]\teval-error:0.159091\ttrain-error:0.018106\n",
      "[11]\teval-error:0.159091\ttrain-error:0.018106\n",
      "[12]\teval-error:0.159091\ttrain-error:0.018106\n",
      "[13]\teval-error:0.159091\ttrain-error:0.018106\n",
      "[14]\teval-error:0.159091\ttrain-error:0.018106\n",
      "[15]\teval-error:0.159091\ttrain-error:0.018106\n",
      "[16]\teval-error:0.159091\ttrain-error:0.018106\n",
      "[17]\teval-error:0.159091\ttrain-error:0.018106\n",
      "[18]\teval-error:0.159091\ttrain-error:0.018106\n",
      "[19]\teval-error:0.159091\ttrain-error:0.018106\n",
      "[20]\teval-error:0.159091\ttrain-error:0.018106\n",
      "[21]\teval-error:0.159091\ttrain-error:0.018106\n",
      "[22]\teval-error:0.159091\ttrain-error:0.018106\n",
      "[23]\teval-error:0.159091\ttrain-error:0.018106\n",
      "[24]\teval-error:0.159091\ttrain-error:0.018106\n",
      "[25]\teval-error:0.159091\ttrain-error:0.018106\n",
      "[26]\teval-error:0.159091\ttrain-error:0.018106\n",
      "[27]\teval-error:0.159091\ttrain-error:0.018106\n",
      "[28]\teval-error:0.159091\ttrain-error:0.018106\n",
      "[29]\teval-error:0.159091\ttrain-error:0.018106\n",
      "[30]\teval-error:0.159091\ttrain-error:0.018106\n",
      "[31]\teval-error:0.147727\ttrain-error:0.018106\n",
      "[32]\teval-error:0.147727\ttrain-error:0.018106\n",
      "[33]\teval-error:0.159091\ttrain-error:0.018106\n",
      "[34]\teval-error:0.170455\ttrain-error:0.018106\n",
      "[35]\teval-error:0.170455\ttrain-error:0.018106\n",
      "[36]\teval-error:0.159091\ttrain-error:0.018106\n",
      "[37]\teval-error:0.159091\ttrain-error:0.018106\n",
      "[38]\teval-error:0.147727\ttrain-error:0.018106\n",
      "[39]\teval-error:0.147727\ttrain-error:0.018106\n",
      "[40]\teval-error:0.147727\ttrain-error:0.018106\n",
      "[41]\teval-error:0.147727\ttrain-error:0.018106\n",
      "[42]\teval-error:0.147727\ttrain-error:0.018106\n",
      "[43]\teval-error:0.147727\ttrain-error:0.018106\n",
      "[44]\teval-error:0.159091\ttrain-error:0.018106\n",
      "[45]\teval-error:0.170455\ttrain-error:0.018106\n",
      "[46]\teval-error:0.159091\ttrain-error:0.018106\n",
      "[47]\teval-error:0.159091\ttrain-error:0.018106\n",
      "[48]\teval-error:0.159091\ttrain-error:0.018106\n",
      "[49]\teval-error:0.159091\ttrain-error:0.018106\n",
      "[50]\teval-error:0.170455\ttrain-error:0.018106\n",
      "[51]\teval-error:0.170455\ttrain-error:0.018106\n",
      "[52]\teval-error:0.159091\ttrain-error:0.018106\n",
      "[53]\teval-error:0.170455\ttrain-error:0.018106\n",
      "[54]\teval-error:0.159091\ttrain-error:0.018106\n",
      "[55]\teval-error:0.159091\ttrain-error:0.018106\n",
      "[56]\teval-error:0.170455\ttrain-error:0.018106\n",
      "[57]\teval-error:0.159091\ttrain-error:0.018106\n",
      "[58]\teval-error:0.147727\ttrain-error:0.018106\n",
      "[59]\teval-error:0.147727\ttrain-error:0.018106\n",
      "[60]\teval-error:0.147727\ttrain-error:0.018106\n",
      "[61]\teval-error:0.147727\ttrain-error:0.018106\n",
      "[62]\teval-error:0.159091\ttrain-error:0.018106\n",
      "[63]\teval-error:0.147727\ttrain-error:0.018106\n",
      "[64]\teval-error:0.147727\ttrain-error:0.018106\n",
      "[65]\teval-error:0.147727\ttrain-error:0.018106\n",
      "[66]\teval-error:0.147727\ttrain-error:0.018106\n",
      "[67]\teval-error:0.147727\ttrain-error:0.018106\n",
      "[68]\teval-error:0.147727\ttrain-error:0.018106\n",
      "[69]\teval-error:0.147727\ttrain-error:0.018106\n",
      "[0]\teval-error:0.319149\ttrain-error:0.154085\n",
      "[1]\teval-error:0.340426\ttrain-error:0.109617\n",
      "[2]\teval-error:0.319149\ttrain-error:0.083764\n",
      "[3]\teval-error:0.287234\ttrain-error:0.070321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4]\teval-error:0.276596\ttrain-error:0.063082\n",
      "[5]\teval-error:0.265957\ttrain-error:0.059979\n",
      "[6]\teval-error:0.265957\ttrain-error:0.057911\n",
      "[7]\teval-error:0.287234\ttrain-error:0.057911\n",
      "[8]\teval-error:0.297872\ttrain-error:0.057911\n",
      "[9]\teval-error:0.297872\ttrain-error:0.057911\n",
      "[10]\teval-error:0.287234\ttrain-error:0.057911\n",
      "[11]\teval-error:0.287234\ttrain-error:0.057911\n",
      "[12]\teval-error:0.276596\ttrain-error:0.057911\n",
      "[13]\teval-error:0.276596\ttrain-error:0.057911\n",
      "[14]\teval-error:0.287234\ttrain-error:0.057911\n",
      "[15]\teval-error:0.276596\ttrain-error:0.057911\n",
      "[16]\teval-error:0.276596\ttrain-error:0.057911\n",
      "[17]\teval-error:0.276596\ttrain-error:0.057911\n",
      "[18]\teval-error:0.265957\ttrain-error:0.057911\n",
      "[19]\teval-error:0.255319\ttrain-error:0.057911\n",
      "[20]\teval-error:0.255319\ttrain-error:0.057911\n",
      "[21]\teval-error:0.265957\ttrain-error:0.057911\n",
      "[22]\teval-error:0.265957\ttrain-error:0.057911\n",
      "[23]\teval-error:0.255319\ttrain-error:0.057911\n",
      "[24]\teval-error:0.265957\ttrain-error:0.057911\n",
      "[25]\teval-error:0.276596\ttrain-error:0.057911\n",
      "[26]\teval-error:0.287234\ttrain-error:0.057911\n",
      "[27]\teval-error:0.276596\ttrain-error:0.057911\n",
      "[28]\teval-error:0.265957\ttrain-error:0.057911\n",
      "[29]\teval-error:0.265957\ttrain-error:0.057911\n",
      "[30]\teval-error:0.265957\ttrain-error:0.057911\n",
      "[31]\teval-error:0.265957\ttrain-error:0.057911\n",
      "[32]\teval-error:0.265957\ttrain-error:0.057911\n",
      "[33]\teval-error:0.265957\ttrain-error:0.057911\n",
      "[34]\teval-error:0.265957\ttrain-error:0.057911\n",
      "[35]\teval-error:0.265957\ttrain-error:0.057911\n",
      "[36]\teval-error:0.265957\ttrain-error:0.057911\n",
      "[37]\teval-error:0.265957\ttrain-error:0.057911\n",
      "[38]\teval-error:0.265957\ttrain-error:0.057911\n",
      "[39]\teval-error:0.276596\ttrain-error:0.057911\n",
      "[40]\teval-error:0.276596\ttrain-error:0.057911\n",
      "[41]\teval-error:0.276596\ttrain-error:0.057911\n",
      "[42]\teval-error:0.276596\ttrain-error:0.057911\n",
      "[43]\teval-error:0.276596\ttrain-error:0.057911\n",
      "[44]\teval-error:0.276596\ttrain-error:0.057911\n",
      "[45]\teval-error:0.287234\ttrain-error:0.057911\n",
      "[46]\teval-error:0.276596\ttrain-error:0.057911\n",
      "[47]\teval-error:0.276596\ttrain-error:0.057911\n",
      "[48]\teval-error:0.276596\ttrain-error:0.057911\n",
      "[49]\teval-error:0.276596\ttrain-error:0.057911\n",
      "[50]\teval-error:0.276596\ttrain-error:0.057911\n",
      "[51]\teval-error:0.276596\ttrain-error:0.057911\n",
      "[52]\teval-error:0.276596\ttrain-error:0.057911\n",
      "[53]\teval-error:0.276596\ttrain-error:0.057911\n",
      "[54]\teval-error:0.276596\ttrain-error:0.057911\n",
      "[55]\teval-error:0.276596\ttrain-error:0.057911\n",
      "[56]\teval-error:0.276596\ttrain-error:0.057911\n",
      "[57]\teval-error:0.265957\ttrain-error:0.057911\n",
      "[58]\teval-error:0.265957\ttrain-error:0.057911\n",
      "[59]\teval-error:0.265957\ttrain-error:0.057911\n",
      "[60]\teval-error:0.265957\ttrain-error:0.057911\n",
      "[61]\teval-error:0.265957\ttrain-error:0.057911\n",
      "[62]\teval-error:0.265957\ttrain-error:0.057911\n",
      "[63]\teval-error:0.265957\ttrain-error:0.057911\n",
      "[64]\teval-error:0.265957\ttrain-error:0.057911\n",
      "[65]\teval-error:0.265957\ttrain-error:0.057911\n",
      "[66]\teval-error:0.276596\ttrain-error:0.057911\n",
      "[67]\teval-error:0.287234\ttrain-error:0.057911\n",
      "[68]\teval-error:0.287234\ttrain-error:0.057911\n",
      "[69]\teval-error:0.265957\ttrain-error:0.057911\n",
      "[0]\teval-error:0.255319\ttrain-error:0.184074\n",
      "[1]\teval-error:0.234043\ttrain-error:0.143744\n",
      "[2]\teval-error:0.212766\ttrain-error:0.109617\n",
      "[3]\teval-error:0.234043\ttrain-error:0.10031\n",
      "[4]\teval-error:0.223404\ttrain-error:0.091003\n",
      "[5]\teval-error:0.223404\ttrain-error:0.085832\n",
      "[6]\teval-error:0.212766\ttrain-error:0.08273\n",
      "[7]\teval-error:0.244681\ttrain-error:0.080662\n",
      "[8]\teval-error:0.234043\ttrain-error:0.078594\n",
      "[9]\teval-error:0.223404\ttrain-error:0.077559\n",
      "[10]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[11]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[12]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[13]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[14]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[15]\teval-error:0.223404\ttrain-error:0.077559\n",
      "[16]\teval-error:0.223404\ttrain-error:0.077559\n",
      "[17]\teval-error:0.223404\ttrain-error:0.077559\n",
      "[18]\teval-error:0.223404\ttrain-error:0.077559\n",
      "[19]\teval-error:0.223404\ttrain-error:0.077559\n",
      "[20]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[21]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[22]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[23]\teval-error:0.223404\ttrain-error:0.077559\n",
      "[24]\teval-error:0.223404\ttrain-error:0.077559\n",
      "[25]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[26]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[27]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[28]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[29]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[30]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[31]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[32]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[33]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[34]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[35]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[36]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[37]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[38]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[39]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[40]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[41]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[42]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[43]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[44]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[45]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[46]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[47]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[48]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[49]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[50]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[51]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[52]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[53]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[54]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[55]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[56]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[57]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[58]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[59]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[60]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[61]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[62]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[63]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[64]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[65]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[66]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[67]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[68]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[69]\teval-error:0.212766\ttrain-error:0.077559\n",
      "[0]\teval-error:0.234043\ttrain-error:0.196484\n",
      "[1]\teval-error:0.223404\ttrain-error:0.174767\n",
      "[2]\teval-error:0.202128\ttrain-error:0.155119\n",
      "[3]\teval-error:0.223404\ttrain-error:0.136505\n",
      "[4]\teval-error:0.223404\ttrain-error:0.125129\n",
      "[5]\teval-error:0.223404\ttrain-error:0.11789\n",
      "[6]\teval-error:0.212766\ttrain-error:0.11272\n",
      "[7]\teval-error:0.202128\ttrain-error:0.11272\n",
      "[8]\teval-error:0.223404\ttrain-error:0.107549\n",
      "[9]\teval-error:0.212766\ttrain-error:0.107549\n",
      "[10]\teval-error:0.234043\ttrain-error:0.106515\n",
      "[11]\teval-error:0.223404\ttrain-error:0.106515\n",
      "[12]\teval-error:0.223404\ttrain-error:0.106515\n",
      "[13]\teval-error:0.234043\ttrain-error:0.106515\n",
      "[14]\teval-error:0.255319\ttrain-error:0.107549\n",
      "[15]\teval-error:0.244681\ttrain-error:0.106515\n",
      "[16]\teval-error:0.255319\ttrain-error:0.106515\n",
      "[17]\teval-error:0.255319\ttrain-error:0.106515\n",
      "[18]\teval-error:0.244681\ttrain-error:0.106515\n",
      "[19]\teval-error:0.255319\ttrain-error:0.105481\n",
      "[20]\teval-error:0.255319\ttrain-error:0.106515\n",
      "[21]\teval-error:0.244681\ttrain-error:0.105481\n",
      "[22]\teval-error:0.234043\ttrain-error:0.105481\n",
      "[23]\teval-error:0.244681\ttrain-error:0.106515\n",
      "[24]\teval-error:0.255319\ttrain-error:0.105481\n",
      "[25]\teval-error:0.244681\ttrain-error:0.105481\n",
      "[26]\teval-error:0.255319\ttrain-error:0.105481\n",
      "[27]\teval-error:0.234043\ttrain-error:0.105481\n",
      "[28]\teval-error:0.255319\ttrain-error:0.105481\n",
      "[29]\teval-error:0.234043\ttrain-error:0.105481\n",
      "[30]\teval-error:0.244681\ttrain-error:0.105481\n",
      "[31]\teval-error:0.244681\ttrain-error:0.105481\n",
      "[32]\teval-error:0.234043\ttrain-error:0.105481\n",
      "[33]\teval-error:0.234043\ttrain-error:0.105481\n",
      "[34]\teval-error:0.234043\ttrain-error:0.105481\n",
      "[35]\teval-error:0.244681\ttrain-error:0.105481\n",
      "[36]\teval-error:0.244681\ttrain-error:0.105481\n",
      "[37]\teval-error:0.244681\ttrain-error:0.105481\n",
      "[38]\teval-error:0.244681\ttrain-error:0.105481\n",
      "[39]\teval-error:0.244681\ttrain-error:0.105481\n",
      "[40]\teval-error:0.244681\ttrain-error:0.105481\n",
      "[41]\teval-error:0.244681\ttrain-error:0.105481\n",
      "[42]\teval-error:0.234043\ttrain-error:0.105481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43]\teval-error:0.244681\ttrain-error:0.105481\n",
      "[44]\teval-error:0.234043\ttrain-error:0.105481\n",
      "[45]\teval-error:0.234043\ttrain-error:0.105481\n",
      "[46]\teval-error:0.234043\ttrain-error:0.105481\n",
      "[47]\teval-error:0.234043\ttrain-error:0.105481\n",
      "[48]\teval-error:0.234043\ttrain-error:0.105481\n",
      "[49]\teval-error:0.234043\ttrain-error:0.105481\n",
      "[50]\teval-error:0.234043\ttrain-error:0.105481\n",
      "[51]\teval-error:0.234043\ttrain-error:0.105481\n",
      "[52]\teval-error:0.234043\ttrain-error:0.105481\n",
      "[53]\teval-error:0.234043\ttrain-error:0.105481\n",
      "[54]\teval-error:0.234043\ttrain-error:0.105481\n",
      "[55]\teval-error:0.234043\ttrain-error:0.105481\n",
      "[56]\teval-error:0.234043\ttrain-error:0.105481\n",
      "[57]\teval-error:0.234043\ttrain-error:0.105481\n",
      "[58]\teval-error:0.234043\ttrain-error:0.105481\n",
      "[59]\teval-error:0.234043\ttrain-error:0.105481\n",
      "[60]\teval-error:0.234043\ttrain-error:0.105481\n",
      "[61]\teval-error:0.234043\ttrain-error:0.105481\n",
      "[62]\teval-error:0.234043\ttrain-error:0.105481\n",
      "[63]\teval-error:0.234043\ttrain-error:0.105481\n",
      "[64]\teval-error:0.234043\ttrain-error:0.105481\n",
      "[65]\teval-error:0.234043\ttrain-error:0.105481\n",
      "[66]\teval-error:0.234043\ttrain-error:0.105481\n",
      "[67]\teval-error:0.234043\ttrain-error:0.105481\n",
      "[68]\teval-error:0.234043\ttrain-error:0.105481\n",
      "[69]\teval-error:0.234043\ttrain-error:0.105481\n",
      "[0]\teval-error:0.361702\ttrain-error:0.14788\n",
      "[1]\teval-error:0.319149\ttrain-error:0.11272\n",
      "[2]\teval-error:0.276596\ttrain-error:0.08273\n",
      "[3]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[4]\teval-error:0.287234\ttrain-error:0.053775\n",
      "[5]\teval-error:0.265957\ttrain-error:0.05274\n",
      "[6]\teval-error:0.276596\ttrain-error:0.05274\n",
      "[7]\teval-error:0.255319\ttrain-error:0.05274\n",
      "[8]\teval-error:0.265957\ttrain-error:0.05274\n",
      "[9]\teval-error:0.287234\ttrain-error:0.05274\n",
      "[10]\teval-error:0.287234\ttrain-error:0.05274\n",
      "[11]\teval-error:0.265957\ttrain-error:0.05274\n",
      "[12]\teval-error:0.276596\ttrain-error:0.05274\n",
      "[13]\teval-error:0.265957\ttrain-error:0.05274\n",
      "[14]\teval-error:0.265957\ttrain-error:0.05274\n",
      "[15]\teval-error:0.287234\ttrain-error:0.05274\n",
      "[16]\teval-error:0.308511\ttrain-error:0.05274\n",
      "[17]\teval-error:0.287234\ttrain-error:0.05274\n",
      "[18]\teval-error:0.276596\ttrain-error:0.05274\n",
      "[19]\teval-error:0.287234\ttrain-error:0.05274\n",
      "[20]\teval-error:0.297872\ttrain-error:0.05274\n",
      "[21]\teval-error:0.308511\ttrain-error:0.05274\n",
      "[22]\teval-error:0.287234\ttrain-error:0.05274\n",
      "[23]\teval-error:0.308511\ttrain-error:0.05274\n",
      "[24]\teval-error:0.308511\ttrain-error:0.05274\n",
      "[25]\teval-error:0.297872\ttrain-error:0.05274\n",
      "[26]\teval-error:0.297872\ttrain-error:0.05274\n",
      "[27]\teval-error:0.308511\ttrain-error:0.05274\n",
      "[28]\teval-error:0.297872\ttrain-error:0.05274\n",
      "[29]\teval-error:0.308511\ttrain-error:0.05274\n",
      "[30]\teval-error:0.308511\ttrain-error:0.05274\n",
      "[31]\teval-error:0.319149\ttrain-error:0.05274\n",
      "[32]\teval-error:0.319149\ttrain-error:0.05274\n",
      "[33]\teval-error:0.319149\ttrain-error:0.05274\n",
      "[34]\teval-error:0.319149\ttrain-error:0.05274\n",
      "[35]\teval-error:0.319149\ttrain-error:0.05274\n",
      "[36]\teval-error:0.297872\ttrain-error:0.05274\n",
      "[37]\teval-error:0.276596\ttrain-error:0.05274\n",
      "[38]\teval-error:0.287234\ttrain-error:0.05274\n",
      "[39]\teval-error:0.297872\ttrain-error:0.05274\n",
      "[40]\teval-error:0.297872\ttrain-error:0.05274\n",
      "[41]\teval-error:0.287234\ttrain-error:0.05274\n",
      "[42]\teval-error:0.287234\ttrain-error:0.05274\n",
      "[43]\teval-error:0.297872\ttrain-error:0.05274\n",
      "[44]\teval-error:0.297872\ttrain-error:0.05274\n",
      "[45]\teval-error:0.287234\ttrain-error:0.05274\n",
      "[46]\teval-error:0.297872\ttrain-error:0.05274\n",
      "[47]\teval-error:0.287234\ttrain-error:0.05274\n",
      "[48]\teval-error:0.276596\ttrain-error:0.05274\n",
      "[49]\teval-error:0.276596\ttrain-error:0.05274\n",
      "[50]\teval-error:0.276596\ttrain-error:0.05274\n",
      "[51]\teval-error:0.287234\ttrain-error:0.05274\n",
      "[52]\teval-error:0.287234\ttrain-error:0.05274\n",
      "[53]\teval-error:0.308511\ttrain-error:0.05274\n",
      "[54]\teval-error:0.297872\ttrain-error:0.05274\n",
      "[55]\teval-error:0.308511\ttrain-error:0.05274\n",
      "[56]\teval-error:0.308511\ttrain-error:0.05274\n",
      "[57]\teval-error:0.287234\ttrain-error:0.05274\n",
      "[58]\teval-error:0.319149\ttrain-error:0.05274\n",
      "[59]\teval-error:0.297872\ttrain-error:0.05274\n",
      "[60]\teval-error:0.329787\ttrain-error:0.05274\n",
      "[61]\teval-error:0.319149\ttrain-error:0.05274\n",
      "[62]\teval-error:0.319149\ttrain-error:0.05274\n",
      "[63]\teval-error:0.329787\ttrain-error:0.05274\n",
      "[64]\teval-error:0.319149\ttrain-error:0.05274\n",
      "[65]\teval-error:0.308511\ttrain-error:0.05274\n",
      "[66]\teval-error:0.308511\ttrain-error:0.05274\n",
      "[67]\teval-error:0.297872\ttrain-error:0.05274\n",
      "[68]\teval-error:0.319149\ttrain-error:0.05274\n",
      "[69]\teval-error:0.319149\ttrain-error:0.05274\n",
      "[0]\teval-error:0.240458\ttrain-error:0.120019\n",
      "[1]\teval-error:0.267176\ttrain-error:0.092809\n",
      "[2]\teval-error:0.251908\ttrain-error:0.075316\n",
      "[3]\teval-error:0.240458\ttrain-error:0.06171\n",
      "[4]\teval-error:0.248092\ttrain-error:0.056365\n",
      "[5]\teval-error:0.236641\ttrain-error:0.051506\n",
      "[6]\teval-error:0.240458\ttrain-error:0.050534\n",
      "[7]\teval-error:0.244275\ttrain-error:0.050049\n",
      "[8]\teval-error:0.240458\ttrain-error:0.049563\n",
      "[9]\teval-error:0.255725\ttrain-error:0.049077\n",
      "[10]\teval-error:0.267176\ttrain-error:0.049077\n",
      "[11]\teval-error:0.240458\ttrain-error:0.049077\n",
      "[12]\teval-error:0.255725\ttrain-error:0.049077\n",
      "[13]\teval-error:0.244275\ttrain-error:0.049077\n",
      "[14]\teval-error:0.244275\ttrain-error:0.049077\n",
      "[15]\teval-error:0.251908\ttrain-error:0.049077\n",
      "[16]\teval-error:0.240458\ttrain-error:0.049077\n",
      "[17]\teval-error:0.244275\ttrain-error:0.049077\n",
      "[18]\teval-error:0.251908\ttrain-error:0.049077\n",
      "[19]\teval-error:0.232824\ttrain-error:0.049077\n",
      "[20]\teval-error:0.248092\ttrain-error:0.049077\n",
      "[21]\teval-error:0.244275\ttrain-error:0.049077\n",
      "[22]\teval-error:0.244275\ttrain-error:0.049077\n",
      "[23]\teval-error:0.259542\ttrain-error:0.049077\n",
      "[24]\teval-error:0.240458\ttrain-error:0.049077\n",
      "[25]\teval-error:0.232824\ttrain-error:0.049077\n",
      "[26]\teval-error:0.240458\ttrain-error:0.049077\n",
      "[27]\teval-error:0.244275\ttrain-error:0.049077\n",
      "[28]\teval-error:0.255725\ttrain-error:0.049077\n",
      "[29]\teval-error:0.232824\ttrain-error:0.049077\n",
      "[30]\teval-error:0.259542\ttrain-error:0.049077\n",
      "[31]\teval-error:0.255725\ttrain-error:0.049077\n",
      "[32]\teval-error:0.244275\ttrain-error:0.049077\n",
      "[33]\teval-error:0.240458\ttrain-error:0.049077\n",
      "[34]\teval-error:0.240458\ttrain-error:0.049077\n",
      "[35]\teval-error:0.232824\ttrain-error:0.049077\n",
      "[36]\teval-error:0.232824\ttrain-error:0.049077\n",
      "[37]\teval-error:0.236641\ttrain-error:0.049077\n",
      "[38]\teval-error:0.240458\ttrain-error:0.049077\n",
      "[39]\teval-error:0.244275\ttrain-error:0.049077\n",
      "[40]\teval-error:0.236641\ttrain-error:0.049077\n",
      "[41]\teval-error:0.251908\ttrain-error:0.049077\n",
      "[42]\teval-error:0.244275\ttrain-error:0.049077\n",
      "[43]\teval-error:0.232824\ttrain-error:0.049077\n",
      "[44]\teval-error:0.225191\ttrain-error:0.049077\n",
      "[45]\teval-error:0.229008\ttrain-error:0.049077\n",
      "[46]\teval-error:0.248092\ttrain-error:0.049077\n",
      "[47]\teval-error:0.232824\ttrain-error:0.049077\n",
      "[48]\teval-error:0.232824\ttrain-error:0.049077\n",
      "[49]\teval-error:0.244275\ttrain-error:0.049077\n",
      "[50]\teval-error:0.232824\ttrain-error:0.049077\n",
      "[51]\teval-error:0.244275\ttrain-error:0.049077\n",
      "[52]\teval-error:0.248092\ttrain-error:0.049077\n",
      "[53]\teval-error:0.244275\ttrain-error:0.049077\n",
      "[54]\teval-error:0.244275\ttrain-error:0.049077\n",
      "[55]\teval-error:0.232824\ttrain-error:0.049077\n",
      "[56]\teval-error:0.229008\ttrain-error:0.049077\n",
      "[57]\teval-error:0.229008\ttrain-error:0.049077\n",
      "[58]\teval-error:0.229008\ttrain-error:0.049077\n",
      "[59]\teval-error:0.229008\ttrain-error:0.049077\n",
      "[60]\teval-error:0.229008\ttrain-error:0.049077\n",
      "[61]\teval-error:0.244275\ttrain-error:0.049077\n",
      "[62]\teval-error:0.236641\ttrain-error:0.049077\n",
      "[63]\teval-error:0.240458\ttrain-error:0.049077\n",
      "[64]\teval-error:0.232824\ttrain-error:0.049077\n",
      "[65]\teval-error:0.232824\ttrain-error:0.049077\n",
      "[66]\teval-error:0.232824\ttrain-error:0.049077\n",
      "[67]\teval-error:0.225191\ttrain-error:0.049077\n",
      "[68]\teval-error:0.251908\ttrain-error:0.049077\n",
      "[69]\teval-error:0.244275\ttrain-error:0.049077\n",
      "[0]\teval-error:0.217557\ttrain-error:0.132167\n",
      "[1]\teval-error:0.232824\ttrain-error:0.104956\n",
      "[2]\teval-error:0.225191\ttrain-error:0.088921\n",
      "[3]\teval-error:0.229008\ttrain-error:0.073858\n",
      "[4]\teval-error:0.221374\ttrain-error:0.068027\n",
      "[5]\teval-error:0.240458\ttrain-error:0.065112\n",
      "[6]\teval-error:0.232824\ttrain-error:0.06171\n",
      "[7]\teval-error:0.229008\ttrain-error:0.059767\n",
      "[8]\teval-error:0.229008\ttrain-error:0.059767\n",
      "[9]\teval-error:0.217557\ttrain-error:0.059281\n",
      "[10]\teval-error:0.229008\ttrain-error:0.058795\n",
      "[11]\teval-error:0.225191\ttrain-error:0.058795\n",
      "[12]\teval-error:0.221374\ttrain-error:0.058795\n",
      "[13]\teval-error:0.221374\ttrain-error:0.058795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14]\teval-error:0.229008\ttrain-error:0.058795\n",
      "[15]\teval-error:0.225191\ttrain-error:0.058795\n",
      "[16]\teval-error:0.21374\ttrain-error:0.058795\n",
      "[17]\teval-error:0.229008\ttrain-error:0.058795\n",
      "[18]\teval-error:0.232824\ttrain-error:0.058795\n",
      "[19]\teval-error:0.225191\ttrain-error:0.058795\n",
      "[20]\teval-error:0.209924\ttrain-error:0.058795\n",
      "[21]\teval-error:0.217557\ttrain-error:0.058795\n",
      "[22]\teval-error:0.21374\ttrain-error:0.058795\n",
      "[23]\teval-error:0.21374\ttrain-error:0.058795\n",
      "[24]\teval-error:0.217557\ttrain-error:0.058795\n",
      "[25]\teval-error:0.20229\ttrain-error:0.058795\n",
      "[26]\teval-error:0.194656\ttrain-error:0.058795\n",
      "[27]\teval-error:0.20229\ttrain-error:0.058795\n",
      "[28]\teval-error:0.209924\ttrain-error:0.058795\n",
      "[29]\teval-error:0.20229\ttrain-error:0.058795\n",
      "[30]\teval-error:0.209924\ttrain-error:0.058795\n",
      "[31]\teval-error:0.21374\ttrain-error:0.058795\n",
      "[32]\teval-error:0.21374\ttrain-error:0.058795\n",
      "[33]\teval-error:0.21374\ttrain-error:0.058795\n",
      "[34]\teval-error:0.221374\ttrain-error:0.058795\n",
      "[35]\teval-error:0.225191\ttrain-error:0.058795\n",
      "[36]\teval-error:0.225191\ttrain-error:0.058795\n",
      "[37]\teval-error:0.225191\ttrain-error:0.058795\n",
      "[38]\teval-error:0.229008\ttrain-error:0.058795\n",
      "[39]\teval-error:0.217557\ttrain-error:0.058795\n",
      "[40]\teval-error:0.225191\ttrain-error:0.058795\n",
      "[41]\teval-error:0.209924\ttrain-error:0.058795\n",
      "[42]\teval-error:0.221374\ttrain-error:0.058795\n",
      "[43]\teval-error:0.225191\ttrain-error:0.058795\n",
      "[44]\teval-error:0.217557\ttrain-error:0.058795\n",
      "[45]\teval-error:0.225191\ttrain-error:0.058795\n",
      "[46]\teval-error:0.229008\ttrain-error:0.058795\n",
      "[47]\teval-error:0.221374\ttrain-error:0.058795\n",
      "[48]\teval-error:0.221374\ttrain-error:0.058795\n",
      "[49]\teval-error:0.240458\ttrain-error:0.058795\n",
      "[50]\teval-error:0.229008\ttrain-error:0.058795\n",
      "[51]\teval-error:0.221374\ttrain-error:0.058795\n",
      "[52]\teval-error:0.209924\ttrain-error:0.058795\n",
      "[53]\teval-error:0.21374\ttrain-error:0.058795\n",
      "[54]\teval-error:0.209924\ttrain-error:0.058795\n",
      "[55]\teval-error:0.221374\ttrain-error:0.058795\n",
      "[56]\teval-error:0.217557\ttrain-error:0.058795\n",
      "[57]\teval-error:0.221374\ttrain-error:0.058795\n",
      "[58]\teval-error:0.217557\ttrain-error:0.058795\n",
      "[59]\teval-error:0.206107\ttrain-error:0.058795\n",
      "[60]\teval-error:0.225191\ttrain-error:0.058795\n",
      "[61]\teval-error:0.21374\ttrain-error:0.058795\n",
      "[62]\teval-error:0.225191\ttrain-error:0.058795\n",
      "[63]\teval-error:0.221374\ttrain-error:0.058795\n",
      "[64]\teval-error:0.225191\ttrain-error:0.058795\n",
      "[65]\teval-error:0.221374\ttrain-error:0.058795\n",
      "[66]\teval-error:0.221374\ttrain-error:0.058795\n",
      "[67]\teval-error:0.217557\ttrain-error:0.058795\n",
      "[68]\teval-error:0.225191\ttrain-error:0.058795\n",
      "[69]\teval-error:0.232824\ttrain-error:0.058795\n",
      "[0]\teval-error:0.229008\ttrain-error:0.151118\n",
      "[1]\teval-error:0.240458\ttrain-error:0.136054\n",
      "[2]\teval-error:0.244275\ttrain-error:0.120991\n",
      "[3]\teval-error:0.248092\ttrain-error:0.1069\n",
      "[4]\teval-error:0.232824\ttrain-error:0.097182\n",
      "[5]\teval-error:0.248092\ttrain-error:0.093294\n",
      "[6]\teval-error:0.251908\ttrain-error:0.091837\n",
      "[7]\teval-error:0.240458\ttrain-error:0.090865\n",
      "[8]\teval-error:0.240458\ttrain-error:0.087949\n",
      "[9]\teval-error:0.251908\ttrain-error:0.088921\n",
      "[10]\teval-error:0.255725\ttrain-error:0.086492\n",
      "[11]\teval-error:0.244275\ttrain-error:0.087949\n",
      "[12]\teval-error:0.251908\ttrain-error:0.086006\n",
      "[13]\teval-error:0.236641\ttrain-error:0.08552\n",
      "[14]\teval-error:0.236641\ttrain-error:0.084548\n",
      "[15]\teval-error:0.236641\ttrain-error:0.084062\n",
      "[16]\teval-error:0.236641\ttrain-error:0.084062\n",
      "[17]\teval-error:0.225191\ttrain-error:0.084062\n",
      "[18]\teval-error:0.232824\ttrain-error:0.084548\n",
      "[19]\teval-error:0.225191\ttrain-error:0.083576\n",
      "[20]\teval-error:0.232824\ttrain-error:0.083576\n",
      "[21]\teval-error:0.236641\ttrain-error:0.083576\n",
      "[22]\teval-error:0.221374\ttrain-error:0.083576\n",
      "[23]\teval-error:0.221374\ttrain-error:0.083576\n",
      "[24]\teval-error:0.221374\ttrain-error:0.083576\n",
      "[25]\teval-error:0.221374\ttrain-error:0.083576\n",
      "[26]\teval-error:0.244275\ttrain-error:0.083576\n",
      "[27]\teval-error:0.229008\ttrain-error:0.083576\n",
      "[28]\teval-error:0.236641\ttrain-error:0.083576\n",
      "[29]\teval-error:0.221374\ttrain-error:0.083576\n",
      "[30]\teval-error:0.229008\ttrain-error:0.083576\n",
      "[31]\teval-error:0.244275\ttrain-error:0.083576\n",
      "[32]\teval-error:0.244275\ttrain-error:0.083576\n",
      "[33]\teval-error:0.225191\ttrain-error:0.083576\n",
      "[34]\teval-error:0.232824\ttrain-error:0.083576\n",
      "[35]\teval-error:0.229008\ttrain-error:0.083576\n",
      "[36]\teval-error:0.240458\ttrain-error:0.083576\n",
      "[37]\teval-error:0.240458\ttrain-error:0.083576\n",
      "[38]\teval-error:0.240458\ttrain-error:0.083576\n",
      "[39]\teval-error:0.251908\ttrain-error:0.083576\n",
      "[40]\teval-error:0.236641\ttrain-error:0.083576\n",
      "[41]\teval-error:0.244275\ttrain-error:0.083576\n",
      "[42]\teval-error:0.225191\ttrain-error:0.083576\n",
      "[43]\teval-error:0.225191\ttrain-error:0.083576\n",
      "[44]\teval-error:0.229008\ttrain-error:0.083576\n",
      "[45]\teval-error:0.225191\ttrain-error:0.083576\n",
      "[46]\teval-error:0.236641\ttrain-error:0.083576\n",
      "[47]\teval-error:0.225191\ttrain-error:0.083576\n",
      "[48]\teval-error:0.225191\ttrain-error:0.083576\n",
      "[49]\teval-error:0.229008\ttrain-error:0.083576\n",
      "[50]\teval-error:0.248092\ttrain-error:0.083576\n",
      "[51]\teval-error:0.240458\ttrain-error:0.083576\n",
      "[52]\teval-error:0.248092\ttrain-error:0.083576\n",
      "[53]\teval-error:0.244275\ttrain-error:0.083576\n",
      "[54]\teval-error:0.244275\ttrain-error:0.083576\n",
      "[55]\teval-error:0.240458\ttrain-error:0.083576\n",
      "[56]\teval-error:0.244275\ttrain-error:0.083576\n",
      "[57]\teval-error:0.225191\ttrain-error:0.083576\n",
      "[58]\teval-error:0.240458\ttrain-error:0.083576\n",
      "[59]\teval-error:0.225191\ttrain-error:0.083576\n",
      "[60]\teval-error:0.240458\ttrain-error:0.083576\n",
      "[61]\teval-error:0.225191\ttrain-error:0.083576\n",
      "[62]\teval-error:0.240458\ttrain-error:0.083576\n",
      "[63]\teval-error:0.229008\ttrain-error:0.083576\n",
      "[64]\teval-error:0.240458\ttrain-error:0.083576\n",
      "[65]\teval-error:0.225191\ttrain-error:0.083576\n",
      "[66]\teval-error:0.225191\ttrain-error:0.083576\n",
      "[67]\teval-error:0.225191\ttrain-error:0.083576\n",
      "[68]\teval-error:0.221374\ttrain-error:0.083576\n",
      "[69]\teval-error:0.236641\ttrain-error:0.083576\n",
      "[0]\teval-error:0.267176\ttrain-error:0.118076\n",
      "[1]\teval-error:0.255725\ttrain-error:0.085034\n",
      "[2]\teval-error:0.255725\ttrain-error:0.062196\n",
      "[3]\teval-error:0.255725\ttrain-error:0.049077\n",
      "[4]\teval-error:0.244275\ttrain-error:0.043246\n",
      "[5]\teval-error:0.232824\ttrain-error:0.04033\n",
      "[6]\teval-error:0.240458\ttrain-error:0.037901\n",
      "[7]\teval-error:0.244275\ttrain-error:0.037415\n",
      "[8]\teval-error:0.244275\ttrain-error:0.037415\n",
      "[9]\teval-error:0.232824\ttrain-error:0.037415\n",
      "[10]\teval-error:0.240458\ttrain-error:0.037415\n",
      "[11]\teval-error:0.244275\ttrain-error:0.037415\n",
      "[12]\teval-error:0.240458\ttrain-error:0.037415\n",
      "[13]\teval-error:0.240458\ttrain-error:0.037415\n",
      "[14]\teval-error:0.248092\ttrain-error:0.037415\n",
      "[15]\teval-error:0.240458\ttrain-error:0.037415\n",
      "[16]\teval-error:0.236641\ttrain-error:0.037415\n",
      "[17]\teval-error:0.240458\ttrain-error:0.037415\n",
      "[18]\teval-error:0.251908\ttrain-error:0.037415\n",
      "[19]\teval-error:0.251908\ttrain-error:0.037415\n",
      "[20]\teval-error:0.251908\ttrain-error:0.037415\n",
      "[21]\teval-error:0.255725\ttrain-error:0.037415\n",
      "[22]\teval-error:0.244275\ttrain-error:0.037415\n",
      "[23]\teval-error:0.251908\ttrain-error:0.037415\n",
      "[24]\teval-error:0.259542\ttrain-error:0.037415\n",
      "[25]\teval-error:0.240458\ttrain-error:0.037415\n",
      "[26]\teval-error:0.240458\ttrain-error:0.037415\n",
      "[27]\teval-error:0.244275\ttrain-error:0.037415\n",
      "[28]\teval-error:0.244275\ttrain-error:0.037415\n",
      "[29]\teval-error:0.240458\ttrain-error:0.037415\n",
      "[30]\teval-error:0.236641\ttrain-error:0.037415\n",
      "[31]\teval-error:0.240458\ttrain-error:0.037415\n",
      "[32]\teval-error:0.255725\ttrain-error:0.037415\n",
      "[33]\teval-error:0.255725\ttrain-error:0.037415\n",
      "[34]\teval-error:0.240458\ttrain-error:0.037415\n",
      "[35]\teval-error:0.248092\ttrain-error:0.037415\n",
      "[36]\teval-error:0.244275\ttrain-error:0.037415\n",
      "[37]\teval-error:0.240458\ttrain-error:0.037415\n",
      "[38]\teval-error:0.240458\ttrain-error:0.037415\n",
      "[39]\teval-error:0.248092\ttrain-error:0.037415\n",
      "[40]\teval-error:0.240458\ttrain-error:0.037415\n",
      "[41]\teval-error:0.244275\ttrain-error:0.037415\n",
      "[42]\teval-error:0.248092\ttrain-error:0.037415\n",
      "[43]\teval-error:0.251908\ttrain-error:0.037415\n",
      "[44]\teval-error:0.248092\ttrain-error:0.037415\n",
      "[45]\teval-error:0.251908\ttrain-error:0.037415\n",
      "[46]\teval-error:0.251908\ttrain-error:0.037415\n",
      "[47]\teval-error:0.255725\ttrain-error:0.037415\n",
      "[48]\teval-error:0.255725\ttrain-error:0.037415\n",
      "[49]\teval-error:0.251908\ttrain-error:0.037415\n",
      "[50]\teval-error:0.248092\ttrain-error:0.037415\n",
      "[51]\teval-error:0.244275\ttrain-error:0.037415\n",
      "[52]\teval-error:0.244275\ttrain-error:0.037415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[53]\teval-error:0.255725\ttrain-error:0.037415\n",
      "[54]\teval-error:0.244275\ttrain-error:0.037415\n",
      "[55]\teval-error:0.244275\ttrain-error:0.037415\n",
      "[56]\teval-error:0.244275\ttrain-error:0.037415\n",
      "[57]\teval-error:0.236641\ttrain-error:0.037415\n",
      "[58]\teval-error:0.244275\ttrain-error:0.037415\n",
      "[59]\teval-error:0.244275\ttrain-error:0.037415\n",
      "[60]\teval-error:0.240458\ttrain-error:0.037415\n",
      "[61]\teval-error:0.240458\ttrain-error:0.037415\n",
      "[62]\teval-error:0.244275\ttrain-error:0.037415\n",
      "[63]\teval-error:0.244275\ttrain-error:0.037415\n",
      "[64]\teval-error:0.244275\ttrain-error:0.037415\n",
      "[65]\teval-error:0.251908\ttrain-error:0.037415\n",
      "[66]\teval-error:0.236641\ttrain-error:0.037415\n",
      "[67]\teval-error:0.244275\ttrain-error:0.037415\n",
      "[68]\teval-error:0.244275\ttrain-error:0.037415\n",
      "[69]\teval-error:0.251908\ttrain-error:0.037415\n",
      "[0]\teval-error:0.272727\ttrain-error:0.108635\n",
      "[1]\teval-error:0.181818\ttrain-error:0.05571\n",
      "[2]\teval-error:0.227273\ttrain-error:0.027855\n",
      "[3]\teval-error:0.181818\ttrain-error:0.008357\n",
      "[4]\teval-error:0.193182\ttrain-error:0.006964\n",
      "[5]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[6]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[7]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[8]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[9]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[10]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[11]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[12]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[13]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[14]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[15]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[16]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[17]\teval-error:0.147727\ttrain-error:0.004178\n",
      "[18]\teval-error:0.159091\ttrain-error:0.004178\n",
      "[19]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[20]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[21]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[22]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[23]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[24]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[25]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[26]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[27]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[28]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[29]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[30]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[31]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[32]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[33]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[34]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[35]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[36]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[37]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[38]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[39]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[40]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[41]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[42]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[43]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[44]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[45]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[46]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[47]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[48]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[49]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[50]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[51]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[52]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[53]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[54]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[55]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[56]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[57]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[58]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[59]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[60]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[61]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[62]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[63]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[64]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[65]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[66]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[67]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[68]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[69]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[0]\teval-error:0.272727\ttrain-error:0.093315\n",
      "[1]\teval-error:0.227273\ttrain-error:0.048747\n",
      "[2]\teval-error:0.238636\ttrain-error:0.026462\n",
      "[3]\teval-error:0.215909\ttrain-error:0.012535\n",
      "[4]\teval-error:0.170455\ttrain-error:0.006964\n",
      "[5]\teval-error:0.204545\ttrain-error:0.005571\n",
      "[6]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[7]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[8]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[9]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[10]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[11]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[12]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[13]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[14]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[15]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[16]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[17]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[18]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[19]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[20]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[21]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[22]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[23]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[24]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[25]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[26]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[27]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[28]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[29]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[30]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[31]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[32]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[33]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[34]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[35]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[36]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[37]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[38]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[39]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[40]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[41]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[42]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[43]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[44]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[45]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[46]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[47]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[48]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[49]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[50]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[51]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[52]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[53]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[54]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[55]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[56]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[57]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[58]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[59]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[60]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[61]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[62]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[63]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[64]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[65]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[66]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[67]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[68]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[69]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[0]\teval-error:0.215909\ttrain-error:0.140669\n",
      "[1]\teval-error:0.227273\ttrain-error:0.090529\n",
      "[2]\teval-error:0.272727\ttrain-error:0.057103\n",
      "[3]\teval-error:0.227273\ttrain-error:0.026462\n",
      "[4]\teval-error:0.227273\ttrain-error:0.020891\n",
      "[5]\teval-error:0.227273\ttrain-error:0.012535\n",
      "[6]\teval-error:0.227273\ttrain-error:0.011142\n",
      "[7]\teval-error:0.227273\ttrain-error:0.008357\n",
      "[8]\teval-error:0.204545\ttrain-error:0.008357\n",
      "[9]\teval-error:0.204545\ttrain-error:0.008357\n",
      "[10]\teval-error:0.215909\ttrain-error:0.008357\n",
      "[11]\teval-error:0.238636\ttrain-error:0.008357\n",
      "[12]\teval-error:0.227273\ttrain-error:0.008357\n",
      "[13]\teval-error:0.193182\ttrain-error:0.008357\n",
      "[14]\teval-error:0.193182\ttrain-error:0.008357\n",
      "[15]\teval-error:0.204545\ttrain-error:0.008357\n",
      "[16]\teval-error:0.204545\ttrain-error:0.006964\n",
      "[17]\teval-error:0.193182\ttrain-error:0.006964\n",
      "[18]\teval-error:0.193182\ttrain-error:0.006964\n",
      "[19]\teval-error:0.181818\ttrain-error:0.006964\n",
      "[20]\teval-error:0.193182\ttrain-error:0.006964\n",
      "[21]\teval-error:0.204545\ttrain-error:0.006964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22]\teval-error:0.204545\ttrain-error:0.006964\n",
      "[23]\teval-error:0.193182\ttrain-error:0.006964\n",
      "[24]\teval-error:0.193182\ttrain-error:0.006964\n",
      "[25]\teval-error:0.204545\ttrain-error:0.006964\n",
      "[26]\teval-error:0.193182\ttrain-error:0.006964\n",
      "[27]\teval-error:0.204545\ttrain-error:0.006964\n",
      "[28]\teval-error:0.193182\ttrain-error:0.006964\n",
      "[29]\teval-error:0.204545\ttrain-error:0.006964\n",
      "[30]\teval-error:0.204545\ttrain-error:0.006964\n",
      "[31]\teval-error:0.204545\ttrain-error:0.006964\n",
      "[32]\teval-error:0.215909\ttrain-error:0.006964\n",
      "[33]\teval-error:0.204545\ttrain-error:0.006964\n",
      "[34]\teval-error:0.204545\ttrain-error:0.006964\n",
      "[35]\teval-error:0.193182\ttrain-error:0.006964\n",
      "[36]\teval-error:0.193182\ttrain-error:0.006964\n",
      "[37]\teval-error:0.204545\ttrain-error:0.006964\n",
      "[38]\teval-error:0.193182\ttrain-error:0.006964\n",
      "[39]\teval-error:0.193182\ttrain-error:0.006964\n",
      "[40]\teval-error:0.204545\ttrain-error:0.006964\n",
      "[41]\teval-error:0.204545\ttrain-error:0.006964\n",
      "[42]\teval-error:0.215909\ttrain-error:0.006964\n",
      "[43]\teval-error:0.215909\ttrain-error:0.006964\n",
      "[44]\teval-error:0.215909\ttrain-error:0.006964\n",
      "[45]\teval-error:0.238636\ttrain-error:0.006964\n",
      "[46]\teval-error:0.227273\ttrain-error:0.006964\n",
      "[47]\teval-error:0.215909\ttrain-error:0.006964\n",
      "[48]\teval-error:0.227273\ttrain-error:0.006964\n",
      "[49]\teval-error:0.238636\ttrain-error:0.006964\n",
      "[50]\teval-error:0.25\ttrain-error:0.006964\n",
      "[51]\teval-error:0.25\ttrain-error:0.006964\n",
      "[52]\teval-error:0.25\ttrain-error:0.006964\n",
      "[53]\teval-error:0.25\ttrain-error:0.006964\n",
      "[54]\teval-error:0.25\ttrain-error:0.006964\n",
      "[55]\teval-error:0.25\ttrain-error:0.006964\n",
      "[56]\teval-error:0.25\ttrain-error:0.006964\n",
      "[57]\teval-error:0.25\ttrain-error:0.006964\n",
      "[58]\teval-error:0.238636\ttrain-error:0.006964\n",
      "[59]\teval-error:0.238636\ttrain-error:0.006964\n",
      "[60]\teval-error:0.238636\ttrain-error:0.006964\n",
      "[61]\teval-error:0.227273\ttrain-error:0.006964\n",
      "[62]\teval-error:0.238636\ttrain-error:0.006964\n",
      "[63]\teval-error:0.238636\ttrain-error:0.006964\n",
      "[64]\teval-error:0.238636\ttrain-error:0.006964\n",
      "[65]\teval-error:0.238636\ttrain-error:0.006964\n",
      "[66]\teval-error:0.238636\ttrain-error:0.006964\n",
      "[67]\teval-error:0.238636\ttrain-error:0.006964\n",
      "[68]\teval-error:0.238636\ttrain-error:0.006964\n",
      "[69]\teval-error:0.238636\ttrain-error:0.006964\n",
      "[0]\teval-error:0.215909\ttrain-error:0.089136\n",
      "[1]\teval-error:0.227273\ttrain-error:0.04039\n",
      "[2]\teval-error:0.181818\ttrain-error:0.020891\n",
      "[3]\teval-error:0.147727\ttrain-error:0.008357\n",
      "[4]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[5]\teval-error:0.147727\ttrain-error:0.004178\n",
      "[6]\teval-error:0.136364\ttrain-error:0.004178\n",
      "[7]\teval-error:0.159091\ttrain-error:0.004178\n",
      "[8]\teval-error:0.147727\ttrain-error:0.004178\n",
      "[9]\teval-error:0.136364\ttrain-error:0.004178\n",
      "[10]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[11]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[12]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[13]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[14]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[15]\teval-error:0.159091\ttrain-error:0.004178\n",
      "[16]\teval-error:0.159091\ttrain-error:0.004178\n",
      "[17]\teval-error:0.159091\ttrain-error:0.004178\n",
      "[18]\teval-error:0.159091\ttrain-error:0.004178\n",
      "[19]\teval-error:0.159091\ttrain-error:0.004178\n",
      "[20]\teval-error:0.147727\ttrain-error:0.004178\n",
      "[21]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[22]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[23]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[24]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[25]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[26]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[27]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[28]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[29]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[30]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[31]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[32]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[33]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[34]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[35]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[36]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[37]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[38]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[39]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[40]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[41]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[42]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[43]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[44]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[45]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[46]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[47]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[48]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[49]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[50]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[51]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[52]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[53]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[54]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[55]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[56]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[57]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[58]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[59]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[60]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[61]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[62]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[63]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[64]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[65]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[66]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[67]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[68]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[69]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[0]\teval-error:0.319149\ttrain-error:0.10031\n",
      "[1]\teval-error:0.255319\ttrain-error:0.051706\n",
      "[2]\teval-error:0.223404\ttrain-error:0.036194\n",
      "[3]\teval-error:0.234043\ttrain-error:0.025853\n",
      "[4]\teval-error:0.234043\ttrain-error:0.024819\n",
      "[5]\teval-error:0.212766\ttrain-error:0.023785\n",
      "[6]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[7]\teval-error:0.223404\ttrain-error:0.023785\n",
      "[8]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[9]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[10]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[11]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[12]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[13]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[14]\teval-error:0.265957\ttrain-error:0.023785\n",
      "[15]\teval-error:0.265957\ttrain-error:0.023785\n",
      "[16]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[17]\teval-error:0.265957\ttrain-error:0.023785\n",
      "[18]\teval-error:0.265957\ttrain-error:0.023785\n",
      "[19]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[20]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[21]\teval-error:0.265957\ttrain-error:0.023785\n",
      "[22]\teval-error:0.276596\ttrain-error:0.023785\n",
      "[23]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[24]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[25]\teval-error:0.265957\ttrain-error:0.023785\n",
      "[26]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[27]\teval-error:0.265957\ttrain-error:0.023785\n",
      "[28]\teval-error:0.265957\ttrain-error:0.023785\n",
      "[29]\teval-error:0.265957\ttrain-error:0.023785\n",
      "[30]\teval-error:0.265957\ttrain-error:0.023785\n",
      "[31]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[32]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[33]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[34]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[35]\teval-error:0.265957\ttrain-error:0.023785\n",
      "[36]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[37]\teval-error:0.265957\ttrain-error:0.023785\n",
      "[38]\teval-error:0.265957\ttrain-error:0.023785\n",
      "[39]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[40]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[41]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[42]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[43]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[44]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[45]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[46]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[47]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[48]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[49]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[50]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[51]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[52]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[53]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[54]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[55]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[56]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[57]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[58]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[59]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[60]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[61]\teval-error:0.234043\ttrain-error:0.023785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[63]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[64]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[65]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[66]\teval-error:0.265957\ttrain-error:0.023785\n",
      "[67]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[68]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[69]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[0]\teval-error:0.244681\ttrain-error:0.111686\n",
      "[1]\teval-error:0.244681\ttrain-error:0.066184\n",
      "[2]\teval-error:0.255319\ttrain-error:0.043433\n",
      "[3]\teval-error:0.287234\ttrain-error:0.033092\n",
      "[4]\teval-error:0.276596\ttrain-error:0.027921\n",
      "[5]\teval-error:0.276596\ttrain-error:0.026887\n",
      "[6]\teval-error:0.244681\ttrain-error:0.026887\n",
      "[7]\teval-error:0.255319\ttrain-error:0.024819\n",
      "[8]\teval-error:0.223404\ttrain-error:0.024819\n",
      "[9]\teval-error:0.244681\ttrain-error:0.024819\n",
      "[10]\teval-error:0.255319\ttrain-error:0.024819\n",
      "[11]\teval-error:0.265957\ttrain-error:0.024819\n",
      "[12]\teval-error:0.255319\ttrain-error:0.024819\n",
      "[13]\teval-error:0.255319\ttrain-error:0.024819\n",
      "[14]\teval-error:0.244681\ttrain-error:0.024819\n",
      "[15]\teval-error:0.244681\ttrain-error:0.024819\n",
      "[16]\teval-error:0.244681\ttrain-error:0.024819\n",
      "[17]\teval-error:0.255319\ttrain-error:0.024819\n",
      "[18]\teval-error:0.255319\ttrain-error:0.024819\n",
      "[19]\teval-error:0.265957\ttrain-error:0.024819\n",
      "[20]\teval-error:0.255319\ttrain-error:0.024819\n",
      "[21]\teval-error:0.265957\ttrain-error:0.024819\n",
      "[22]\teval-error:0.265957\ttrain-error:0.024819\n",
      "[23]\teval-error:0.276596\ttrain-error:0.024819\n",
      "[24]\teval-error:0.276596\ttrain-error:0.024819\n",
      "[25]\teval-error:0.276596\ttrain-error:0.024819\n",
      "[26]\teval-error:0.287234\ttrain-error:0.024819\n",
      "[27]\teval-error:0.287234\ttrain-error:0.024819\n",
      "[28]\teval-error:0.297872\ttrain-error:0.024819\n",
      "[29]\teval-error:0.287234\ttrain-error:0.024819\n",
      "[30]\teval-error:0.287234\ttrain-error:0.024819\n",
      "[31]\teval-error:0.287234\ttrain-error:0.024819\n",
      "[32]\teval-error:0.287234\ttrain-error:0.024819\n",
      "[33]\teval-error:0.276596\ttrain-error:0.024819\n",
      "[34]\teval-error:0.276596\ttrain-error:0.024819\n",
      "[35]\teval-error:0.276596\ttrain-error:0.024819\n",
      "[36]\teval-error:0.276596\ttrain-error:0.024819\n",
      "[37]\teval-error:0.287234\ttrain-error:0.024819\n",
      "[38]\teval-error:0.276596\ttrain-error:0.024819\n",
      "[39]\teval-error:0.276596\ttrain-error:0.024819\n",
      "[40]\teval-error:0.276596\ttrain-error:0.024819\n",
      "[41]\teval-error:0.276596\ttrain-error:0.024819\n",
      "[42]\teval-error:0.276596\ttrain-error:0.024819\n",
      "[43]\teval-error:0.287234\ttrain-error:0.024819\n",
      "[44]\teval-error:0.276596\ttrain-error:0.024819\n",
      "[45]\teval-error:0.276596\ttrain-error:0.024819\n",
      "[46]\teval-error:0.287234\ttrain-error:0.024819\n",
      "[47]\teval-error:0.276596\ttrain-error:0.024819\n",
      "[48]\teval-error:0.276596\ttrain-error:0.024819\n",
      "[49]\teval-error:0.276596\ttrain-error:0.024819\n",
      "[50]\teval-error:0.276596\ttrain-error:0.024819\n",
      "[51]\teval-error:0.276596\ttrain-error:0.024819\n",
      "[52]\teval-error:0.276596\ttrain-error:0.024819\n",
      "[53]\teval-error:0.287234\ttrain-error:0.024819\n",
      "[54]\teval-error:0.276596\ttrain-error:0.024819\n",
      "[55]\teval-error:0.276596\ttrain-error:0.024819\n",
      "[56]\teval-error:0.276596\ttrain-error:0.024819\n",
      "[57]\teval-error:0.276596\ttrain-error:0.024819\n",
      "[58]\teval-error:0.276596\ttrain-error:0.024819\n",
      "[59]\teval-error:0.276596\ttrain-error:0.024819\n",
      "[60]\teval-error:0.276596\ttrain-error:0.024819\n",
      "[61]\teval-error:0.276596\ttrain-error:0.024819\n",
      "[62]\teval-error:0.287234\ttrain-error:0.024819\n",
      "[63]\teval-error:0.276596\ttrain-error:0.024819\n",
      "[64]\teval-error:0.276596\ttrain-error:0.024819\n",
      "[65]\teval-error:0.276596\ttrain-error:0.024819\n",
      "[66]\teval-error:0.287234\ttrain-error:0.024819\n",
      "[67]\teval-error:0.276596\ttrain-error:0.024819\n",
      "[68]\teval-error:0.276596\ttrain-error:0.024819\n",
      "[69]\teval-error:0.276596\ttrain-error:0.024819\n",
      "[0]\teval-error:0.276596\ttrain-error:0.140641\n",
      "[1]\teval-error:0.351064\ttrain-error:0.096174\n",
      "[2]\teval-error:0.308511\ttrain-error:0.059979\n",
      "[3]\teval-error:0.287234\ttrain-error:0.048604\n",
      "[4]\teval-error:0.287234\ttrain-error:0.036194\n",
      "[5]\teval-error:0.308511\ttrain-error:0.034126\n",
      "[6]\teval-error:0.276596\ttrain-error:0.028956\n",
      "[7]\teval-error:0.297872\ttrain-error:0.02999\n",
      "[8]\teval-error:0.276596\ttrain-error:0.02999\n",
      "[9]\teval-error:0.255319\ttrain-error:0.028956\n",
      "[10]\teval-error:0.265957\ttrain-error:0.027921\n",
      "[11]\teval-error:0.276596\ttrain-error:0.027921\n",
      "[12]\teval-error:0.265957\ttrain-error:0.028956\n",
      "[13]\teval-error:0.244681\ttrain-error:0.027921\n",
      "[14]\teval-error:0.244681\ttrain-error:0.028956\n",
      "[15]\teval-error:0.234043\ttrain-error:0.027921\n",
      "[16]\teval-error:0.244681\ttrain-error:0.028956\n",
      "[17]\teval-error:0.234043\ttrain-error:0.027921\n",
      "[18]\teval-error:0.223404\ttrain-error:0.027921\n",
      "[19]\teval-error:0.234043\ttrain-error:0.027921\n",
      "[20]\teval-error:0.244681\ttrain-error:0.027921\n",
      "[21]\teval-error:0.244681\ttrain-error:0.028956\n",
      "[22]\teval-error:0.244681\ttrain-error:0.027921\n",
      "[23]\teval-error:0.244681\ttrain-error:0.027921\n",
      "[24]\teval-error:0.234043\ttrain-error:0.027921\n",
      "[25]\teval-error:0.234043\ttrain-error:0.027921\n",
      "[26]\teval-error:0.234043\ttrain-error:0.027921\n",
      "[27]\teval-error:0.234043\ttrain-error:0.027921\n",
      "[28]\teval-error:0.244681\ttrain-error:0.027921\n",
      "[29]\teval-error:0.234043\ttrain-error:0.027921\n",
      "[30]\teval-error:0.234043\ttrain-error:0.027921\n",
      "[31]\teval-error:0.244681\ttrain-error:0.027921\n",
      "[32]\teval-error:0.244681\ttrain-error:0.027921\n",
      "[33]\teval-error:0.244681\ttrain-error:0.027921\n",
      "[34]\teval-error:0.244681\ttrain-error:0.027921\n",
      "[35]\teval-error:0.244681\ttrain-error:0.027921\n",
      "[36]\teval-error:0.244681\ttrain-error:0.027921\n",
      "[37]\teval-error:0.244681\ttrain-error:0.027921\n",
      "[38]\teval-error:0.234043\ttrain-error:0.027921\n",
      "[39]\teval-error:0.244681\ttrain-error:0.027921\n",
      "[40]\teval-error:0.234043\ttrain-error:0.027921\n",
      "[41]\teval-error:0.234043\ttrain-error:0.027921\n",
      "[42]\teval-error:0.244681\ttrain-error:0.027921\n",
      "[43]\teval-error:0.234043\ttrain-error:0.027921\n",
      "[44]\teval-error:0.234043\ttrain-error:0.027921\n",
      "[45]\teval-error:0.234043\ttrain-error:0.027921\n",
      "[46]\teval-error:0.244681\ttrain-error:0.027921\n",
      "[47]\teval-error:0.244681\ttrain-error:0.027921\n",
      "[48]\teval-error:0.244681\ttrain-error:0.027921\n",
      "[49]\teval-error:0.244681\ttrain-error:0.027921\n",
      "[50]\teval-error:0.234043\ttrain-error:0.027921\n",
      "[51]\teval-error:0.234043\ttrain-error:0.027921\n",
      "[52]\teval-error:0.244681\ttrain-error:0.027921\n",
      "[53]\teval-error:0.234043\ttrain-error:0.027921\n",
      "[54]\teval-error:0.234043\ttrain-error:0.027921\n",
      "[55]\teval-error:0.234043\ttrain-error:0.027921\n",
      "[56]\teval-error:0.234043\ttrain-error:0.027921\n",
      "[57]\teval-error:0.234043\ttrain-error:0.027921\n",
      "[58]\teval-error:0.234043\ttrain-error:0.027921\n",
      "[59]\teval-error:0.234043\ttrain-error:0.027921\n",
      "[60]\teval-error:0.234043\ttrain-error:0.027921\n",
      "[61]\teval-error:0.234043\ttrain-error:0.027921\n",
      "[62]\teval-error:0.234043\ttrain-error:0.027921\n",
      "[63]\teval-error:0.234043\ttrain-error:0.027921\n",
      "[64]\teval-error:0.234043\ttrain-error:0.027921\n",
      "[65]\teval-error:0.234043\ttrain-error:0.027921\n",
      "[66]\teval-error:0.234043\ttrain-error:0.027921\n",
      "[67]\teval-error:0.234043\ttrain-error:0.027921\n",
      "[68]\teval-error:0.234043\ttrain-error:0.027921\n",
      "[69]\teval-error:0.234043\ttrain-error:0.027921\n",
      "[0]\teval-error:0.382979\ttrain-error:0.107549\n",
      "[1]\teval-error:0.319149\ttrain-error:0.059979\n",
      "[2]\teval-error:0.308511\ttrain-error:0.036194\n",
      "[3]\teval-error:0.308511\ttrain-error:0.02999\n",
      "[4]\teval-error:0.308511\ttrain-error:0.024819\n",
      "[5]\teval-error:0.329787\ttrain-error:0.023785\n",
      "[6]\teval-error:0.319149\ttrain-error:0.023785\n",
      "[7]\teval-error:0.319149\ttrain-error:0.023785\n",
      "[8]\teval-error:0.308511\ttrain-error:0.023785\n",
      "[9]\teval-error:0.308511\ttrain-error:0.023785\n",
      "[10]\teval-error:0.297872\ttrain-error:0.023785\n",
      "[11]\teval-error:0.308511\ttrain-error:0.023785\n",
      "[12]\teval-error:0.297872\ttrain-error:0.023785\n",
      "[13]\teval-error:0.319149\ttrain-error:0.023785\n",
      "[14]\teval-error:0.297872\ttrain-error:0.023785\n",
      "[15]\teval-error:0.287234\ttrain-error:0.023785\n",
      "[16]\teval-error:0.287234\ttrain-error:0.023785\n",
      "[17]\teval-error:0.287234\ttrain-error:0.023785\n",
      "[18]\teval-error:0.287234\ttrain-error:0.023785\n",
      "[19]\teval-error:0.265957\ttrain-error:0.023785\n",
      "[20]\teval-error:0.276596\ttrain-error:0.023785\n",
      "[21]\teval-error:0.287234\ttrain-error:0.023785\n",
      "[22]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[23]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[24]\teval-error:0.276596\ttrain-error:0.023785\n",
      "[25]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[26]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[27]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[28]\teval-error:0.223404\ttrain-error:0.023785\n",
      "[29]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[30]\teval-error:0.212766\ttrain-error:0.023785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[32]\teval-error:0.223404\ttrain-error:0.023785\n",
      "[33]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[34]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[35]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[36]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[37]\teval-error:0.212766\ttrain-error:0.023785\n",
      "[38]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[39]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[40]\teval-error:0.212766\ttrain-error:0.023785\n",
      "[41]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[42]\teval-error:0.223404\ttrain-error:0.023785\n",
      "[43]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[44]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[45]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[46]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[47]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[48]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[49]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[50]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[51]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[52]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[53]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[54]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[55]\teval-error:0.223404\ttrain-error:0.023785\n",
      "[56]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[57]\teval-error:0.223404\ttrain-error:0.023785\n",
      "[58]\teval-error:0.223404\ttrain-error:0.023785\n",
      "[59]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[60]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[61]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[62]\teval-error:0.223404\ttrain-error:0.023785\n",
      "[63]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[64]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[65]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[66]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[67]\teval-error:0.223404\ttrain-error:0.023785\n",
      "[68]\teval-error:0.223404\ttrain-error:0.023785\n",
      "[69]\teval-error:0.223404\ttrain-error:0.023785\n",
      "[0]\teval-error:0.251908\ttrain-error:0.100583\n",
      "[1]\teval-error:0.232824\ttrain-error:0.068999\n",
      "[2]\teval-error:0.244275\ttrain-error:0.04276\n",
      "[3]\teval-error:0.236641\ttrain-error:0.030126\n",
      "[4]\teval-error:0.232824\ttrain-error:0.025267\n",
      "[5]\teval-error:0.229008\ttrain-error:0.02381\n",
      "[6]\teval-error:0.232824\ttrain-error:0.02381\n",
      "[7]\teval-error:0.248092\ttrain-error:0.023324\n",
      "[8]\teval-error:0.244275\ttrain-error:0.023324\n",
      "[9]\teval-error:0.251908\ttrain-error:0.023324\n",
      "[10]\teval-error:0.244275\ttrain-error:0.022838\n",
      "[11]\teval-error:0.251908\ttrain-error:0.022838\n",
      "[12]\teval-error:0.240458\ttrain-error:0.022838\n",
      "[13]\teval-error:0.244275\ttrain-error:0.022838\n",
      "[14]\teval-error:0.248092\ttrain-error:0.022838\n",
      "[15]\teval-error:0.244275\ttrain-error:0.022838\n",
      "[16]\teval-error:0.236641\ttrain-error:0.022838\n",
      "[17]\teval-error:0.236641\ttrain-error:0.022838\n",
      "[18]\teval-error:0.240458\ttrain-error:0.022838\n",
      "[19]\teval-error:0.236641\ttrain-error:0.022838\n",
      "[20]\teval-error:0.236641\ttrain-error:0.022838\n",
      "[21]\teval-error:0.236641\ttrain-error:0.022838\n",
      "[22]\teval-error:0.236641\ttrain-error:0.022838\n",
      "[23]\teval-error:0.232824\ttrain-error:0.022838\n",
      "[24]\teval-error:0.236641\ttrain-error:0.022838\n",
      "[25]\teval-error:0.229008\ttrain-error:0.022838\n",
      "[26]\teval-error:0.232824\ttrain-error:0.022838\n",
      "[27]\teval-error:0.232824\ttrain-error:0.022838\n",
      "[28]\teval-error:0.229008\ttrain-error:0.022838\n",
      "[29]\teval-error:0.225191\ttrain-error:0.022838\n",
      "[30]\teval-error:0.229008\ttrain-error:0.022838\n",
      "[31]\teval-error:0.229008\ttrain-error:0.022838\n",
      "[32]\teval-error:0.240458\ttrain-error:0.022838\n",
      "[33]\teval-error:0.229008\ttrain-error:0.022838\n",
      "[34]\teval-error:0.229008\ttrain-error:0.022838\n",
      "[35]\teval-error:0.236641\ttrain-error:0.022838\n",
      "[36]\teval-error:0.232824\ttrain-error:0.022838\n",
      "[37]\teval-error:0.229008\ttrain-error:0.022838\n",
      "[38]\teval-error:0.240458\ttrain-error:0.022838\n",
      "[39]\teval-error:0.248092\ttrain-error:0.022838\n",
      "[40]\teval-error:0.248092\ttrain-error:0.022838\n",
      "[41]\teval-error:0.236641\ttrain-error:0.022838\n",
      "[42]\teval-error:0.236641\ttrain-error:0.022838\n",
      "[43]\teval-error:0.244275\ttrain-error:0.022838\n",
      "[44]\teval-error:0.232824\ttrain-error:0.022838\n",
      "[45]\teval-error:0.236641\ttrain-error:0.022838\n",
      "[46]\teval-error:0.244275\ttrain-error:0.022838\n",
      "[47]\teval-error:0.232824\ttrain-error:0.022838\n",
      "[48]\teval-error:0.229008\ttrain-error:0.022838\n",
      "[49]\teval-error:0.229008\ttrain-error:0.022838\n",
      "[50]\teval-error:0.232824\ttrain-error:0.022838\n",
      "[51]\teval-error:0.232824\ttrain-error:0.022838\n",
      "[52]\teval-error:0.225191\ttrain-error:0.022838\n",
      "[53]\teval-error:0.232824\ttrain-error:0.022838\n",
      "[54]\teval-error:0.232824\ttrain-error:0.022838\n",
      "[55]\teval-error:0.229008\ttrain-error:0.022838\n",
      "[56]\teval-error:0.229008\ttrain-error:0.022838\n",
      "[57]\teval-error:0.229008\ttrain-error:0.022838\n",
      "[58]\teval-error:0.225191\ttrain-error:0.022838\n",
      "[59]\teval-error:0.229008\ttrain-error:0.022838\n",
      "[60]\teval-error:0.232824\ttrain-error:0.022838\n",
      "[61]\teval-error:0.232824\ttrain-error:0.022838\n",
      "[62]\teval-error:0.232824\ttrain-error:0.022838\n",
      "[63]\teval-error:0.229008\ttrain-error:0.022838\n",
      "[64]\teval-error:0.221374\ttrain-error:0.022838\n",
      "[65]\teval-error:0.229008\ttrain-error:0.022838\n",
      "[66]\teval-error:0.236641\ttrain-error:0.022838\n",
      "[67]\teval-error:0.229008\ttrain-error:0.022838\n",
      "[68]\teval-error:0.232824\ttrain-error:0.022838\n",
      "[69]\teval-error:0.225191\ttrain-error:0.022838\n",
      "[0]\teval-error:0.259542\ttrain-error:0.097668\n",
      "[1]\teval-error:0.206107\ttrain-error:0.058309\n",
      "[2]\teval-error:0.209924\ttrain-error:0.04033\n",
      "[3]\teval-error:0.229008\ttrain-error:0.03207\n",
      "[4]\teval-error:0.229008\ttrain-error:0.028183\n",
      "[5]\teval-error:0.232824\ttrain-error:0.027211\n",
      "[6]\teval-error:0.236641\ttrain-error:0.025753\n",
      "[7]\teval-error:0.217557\ttrain-error:0.025753\n",
      "[8]\teval-error:0.217557\ttrain-error:0.025267\n",
      "[9]\teval-error:0.229008\ttrain-error:0.025267\n",
      "[10]\teval-error:0.225191\ttrain-error:0.025267\n",
      "[11]\teval-error:0.217557\ttrain-error:0.024781\n",
      "[12]\teval-error:0.20229\ttrain-error:0.024781\n",
      "[13]\teval-error:0.206107\ttrain-error:0.024781\n",
      "[14]\teval-error:0.20229\ttrain-error:0.024781\n",
      "[15]\teval-error:0.198473\ttrain-error:0.024781\n",
      "[16]\teval-error:0.20229\ttrain-error:0.024781\n",
      "[17]\teval-error:0.198473\ttrain-error:0.024781\n",
      "[18]\teval-error:0.194656\ttrain-error:0.024781\n",
      "[19]\teval-error:0.194656\ttrain-error:0.024781\n",
      "[20]\teval-error:0.19084\ttrain-error:0.024781\n",
      "[21]\teval-error:0.194656\ttrain-error:0.024781\n",
      "[22]\teval-error:0.198473\ttrain-error:0.024781\n",
      "[23]\teval-error:0.20229\ttrain-error:0.024781\n",
      "[24]\teval-error:0.187023\ttrain-error:0.024781\n",
      "[25]\teval-error:0.198473\ttrain-error:0.024781\n",
      "[26]\teval-error:0.194656\ttrain-error:0.024781\n",
      "[27]\teval-error:0.19084\ttrain-error:0.024781\n",
      "[28]\teval-error:0.19084\ttrain-error:0.024781\n",
      "[29]\teval-error:0.206107\ttrain-error:0.024781\n",
      "[30]\teval-error:0.20229\ttrain-error:0.024781\n",
      "[31]\teval-error:0.194656\ttrain-error:0.024781\n",
      "[32]\teval-error:0.194656\ttrain-error:0.024781\n",
      "[33]\teval-error:0.198473\ttrain-error:0.024781\n",
      "[34]\teval-error:0.198473\ttrain-error:0.024781\n",
      "[35]\teval-error:0.198473\ttrain-error:0.024781\n",
      "[36]\teval-error:0.198473\ttrain-error:0.024781\n",
      "[37]\teval-error:0.194656\ttrain-error:0.024781\n",
      "[38]\teval-error:0.194656\ttrain-error:0.024781\n",
      "[39]\teval-error:0.20229\ttrain-error:0.024781\n",
      "[40]\teval-error:0.194656\ttrain-error:0.024781\n",
      "[41]\teval-error:0.194656\ttrain-error:0.024781\n",
      "[42]\teval-error:0.20229\ttrain-error:0.024781\n",
      "[43]\teval-error:0.209924\ttrain-error:0.024781\n",
      "[44]\teval-error:0.198473\ttrain-error:0.024781\n",
      "[45]\teval-error:0.20229\ttrain-error:0.024781\n",
      "[46]\teval-error:0.198473\ttrain-error:0.024781\n",
      "[47]\teval-error:0.206107\ttrain-error:0.024781\n",
      "[48]\teval-error:0.20229\ttrain-error:0.024781\n",
      "[49]\teval-error:0.198473\ttrain-error:0.024781\n",
      "[50]\teval-error:0.198473\ttrain-error:0.024781\n",
      "[51]\teval-error:0.206107\ttrain-error:0.024781\n",
      "[52]\teval-error:0.198473\ttrain-error:0.024781\n",
      "[53]\teval-error:0.198473\ttrain-error:0.024781\n",
      "[54]\teval-error:0.206107\ttrain-error:0.024781\n",
      "[55]\teval-error:0.194656\ttrain-error:0.024781\n",
      "[56]\teval-error:0.198473\ttrain-error:0.024781\n",
      "[57]\teval-error:0.206107\ttrain-error:0.024781\n",
      "[58]\teval-error:0.198473\ttrain-error:0.024781\n",
      "[59]\teval-error:0.209924\ttrain-error:0.024781\n",
      "[60]\teval-error:0.198473\ttrain-error:0.024781\n",
      "[61]\teval-error:0.206107\ttrain-error:0.024781\n",
      "[62]\teval-error:0.209924\ttrain-error:0.024781\n",
      "[63]\teval-error:0.20229\ttrain-error:0.024781\n",
      "[64]\teval-error:0.20229\ttrain-error:0.024781\n",
      "[65]\teval-error:0.206107\ttrain-error:0.024781\n",
      "[66]\teval-error:0.206107\ttrain-error:0.024781\n",
      "[67]\teval-error:0.20229\ttrain-error:0.024781\n",
      "[68]\teval-error:0.209924\ttrain-error:0.024781\n",
      "[69]\teval-error:0.20229\ttrain-error:0.024781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-error:0.244275\ttrain-error:0.116132\n",
      "[1]\teval-error:0.251908\ttrain-error:0.076774\n",
      "[2]\teval-error:0.236641\ttrain-error:0.048591\n",
      "[3]\teval-error:0.20229\ttrain-error:0.035471\n",
      "[4]\teval-error:0.206107\ttrain-error:0.030612\n",
      "[5]\teval-error:0.206107\ttrain-error:0.030126\n",
      "[6]\teval-error:0.21374\ttrain-error:0.028669\n",
      "[7]\teval-error:0.225191\ttrain-error:0.028669\n",
      "[8]\teval-error:0.232824\ttrain-error:0.028669\n",
      "[9]\teval-error:0.225191\ttrain-error:0.029155\n",
      "[10]\teval-error:0.225191\ttrain-error:0.028669\n",
      "[11]\teval-error:0.21374\ttrain-error:0.028669\n",
      "[12]\teval-error:0.225191\ttrain-error:0.028669\n",
      "[13]\teval-error:0.221374\ttrain-error:0.028669\n",
      "[14]\teval-error:0.229008\ttrain-error:0.028669\n",
      "[15]\teval-error:0.229008\ttrain-error:0.028669\n",
      "[16]\teval-error:0.221374\ttrain-error:0.028669\n",
      "[17]\teval-error:0.217557\ttrain-error:0.028669\n",
      "[18]\teval-error:0.225191\ttrain-error:0.028669\n",
      "[19]\teval-error:0.225191\ttrain-error:0.028669\n",
      "[20]\teval-error:0.221374\ttrain-error:0.028669\n",
      "[21]\teval-error:0.221374\ttrain-error:0.028669\n",
      "[22]\teval-error:0.217557\ttrain-error:0.028669\n",
      "[23]\teval-error:0.221374\ttrain-error:0.028669\n",
      "[24]\teval-error:0.229008\ttrain-error:0.028669\n",
      "[25]\teval-error:0.225191\ttrain-error:0.028669\n",
      "[26]\teval-error:0.221374\ttrain-error:0.028669\n",
      "[27]\teval-error:0.225191\ttrain-error:0.028669\n",
      "[28]\teval-error:0.206107\ttrain-error:0.028669\n",
      "[29]\teval-error:0.209924\ttrain-error:0.028669\n",
      "[30]\teval-error:0.21374\ttrain-error:0.028669\n",
      "[31]\teval-error:0.209924\ttrain-error:0.028669\n",
      "[32]\teval-error:0.209924\ttrain-error:0.028669\n",
      "[33]\teval-error:0.21374\ttrain-error:0.028669\n",
      "[34]\teval-error:0.20229\ttrain-error:0.028669\n",
      "[35]\teval-error:0.20229\ttrain-error:0.028669\n",
      "[36]\teval-error:0.20229\ttrain-error:0.028669\n",
      "[37]\teval-error:0.209924\ttrain-error:0.028669\n",
      "[38]\teval-error:0.209924\ttrain-error:0.028669\n",
      "[39]\teval-error:0.206107\ttrain-error:0.028669\n",
      "[40]\teval-error:0.21374\ttrain-error:0.028669\n",
      "[41]\teval-error:0.209924\ttrain-error:0.028669\n",
      "[42]\teval-error:0.209924\ttrain-error:0.028669\n",
      "[43]\teval-error:0.206107\ttrain-error:0.028669\n",
      "[44]\teval-error:0.206107\ttrain-error:0.028669\n",
      "[45]\teval-error:0.217557\ttrain-error:0.028669\n",
      "[46]\teval-error:0.217557\ttrain-error:0.028669\n",
      "[47]\teval-error:0.21374\ttrain-error:0.028669\n",
      "[48]\teval-error:0.209924\ttrain-error:0.028669\n",
      "[49]\teval-error:0.209924\ttrain-error:0.028669\n",
      "[50]\teval-error:0.20229\ttrain-error:0.028669\n",
      "[51]\teval-error:0.21374\ttrain-error:0.028669\n",
      "[52]\teval-error:0.206107\ttrain-error:0.028669\n",
      "[53]\teval-error:0.209924\ttrain-error:0.028669\n",
      "[54]\teval-error:0.206107\ttrain-error:0.028669\n",
      "[55]\teval-error:0.221374\ttrain-error:0.028669\n",
      "[56]\teval-error:0.21374\ttrain-error:0.028669\n",
      "[57]\teval-error:0.21374\ttrain-error:0.028669\n",
      "[58]\teval-error:0.206107\ttrain-error:0.028669\n",
      "[59]\teval-error:0.209924\ttrain-error:0.028669\n",
      "[60]\teval-error:0.221374\ttrain-error:0.028669\n",
      "[61]\teval-error:0.21374\ttrain-error:0.028669\n",
      "[62]\teval-error:0.217557\ttrain-error:0.028669\n",
      "[63]\teval-error:0.21374\ttrain-error:0.028669\n",
      "[64]\teval-error:0.209924\ttrain-error:0.028669\n",
      "[65]\teval-error:0.21374\ttrain-error:0.028669\n",
      "[66]\teval-error:0.20229\ttrain-error:0.028669\n",
      "[67]\teval-error:0.209924\ttrain-error:0.028669\n",
      "[68]\teval-error:0.217557\ttrain-error:0.028669\n",
      "[69]\teval-error:0.217557\ttrain-error:0.028669\n",
      "[0]\teval-error:0.267176\ttrain-error:0.101555\n",
      "[1]\teval-error:0.251908\ttrain-error:0.065112\n",
      "[2]\teval-error:0.255725\ttrain-error:0.038873\n",
      "[3]\teval-error:0.255725\ttrain-error:0.025753\n",
      "[4]\teval-error:0.244275\ttrain-error:0.025267\n",
      "[5]\teval-error:0.259542\ttrain-error:0.023324\n",
      "[6]\teval-error:0.270992\ttrain-error:0.022838\n",
      "[7]\teval-error:0.267176\ttrain-error:0.021866\n",
      "[8]\teval-error:0.251908\ttrain-error:0.021866\n",
      "[9]\teval-error:0.251908\ttrain-error:0.021866\n",
      "[10]\teval-error:0.263359\ttrain-error:0.021866\n",
      "[11]\teval-error:0.255725\ttrain-error:0.021866\n",
      "[12]\teval-error:0.263359\ttrain-error:0.021866\n",
      "[13]\teval-error:0.263359\ttrain-error:0.021866\n",
      "[14]\teval-error:0.251908\ttrain-error:0.021866\n",
      "[15]\teval-error:0.267176\ttrain-error:0.021866\n",
      "[16]\teval-error:0.267176\ttrain-error:0.021866\n",
      "[17]\teval-error:0.259542\ttrain-error:0.021866\n",
      "[18]\teval-error:0.270992\ttrain-error:0.021866\n",
      "[19]\teval-error:0.248092\ttrain-error:0.021866\n",
      "[20]\teval-error:0.251908\ttrain-error:0.021866\n",
      "[21]\teval-error:0.251908\ttrain-error:0.021866\n",
      "[22]\teval-error:0.255725\ttrain-error:0.021866\n",
      "[23]\teval-error:0.244275\ttrain-error:0.021866\n",
      "[24]\teval-error:0.244275\ttrain-error:0.021866\n",
      "[25]\teval-error:0.248092\ttrain-error:0.021866\n",
      "[26]\teval-error:0.255725\ttrain-error:0.021866\n",
      "[27]\teval-error:0.259542\ttrain-error:0.021866\n",
      "[28]\teval-error:0.259542\ttrain-error:0.021866\n",
      "[29]\teval-error:0.267176\ttrain-error:0.021866\n",
      "[30]\teval-error:0.263359\ttrain-error:0.021866\n",
      "[31]\teval-error:0.259542\ttrain-error:0.021866\n",
      "[32]\teval-error:0.259542\ttrain-error:0.021866\n",
      "[33]\teval-error:0.255725\ttrain-error:0.021866\n",
      "[34]\teval-error:0.255725\ttrain-error:0.021866\n",
      "[35]\teval-error:0.255725\ttrain-error:0.021866\n",
      "[36]\teval-error:0.255725\ttrain-error:0.021866\n",
      "[37]\teval-error:0.255725\ttrain-error:0.021866\n",
      "[38]\teval-error:0.251908\ttrain-error:0.021866\n",
      "[39]\teval-error:0.251908\ttrain-error:0.021866\n",
      "[40]\teval-error:0.251908\ttrain-error:0.021866\n",
      "[41]\teval-error:0.267176\ttrain-error:0.021866\n",
      "[42]\teval-error:0.251908\ttrain-error:0.021866\n",
      "[43]\teval-error:0.251908\ttrain-error:0.021866\n",
      "[44]\teval-error:0.259542\ttrain-error:0.021866\n",
      "[45]\teval-error:0.259542\ttrain-error:0.021866\n",
      "[46]\teval-error:0.255725\ttrain-error:0.021866\n",
      "[47]\teval-error:0.259542\ttrain-error:0.021866\n",
      "[48]\teval-error:0.255725\ttrain-error:0.021866\n",
      "[49]\teval-error:0.259542\ttrain-error:0.021866\n",
      "[50]\teval-error:0.255725\ttrain-error:0.021866\n",
      "[51]\teval-error:0.259542\ttrain-error:0.021866\n",
      "[52]\teval-error:0.255725\ttrain-error:0.021866\n",
      "[53]\teval-error:0.259542\ttrain-error:0.021866\n",
      "[54]\teval-error:0.259542\ttrain-error:0.021866\n",
      "[55]\teval-error:0.255725\ttrain-error:0.021866\n",
      "[56]\teval-error:0.255725\ttrain-error:0.021866\n",
      "[57]\teval-error:0.259542\ttrain-error:0.021866\n",
      "[58]\teval-error:0.259542\ttrain-error:0.021866\n",
      "[59]\teval-error:0.263359\ttrain-error:0.021866\n",
      "[60]\teval-error:0.263359\ttrain-error:0.021866\n",
      "[61]\teval-error:0.255725\ttrain-error:0.021866\n",
      "[62]\teval-error:0.255725\ttrain-error:0.021866\n",
      "[63]\teval-error:0.255725\ttrain-error:0.021866\n",
      "[64]\teval-error:0.263359\ttrain-error:0.021866\n",
      "[65]\teval-error:0.259542\ttrain-error:0.021866\n",
      "[66]\teval-error:0.259542\ttrain-error:0.021866\n",
      "[67]\teval-error:0.263359\ttrain-error:0.021866\n",
      "[68]\teval-error:0.259542\ttrain-error:0.021866\n",
      "[69]\teval-error:0.263359\ttrain-error:0.021866\n",
      "[0]\teval-error:0.204545\ttrain-error:0.143454\n",
      "[1]\teval-error:0.193182\ttrain-error:0.119777\n",
      "[2]\teval-error:0.181818\ttrain-error:0.0961\n",
      "[3]\teval-error:0.193182\ttrain-error:0.066852\n",
      "[4]\teval-error:0.193182\ttrain-error:0.057103\n",
      "[5]\teval-error:0.204545\ttrain-error:0.050139\n",
      "[6]\teval-error:0.193182\ttrain-error:0.044568\n",
      "[7]\teval-error:0.204545\ttrain-error:0.045961\n",
      "[8]\teval-error:0.193182\ttrain-error:0.043175\n",
      "[9]\teval-error:0.204545\ttrain-error:0.04039\n",
      "[10]\teval-error:0.204545\ttrain-error:0.04039\n",
      "[11]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[12]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[13]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[14]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[15]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[16]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[17]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[18]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[19]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[20]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[21]\teval-error:0.193182\ttrain-error:0.038997\n",
      "[22]\teval-error:0.193182\ttrain-error:0.038997\n",
      "[23]\teval-error:0.193182\ttrain-error:0.038997\n",
      "[24]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[25]\teval-error:0.204545\ttrain-error:0.04039\n",
      "[26]\teval-error:0.204545\ttrain-error:0.04039\n",
      "[27]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[28]\teval-error:0.193182\ttrain-error:0.038997\n",
      "[29]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[30]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[31]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[32]\teval-error:0.193182\ttrain-error:0.038997\n",
      "[33]\teval-error:0.193182\ttrain-error:0.038997\n",
      "[34]\teval-error:0.193182\ttrain-error:0.038997\n",
      "[35]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[36]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[37]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[38]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[39]\teval-error:0.204545\ttrain-error:0.038997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[41]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[42]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[43]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[44]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[45]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[46]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[47]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[48]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[49]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[50]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[51]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[52]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[53]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[54]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[55]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[56]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[57]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[58]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[59]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[60]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[61]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[62]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[63]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[64]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[65]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[66]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[67]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[68]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[69]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[0]\teval-error:0.204545\ttrain-error:0.143454\n",
      "[1]\teval-error:0.193182\ttrain-error:0.119777\n",
      "[2]\teval-error:0.181818\ttrain-error:0.0961\n",
      "[3]\teval-error:0.193182\ttrain-error:0.066852\n",
      "[4]\teval-error:0.193182\ttrain-error:0.057103\n",
      "[5]\teval-error:0.204545\ttrain-error:0.050139\n",
      "[6]\teval-error:0.193182\ttrain-error:0.044568\n",
      "[7]\teval-error:0.204545\ttrain-error:0.045961\n",
      "[8]\teval-error:0.193182\ttrain-error:0.043175\n",
      "[9]\teval-error:0.204545\ttrain-error:0.04039\n",
      "[10]\teval-error:0.204545\ttrain-error:0.04039\n",
      "[11]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[12]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[13]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[14]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[15]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[16]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[17]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[18]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[19]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[20]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[21]\teval-error:0.193182\ttrain-error:0.038997\n",
      "[22]\teval-error:0.193182\ttrain-error:0.038997\n",
      "[23]\teval-error:0.193182\ttrain-error:0.038997\n",
      "[24]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[25]\teval-error:0.204545\ttrain-error:0.04039\n",
      "[26]\teval-error:0.204545\ttrain-error:0.04039\n",
      "[27]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[28]\teval-error:0.193182\ttrain-error:0.038997\n",
      "[29]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[30]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[31]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[32]\teval-error:0.193182\ttrain-error:0.038997\n",
      "[33]\teval-error:0.193182\ttrain-error:0.038997\n",
      "[34]\teval-error:0.193182\ttrain-error:0.038997\n",
      "[35]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[36]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[37]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[38]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[39]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[40]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[41]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[42]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[43]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[44]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[45]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[46]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[47]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[48]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[49]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[50]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[51]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[52]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[53]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[54]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[55]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[56]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[57]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[58]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[59]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[60]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[61]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[62]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[63]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[64]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[65]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[66]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[67]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[68]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[69]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[0]\teval-error:0.204545\ttrain-error:0.143454\n",
      "[1]\teval-error:0.193182\ttrain-error:0.119777\n",
      "[2]\teval-error:0.181818\ttrain-error:0.0961\n",
      "[3]\teval-error:0.193182\ttrain-error:0.066852\n",
      "[4]\teval-error:0.193182\ttrain-error:0.057103\n",
      "[5]\teval-error:0.204545\ttrain-error:0.050139\n",
      "[6]\teval-error:0.193182\ttrain-error:0.044568\n",
      "[7]\teval-error:0.204545\ttrain-error:0.045961\n",
      "[8]\teval-error:0.193182\ttrain-error:0.043175\n",
      "[9]\teval-error:0.204545\ttrain-error:0.04039\n",
      "[10]\teval-error:0.204545\ttrain-error:0.04039\n",
      "[11]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[12]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[13]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[14]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[15]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[16]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[17]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[18]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[19]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[20]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[21]\teval-error:0.193182\ttrain-error:0.038997\n",
      "[22]\teval-error:0.193182\ttrain-error:0.038997\n",
      "[23]\teval-error:0.193182\ttrain-error:0.038997\n",
      "[24]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[25]\teval-error:0.204545\ttrain-error:0.04039\n",
      "[26]\teval-error:0.204545\ttrain-error:0.04039\n",
      "[27]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[28]\teval-error:0.193182\ttrain-error:0.038997\n",
      "[29]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[30]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[31]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[32]\teval-error:0.193182\ttrain-error:0.038997\n",
      "[33]\teval-error:0.193182\ttrain-error:0.038997\n",
      "[34]\teval-error:0.193182\ttrain-error:0.038997\n",
      "[35]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[36]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[37]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[38]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[39]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[40]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[41]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[42]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[43]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[44]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[45]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[46]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[47]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[48]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[49]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[50]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[51]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[52]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[53]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[54]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[55]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[56]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[57]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[58]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[59]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[60]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[61]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[62]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[63]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[64]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[65]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[66]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[67]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[68]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[69]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[0]\teval-error:0.204545\ttrain-error:0.143454\n",
      "[1]\teval-error:0.193182\ttrain-error:0.119777\n",
      "[2]\teval-error:0.181818\ttrain-error:0.0961\n",
      "[3]\teval-error:0.193182\ttrain-error:0.066852\n",
      "[4]\teval-error:0.193182\ttrain-error:0.057103\n",
      "[5]\teval-error:0.204545\ttrain-error:0.050139\n",
      "[6]\teval-error:0.193182\ttrain-error:0.044568\n",
      "[7]\teval-error:0.204545\ttrain-error:0.045961\n",
      "[8]\teval-error:0.193182\ttrain-error:0.043175\n",
      "[9]\teval-error:0.204545\ttrain-error:0.04039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\teval-error:0.204545\ttrain-error:0.04039\n",
      "[11]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[12]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[13]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[14]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[15]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[16]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[17]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[18]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[19]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[20]\teval-error:0.193182\ttrain-error:0.04039\n",
      "[21]\teval-error:0.193182\ttrain-error:0.038997\n",
      "[22]\teval-error:0.193182\ttrain-error:0.038997\n",
      "[23]\teval-error:0.193182\ttrain-error:0.038997\n",
      "[24]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[25]\teval-error:0.204545\ttrain-error:0.04039\n",
      "[26]\teval-error:0.204545\ttrain-error:0.04039\n",
      "[27]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[28]\teval-error:0.193182\ttrain-error:0.038997\n",
      "[29]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[30]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[31]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[32]\teval-error:0.193182\ttrain-error:0.038997\n",
      "[33]\teval-error:0.193182\ttrain-error:0.038997\n",
      "[34]\teval-error:0.193182\ttrain-error:0.038997\n",
      "[35]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[36]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[37]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[38]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[39]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[40]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[41]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[42]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[43]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[44]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[45]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[46]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[47]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[48]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[49]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[50]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[51]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[52]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[53]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[54]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[55]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[56]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[57]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[58]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[59]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[60]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[61]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[62]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[63]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[64]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[65]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[66]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[67]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[68]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[69]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[0]\teval-error:0.202128\ttrain-error:0.161324\n",
      "[1]\teval-error:0.255319\ttrain-error:0.120993\n",
      "[2]\teval-error:0.276596\ttrain-error:0.088935\n",
      "[3]\teval-error:0.287234\ttrain-error:0.068252\n",
      "[4]\teval-error:0.265957\ttrain-error:0.055843\n",
      "[5]\teval-error:0.276596\ttrain-error:0.050672\n",
      "[6]\teval-error:0.255319\ttrain-error:0.046536\n",
      "[7]\teval-error:0.265957\ttrain-error:0.042399\n",
      "[8]\teval-error:0.265957\ttrain-error:0.042399\n",
      "[9]\teval-error:0.276596\ttrain-error:0.041365\n",
      "[10]\teval-error:0.276596\ttrain-error:0.039297\n",
      "[11]\teval-error:0.244681\ttrain-error:0.039297\n",
      "[12]\teval-error:0.276596\ttrain-error:0.039297\n",
      "[13]\teval-error:0.255319\ttrain-error:0.039297\n",
      "[14]\teval-error:0.265957\ttrain-error:0.039297\n",
      "[15]\teval-error:0.287234\ttrain-error:0.039297\n",
      "[16]\teval-error:0.265957\ttrain-error:0.039297\n",
      "[17]\teval-error:0.255319\ttrain-error:0.039297\n",
      "[18]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[19]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[20]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[21]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[22]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[23]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[24]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[25]\teval-error:0.244681\ttrain-error:0.038263\n",
      "[26]\teval-error:0.255319\ttrain-error:0.038263\n",
      "[27]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[28]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[29]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[30]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[31]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[32]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[33]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[34]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[35]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[36]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[37]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[38]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[39]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[40]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[41]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[42]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[43]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[44]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[45]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[46]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[47]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[48]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[49]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[50]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[51]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[52]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[53]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[54]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[55]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[56]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[57]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[58]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[59]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[60]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[61]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[62]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[63]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[64]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[65]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[66]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[67]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[68]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[69]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[0]\teval-error:0.202128\ttrain-error:0.161324\n",
      "[1]\teval-error:0.255319\ttrain-error:0.120993\n",
      "[2]\teval-error:0.276596\ttrain-error:0.088935\n",
      "[3]\teval-error:0.287234\ttrain-error:0.068252\n",
      "[4]\teval-error:0.265957\ttrain-error:0.055843\n",
      "[5]\teval-error:0.276596\ttrain-error:0.050672\n",
      "[6]\teval-error:0.255319\ttrain-error:0.046536\n",
      "[7]\teval-error:0.265957\ttrain-error:0.042399\n",
      "[8]\teval-error:0.265957\ttrain-error:0.042399\n",
      "[9]\teval-error:0.276596\ttrain-error:0.041365\n",
      "[10]\teval-error:0.276596\ttrain-error:0.039297\n",
      "[11]\teval-error:0.244681\ttrain-error:0.039297\n",
      "[12]\teval-error:0.276596\ttrain-error:0.039297\n",
      "[13]\teval-error:0.255319\ttrain-error:0.039297\n",
      "[14]\teval-error:0.265957\ttrain-error:0.039297\n",
      "[15]\teval-error:0.287234\ttrain-error:0.039297\n",
      "[16]\teval-error:0.265957\ttrain-error:0.039297\n",
      "[17]\teval-error:0.255319\ttrain-error:0.039297\n",
      "[18]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[19]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[20]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[21]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[22]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[23]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[24]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[25]\teval-error:0.244681\ttrain-error:0.038263\n",
      "[26]\teval-error:0.255319\ttrain-error:0.038263\n",
      "[27]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[28]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[29]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[30]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[31]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[32]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[33]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[34]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[35]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[36]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[37]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[38]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[39]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[40]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[41]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[42]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[43]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[44]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[45]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[46]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[47]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[48]\teval-error:0.276596\ttrain-error:0.038263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[50]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[51]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[52]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[53]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[54]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[55]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[56]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[57]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[58]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[59]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[60]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[61]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[62]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[63]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[64]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[65]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[66]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[67]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[68]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[69]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[0]\teval-error:0.202128\ttrain-error:0.161324\n",
      "[1]\teval-error:0.255319\ttrain-error:0.120993\n",
      "[2]\teval-error:0.276596\ttrain-error:0.088935\n",
      "[3]\teval-error:0.287234\ttrain-error:0.068252\n",
      "[4]\teval-error:0.265957\ttrain-error:0.055843\n",
      "[5]\teval-error:0.276596\ttrain-error:0.050672\n",
      "[6]\teval-error:0.255319\ttrain-error:0.046536\n",
      "[7]\teval-error:0.265957\ttrain-error:0.042399\n",
      "[8]\teval-error:0.265957\ttrain-error:0.042399\n",
      "[9]\teval-error:0.276596\ttrain-error:0.041365\n",
      "[10]\teval-error:0.276596\ttrain-error:0.039297\n",
      "[11]\teval-error:0.244681\ttrain-error:0.039297\n",
      "[12]\teval-error:0.276596\ttrain-error:0.039297\n",
      "[13]\teval-error:0.255319\ttrain-error:0.039297\n",
      "[14]\teval-error:0.265957\ttrain-error:0.039297\n",
      "[15]\teval-error:0.287234\ttrain-error:0.039297\n",
      "[16]\teval-error:0.265957\ttrain-error:0.039297\n",
      "[17]\teval-error:0.255319\ttrain-error:0.039297\n",
      "[18]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[19]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[20]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[21]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[22]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[23]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[24]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[25]\teval-error:0.244681\ttrain-error:0.038263\n",
      "[26]\teval-error:0.255319\ttrain-error:0.038263\n",
      "[27]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[28]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[29]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[30]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[31]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[32]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[33]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[34]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[35]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[36]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[37]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[38]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[39]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[40]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[41]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[42]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[43]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[44]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[45]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[46]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[47]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[48]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[49]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[50]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[51]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[52]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[53]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[54]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[55]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[56]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[57]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[58]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[59]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[60]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[61]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[62]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[63]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[64]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[65]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[66]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[67]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[68]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[69]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[0]\teval-error:0.202128\ttrain-error:0.161324\n",
      "[1]\teval-error:0.255319\ttrain-error:0.120993\n",
      "[2]\teval-error:0.276596\ttrain-error:0.088935\n",
      "[3]\teval-error:0.287234\ttrain-error:0.068252\n",
      "[4]\teval-error:0.265957\ttrain-error:0.055843\n",
      "[5]\teval-error:0.276596\ttrain-error:0.050672\n",
      "[6]\teval-error:0.255319\ttrain-error:0.046536\n",
      "[7]\teval-error:0.265957\ttrain-error:0.042399\n",
      "[8]\teval-error:0.265957\ttrain-error:0.042399\n",
      "[9]\teval-error:0.276596\ttrain-error:0.041365\n",
      "[10]\teval-error:0.276596\ttrain-error:0.039297\n",
      "[11]\teval-error:0.244681\ttrain-error:0.039297\n",
      "[12]\teval-error:0.276596\ttrain-error:0.039297\n",
      "[13]\teval-error:0.255319\ttrain-error:0.039297\n",
      "[14]\teval-error:0.265957\ttrain-error:0.039297\n",
      "[15]\teval-error:0.287234\ttrain-error:0.039297\n",
      "[16]\teval-error:0.265957\ttrain-error:0.039297\n",
      "[17]\teval-error:0.255319\ttrain-error:0.039297\n",
      "[18]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[19]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[20]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[21]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[22]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[23]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[24]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[25]\teval-error:0.244681\ttrain-error:0.038263\n",
      "[26]\teval-error:0.255319\ttrain-error:0.038263\n",
      "[27]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[28]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[29]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[30]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[31]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[32]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[33]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[34]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[35]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[36]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[37]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[38]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[39]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[40]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[41]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[42]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[43]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[44]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[45]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[46]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[47]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[48]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[49]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[50]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[51]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[52]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[53]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[54]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[55]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[56]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[57]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[58]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[59]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[60]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[61]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[62]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[63]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[64]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[65]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[66]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[67]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[68]\teval-error:0.265957\ttrain-error:0.038263\n",
      "[69]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[0]\teval-error:0.263359\ttrain-error:0.145287\n",
      "[1]\teval-error:0.274809\ttrain-error:0.126336\n",
      "[2]\teval-error:0.267176\ttrain-error:0.088435\n",
      "[3]\teval-error:0.251908\ttrain-error:0.074344\n",
      "[4]\teval-error:0.240458\ttrain-error:0.064626\n",
      "[5]\teval-error:0.236641\ttrain-error:0.057337\n",
      "[6]\teval-error:0.240458\ttrain-error:0.052964\n",
      "[7]\teval-error:0.236641\ttrain-error:0.052478\n",
      "[8]\teval-error:0.236641\ttrain-error:0.050534\n",
      "[9]\teval-error:0.240458\ttrain-error:0.048591\n",
      "[10]\teval-error:0.232824\ttrain-error:0.049077\n",
      "[11]\teval-error:0.236641\ttrain-error:0.048105\n",
      "[12]\teval-error:0.240458\ttrain-error:0.048105\n",
      "[13]\teval-error:0.248092\ttrain-error:0.049077\n",
      "[14]\teval-error:0.244275\ttrain-error:0.047619\n",
      "[15]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[16]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[17]\teval-error:0.240458\ttrain-error:0.047619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18]\teval-error:0.244275\ttrain-error:0.047619\n",
      "[19]\teval-error:0.244275\ttrain-error:0.047619\n",
      "[20]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[21]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[22]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[23]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[24]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[25]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[26]\teval-error:0.244275\ttrain-error:0.047619\n",
      "[27]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[28]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[29]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[30]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[31]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[32]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[33]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[34]\teval-error:0.244275\ttrain-error:0.047619\n",
      "[35]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[36]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[37]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[38]\teval-error:0.229008\ttrain-error:0.047619\n",
      "[39]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[40]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[41]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[42]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[43]\teval-error:0.229008\ttrain-error:0.047619\n",
      "[44]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[45]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[46]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[47]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[48]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[49]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[50]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[51]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[52]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[53]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[54]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[55]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[56]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[57]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[58]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[59]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[60]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[61]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[62]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[63]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[64]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[65]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[66]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[67]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[68]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[69]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[0]\teval-error:0.263359\ttrain-error:0.145287\n",
      "[1]\teval-error:0.274809\ttrain-error:0.126336\n",
      "[2]\teval-error:0.267176\ttrain-error:0.088435\n",
      "[3]\teval-error:0.251908\ttrain-error:0.074344\n",
      "[4]\teval-error:0.240458\ttrain-error:0.064626\n",
      "[5]\teval-error:0.236641\ttrain-error:0.057337\n",
      "[6]\teval-error:0.240458\ttrain-error:0.052964\n",
      "[7]\teval-error:0.236641\ttrain-error:0.052478\n",
      "[8]\teval-error:0.236641\ttrain-error:0.050534\n",
      "[9]\teval-error:0.240458\ttrain-error:0.048591\n",
      "[10]\teval-error:0.232824\ttrain-error:0.049077\n",
      "[11]\teval-error:0.236641\ttrain-error:0.048105\n",
      "[12]\teval-error:0.240458\ttrain-error:0.048105\n",
      "[13]\teval-error:0.248092\ttrain-error:0.049077\n",
      "[14]\teval-error:0.244275\ttrain-error:0.047619\n",
      "[15]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[16]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[17]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[18]\teval-error:0.244275\ttrain-error:0.047619\n",
      "[19]\teval-error:0.244275\ttrain-error:0.047619\n",
      "[20]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[21]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[22]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[23]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[24]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[25]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[26]\teval-error:0.244275\ttrain-error:0.047619\n",
      "[27]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[28]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[29]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[30]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[31]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[32]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[33]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[34]\teval-error:0.244275\ttrain-error:0.047619\n",
      "[35]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[36]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[37]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[38]\teval-error:0.229008\ttrain-error:0.047619\n",
      "[39]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[40]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[41]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[42]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[43]\teval-error:0.229008\ttrain-error:0.047619\n",
      "[44]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[45]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[46]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[47]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[48]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[49]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[50]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[51]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[52]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[53]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[54]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[55]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[56]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[57]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[58]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[59]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[60]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[61]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[62]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[63]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[64]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[65]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[66]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[67]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[68]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[69]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[0]\teval-error:0.263359\ttrain-error:0.145287\n",
      "[1]\teval-error:0.274809\ttrain-error:0.126336\n",
      "[2]\teval-error:0.267176\ttrain-error:0.088435\n",
      "[3]\teval-error:0.251908\ttrain-error:0.074344\n",
      "[4]\teval-error:0.240458\ttrain-error:0.064626\n",
      "[5]\teval-error:0.236641\ttrain-error:0.057337\n",
      "[6]\teval-error:0.240458\ttrain-error:0.052964\n",
      "[7]\teval-error:0.236641\ttrain-error:0.052478\n",
      "[8]\teval-error:0.236641\ttrain-error:0.050534\n",
      "[9]\teval-error:0.240458\ttrain-error:0.048591\n",
      "[10]\teval-error:0.232824\ttrain-error:0.049077\n",
      "[11]\teval-error:0.236641\ttrain-error:0.048105\n",
      "[12]\teval-error:0.240458\ttrain-error:0.048105\n",
      "[13]\teval-error:0.248092\ttrain-error:0.049077\n",
      "[14]\teval-error:0.244275\ttrain-error:0.047619\n",
      "[15]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[16]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[17]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[18]\teval-error:0.244275\ttrain-error:0.047619\n",
      "[19]\teval-error:0.244275\ttrain-error:0.047619\n",
      "[20]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[21]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[22]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[23]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[24]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[25]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[26]\teval-error:0.244275\ttrain-error:0.047619\n",
      "[27]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[28]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[29]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[30]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[31]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[32]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[33]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[34]\teval-error:0.244275\ttrain-error:0.047619\n",
      "[35]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[36]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[37]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[38]\teval-error:0.229008\ttrain-error:0.047619\n",
      "[39]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[40]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[41]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[42]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[43]\teval-error:0.229008\ttrain-error:0.047619\n",
      "[44]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[45]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[46]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[47]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[48]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[49]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[50]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[51]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[52]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[53]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[54]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[55]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[56]\teval-error:0.232824\ttrain-error:0.047619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[58]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[59]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[60]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[61]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[62]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[63]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[64]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[65]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[66]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[67]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[68]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[69]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[0]\teval-error:0.263359\ttrain-error:0.145287\n",
      "[1]\teval-error:0.274809\ttrain-error:0.126336\n",
      "[2]\teval-error:0.267176\ttrain-error:0.088435\n",
      "[3]\teval-error:0.251908\ttrain-error:0.074344\n",
      "[4]\teval-error:0.240458\ttrain-error:0.064626\n",
      "[5]\teval-error:0.236641\ttrain-error:0.057337\n",
      "[6]\teval-error:0.240458\ttrain-error:0.052964\n",
      "[7]\teval-error:0.236641\ttrain-error:0.052478\n",
      "[8]\teval-error:0.236641\ttrain-error:0.050534\n",
      "[9]\teval-error:0.240458\ttrain-error:0.048591\n",
      "[10]\teval-error:0.232824\ttrain-error:0.049077\n",
      "[11]\teval-error:0.236641\ttrain-error:0.048105\n",
      "[12]\teval-error:0.240458\ttrain-error:0.048105\n",
      "[13]\teval-error:0.248092\ttrain-error:0.049077\n",
      "[14]\teval-error:0.244275\ttrain-error:0.047619\n",
      "[15]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[16]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[17]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[18]\teval-error:0.244275\ttrain-error:0.047619\n",
      "[19]\teval-error:0.244275\ttrain-error:0.047619\n",
      "[20]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[21]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[22]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[23]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[24]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[25]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[26]\teval-error:0.244275\ttrain-error:0.047619\n",
      "[27]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[28]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[29]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[30]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[31]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[32]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[33]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[34]\teval-error:0.244275\ttrain-error:0.047619\n",
      "[35]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[36]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[37]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[38]\teval-error:0.229008\ttrain-error:0.047619\n",
      "[39]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[40]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[41]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[42]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[43]\teval-error:0.229008\ttrain-error:0.047619\n",
      "[44]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[45]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[46]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[47]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[48]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[49]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[50]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[51]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[52]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[53]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[54]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[55]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[56]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[57]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[58]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[59]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[60]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[61]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[62]\teval-error:0.232824\ttrain-error:0.047619\n",
      "[63]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[64]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[65]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[66]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[67]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[68]\teval-error:0.236641\ttrain-error:0.047619\n",
      "[69]\teval-error:0.240458\ttrain-error:0.047619\n",
      "[0]\teval-error:0.272727\ttrain-error:0.150418\n",
      "[1]\teval-error:0.204545\ttrain-error:0.116992\n",
      "[2]\teval-error:0.261364\ttrain-error:0.084958\n",
      "[3]\teval-error:0.284091\ttrain-error:0.054318\n",
      "[4]\teval-error:0.272727\ttrain-error:0.029248\n",
      "[5]\teval-error:0.261364\ttrain-error:0.020891\n",
      "[6]\teval-error:0.261364\ttrain-error:0.019499\n",
      "[7]\teval-error:0.261364\ttrain-error:0.01532\n",
      "[8]\teval-error:0.238636\ttrain-error:0.013928\n",
      "[9]\teval-error:0.215909\ttrain-error:0.013928\n",
      "[10]\teval-error:0.238636\ttrain-error:0.013928\n",
      "[11]\teval-error:0.238636\ttrain-error:0.013928\n",
      "[12]\teval-error:0.227273\ttrain-error:0.013928\n",
      "[13]\teval-error:0.227273\ttrain-error:0.013928\n",
      "[14]\teval-error:0.227273\ttrain-error:0.013928\n",
      "[15]\teval-error:0.238636\ttrain-error:0.013928\n",
      "[16]\teval-error:0.215909\ttrain-error:0.013928\n",
      "[17]\teval-error:0.215909\ttrain-error:0.013928\n",
      "[18]\teval-error:0.215909\ttrain-error:0.013928\n",
      "[19]\teval-error:0.227273\ttrain-error:0.013928\n",
      "[20]\teval-error:0.227273\ttrain-error:0.013928\n",
      "[21]\teval-error:0.215909\ttrain-error:0.013928\n",
      "[22]\teval-error:0.204545\ttrain-error:0.013928\n",
      "[23]\teval-error:0.227273\ttrain-error:0.013928\n",
      "[24]\teval-error:0.215909\ttrain-error:0.013928\n",
      "[25]\teval-error:0.204545\ttrain-error:0.013928\n",
      "[26]\teval-error:0.215909\ttrain-error:0.013928\n",
      "[27]\teval-error:0.215909\ttrain-error:0.013928\n",
      "[28]\teval-error:0.204545\ttrain-error:0.013928\n",
      "[29]\teval-error:0.215909\ttrain-error:0.013928\n",
      "[30]\teval-error:0.215909\ttrain-error:0.013928\n",
      "[31]\teval-error:0.227273\ttrain-error:0.013928\n",
      "[32]\teval-error:0.204545\ttrain-error:0.013928\n",
      "[33]\teval-error:0.204545\ttrain-error:0.013928\n",
      "[34]\teval-error:0.204545\ttrain-error:0.013928\n",
      "[35]\teval-error:0.215909\ttrain-error:0.013928\n",
      "[36]\teval-error:0.204545\ttrain-error:0.013928\n",
      "[37]\teval-error:0.204545\ttrain-error:0.013928\n",
      "[38]\teval-error:0.204545\ttrain-error:0.013928\n",
      "[39]\teval-error:0.204545\ttrain-error:0.013928\n",
      "[40]\teval-error:0.215909\ttrain-error:0.013928\n",
      "[41]\teval-error:0.204545\ttrain-error:0.013928\n",
      "[42]\teval-error:0.215909\ttrain-error:0.013928\n",
      "[43]\teval-error:0.215909\ttrain-error:0.013928\n",
      "[44]\teval-error:0.215909\ttrain-error:0.013928\n",
      "[45]\teval-error:0.215909\ttrain-error:0.013928\n",
      "[46]\teval-error:0.204545\ttrain-error:0.013928\n",
      "[47]\teval-error:0.204545\ttrain-error:0.013928\n",
      "[48]\teval-error:0.204545\ttrain-error:0.013928\n",
      "[49]\teval-error:0.204545\ttrain-error:0.013928\n",
      "[50]\teval-error:0.204545\ttrain-error:0.013928\n",
      "[51]\teval-error:0.227273\ttrain-error:0.013928\n",
      "[52]\teval-error:0.215909\ttrain-error:0.013928\n",
      "[53]\teval-error:0.215909\ttrain-error:0.013928\n",
      "[54]\teval-error:0.204545\ttrain-error:0.013928\n",
      "[55]\teval-error:0.215909\ttrain-error:0.013928\n",
      "[56]\teval-error:0.227273\ttrain-error:0.013928\n",
      "[57]\teval-error:0.204545\ttrain-error:0.013928\n",
      "[58]\teval-error:0.215909\ttrain-error:0.013928\n",
      "[59]\teval-error:0.215909\ttrain-error:0.013928\n",
      "[60]\teval-error:0.215909\ttrain-error:0.013928\n",
      "[61]\teval-error:0.215909\ttrain-error:0.013928\n",
      "[62]\teval-error:0.215909\ttrain-error:0.013928\n",
      "[63]\teval-error:0.215909\ttrain-error:0.013928\n",
      "[64]\teval-error:0.204545\ttrain-error:0.013928\n",
      "[65]\teval-error:0.204545\ttrain-error:0.013928\n",
      "[66]\teval-error:0.215909\ttrain-error:0.013928\n",
      "[67]\teval-error:0.227273\ttrain-error:0.013928\n",
      "[68]\teval-error:0.215909\ttrain-error:0.013928\n",
      "[69]\teval-error:0.227273\ttrain-error:0.013928\n",
      "[0]\teval-error:0.215909\ttrain-error:0.16156\n",
      "[1]\teval-error:0.204545\ttrain-error:0.139276\n",
      "[2]\teval-error:0.193182\ttrain-error:0.0961\n",
      "[3]\teval-error:0.193182\ttrain-error:0.072423\n",
      "[4]\teval-error:0.170455\ttrain-error:0.069638\n",
      "[5]\teval-error:0.170455\ttrain-error:0.058496\n",
      "[6]\teval-error:0.147727\ttrain-error:0.047354\n",
      "[7]\teval-error:0.159091\ttrain-error:0.047354\n",
      "[8]\teval-error:0.159091\ttrain-error:0.045961\n",
      "[9]\teval-error:0.159091\ttrain-error:0.044568\n",
      "[10]\teval-error:0.159091\ttrain-error:0.043175\n",
      "[11]\teval-error:0.170455\ttrain-error:0.043175\n",
      "[12]\teval-error:0.159091\ttrain-error:0.043175\n",
      "[13]\teval-error:0.170455\ttrain-error:0.043175\n",
      "[14]\teval-error:0.159091\ttrain-error:0.043175\n",
      "[15]\teval-error:0.170455\ttrain-error:0.043175\n",
      "[16]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[17]\teval-error:0.193182\ttrain-error:0.043175\n",
      "[18]\teval-error:0.193182\ttrain-error:0.043175\n",
      "[19]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[20]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[21]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[22]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[23]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[24]\teval-error:0.170455\ttrain-error:0.043175\n",
      "[25]\teval-error:0.181818\ttrain-error:0.043175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[27]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[28]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[29]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[30]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[31]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[32]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[33]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[34]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[35]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[36]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[37]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[38]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[39]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[40]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[41]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[42]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[43]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[44]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[45]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[46]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[47]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[48]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[49]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[50]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[51]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[52]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[53]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[54]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[55]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[56]\teval-error:0.170455\ttrain-error:0.043175\n",
      "[57]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[58]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[59]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[60]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[61]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[62]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[63]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[64]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[65]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[66]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[67]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[68]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[69]\teval-error:0.181818\ttrain-error:0.043175\n",
      "[0]\teval-error:0.25\ttrain-error:0.168524\n",
      "[1]\teval-error:0.204545\ttrain-error:0.143454\n",
      "[2]\teval-error:0.204545\ttrain-error:0.142061\n",
      "[3]\teval-error:0.204545\ttrain-error:0.129526\n",
      "[4]\teval-error:0.215909\ttrain-error:0.118384\n",
      "[5]\teval-error:0.215909\ttrain-error:0.114206\n",
      "[6]\teval-error:0.204545\ttrain-error:0.110028\n",
      "[7]\teval-error:0.204545\ttrain-error:0.103064\n",
      "[8]\teval-error:0.204545\ttrain-error:0.104457\n",
      "[9]\teval-error:0.204545\ttrain-error:0.098886\n",
      "[10]\teval-error:0.204545\ttrain-error:0.100279\n",
      "[11]\teval-error:0.204545\ttrain-error:0.100279\n",
      "[12]\teval-error:0.204545\ttrain-error:0.097493\n",
      "[13]\teval-error:0.204545\ttrain-error:0.097493\n",
      "[14]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[15]\teval-error:0.215909\ttrain-error:0.0961\n",
      "[16]\teval-error:0.204545\ttrain-error:0.097493\n",
      "[17]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[18]\teval-error:0.215909\ttrain-error:0.0961\n",
      "[19]\teval-error:0.215909\ttrain-error:0.0961\n",
      "[20]\teval-error:0.215909\ttrain-error:0.0961\n",
      "[21]\teval-error:0.215909\ttrain-error:0.0961\n",
      "[22]\teval-error:0.215909\ttrain-error:0.0961\n",
      "[23]\teval-error:0.215909\ttrain-error:0.0961\n",
      "[24]\teval-error:0.215909\ttrain-error:0.0961\n",
      "[25]\teval-error:0.215909\ttrain-error:0.0961\n",
      "[26]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[27]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[28]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[29]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[30]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[31]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[32]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[33]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[34]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[35]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[36]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[37]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[38]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[39]\teval-error:0.238636\ttrain-error:0.0961\n",
      "[40]\teval-error:0.238636\ttrain-error:0.0961\n",
      "[41]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[42]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[43]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[44]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[45]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[46]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[47]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[48]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[49]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[50]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[51]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[52]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[53]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[54]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[55]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[56]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[57]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[58]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[59]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[60]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[61]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[62]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[63]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[64]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[65]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[66]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[67]\teval-error:0.238636\ttrain-error:0.0961\n",
      "[68]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[69]\teval-error:0.204545\ttrain-error:0.0961\n",
      "[0]\teval-error:0.227273\ttrain-error:0.135097\n",
      "[1]\teval-error:0.193182\ttrain-error:0.0961\n",
      "[2]\teval-error:0.181818\ttrain-error:0.06546\n",
      "[3]\teval-error:0.204545\ttrain-error:0.044568\n",
      "[4]\teval-error:0.215909\ttrain-error:0.027855\n",
      "[5]\teval-error:0.204545\ttrain-error:0.02507\n",
      "[6]\teval-error:0.204545\ttrain-error:0.01532\n",
      "[7]\teval-error:0.215909\ttrain-error:0.011142\n",
      "[8]\teval-error:0.227273\ttrain-error:0.011142\n",
      "[9]\teval-error:0.215909\ttrain-error:0.011142\n",
      "[10]\teval-error:0.193182\ttrain-error:0.011142\n",
      "[11]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[12]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[13]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[14]\teval-error:0.215909\ttrain-error:0.011142\n",
      "[15]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[16]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[17]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[18]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[19]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[20]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[21]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[22]\teval-error:0.215909\ttrain-error:0.011142\n",
      "[23]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[24]\teval-error:0.215909\ttrain-error:0.011142\n",
      "[25]\teval-error:0.215909\ttrain-error:0.011142\n",
      "[26]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[27]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[28]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[29]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[30]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[31]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[32]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[33]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[34]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[35]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[36]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[37]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[38]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[39]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[40]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[41]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[42]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[43]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[44]\teval-error:0.215909\ttrain-error:0.011142\n",
      "[45]\teval-error:0.215909\ttrain-error:0.011142\n",
      "[46]\teval-error:0.215909\ttrain-error:0.011142\n",
      "[47]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[48]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[49]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[50]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[51]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[52]\teval-error:0.215909\ttrain-error:0.011142\n",
      "[53]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[54]\teval-error:0.193182\ttrain-error:0.011142\n",
      "[55]\teval-error:0.193182\ttrain-error:0.011142\n",
      "[56]\teval-error:0.193182\ttrain-error:0.011142\n",
      "[57]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[58]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[59]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[60]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[61]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[62]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[63]\teval-error:0.204545\ttrain-error:0.011142\n",
      "[64]\teval-error:0.193182\ttrain-error:0.011142\n",
      "[65]\teval-error:0.193182\ttrain-error:0.011142\n",
      "[66]\teval-error:0.193182\ttrain-error:0.011142\n",
      "[67]\teval-error:0.193182\ttrain-error:0.011142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[68]\teval-error:0.193182\ttrain-error:0.011142\n",
      "[69]\teval-error:0.193182\ttrain-error:0.011142\n",
      "[0]\teval-error:0.319149\ttrain-error:0.182006\n",
      "[1]\teval-error:0.319149\ttrain-error:0.144778\n",
      "[2]\teval-error:0.319149\ttrain-error:0.096174\n",
      "[3]\teval-error:0.276596\ttrain-error:0.073423\n",
      "[4]\teval-error:0.297872\ttrain-error:0.056877\n",
      "[5]\teval-error:0.319149\ttrain-error:0.04757\n",
      "[6]\teval-error:0.329787\ttrain-error:0.042399\n",
      "[7]\teval-error:0.329787\ttrain-error:0.043433\n",
      "[8]\teval-error:0.329787\ttrain-error:0.039297\n",
      "[9]\teval-error:0.329787\ttrain-error:0.039297\n",
      "[10]\teval-error:0.319149\ttrain-error:0.039297\n",
      "[11]\teval-error:0.319149\ttrain-error:0.038263\n",
      "[12]\teval-error:0.308511\ttrain-error:0.038263\n",
      "[13]\teval-error:0.319149\ttrain-error:0.038263\n",
      "[14]\teval-error:0.308511\ttrain-error:0.038263\n",
      "[15]\teval-error:0.308511\ttrain-error:0.038263\n",
      "[16]\teval-error:0.319149\ttrain-error:0.038263\n",
      "[17]\teval-error:0.308511\ttrain-error:0.038263\n",
      "[18]\teval-error:0.319149\ttrain-error:0.038263\n",
      "[19]\teval-error:0.319149\ttrain-error:0.038263\n",
      "[20]\teval-error:0.319149\ttrain-error:0.038263\n",
      "[21]\teval-error:0.319149\ttrain-error:0.038263\n",
      "[22]\teval-error:0.308511\ttrain-error:0.038263\n",
      "[23]\teval-error:0.308511\ttrain-error:0.038263\n",
      "[24]\teval-error:0.319149\ttrain-error:0.038263\n",
      "[25]\teval-error:0.319149\ttrain-error:0.038263\n",
      "[26]\teval-error:0.308511\ttrain-error:0.038263\n",
      "[27]\teval-error:0.308511\ttrain-error:0.038263\n",
      "[28]\teval-error:0.308511\ttrain-error:0.038263\n",
      "[29]\teval-error:0.308511\ttrain-error:0.038263\n",
      "[30]\teval-error:0.297872\ttrain-error:0.038263\n",
      "[31]\teval-error:0.308511\ttrain-error:0.038263\n",
      "[32]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[33]\teval-error:0.297872\ttrain-error:0.038263\n",
      "[34]\teval-error:0.297872\ttrain-error:0.038263\n",
      "[35]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[36]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[37]\teval-error:0.297872\ttrain-error:0.038263\n",
      "[38]\teval-error:0.297872\ttrain-error:0.038263\n",
      "[39]\teval-error:0.297872\ttrain-error:0.038263\n",
      "[40]\teval-error:0.297872\ttrain-error:0.038263\n",
      "[41]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[42]\teval-error:0.297872\ttrain-error:0.038263\n",
      "[43]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[44]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[45]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[46]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[47]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[48]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[49]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[50]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[51]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[52]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[53]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[54]\teval-error:0.276596\ttrain-error:0.038263\n",
      "[55]\teval-error:0.297872\ttrain-error:0.038263\n",
      "[56]\teval-error:0.297872\ttrain-error:0.038263\n",
      "[57]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[58]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[59]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[60]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[61]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[62]\teval-error:0.297872\ttrain-error:0.038263\n",
      "[63]\teval-error:0.297872\ttrain-error:0.038263\n",
      "[64]\teval-error:0.297872\ttrain-error:0.038263\n",
      "[65]\teval-error:0.297872\ttrain-error:0.038263\n",
      "[66]\teval-error:0.297872\ttrain-error:0.038263\n",
      "[67]\teval-error:0.308511\ttrain-error:0.038263\n",
      "[68]\teval-error:0.297872\ttrain-error:0.038263\n",
      "[69]\teval-error:0.287234\ttrain-error:0.038263\n",
      "[0]\teval-error:0.223404\ttrain-error:0.174767\n",
      "[1]\teval-error:0.244681\ttrain-error:0.137539\n",
      "[2]\teval-error:0.244681\ttrain-error:0.109617\n",
      "[3]\teval-error:0.244681\ttrain-error:0.093071\n",
      "[4]\teval-error:0.244681\ttrain-error:0.083764\n",
      "[5]\teval-error:0.244681\ttrain-error:0.076525\n",
      "[6]\teval-error:0.244681\ttrain-error:0.071355\n",
      "[7]\teval-error:0.255319\ttrain-error:0.070321\n",
      "[8]\teval-error:0.255319\ttrain-error:0.068252\n",
      "[9]\teval-error:0.255319\ttrain-error:0.066184\n",
      "[10]\teval-error:0.255319\ttrain-error:0.064116\n",
      "[11]\teval-error:0.255319\ttrain-error:0.064116\n",
      "[12]\teval-error:0.255319\ttrain-error:0.064116\n",
      "[13]\teval-error:0.255319\ttrain-error:0.064116\n",
      "[14]\teval-error:0.255319\ttrain-error:0.064116\n",
      "[15]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[16]\teval-error:0.276596\ttrain-error:0.064116\n",
      "[17]\teval-error:0.276596\ttrain-error:0.064116\n",
      "[18]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[19]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[20]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[21]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[22]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[23]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[24]\teval-error:0.276596\ttrain-error:0.064116\n",
      "[25]\teval-error:0.276596\ttrain-error:0.064116\n",
      "[26]\teval-error:0.276596\ttrain-error:0.064116\n",
      "[27]\teval-error:0.276596\ttrain-error:0.064116\n",
      "[28]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[29]\teval-error:0.276596\ttrain-error:0.064116\n",
      "[30]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[31]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[32]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[33]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[34]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[35]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[36]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[37]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[38]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[39]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[40]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[41]\teval-error:0.276596\ttrain-error:0.064116\n",
      "[42]\teval-error:0.276596\ttrain-error:0.064116\n",
      "[43]\teval-error:0.276596\ttrain-error:0.064116\n",
      "[44]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[45]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[46]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[47]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[48]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[49]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[50]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[51]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[52]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[53]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[54]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[55]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[56]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[57]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[58]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[59]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[60]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[61]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[62]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[63]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[64]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[65]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[66]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[67]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[68]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[69]\teval-error:0.287234\ttrain-error:0.064116\n",
      "[0]\teval-error:0.244681\ttrain-error:0.210962\n",
      "[1]\teval-error:0.265957\ttrain-error:0.189245\n",
      "[2]\teval-error:0.255319\ttrain-error:0.173733\n",
      "[3]\teval-error:0.265957\ttrain-error:0.16546\n",
      "[4]\teval-error:0.276596\ttrain-error:0.156153\n",
      "[5]\teval-error:0.276596\ttrain-error:0.149948\n",
      "[6]\teval-error:0.276596\ttrain-error:0.139607\n",
      "[7]\teval-error:0.276596\ttrain-error:0.140641\n",
      "[8]\teval-error:0.255319\ttrain-error:0.132368\n",
      "[9]\teval-error:0.265957\ttrain-error:0.132368\n",
      "[10]\teval-error:0.276596\ttrain-error:0.1303\n",
      "[11]\teval-error:0.276596\ttrain-error:0.1303\n",
      "[12]\teval-error:0.287234\ttrain-error:0.131334\n",
      "[13]\teval-error:0.276596\ttrain-error:0.131334\n",
      "[14]\teval-error:0.265957\ttrain-error:0.1303\n",
      "[15]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[16]\teval-error:0.287234\ttrain-error:0.131334\n",
      "[17]\teval-error:0.276596\ttrain-error:0.131334\n",
      "[18]\teval-error:0.276596\ttrain-error:0.131334\n",
      "[19]\teval-error:0.276596\ttrain-error:0.1303\n",
      "[20]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[21]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[22]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[23]\teval-error:0.276596\ttrain-error:0.1303\n",
      "[24]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[25]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[26]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[27]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[28]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[29]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[30]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[31]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[32]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[33]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[34]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[35]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[36]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[37]\teval-error:0.287234\ttrain-error:0.1303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[39]\teval-error:0.297872\ttrain-error:0.1303\n",
      "[40]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[41]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[42]\teval-error:0.297872\ttrain-error:0.1303\n",
      "[43]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[44]\teval-error:0.297872\ttrain-error:0.1303\n",
      "[45]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[46]\teval-error:0.297872\ttrain-error:0.1303\n",
      "[47]\teval-error:0.297872\ttrain-error:0.1303\n",
      "[48]\teval-error:0.297872\ttrain-error:0.1303\n",
      "[49]\teval-error:0.297872\ttrain-error:0.1303\n",
      "[50]\teval-error:0.297872\ttrain-error:0.1303\n",
      "[51]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[52]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[53]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[54]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[55]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[56]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[57]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[58]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[59]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[60]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[61]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[62]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[63]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[64]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[65]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[66]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[67]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[68]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[69]\teval-error:0.287234\ttrain-error:0.1303\n",
      "[0]\teval-error:0.351064\ttrain-error:0.17787\n",
      "[1]\teval-error:0.351064\ttrain-error:0.136505\n",
      "[2]\teval-error:0.37234\ttrain-error:0.096174\n",
      "[3]\teval-error:0.37234\ttrain-error:0.070321\n",
      "[4]\teval-error:0.319149\ttrain-error:0.049638\n",
      "[5]\teval-error:0.329787\ttrain-error:0.040331\n",
      "[6]\teval-error:0.340426\ttrain-error:0.041365\n",
      "[7]\teval-error:0.340426\ttrain-error:0.039297\n",
      "[8]\teval-error:0.351064\ttrain-error:0.038263\n",
      "[9]\teval-error:0.340426\ttrain-error:0.038263\n",
      "[10]\teval-error:0.340426\ttrain-error:0.038263\n",
      "[11]\teval-error:0.351064\ttrain-error:0.038263\n",
      "[12]\teval-error:0.351064\ttrain-error:0.038263\n",
      "[13]\teval-error:0.340426\ttrain-error:0.038263\n",
      "[14]\teval-error:0.351064\ttrain-error:0.038263\n",
      "[15]\teval-error:0.340426\ttrain-error:0.038263\n",
      "[16]\teval-error:0.329787\ttrain-error:0.038263\n",
      "[17]\teval-error:0.340426\ttrain-error:0.038263\n",
      "[18]\teval-error:0.329787\ttrain-error:0.038263\n",
      "[19]\teval-error:0.329787\ttrain-error:0.038263\n",
      "[20]\teval-error:0.340426\ttrain-error:0.038263\n",
      "[21]\teval-error:0.340426\ttrain-error:0.038263\n",
      "[22]\teval-error:0.340426\ttrain-error:0.038263\n",
      "[23]\teval-error:0.340426\ttrain-error:0.038263\n",
      "[24]\teval-error:0.329787\ttrain-error:0.038263\n",
      "[25]\teval-error:0.329787\ttrain-error:0.038263\n",
      "[26]\teval-error:0.329787\ttrain-error:0.038263\n",
      "[27]\teval-error:0.329787\ttrain-error:0.038263\n",
      "[28]\teval-error:0.329787\ttrain-error:0.038263\n",
      "[29]\teval-error:0.340426\ttrain-error:0.038263\n",
      "[30]\teval-error:0.329787\ttrain-error:0.038263\n",
      "[31]\teval-error:0.319149\ttrain-error:0.038263\n",
      "[32]\teval-error:0.319149\ttrain-error:0.038263\n",
      "[33]\teval-error:0.319149\ttrain-error:0.038263\n",
      "[34]\teval-error:0.319149\ttrain-error:0.038263\n",
      "[35]\teval-error:0.319149\ttrain-error:0.038263\n",
      "[36]\teval-error:0.319149\ttrain-error:0.038263\n",
      "[37]\teval-error:0.319149\ttrain-error:0.038263\n",
      "[38]\teval-error:0.329787\ttrain-error:0.038263\n",
      "[39]\teval-error:0.329787\ttrain-error:0.038263\n",
      "[40]\teval-error:0.340426\ttrain-error:0.038263\n",
      "[41]\teval-error:0.329787\ttrain-error:0.038263\n",
      "[42]\teval-error:0.329787\ttrain-error:0.038263\n",
      "[43]\teval-error:0.329787\ttrain-error:0.038263\n",
      "[44]\teval-error:0.319149\ttrain-error:0.038263\n",
      "[45]\teval-error:0.308511\ttrain-error:0.038263\n",
      "[46]\teval-error:0.319149\ttrain-error:0.038263\n",
      "[47]\teval-error:0.319149\ttrain-error:0.038263\n",
      "[48]\teval-error:0.319149\ttrain-error:0.038263\n",
      "[49]\teval-error:0.308511\ttrain-error:0.038263\n",
      "[50]\teval-error:0.308511\ttrain-error:0.038263\n",
      "[51]\teval-error:0.319149\ttrain-error:0.038263\n",
      "[52]\teval-error:0.319149\ttrain-error:0.038263\n",
      "[53]\teval-error:0.329787\ttrain-error:0.038263\n",
      "[54]\teval-error:0.319149\ttrain-error:0.038263\n",
      "[55]\teval-error:0.319149\ttrain-error:0.038263\n",
      "[56]\teval-error:0.319149\ttrain-error:0.038263\n",
      "[57]\teval-error:0.319149\ttrain-error:0.038263\n",
      "[58]\teval-error:0.319149\ttrain-error:0.038263\n",
      "[59]\teval-error:0.308511\ttrain-error:0.038263\n",
      "[60]\teval-error:0.319149\ttrain-error:0.038263\n",
      "[61]\teval-error:0.308511\ttrain-error:0.038263\n",
      "[62]\teval-error:0.319149\ttrain-error:0.038263\n",
      "[63]\teval-error:0.329787\ttrain-error:0.038263\n",
      "[64]\teval-error:0.329787\ttrain-error:0.038263\n",
      "[65]\teval-error:0.329787\ttrain-error:0.038263\n",
      "[66]\teval-error:0.329787\ttrain-error:0.038263\n",
      "[67]\teval-error:0.319149\ttrain-error:0.038263\n",
      "[68]\teval-error:0.319149\ttrain-error:0.038263\n",
      "[69]\teval-error:0.319149\ttrain-error:0.038263\n",
      "[0]\teval-error:0.251908\ttrain-error:0.148688\n",
      "[1]\teval-error:0.248092\ttrain-error:0.110787\n",
      "[2]\teval-error:0.274809\ttrain-error:0.073858\n",
      "[3]\teval-error:0.278626\ttrain-error:0.055879\n",
      "[4]\teval-error:0.274809\ttrain-error:0.042274\n",
      "[5]\teval-error:0.255725\ttrain-error:0.035471\n",
      "[6]\teval-error:0.267176\ttrain-error:0.034014\n",
      "[7]\teval-error:0.270992\ttrain-error:0.031098\n",
      "[8]\teval-error:0.263359\ttrain-error:0.030126\n",
      "[9]\teval-error:0.274809\ttrain-error:0.030126\n",
      "[10]\teval-error:0.278626\ttrain-error:0.030126\n",
      "[11]\teval-error:0.282443\ttrain-error:0.02964\n",
      "[12]\teval-error:0.274809\ttrain-error:0.02964\n",
      "[13]\teval-error:0.278626\ttrain-error:0.02964\n",
      "[14]\teval-error:0.267176\ttrain-error:0.02964\n",
      "[15]\teval-error:0.267176\ttrain-error:0.02964\n",
      "[16]\teval-error:0.274809\ttrain-error:0.02964\n",
      "[17]\teval-error:0.274809\ttrain-error:0.02964\n",
      "[18]\teval-error:0.278626\ttrain-error:0.02964\n",
      "[19]\teval-error:0.28626\ttrain-error:0.02964\n",
      "[20]\teval-error:0.278626\ttrain-error:0.02964\n",
      "[21]\teval-error:0.282443\ttrain-error:0.02964\n",
      "[22]\teval-error:0.278626\ttrain-error:0.02964\n",
      "[23]\teval-error:0.274809\ttrain-error:0.02964\n",
      "[24]\teval-error:0.267176\ttrain-error:0.02964\n",
      "[25]\teval-error:0.267176\ttrain-error:0.02964\n",
      "[26]\teval-error:0.263359\ttrain-error:0.02964\n",
      "[27]\teval-error:0.278626\ttrain-error:0.02964\n",
      "[28]\teval-error:0.270992\ttrain-error:0.02964\n",
      "[29]\teval-error:0.282443\ttrain-error:0.02964\n",
      "[30]\teval-error:0.282443\ttrain-error:0.02964\n",
      "[31]\teval-error:0.28626\ttrain-error:0.02964\n",
      "[32]\teval-error:0.267176\ttrain-error:0.02964\n",
      "[33]\teval-error:0.278626\ttrain-error:0.02964\n",
      "[34]\teval-error:0.270992\ttrain-error:0.02964\n",
      "[35]\teval-error:0.282443\ttrain-error:0.02964\n",
      "[36]\teval-error:0.28626\ttrain-error:0.02964\n",
      "[37]\teval-error:0.282443\ttrain-error:0.02964\n",
      "[38]\teval-error:0.278626\ttrain-error:0.02964\n",
      "[39]\teval-error:0.28626\ttrain-error:0.02964\n",
      "[40]\teval-error:0.28626\ttrain-error:0.02964\n",
      "[41]\teval-error:0.282443\ttrain-error:0.02964\n",
      "[42]\teval-error:0.282443\ttrain-error:0.02964\n",
      "[43]\teval-error:0.274809\ttrain-error:0.02964\n",
      "[44]\teval-error:0.267176\ttrain-error:0.02964\n",
      "[45]\teval-error:0.282443\ttrain-error:0.02964\n",
      "[46]\teval-error:0.278626\ttrain-error:0.02964\n",
      "[47]\teval-error:0.267176\ttrain-error:0.02964\n",
      "[48]\teval-error:0.267176\ttrain-error:0.02964\n",
      "[49]\teval-error:0.282443\ttrain-error:0.02964\n",
      "[50]\teval-error:0.278626\ttrain-error:0.02964\n",
      "[51]\teval-error:0.274809\ttrain-error:0.02964\n",
      "[52]\teval-error:0.290076\ttrain-error:0.02964\n",
      "[53]\teval-error:0.28626\ttrain-error:0.02964\n",
      "[54]\teval-error:0.274809\ttrain-error:0.02964\n",
      "[55]\teval-error:0.278626\ttrain-error:0.02964\n",
      "[56]\teval-error:0.278626\ttrain-error:0.02964\n",
      "[57]\teval-error:0.274809\ttrain-error:0.02964\n",
      "[58]\teval-error:0.282443\ttrain-error:0.02964\n",
      "[59]\teval-error:0.267176\ttrain-error:0.02964\n",
      "[60]\teval-error:0.278626\ttrain-error:0.02964\n",
      "[61]\teval-error:0.28626\ttrain-error:0.02964\n",
      "[62]\teval-error:0.274809\ttrain-error:0.02964\n",
      "[63]\teval-error:0.278626\ttrain-error:0.02964\n",
      "[64]\teval-error:0.282443\ttrain-error:0.02964\n",
      "[65]\teval-error:0.282443\ttrain-error:0.02964\n",
      "[66]\teval-error:0.28626\ttrain-error:0.02964\n",
      "[67]\teval-error:0.278626\ttrain-error:0.02964\n",
      "[68]\teval-error:0.278626\ttrain-error:0.02964\n",
      "[69]\teval-error:0.267176\ttrain-error:0.02964\n",
      "[0]\teval-error:0.278626\ttrain-error:0.14723\n",
      "[1]\teval-error:0.274809\ttrain-error:0.122449\n",
      "[2]\teval-error:0.263359\ttrain-error:0.103499\n",
      "[3]\teval-error:0.267176\ttrain-error:0.086492\n",
      "[4]\teval-error:0.278626\ttrain-error:0.074344\n",
      "[5]\teval-error:0.282443\ttrain-error:0.068999\n",
      "[6]\teval-error:0.28626\ttrain-error:0.062682\n",
      "[7]\teval-error:0.282443\ttrain-error:0.06171\n",
      "[8]\teval-error:0.278626\ttrain-error:0.060739\n",
      "[9]\teval-error:0.278626\ttrain-error:0.060739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\teval-error:0.267176\ttrain-error:0.060739\n",
      "[11]\teval-error:0.28626\ttrain-error:0.059767\n",
      "[12]\teval-error:0.278626\ttrain-error:0.059767\n",
      "[13]\teval-error:0.278626\ttrain-error:0.059767\n",
      "[14]\teval-error:0.270992\ttrain-error:0.059767\n",
      "[15]\teval-error:0.274809\ttrain-error:0.059767\n",
      "[16]\teval-error:0.274809\ttrain-error:0.059767\n",
      "[17]\teval-error:0.278626\ttrain-error:0.059767\n",
      "[18]\teval-error:0.274809\ttrain-error:0.059767\n",
      "[19]\teval-error:0.267176\ttrain-error:0.059767\n",
      "[20]\teval-error:0.270992\ttrain-error:0.059767\n",
      "[21]\teval-error:0.267176\ttrain-error:0.059767\n",
      "[22]\teval-error:0.267176\ttrain-error:0.059767\n",
      "[23]\teval-error:0.270992\ttrain-error:0.059767\n",
      "[24]\teval-error:0.270992\ttrain-error:0.059767\n",
      "[25]\teval-error:0.263359\ttrain-error:0.059767\n",
      "[26]\teval-error:0.270992\ttrain-error:0.059767\n",
      "[27]\teval-error:0.270992\ttrain-error:0.059767\n",
      "[28]\teval-error:0.267176\ttrain-error:0.059767\n",
      "[29]\teval-error:0.267176\ttrain-error:0.059767\n",
      "[30]\teval-error:0.274809\ttrain-error:0.059767\n",
      "[31]\teval-error:0.270992\ttrain-error:0.059767\n",
      "[32]\teval-error:0.267176\ttrain-error:0.059767\n",
      "[33]\teval-error:0.267176\ttrain-error:0.059767\n",
      "[34]\teval-error:0.274809\ttrain-error:0.059767\n",
      "[35]\teval-error:0.263359\ttrain-error:0.059767\n",
      "[36]\teval-error:0.259542\ttrain-error:0.059767\n",
      "[37]\teval-error:0.267176\ttrain-error:0.059767\n",
      "[38]\teval-error:0.255725\ttrain-error:0.059767\n",
      "[39]\teval-error:0.259542\ttrain-error:0.059767\n",
      "[40]\teval-error:0.251908\ttrain-error:0.059767\n",
      "[41]\teval-error:0.255725\ttrain-error:0.059767\n",
      "[42]\teval-error:0.263359\ttrain-error:0.059767\n",
      "[43]\teval-error:0.267176\ttrain-error:0.059767\n",
      "[44]\teval-error:0.259542\ttrain-error:0.059767\n",
      "[45]\teval-error:0.255725\ttrain-error:0.059767\n",
      "[46]\teval-error:0.255725\ttrain-error:0.059767\n",
      "[47]\teval-error:0.251908\ttrain-error:0.059767\n",
      "[48]\teval-error:0.251908\ttrain-error:0.059767\n",
      "[49]\teval-error:0.263359\ttrain-error:0.059767\n",
      "[50]\teval-error:0.255725\ttrain-error:0.059767\n",
      "[51]\teval-error:0.263359\ttrain-error:0.059767\n",
      "[52]\teval-error:0.259542\ttrain-error:0.059767\n",
      "[53]\teval-error:0.251908\ttrain-error:0.059767\n",
      "[54]\teval-error:0.251908\ttrain-error:0.059767\n",
      "[55]\teval-error:0.251908\ttrain-error:0.059767\n",
      "[56]\teval-error:0.251908\ttrain-error:0.059767\n",
      "[57]\teval-error:0.251908\ttrain-error:0.059767\n",
      "[58]\teval-error:0.251908\ttrain-error:0.059767\n",
      "[59]\teval-error:0.255725\ttrain-error:0.059767\n",
      "[60]\teval-error:0.251908\ttrain-error:0.059767\n",
      "[61]\teval-error:0.251908\ttrain-error:0.059767\n",
      "[62]\teval-error:0.255725\ttrain-error:0.059767\n",
      "[63]\teval-error:0.255725\ttrain-error:0.059767\n",
      "[64]\teval-error:0.259542\ttrain-error:0.059767\n",
      "[65]\teval-error:0.255725\ttrain-error:0.059767\n",
      "[66]\teval-error:0.259542\ttrain-error:0.059767\n",
      "[67]\teval-error:0.255725\ttrain-error:0.059767\n",
      "[68]\teval-error:0.259542\ttrain-error:0.059767\n",
      "[69]\teval-error:0.255725\ttrain-error:0.059767\n",
      "[0]\teval-error:0.217557\ttrain-error:0.166181\n",
      "[1]\teval-error:0.232824\ttrain-error:0.154033\n",
      "[2]\teval-error:0.248092\ttrain-error:0.144315\n",
      "[3]\teval-error:0.240458\ttrain-error:0.126336\n",
      "[4]\teval-error:0.244275\ttrain-error:0.118076\n",
      "[5]\teval-error:0.244275\ttrain-error:0.114189\n",
      "[6]\teval-error:0.240458\ttrain-error:0.109815\n",
      "[7]\teval-error:0.251908\ttrain-error:0.109815\n",
      "[8]\teval-error:0.259542\ttrain-error:0.10447\n",
      "[9]\teval-error:0.255725\ttrain-error:0.103499\n",
      "[10]\teval-error:0.251908\ttrain-error:0.102527\n",
      "[11]\teval-error:0.248092\ttrain-error:0.100097\n",
      "[12]\teval-error:0.236641\ttrain-error:0.100097\n",
      "[13]\teval-error:0.248092\ttrain-error:0.099611\n",
      "[14]\teval-error:0.255725\ttrain-error:0.099125\n",
      "[15]\teval-error:0.255725\ttrain-error:0.099125\n",
      "[16]\teval-error:0.248092\ttrain-error:0.099611\n",
      "[17]\teval-error:0.244275\ttrain-error:0.099125\n",
      "[18]\teval-error:0.255725\ttrain-error:0.099125\n",
      "[19]\teval-error:0.248092\ttrain-error:0.099125\n",
      "[20]\teval-error:0.248092\ttrain-error:0.099125\n",
      "[21]\teval-error:0.248092\ttrain-error:0.099125\n",
      "[22]\teval-error:0.251908\ttrain-error:0.099125\n",
      "[23]\teval-error:0.251908\ttrain-error:0.099125\n",
      "[24]\teval-error:0.240458\ttrain-error:0.099125\n",
      "[25]\teval-error:0.236641\ttrain-error:0.099125\n",
      "[26]\teval-error:0.240458\ttrain-error:0.099125\n",
      "[27]\teval-error:0.248092\ttrain-error:0.099125\n",
      "[28]\teval-error:0.248092\ttrain-error:0.099125\n",
      "[29]\teval-error:0.248092\ttrain-error:0.099125\n",
      "[30]\teval-error:0.240458\ttrain-error:0.099125\n",
      "[31]\teval-error:0.251908\ttrain-error:0.099125\n",
      "[32]\teval-error:0.251908\ttrain-error:0.099125\n",
      "[33]\teval-error:0.248092\ttrain-error:0.099125\n",
      "[34]\teval-error:0.248092\ttrain-error:0.099125\n",
      "[35]\teval-error:0.248092\ttrain-error:0.099125\n",
      "[36]\teval-error:0.251908\ttrain-error:0.099125\n",
      "[37]\teval-error:0.251908\ttrain-error:0.099125\n",
      "[38]\teval-error:0.251908\ttrain-error:0.099125\n",
      "[39]\teval-error:0.255725\ttrain-error:0.099125\n",
      "[40]\teval-error:0.263359\ttrain-error:0.099125\n",
      "[41]\teval-error:0.259542\ttrain-error:0.099125\n",
      "[42]\teval-error:0.248092\ttrain-error:0.099125\n",
      "[43]\teval-error:0.259542\ttrain-error:0.099125\n",
      "[44]\teval-error:0.259542\ttrain-error:0.099125\n",
      "[45]\teval-error:0.255725\ttrain-error:0.099125\n",
      "[46]\teval-error:0.255725\ttrain-error:0.099125\n",
      "[47]\teval-error:0.255725\ttrain-error:0.099125\n",
      "[48]\teval-error:0.263359\ttrain-error:0.099125\n",
      "[49]\teval-error:0.255725\ttrain-error:0.099125\n",
      "[50]\teval-error:0.251908\ttrain-error:0.099125\n",
      "[51]\teval-error:0.251908\ttrain-error:0.099125\n",
      "[52]\teval-error:0.255725\ttrain-error:0.099125\n",
      "[53]\teval-error:0.255725\ttrain-error:0.099125\n",
      "[54]\teval-error:0.259542\ttrain-error:0.099125\n",
      "[55]\teval-error:0.259542\ttrain-error:0.099125\n",
      "[56]\teval-error:0.251908\ttrain-error:0.099125\n",
      "[57]\teval-error:0.259542\ttrain-error:0.099125\n",
      "[58]\teval-error:0.259542\ttrain-error:0.099125\n",
      "[59]\teval-error:0.251908\ttrain-error:0.099125\n",
      "[60]\teval-error:0.263359\ttrain-error:0.099125\n",
      "[61]\teval-error:0.259542\ttrain-error:0.099125\n",
      "[62]\teval-error:0.259542\ttrain-error:0.099125\n",
      "[63]\teval-error:0.255725\ttrain-error:0.099125\n",
      "[64]\teval-error:0.263359\ttrain-error:0.099125\n",
      "[65]\teval-error:0.267176\ttrain-error:0.099125\n",
      "[66]\teval-error:0.259542\ttrain-error:0.099125\n",
      "[67]\teval-error:0.259542\ttrain-error:0.099125\n",
      "[68]\teval-error:0.251908\ttrain-error:0.099125\n",
      "[69]\teval-error:0.251908\ttrain-error:0.099125\n",
      "[0]\teval-error:0.301527\ttrain-error:0.134111\n",
      "[1]\teval-error:0.267176\ttrain-error:0.100583\n",
      "[2]\teval-error:0.263359\ttrain-error:0.073858\n",
      "[3]\teval-error:0.263359\ttrain-error:0.05102\n",
      "[4]\teval-error:0.274809\ttrain-error:0.04033\n",
      "[5]\teval-error:0.263359\ttrain-error:0.033528\n",
      "[6]\teval-error:0.270992\ttrain-error:0.028669\n",
      "[7]\teval-error:0.270992\ttrain-error:0.029155\n",
      "[8]\teval-error:0.282443\ttrain-error:0.028669\n",
      "[9]\teval-error:0.270992\ttrain-error:0.028669\n",
      "[10]\teval-error:0.274809\ttrain-error:0.028669\n",
      "[11]\teval-error:0.267176\ttrain-error:0.028669\n",
      "[12]\teval-error:0.278626\ttrain-error:0.028669\n",
      "[13]\teval-error:0.282443\ttrain-error:0.028669\n",
      "[14]\teval-error:0.28626\ttrain-error:0.028669\n",
      "[15]\teval-error:0.28626\ttrain-error:0.028669\n",
      "[16]\teval-error:0.270992\ttrain-error:0.028669\n",
      "[17]\teval-error:0.282443\ttrain-error:0.028669\n",
      "[18]\teval-error:0.274809\ttrain-error:0.028669\n",
      "[19]\teval-error:0.274809\ttrain-error:0.028669\n",
      "[20]\teval-error:0.278626\ttrain-error:0.028669\n",
      "[21]\teval-error:0.278626\ttrain-error:0.028669\n",
      "[22]\teval-error:0.278626\ttrain-error:0.028669\n",
      "[23]\teval-error:0.282443\ttrain-error:0.028669\n",
      "[24]\teval-error:0.278626\ttrain-error:0.028669\n",
      "[25]\teval-error:0.270992\ttrain-error:0.028669\n",
      "[26]\teval-error:0.274809\ttrain-error:0.028669\n",
      "[27]\teval-error:0.28626\ttrain-error:0.028669\n",
      "[28]\teval-error:0.282443\ttrain-error:0.028669\n",
      "[29]\teval-error:0.28626\ttrain-error:0.028669\n",
      "[30]\teval-error:0.282443\ttrain-error:0.028669\n",
      "[31]\teval-error:0.28626\ttrain-error:0.028669\n",
      "[32]\teval-error:0.290076\ttrain-error:0.028669\n",
      "[33]\teval-error:0.282443\ttrain-error:0.028669\n",
      "[34]\teval-error:0.282443\ttrain-error:0.028669\n",
      "[35]\teval-error:0.282443\ttrain-error:0.028669\n",
      "[36]\teval-error:0.278626\ttrain-error:0.028669\n",
      "[37]\teval-error:0.278626\ttrain-error:0.028669\n",
      "[38]\teval-error:0.270992\ttrain-error:0.028669\n",
      "[39]\teval-error:0.274809\ttrain-error:0.028669\n",
      "[40]\teval-error:0.278626\ttrain-error:0.028669\n",
      "[41]\teval-error:0.282443\ttrain-error:0.028669\n",
      "[42]\teval-error:0.282443\ttrain-error:0.028669\n",
      "[43]\teval-error:0.282443\ttrain-error:0.028669\n",
      "[44]\teval-error:0.282443\ttrain-error:0.028669\n",
      "[45]\teval-error:0.274809\ttrain-error:0.028669\n",
      "[46]\teval-error:0.278626\ttrain-error:0.028669\n",
      "[47]\teval-error:0.278626\ttrain-error:0.028669\n",
      "[48]\teval-error:0.270992\ttrain-error:0.028669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49]\teval-error:0.274809\ttrain-error:0.028669\n",
      "[50]\teval-error:0.278626\ttrain-error:0.028669\n",
      "[51]\teval-error:0.274809\ttrain-error:0.028669\n",
      "[52]\teval-error:0.270992\ttrain-error:0.028669\n",
      "[53]\teval-error:0.270992\ttrain-error:0.028669\n",
      "[54]\teval-error:0.274809\ttrain-error:0.028669\n",
      "[55]\teval-error:0.274809\ttrain-error:0.028669\n",
      "[56]\teval-error:0.278626\ttrain-error:0.028669\n",
      "[57]\teval-error:0.270992\ttrain-error:0.028669\n",
      "[58]\teval-error:0.270992\ttrain-error:0.028669\n",
      "[59]\teval-error:0.270992\ttrain-error:0.028669\n",
      "[60]\teval-error:0.270992\ttrain-error:0.028669\n",
      "[61]\teval-error:0.270992\ttrain-error:0.028669\n",
      "[62]\teval-error:0.270992\ttrain-error:0.028669\n",
      "[63]\teval-error:0.270992\ttrain-error:0.028669\n",
      "[64]\teval-error:0.270992\ttrain-error:0.028669\n",
      "[65]\teval-error:0.274809\ttrain-error:0.028669\n",
      "[66]\teval-error:0.270992\ttrain-error:0.028669\n",
      "[67]\teval-error:0.267176\ttrain-error:0.028669\n",
      "[68]\teval-error:0.267176\ttrain-error:0.028669\n",
      "[69]\teval-error:0.270992\ttrain-error:0.028669\n",
      "[0]\teval-error:0.238636\ttrain-error:0.10585\n",
      "[1]\teval-error:0.204545\ttrain-error:0.036212\n",
      "[2]\teval-error:0.181818\ttrain-error:0.013928\n",
      "[3]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[4]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[5]\teval-error:0.204545\ttrain-error:0.002786\n",
      "[6]\teval-error:0.204545\ttrain-error:0.002786\n",
      "[7]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[8]\teval-error:0.204545\ttrain-error:0.002786\n",
      "[9]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[10]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[11]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[12]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[13]\teval-error:0.181818\ttrain-error:0.002786\n",
      "[14]\teval-error:0.181818\ttrain-error:0.002786\n",
      "[15]\teval-error:0.181818\ttrain-error:0.002786\n",
      "[16]\teval-error:0.181818\ttrain-error:0.002786\n",
      "[17]\teval-error:0.181818\ttrain-error:0.002786\n",
      "[18]\teval-error:0.181818\ttrain-error:0.002786\n",
      "[19]\teval-error:0.181818\ttrain-error:0.002786\n",
      "[20]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[21]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[22]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[23]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[24]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[25]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[26]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[27]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[28]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[29]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[30]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[31]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[32]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[33]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[34]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[35]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[36]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[37]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[38]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[39]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[40]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[41]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[42]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[43]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[44]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[45]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[46]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[47]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[48]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[49]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[50]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[51]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[52]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[53]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[54]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[55]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[56]\teval-error:0.204545\ttrain-error:0.002786\n",
      "[57]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[58]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[59]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[60]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[61]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[62]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[63]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[64]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[65]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[66]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[67]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[68]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[69]\teval-error:0.193182\ttrain-error:0.002786\n",
      "[0]\teval-error:0.227273\ttrain-error:0.082173\n",
      "[1]\teval-error:0.238636\ttrain-error:0.036212\n",
      "[2]\teval-error:0.204545\ttrain-error:0.016713\n",
      "[3]\teval-error:0.215909\ttrain-error:0.006964\n",
      "[4]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[5]\teval-error:0.181818\ttrain-error:0.002786\n",
      "[6]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[7]\teval-error:0.181818\ttrain-error:0.002786\n",
      "[8]\teval-error:0.159091\ttrain-error:0.002786\n",
      "[9]\teval-error:0.147727\ttrain-error:0.002786\n",
      "[10]\teval-error:0.159091\ttrain-error:0.002786\n",
      "[11]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[12]\teval-error:0.147727\ttrain-error:0.002786\n",
      "[13]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[14]\teval-error:0.159091\ttrain-error:0.002786\n",
      "[15]\teval-error:0.181818\ttrain-error:0.002786\n",
      "[16]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[17]\teval-error:0.181818\ttrain-error:0.002786\n",
      "[18]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[19]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[20]\teval-error:0.181818\ttrain-error:0.002786\n",
      "[21]\teval-error:0.159091\ttrain-error:0.002786\n",
      "[22]\teval-error:0.181818\ttrain-error:0.002786\n",
      "[23]\teval-error:0.159091\ttrain-error:0.002786\n",
      "[24]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[25]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[26]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[27]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[28]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[29]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[30]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[31]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[32]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[33]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[34]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[35]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[36]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[37]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[38]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[39]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[40]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[41]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[42]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[43]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[44]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[45]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[46]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[47]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[48]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[49]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[50]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[51]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[52]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[53]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[54]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[55]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[56]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[57]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[58]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[59]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[60]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[61]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[62]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[63]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[64]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[65]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[66]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[67]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[68]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[69]\teval-error:0.170455\ttrain-error:0.002786\n",
      "[0]\teval-error:0.352273\ttrain-error:0.107242\n",
      "[1]\teval-error:0.272727\ttrain-error:0.061281\n",
      "[2]\teval-error:0.25\ttrain-error:0.027855\n",
      "[3]\teval-error:0.272727\ttrain-error:0.013928\n",
      "[4]\teval-error:0.261364\ttrain-error:0.004178\n",
      "[5]\teval-error:0.272727\ttrain-error:0.004178\n",
      "[6]\teval-error:0.272727\ttrain-error:0.004178\n",
      "[7]\teval-error:0.284091\ttrain-error:0.004178\n",
      "[8]\teval-error:0.284091\ttrain-error:0.004178\n",
      "[9]\teval-error:0.272727\ttrain-error:0.002786\n",
      "[10]\teval-error:0.25\ttrain-error:0.002786\n",
      "[11]\teval-error:0.25\ttrain-error:0.002786\n",
      "[12]\teval-error:0.272727\ttrain-error:0.002786\n",
      "[13]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[14]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[15]\teval-error:0.284091\ttrain-error:0.002786\n",
      "[16]\teval-error:0.272727\ttrain-error:0.002786\n",
      "[17]\teval-error:0.272727\ttrain-error:0.002786\n",
      "[18]\teval-error:0.261364\ttrain-error:0.002786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[20]\teval-error:0.25\ttrain-error:0.002786\n",
      "[21]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[22]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[23]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[24]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[25]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[26]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[27]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[28]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[29]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[30]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[31]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[32]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[33]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[34]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[35]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[36]\teval-error:0.272727\ttrain-error:0.002786\n",
      "[37]\teval-error:0.272727\ttrain-error:0.002786\n",
      "[38]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[39]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[40]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[41]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[42]\teval-error:0.272727\ttrain-error:0.002786\n",
      "[43]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[44]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[45]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[46]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[47]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[48]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[49]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[50]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[51]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[52]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[53]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[54]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[55]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[56]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[57]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[58]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[59]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[60]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[61]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[62]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[63]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[64]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[65]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[66]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[67]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[68]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[69]\teval-error:0.261364\ttrain-error:0.002786\n",
      "[0]\teval-error:0.25\ttrain-error:0.089136\n",
      "[1]\teval-error:0.170455\ttrain-error:0.038997\n",
      "[2]\teval-error:0.159091\ttrain-error:0.013928\n",
      "[3]\teval-error:0.159091\ttrain-error:0.004178\n",
      "[4]\teval-error:0.159091\ttrain-error:0.004178\n",
      "[5]\teval-error:0.159091\ttrain-error:0.002786\n",
      "[6]\teval-error:0.136364\ttrain-error:0.004178\n",
      "[7]\teval-error:0.136364\ttrain-error:0.004178\n",
      "[8]\teval-error:0.136364\ttrain-error:0.004178\n",
      "[9]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[10]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[11]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[12]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[13]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[14]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[15]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[16]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[17]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[18]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[19]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[20]\teval-error:0.147727\ttrain-error:0.002786\n",
      "[21]\teval-error:0.147727\ttrain-error:0.002786\n",
      "[22]\teval-error:0.147727\ttrain-error:0.002786\n",
      "[23]\teval-error:0.147727\ttrain-error:0.002786\n",
      "[24]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[25]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[26]\teval-error:0.147727\ttrain-error:0.002786\n",
      "[27]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[28]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[29]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[30]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[31]\teval-error:0.147727\ttrain-error:0.002786\n",
      "[32]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[33]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[34]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[35]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[36]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[37]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[38]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[39]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[40]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[41]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[42]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[43]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[44]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[45]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[46]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[47]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[48]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[49]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[50]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[51]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[52]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[53]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[54]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[55]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[56]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[57]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[58]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[59]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[60]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[61]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[62]\teval-error:0.147727\ttrain-error:0.002786\n",
      "[63]\teval-error:0.147727\ttrain-error:0.002786\n",
      "[64]\teval-error:0.147727\ttrain-error:0.002786\n",
      "[65]\teval-error:0.147727\ttrain-error:0.002786\n",
      "[66]\teval-error:0.136364\ttrain-error:0.002786\n",
      "[67]\teval-error:0.147727\ttrain-error:0.002786\n",
      "[68]\teval-error:0.147727\ttrain-error:0.002786\n",
      "[69]\teval-error:0.147727\ttrain-error:0.002786\n",
      "[0]\teval-error:0.297872\ttrain-error:0.104447\n",
      "[1]\teval-error:0.234043\ttrain-error:0.048604\n",
      "[2]\teval-error:0.244681\ttrain-error:0.026887\n",
      "[3]\teval-error:0.255319\ttrain-error:0.01758\n",
      "[4]\teval-error:0.244681\ttrain-error:0.015512\n",
      "[5]\teval-error:0.223404\ttrain-error:0.015512\n",
      "[6]\teval-error:0.244681\ttrain-error:0.015512\n",
      "[7]\teval-error:0.265957\ttrain-error:0.014478\n",
      "[8]\teval-error:0.255319\ttrain-error:0.014478\n",
      "[9]\teval-error:0.255319\ttrain-error:0.014478\n",
      "[10]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[11]\teval-error:0.265957\ttrain-error:0.014478\n",
      "[12]\teval-error:0.255319\ttrain-error:0.014478\n",
      "[13]\teval-error:0.223404\ttrain-error:0.014478\n",
      "[14]\teval-error:0.234043\ttrain-error:0.014478\n",
      "[15]\teval-error:0.234043\ttrain-error:0.014478\n",
      "[16]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[17]\teval-error:0.234043\ttrain-error:0.014478\n",
      "[18]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[19]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[20]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[21]\teval-error:0.223404\ttrain-error:0.014478\n",
      "[22]\teval-error:0.223404\ttrain-error:0.014478\n",
      "[23]\teval-error:0.234043\ttrain-error:0.014478\n",
      "[24]\teval-error:0.223404\ttrain-error:0.014478\n",
      "[25]\teval-error:0.234043\ttrain-error:0.014478\n",
      "[26]\teval-error:0.234043\ttrain-error:0.014478\n",
      "[27]\teval-error:0.234043\ttrain-error:0.014478\n",
      "[28]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[29]\teval-error:0.234043\ttrain-error:0.014478\n",
      "[30]\teval-error:0.234043\ttrain-error:0.014478\n",
      "[31]\teval-error:0.234043\ttrain-error:0.014478\n",
      "[32]\teval-error:0.234043\ttrain-error:0.014478\n",
      "[33]\teval-error:0.234043\ttrain-error:0.014478\n",
      "[34]\teval-error:0.234043\ttrain-error:0.014478\n",
      "[35]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[36]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[37]\teval-error:0.234043\ttrain-error:0.014478\n",
      "[38]\teval-error:0.255319\ttrain-error:0.014478\n",
      "[39]\teval-error:0.255319\ttrain-error:0.014478\n",
      "[40]\teval-error:0.255319\ttrain-error:0.014478\n",
      "[41]\teval-error:0.255319\ttrain-error:0.014478\n",
      "[42]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[43]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[44]\teval-error:0.234043\ttrain-error:0.014478\n",
      "[45]\teval-error:0.234043\ttrain-error:0.014478\n",
      "[46]\teval-error:0.234043\ttrain-error:0.014478\n",
      "[47]\teval-error:0.234043\ttrain-error:0.014478\n",
      "[48]\teval-error:0.234043\ttrain-error:0.014478\n",
      "[49]\teval-error:0.234043\ttrain-error:0.014478\n",
      "[50]\teval-error:0.234043\ttrain-error:0.014478\n",
      "[51]\teval-error:0.223404\ttrain-error:0.014478\n",
      "[52]\teval-error:0.234043\ttrain-error:0.014478\n",
      "[53]\teval-error:0.234043\ttrain-error:0.014478\n",
      "[54]\teval-error:0.234043\ttrain-error:0.014478\n",
      "[55]\teval-error:0.234043\ttrain-error:0.014478\n",
      "[56]\teval-error:0.234043\ttrain-error:0.014478\n",
      "[57]\teval-error:0.234043\ttrain-error:0.014478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58]\teval-error:0.234043\ttrain-error:0.014478\n",
      "[59]\teval-error:0.234043\ttrain-error:0.014478\n",
      "[60]\teval-error:0.223404\ttrain-error:0.014478\n",
      "[61]\teval-error:0.223404\ttrain-error:0.014478\n",
      "[62]\teval-error:0.223404\ttrain-error:0.014478\n",
      "[63]\teval-error:0.223404\ttrain-error:0.014478\n",
      "[64]\teval-error:0.223404\ttrain-error:0.014478\n",
      "[65]\teval-error:0.223404\ttrain-error:0.014478\n",
      "[66]\teval-error:0.223404\ttrain-error:0.014478\n",
      "[67]\teval-error:0.212766\ttrain-error:0.014478\n",
      "[68]\teval-error:0.212766\ttrain-error:0.014478\n",
      "[69]\teval-error:0.212766\ttrain-error:0.014478\n",
      "[0]\teval-error:0.297872\ttrain-error:0.110652\n",
      "[1]\teval-error:0.297872\ttrain-error:0.054809\n",
      "[2]\teval-error:0.276596\ttrain-error:0.028956\n",
      "[3]\teval-error:0.255319\ttrain-error:0.020683\n",
      "[4]\teval-error:0.265957\ttrain-error:0.01758\n",
      "[5]\teval-error:0.255319\ttrain-error:0.014478\n",
      "[6]\teval-error:0.265957\ttrain-error:0.014478\n",
      "[7]\teval-error:0.223404\ttrain-error:0.014478\n",
      "[8]\teval-error:0.234043\ttrain-error:0.014478\n",
      "[9]\teval-error:0.234043\ttrain-error:0.014478\n",
      "[10]\teval-error:0.265957\ttrain-error:0.014478\n",
      "[11]\teval-error:0.276596\ttrain-error:0.014478\n",
      "[12]\teval-error:0.265957\ttrain-error:0.014478\n",
      "[13]\teval-error:0.265957\ttrain-error:0.014478\n",
      "[14]\teval-error:0.265957\ttrain-error:0.014478\n",
      "[15]\teval-error:0.255319\ttrain-error:0.014478\n",
      "[16]\teval-error:0.255319\ttrain-error:0.014478\n",
      "[17]\teval-error:0.265957\ttrain-error:0.014478\n",
      "[18]\teval-error:0.265957\ttrain-error:0.014478\n",
      "[19]\teval-error:0.255319\ttrain-error:0.014478\n",
      "[20]\teval-error:0.255319\ttrain-error:0.014478\n",
      "[21]\teval-error:0.255319\ttrain-error:0.014478\n",
      "[22]\teval-error:0.265957\ttrain-error:0.014478\n",
      "[23]\teval-error:0.255319\ttrain-error:0.014478\n",
      "[24]\teval-error:0.255319\ttrain-error:0.014478\n",
      "[25]\teval-error:0.255319\ttrain-error:0.014478\n",
      "[26]\teval-error:0.255319\ttrain-error:0.014478\n",
      "[27]\teval-error:0.255319\ttrain-error:0.014478\n",
      "[28]\teval-error:0.255319\ttrain-error:0.014478\n",
      "[29]\teval-error:0.255319\ttrain-error:0.014478\n",
      "[30]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[31]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[32]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[33]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[34]\teval-error:0.255319\ttrain-error:0.014478\n",
      "[35]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[36]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[37]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[38]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[39]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[40]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[41]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[42]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[43]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[44]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[45]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[46]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[47]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[48]\teval-error:0.255319\ttrain-error:0.014478\n",
      "[49]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[50]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[51]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[52]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[53]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[54]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[55]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[56]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[57]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[58]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[59]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[60]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[61]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[62]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[63]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[64]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[65]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[66]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[67]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[68]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[69]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[0]\teval-error:0.361702\ttrain-error:0.113754\n",
      "[1]\teval-error:0.308511\ttrain-error:0.06515\n",
      "[2]\teval-error:0.308511\ttrain-error:0.042399\n",
      "[3]\teval-error:0.287234\ttrain-error:0.022751\n",
      "[4]\teval-error:0.276596\ttrain-error:0.018614\n",
      "[5]\teval-error:0.276596\ttrain-error:0.01758\n",
      "[6]\teval-error:0.276596\ttrain-error:0.01758\n",
      "[7]\teval-error:0.255319\ttrain-error:0.016546\n",
      "[8]\teval-error:0.265957\ttrain-error:0.016546\n",
      "[9]\teval-error:0.255319\ttrain-error:0.016546\n",
      "[10]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[11]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[12]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[13]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[14]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[15]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[16]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[17]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[18]\teval-error:0.255319\ttrain-error:0.016546\n",
      "[19]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[20]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[21]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[22]\teval-error:0.234043\ttrain-error:0.016546\n",
      "[23]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[24]\teval-error:0.234043\ttrain-error:0.016546\n",
      "[25]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[26]\teval-error:0.234043\ttrain-error:0.016546\n",
      "[27]\teval-error:0.234043\ttrain-error:0.016546\n",
      "[28]\teval-error:0.234043\ttrain-error:0.016546\n",
      "[29]\teval-error:0.234043\ttrain-error:0.016546\n",
      "[30]\teval-error:0.234043\ttrain-error:0.016546\n",
      "[31]\teval-error:0.234043\ttrain-error:0.016546\n",
      "[32]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[33]\teval-error:0.234043\ttrain-error:0.016546\n",
      "[34]\teval-error:0.234043\ttrain-error:0.016546\n",
      "[35]\teval-error:0.234043\ttrain-error:0.016546\n",
      "[36]\teval-error:0.234043\ttrain-error:0.016546\n",
      "[37]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[38]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[39]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[40]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[41]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[42]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[43]\teval-error:0.234043\ttrain-error:0.016546\n",
      "[44]\teval-error:0.234043\ttrain-error:0.016546\n",
      "[45]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[46]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[47]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[48]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[49]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[50]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[51]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[52]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[53]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[54]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[55]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[56]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[57]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[58]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[59]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[60]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[61]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[62]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[63]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[64]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[65]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[66]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[67]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[68]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[69]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[0]\teval-error:0.297872\ttrain-error:0.105481\n",
      "[1]\teval-error:0.234043\ttrain-error:0.055843\n",
      "[2]\teval-error:0.255319\ttrain-error:0.022751\n",
      "[3]\teval-error:0.244681\ttrain-error:0.018614\n",
      "[4]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[5]\teval-error:0.234043\ttrain-error:0.016546\n",
      "[6]\teval-error:0.234043\ttrain-error:0.015512\n",
      "[7]\teval-error:0.234043\ttrain-error:0.015512\n",
      "[8]\teval-error:0.255319\ttrain-error:0.015512\n",
      "[9]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[10]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[11]\teval-error:0.212766\ttrain-error:0.014478\n",
      "[12]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[13]\teval-error:0.234043\ttrain-error:0.014478\n",
      "[14]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[15]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[16]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[17]\teval-error:0.234043\ttrain-error:0.014478\n",
      "[18]\teval-error:0.234043\ttrain-error:0.014478\n",
      "[19]\teval-error:0.255319\ttrain-error:0.014478\n",
      "[20]\teval-error:0.234043\ttrain-error:0.014478\n",
      "[21]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[22]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[23]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[24]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[25]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[26]\teval-error:0.255319\ttrain-error:0.014478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27]\teval-error:0.255319\ttrain-error:0.014478\n",
      "[28]\teval-error:0.255319\ttrain-error:0.014478\n",
      "[29]\teval-error:0.255319\ttrain-error:0.014478\n",
      "[30]\teval-error:0.255319\ttrain-error:0.014478\n",
      "[31]\teval-error:0.244681\ttrain-error:0.014478\n",
      "[32]\teval-error:0.234043\ttrain-error:0.014478\n",
      "[33]\teval-error:0.234043\ttrain-error:0.014478\n",
      "[34]\teval-error:0.234043\ttrain-error:0.014478\n",
      "[35]\teval-error:0.234043\ttrain-error:0.014478\n",
      "[36]\teval-error:0.234043\ttrain-error:0.014478\n",
      "[37]\teval-error:0.223404\ttrain-error:0.014478\n",
      "[38]\teval-error:0.212766\ttrain-error:0.014478\n",
      "[39]\teval-error:0.212766\ttrain-error:0.014478\n",
      "[40]\teval-error:0.223404\ttrain-error:0.014478\n",
      "[41]\teval-error:0.223404\ttrain-error:0.014478\n",
      "[42]\teval-error:0.223404\ttrain-error:0.014478\n",
      "[43]\teval-error:0.223404\ttrain-error:0.014478\n",
      "[44]\teval-error:0.223404\ttrain-error:0.014478\n",
      "[45]\teval-error:0.223404\ttrain-error:0.014478\n",
      "[46]\teval-error:0.223404\ttrain-error:0.014478\n",
      "[47]\teval-error:0.223404\ttrain-error:0.014478\n",
      "[48]\teval-error:0.223404\ttrain-error:0.014478\n",
      "[49]\teval-error:0.223404\ttrain-error:0.014478\n",
      "[50]\teval-error:0.223404\ttrain-error:0.014478\n",
      "[51]\teval-error:0.223404\ttrain-error:0.014478\n",
      "[52]\teval-error:0.223404\ttrain-error:0.014478\n",
      "[53]\teval-error:0.223404\ttrain-error:0.014478\n",
      "[54]\teval-error:0.223404\ttrain-error:0.014478\n",
      "[55]\teval-error:0.223404\ttrain-error:0.014478\n",
      "[56]\teval-error:0.223404\ttrain-error:0.014478\n",
      "[57]\teval-error:0.223404\ttrain-error:0.014478\n",
      "[58]\teval-error:0.223404\ttrain-error:0.014478\n",
      "[59]\teval-error:0.212766\ttrain-error:0.014478\n",
      "[60]\teval-error:0.212766\ttrain-error:0.014478\n",
      "[61]\teval-error:0.212766\ttrain-error:0.014478\n",
      "[62]\teval-error:0.212766\ttrain-error:0.014478\n",
      "[63]\teval-error:0.212766\ttrain-error:0.014478\n",
      "[64]\teval-error:0.223404\ttrain-error:0.014478\n",
      "[65]\teval-error:0.223404\ttrain-error:0.014478\n",
      "[66]\teval-error:0.223404\ttrain-error:0.014478\n",
      "[67]\teval-error:0.223404\ttrain-error:0.014478\n",
      "[68]\teval-error:0.223404\ttrain-error:0.014478\n",
      "[69]\teval-error:0.223404\ttrain-error:0.014478\n",
      "[0]\teval-error:0.270992\ttrain-error:0.089407\n",
      "[1]\teval-error:0.236641\ttrain-error:0.049563\n",
      "[2]\teval-error:0.251908\ttrain-error:0.031098\n",
      "[3]\teval-error:0.248092\ttrain-error:0.023324\n",
      "[4]\teval-error:0.251908\ttrain-error:0.019922\n",
      "[5]\teval-error:0.240458\ttrain-error:0.01895\n",
      "[6]\teval-error:0.240458\ttrain-error:0.018465\n",
      "[7]\teval-error:0.236641\ttrain-error:0.018465\n",
      "[8]\teval-error:0.244275\ttrain-error:0.018465\n",
      "[9]\teval-error:0.251908\ttrain-error:0.018465\n",
      "[10]\teval-error:0.248092\ttrain-error:0.018465\n",
      "[11]\teval-error:0.255725\ttrain-error:0.018465\n",
      "[12]\teval-error:0.255725\ttrain-error:0.018465\n",
      "[13]\teval-error:0.240458\ttrain-error:0.018465\n",
      "[14]\teval-error:0.244275\ttrain-error:0.018465\n",
      "[15]\teval-error:0.251908\ttrain-error:0.018465\n",
      "[16]\teval-error:0.255725\ttrain-error:0.018465\n",
      "[17]\teval-error:0.251908\ttrain-error:0.018465\n",
      "[18]\teval-error:0.251908\ttrain-error:0.018465\n",
      "[19]\teval-error:0.251908\ttrain-error:0.018465\n",
      "[20]\teval-error:0.251908\ttrain-error:0.018465\n",
      "[21]\teval-error:0.255725\ttrain-error:0.018465\n",
      "[22]\teval-error:0.251908\ttrain-error:0.018465\n",
      "[23]\teval-error:0.259542\ttrain-error:0.018465\n",
      "[24]\teval-error:0.259542\ttrain-error:0.018465\n",
      "[25]\teval-error:0.259542\ttrain-error:0.018465\n",
      "[26]\teval-error:0.240458\ttrain-error:0.018465\n",
      "[27]\teval-error:0.244275\ttrain-error:0.018465\n",
      "[28]\teval-error:0.240458\ttrain-error:0.018465\n",
      "[29]\teval-error:0.244275\ttrain-error:0.018465\n",
      "[30]\teval-error:0.236641\ttrain-error:0.018465\n",
      "[31]\teval-error:0.240458\ttrain-error:0.018465\n",
      "[32]\teval-error:0.255725\ttrain-error:0.018465\n",
      "[33]\teval-error:0.240458\ttrain-error:0.018465\n",
      "[34]\teval-error:0.248092\ttrain-error:0.018465\n",
      "[35]\teval-error:0.244275\ttrain-error:0.018465\n",
      "[36]\teval-error:0.236641\ttrain-error:0.018465\n",
      "[37]\teval-error:0.248092\ttrain-error:0.018465\n",
      "[38]\teval-error:0.251908\ttrain-error:0.018465\n",
      "[39]\teval-error:0.255725\ttrain-error:0.018465\n",
      "[40]\teval-error:0.244275\ttrain-error:0.018465\n",
      "[41]\teval-error:0.244275\ttrain-error:0.018465\n",
      "[42]\teval-error:0.240458\ttrain-error:0.018465\n",
      "[43]\teval-error:0.248092\ttrain-error:0.018465\n",
      "[44]\teval-error:0.232824\ttrain-error:0.018465\n",
      "[45]\teval-error:0.232824\ttrain-error:0.018465\n",
      "[46]\teval-error:0.236641\ttrain-error:0.018465\n",
      "[47]\teval-error:0.240458\ttrain-error:0.018465\n",
      "[48]\teval-error:0.244275\ttrain-error:0.018465\n",
      "[49]\teval-error:0.244275\ttrain-error:0.018465\n",
      "[50]\teval-error:0.244275\ttrain-error:0.018465\n",
      "[51]\teval-error:0.248092\ttrain-error:0.018465\n",
      "[52]\teval-error:0.248092\ttrain-error:0.018465\n",
      "[53]\teval-error:0.251908\ttrain-error:0.018465\n",
      "[54]\teval-error:0.240458\ttrain-error:0.018465\n",
      "[55]\teval-error:0.248092\ttrain-error:0.018465\n",
      "[56]\teval-error:0.251908\ttrain-error:0.018465\n",
      "[57]\teval-error:0.255725\ttrain-error:0.018465\n",
      "[58]\teval-error:0.259542\ttrain-error:0.018465\n",
      "[59]\teval-error:0.251908\ttrain-error:0.018465\n",
      "[60]\teval-error:0.251908\ttrain-error:0.018465\n",
      "[61]\teval-error:0.259542\ttrain-error:0.018465\n",
      "[62]\teval-error:0.248092\ttrain-error:0.018465\n",
      "[63]\teval-error:0.248092\ttrain-error:0.018465\n",
      "[64]\teval-error:0.251908\ttrain-error:0.018465\n",
      "[65]\teval-error:0.255725\ttrain-error:0.018465\n",
      "[66]\teval-error:0.259542\ttrain-error:0.018465\n",
      "[67]\teval-error:0.259542\ttrain-error:0.018465\n",
      "[68]\teval-error:0.251908\ttrain-error:0.018465\n",
      "[69]\teval-error:0.255725\ttrain-error:0.018465\n",
      "[0]\teval-error:0.244275\ttrain-error:0.095238\n",
      "[1]\teval-error:0.217557\ttrain-error:0.051992\n",
      "[2]\teval-error:0.21374\ttrain-error:0.038387\n",
      "[3]\teval-error:0.21374\ttrain-error:0.025753\n",
      "[4]\teval-error:0.225191\ttrain-error:0.021866\n",
      "[5]\teval-error:0.232824\ttrain-error:0.019922\n",
      "[6]\teval-error:0.229008\ttrain-error:0.019436\n",
      "[7]\teval-error:0.225191\ttrain-error:0.019436\n",
      "[8]\teval-error:0.21374\ttrain-error:0.019436\n",
      "[9]\teval-error:0.244275\ttrain-error:0.019436\n",
      "[10]\teval-error:0.232824\ttrain-error:0.019436\n",
      "[11]\teval-error:0.221374\ttrain-error:0.01895\n",
      "[12]\teval-error:0.217557\ttrain-error:0.01895\n",
      "[13]\teval-error:0.217557\ttrain-error:0.01895\n",
      "[14]\teval-error:0.209924\ttrain-error:0.01895\n",
      "[15]\teval-error:0.217557\ttrain-error:0.01895\n",
      "[16]\teval-error:0.209924\ttrain-error:0.01895\n",
      "[17]\teval-error:0.21374\ttrain-error:0.01895\n",
      "[18]\teval-error:0.21374\ttrain-error:0.01895\n",
      "[19]\teval-error:0.206107\ttrain-error:0.01895\n",
      "[20]\teval-error:0.21374\ttrain-error:0.01895\n",
      "[21]\teval-error:0.221374\ttrain-error:0.01895\n",
      "[22]\teval-error:0.221374\ttrain-error:0.01895\n",
      "[23]\teval-error:0.225191\ttrain-error:0.01895\n",
      "[24]\teval-error:0.221374\ttrain-error:0.01895\n",
      "[25]\teval-error:0.217557\ttrain-error:0.01895\n",
      "[26]\teval-error:0.217557\ttrain-error:0.01895\n",
      "[27]\teval-error:0.225191\ttrain-error:0.01895\n",
      "[28]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[29]\teval-error:0.225191\ttrain-error:0.01895\n",
      "[30]\teval-error:0.225191\ttrain-error:0.01895\n",
      "[31]\teval-error:0.229008\ttrain-error:0.01895\n",
      "[32]\teval-error:0.236641\ttrain-error:0.01895\n",
      "[33]\teval-error:0.225191\ttrain-error:0.01895\n",
      "[34]\teval-error:0.221374\ttrain-error:0.01895\n",
      "[35]\teval-error:0.225191\ttrain-error:0.01895\n",
      "[36]\teval-error:0.225191\ttrain-error:0.01895\n",
      "[37]\teval-error:0.229008\ttrain-error:0.01895\n",
      "[38]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[39]\teval-error:0.240458\ttrain-error:0.01895\n",
      "[40]\teval-error:0.236641\ttrain-error:0.01895\n",
      "[41]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[42]\teval-error:0.221374\ttrain-error:0.01895\n",
      "[43]\teval-error:0.225191\ttrain-error:0.01895\n",
      "[44]\teval-error:0.221374\ttrain-error:0.01895\n",
      "[45]\teval-error:0.217557\ttrain-error:0.01895\n",
      "[46]\teval-error:0.229008\ttrain-error:0.01895\n",
      "[47]\teval-error:0.225191\ttrain-error:0.01895\n",
      "[48]\teval-error:0.229008\ttrain-error:0.01895\n",
      "[49]\teval-error:0.229008\ttrain-error:0.01895\n",
      "[50]\teval-error:0.221374\ttrain-error:0.01895\n",
      "[51]\teval-error:0.217557\ttrain-error:0.01895\n",
      "[52]\teval-error:0.217557\ttrain-error:0.01895\n",
      "[53]\teval-error:0.225191\ttrain-error:0.01895\n",
      "[54]\teval-error:0.225191\ttrain-error:0.01895\n",
      "[55]\teval-error:0.229008\ttrain-error:0.01895\n",
      "[56]\teval-error:0.221374\ttrain-error:0.01895\n",
      "[57]\teval-error:0.229008\ttrain-error:0.01895\n",
      "[58]\teval-error:0.217557\ttrain-error:0.01895\n",
      "[59]\teval-error:0.217557\ttrain-error:0.01895\n",
      "[60]\teval-error:0.217557\ttrain-error:0.01895\n",
      "[61]\teval-error:0.217557\ttrain-error:0.01895\n",
      "[62]\teval-error:0.229008\ttrain-error:0.01895\n",
      "[63]\teval-error:0.221374\ttrain-error:0.01895\n",
      "[64]\teval-error:0.229008\ttrain-error:0.01895\n",
      "[65]\teval-error:0.225191\ttrain-error:0.01895\n",
      "[66]\teval-error:0.225191\ttrain-error:0.01895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[67]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[68]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[69]\teval-error:0.225191\ttrain-error:0.01895\n",
      "[0]\teval-error:0.267176\ttrain-error:0.095724\n",
      "[1]\teval-error:0.278626\ttrain-error:0.058795\n",
      "[2]\teval-error:0.232824\ttrain-error:0.032556\n",
      "[3]\teval-error:0.236641\ttrain-error:0.027211\n",
      "[4]\teval-error:0.236641\ttrain-error:0.023324\n",
      "[5]\teval-error:0.221374\ttrain-error:0.020894\n",
      "[6]\teval-error:0.221374\ttrain-error:0.019922\n",
      "[7]\teval-error:0.229008\ttrain-error:0.019922\n",
      "[8]\teval-error:0.229008\ttrain-error:0.019922\n",
      "[9]\teval-error:0.236641\ttrain-error:0.019922\n",
      "[10]\teval-error:0.236641\ttrain-error:0.019922\n",
      "[11]\teval-error:0.236641\ttrain-error:0.019922\n",
      "[12]\teval-error:0.232824\ttrain-error:0.019436\n",
      "[13]\teval-error:0.236641\ttrain-error:0.019436\n",
      "[14]\teval-error:0.229008\ttrain-error:0.019436\n",
      "[15]\teval-error:0.229008\ttrain-error:0.019436\n",
      "[16]\teval-error:0.229008\ttrain-error:0.019436\n",
      "[17]\teval-error:0.225191\ttrain-error:0.019436\n",
      "[18]\teval-error:0.232824\ttrain-error:0.019436\n",
      "[19]\teval-error:0.229008\ttrain-error:0.019436\n",
      "[20]\teval-error:0.236641\ttrain-error:0.019436\n",
      "[21]\teval-error:0.232824\ttrain-error:0.019436\n",
      "[22]\teval-error:0.232824\ttrain-error:0.019436\n",
      "[23]\teval-error:0.229008\ttrain-error:0.019436\n",
      "[24]\teval-error:0.236641\ttrain-error:0.019436\n",
      "[25]\teval-error:0.232824\ttrain-error:0.019436\n",
      "[26]\teval-error:0.240458\ttrain-error:0.019436\n",
      "[27]\teval-error:0.240458\ttrain-error:0.019436\n",
      "[28]\teval-error:0.236641\ttrain-error:0.019436\n",
      "[29]\teval-error:0.236641\ttrain-error:0.019436\n",
      "[30]\teval-error:0.240458\ttrain-error:0.019436\n",
      "[31]\teval-error:0.244275\ttrain-error:0.019436\n",
      "[32]\teval-error:0.240458\ttrain-error:0.019436\n",
      "[33]\teval-error:0.236641\ttrain-error:0.019436\n",
      "[34]\teval-error:0.244275\ttrain-error:0.019436\n",
      "[35]\teval-error:0.240458\ttrain-error:0.019436\n",
      "[36]\teval-error:0.240458\ttrain-error:0.019436\n",
      "[37]\teval-error:0.240458\ttrain-error:0.019436\n",
      "[38]\teval-error:0.240458\ttrain-error:0.019436\n",
      "[39]\teval-error:0.236641\ttrain-error:0.019436\n",
      "[40]\teval-error:0.244275\ttrain-error:0.019436\n",
      "[41]\teval-error:0.236641\ttrain-error:0.019436\n",
      "[42]\teval-error:0.244275\ttrain-error:0.019436\n",
      "[43]\teval-error:0.240458\ttrain-error:0.019436\n",
      "[44]\teval-error:0.236641\ttrain-error:0.019436\n",
      "[45]\teval-error:0.236641\ttrain-error:0.019436\n",
      "[46]\teval-error:0.244275\ttrain-error:0.019436\n",
      "[47]\teval-error:0.236641\ttrain-error:0.019436\n",
      "[48]\teval-error:0.240458\ttrain-error:0.019436\n",
      "[49]\teval-error:0.240458\ttrain-error:0.019436\n",
      "[50]\teval-error:0.236641\ttrain-error:0.019436\n",
      "[51]\teval-error:0.232824\ttrain-error:0.019436\n",
      "[52]\teval-error:0.232824\ttrain-error:0.019436\n",
      "[53]\teval-error:0.236641\ttrain-error:0.019436\n",
      "[54]\teval-error:0.240458\ttrain-error:0.019436\n",
      "[55]\teval-error:0.236641\ttrain-error:0.019436\n",
      "[56]\teval-error:0.232824\ttrain-error:0.019436\n",
      "[57]\teval-error:0.240458\ttrain-error:0.019436\n",
      "[58]\teval-error:0.244275\ttrain-error:0.019436\n",
      "[59]\teval-error:0.240458\ttrain-error:0.019436\n",
      "[60]\teval-error:0.236641\ttrain-error:0.019436\n",
      "[61]\teval-error:0.244275\ttrain-error:0.019436\n",
      "[62]\teval-error:0.240458\ttrain-error:0.019436\n",
      "[63]\teval-error:0.236641\ttrain-error:0.019436\n",
      "[64]\teval-error:0.236641\ttrain-error:0.019436\n",
      "[65]\teval-error:0.236641\ttrain-error:0.019436\n",
      "[66]\teval-error:0.236641\ttrain-error:0.019436\n",
      "[67]\teval-error:0.240458\ttrain-error:0.019436\n",
      "[68]\teval-error:0.244275\ttrain-error:0.019436\n",
      "[69]\teval-error:0.240458\ttrain-error:0.019436\n",
      "[0]\teval-error:0.282443\ttrain-error:0.091837\n",
      "[1]\teval-error:0.248092\ttrain-error:0.043246\n",
      "[2]\teval-error:0.251908\ttrain-error:0.028183\n",
      "[3]\teval-error:0.244275\ttrain-error:0.02381\n",
      "[4]\teval-error:0.236641\ttrain-error:0.020408\n",
      "[5]\teval-error:0.248092\ttrain-error:0.019922\n",
      "[6]\teval-error:0.225191\ttrain-error:0.01895\n",
      "[7]\teval-error:0.236641\ttrain-error:0.01895\n",
      "[8]\teval-error:0.229008\ttrain-error:0.018465\n",
      "[9]\teval-error:0.240458\ttrain-error:0.018465\n",
      "[10]\teval-error:0.229008\ttrain-error:0.018465\n",
      "[11]\teval-error:0.229008\ttrain-error:0.018465\n",
      "[12]\teval-error:0.232824\ttrain-error:0.018465\n",
      "[13]\teval-error:0.248092\ttrain-error:0.018465\n",
      "[14]\teval-error:0.236641\ttrain-error:0.018465\n",
      "[15]\teval-error:0.236641\ttrain-error:0.018465\n",
      "[16]\teval-error:0.236641\ttrain-error:0.018465\n",
      "[17]\teval-error:0.229008\ttrain-error:0.018465\n",
      "[18]\teval-error:0.221374\ttrain-error:0.018465\n",
      "[19]\teval-error:0.225191\ttrain-error:0.018465\n",
      "[20]\teval-error:0.217557\ttrain-error:0.018465\n",
      "[21]\teval-error:0.229008\ttrain-error:0.018465\n",
      "[22]\teval-error:0.229008\ttrain-error:0.018465\n",
      "[23]\teval-error:0.229008\ttrain-error:0.018465\n",
      "[24]\teval-error:0.236641\ttrain-error:0.018465\n",
      "[25]\teval-error:0.232824\ttrain-error:0.018465\n",
      "[26]\teval-error:0.229008\ttrain-error:0.018465\n",
      "[27]\teval-error:0.229008\ttrain-error:0.018465\n",
      "[28]\teval-error:0.232824\ttrain-error:0.018465\n",
      "[29]\teval-error:0.232824\ttrain-error:0.018465\n",
      "[30]\teval-error:0.225191\ttrain-error:0.018465\n",
      "[31]\teval-error:0.232824\ttrain-error:0.018465\n",
      "[32]\teval-error:0.229008\ttrain-error:0.018465\n",
      "[33]\teval-error:0.225191\ttrain-error:0.018465\n",
      "[34]\teval-error:0.229008\ttrain-error:0.018465\n",
      "[35]\teval-error:0.221374\ttrain-error:0.018465\n",
      "[36]\teval-error:0.229008\ttrain-error:0.018465\n",
      "[37]\teval-error:0.229008\ttrain-error:0.018465\n",
      "[38]\teval-error:0.232824\ttrain-error:0.018465\n",
      "[39]\teval-error:0.225191\ttrain-error:0.018465\n",
      "[40]\teval-error:0.229008\ttrain-error:0.018465\n",
      "[41]\teval-error:0.217557\ttrain-error:0.018465\n",
      "[42]\teval-error:0.232824\ttrain-error:0.018465\n",
      "[43]\teval-error:0.229008\ttrain-error:0.018465\n",
      "[44]\teval-error:0.221374\ttrain-error:0.018465\n",
      "[45]\teval-error:0.221374\ttrain-error:0.018465\n",
      "[46]\teval-error:0.217557\ttrain-error:0.018465\n",
      "[47]\teval-error:0.217557\ttrain-error:0.018465\n",
      "[48]\teval-error:0.221374\ttrain-error:0.018465\n",
      "[49]\teval-error:0.21374\ttrain-error:0.018465\n",
      "[50]\teval-error:0.21374\ttrain-error:0.018465\n",
      "[51]\teval-error:0.221374\ttrain-error:0.018465\n",
      "[52]\teval-error:0.221374\ttrain-error:0.018465\n",
      "[53]\teval-error:0.225191\ttrain-error:0.018465\n",
      "[54]\teval-error:0.221374\ttrain-error:0.018465\n",
      "[55]\teval-error:0.221374\ttrain-error:0.018465\n",
      "[56]\teval-error:0.221374\ttrain-error:0.018465\n",
      "[57]\teval-error:0.221374\ttrain-error:0.018465\n",
      "[58]\teval-error:0.221374\ttrain-error:0.018465\n",
      "[59]\teval-error:0.221374\ttrain-error:0.018465\n",
      "[60]\teval-error:0.217557\ttrain-error:0.018465\n",
      "[61]\teval-error:0.21374\ttrain-error:0.018465\n",
      "[62]\teval-error:0.21374\ttrain-error:0.018465\n",
      "[63]\teval-error:0.225191\ttrain-error:0.018465\n",
      "[64]\teval-error:0.221374\ttrain-error:0.018465\n",
      "[65]\teval-error:0.232824\ttrain-error:0.018465\n",
      "[66]\teval-error:0.221374\ttrain-error:0.018465\n",
      "[67]\teval-error:0.221374\ttrain-error:0.018465\n",
      "[68]\teval-error:0.21374\ttrain-error:0.018465\n",
      "[69]\teval-error:0.217557\ttrain-error:0.018465\n",
      "[0]\teval-error:0.227273\ttrain-error:0.181058\n",
      "[1]\teval-error:0.193182\ttrain-error:0.178273\n",
      "[2]\teval-error:0.181818\ttrain-error:0.167131\n",
      "[3]\teval-error:0.181818\ttrain-error:0.162953\n",
      "[4]\teval-error:0.181818\ttrain-error:0.158774\n",
      "[5]\teval-error:0.204545\ttrain-error:0.158774\n",
      "[6]\teval-error:0.193182\ttrain-error:0.155989\n",
      "[7]\teval-error:0.215909\ttrain-error:0.154596\n",
      "[8]\teval-error:0.193182\ttrain-error:0.153203\n",
      "[9]\teval-error:0.193182\ttrain-error:0.153203\n",
      "[10]\teval-error:0.215909\ttrain-error:0.153203\n",
      "[11]\teval-error:0.193182\ttrain-error:0.151811\n",
      "[12]\teval-error:0.193182\ttrain-error:0.151811\n",
      "[13]\teval-error:0.193182\ttrain-error:0.150418\n",
      "[14]\teval-error:0.181818\ttrain-error:0.150418\n",
      "[15]\teval-error:0.204545\ttrain-error:0.150418\n",
      "[16]\teval-error:0.204545\ttrain-error:0.150418\n",
      "[17]\teval-error:0.193182\ttrain-error:0.150418\n",
      "[18]\teval-error:0.193182\ttrain-error:0.149025\n",
      "[19]\teval-error:0.215909\ttrain-error:0.149025\n",
      "[20]\teval-error:0.193182\ttrain-error:0.147632\n",
      "[21]\teval-error:0.181818\ttrain-error:0.147632\n",
      "[22]\teval-error:0.193182\ttrain-error:0.147632\n",
      "[23]\teval-error:0.193182\ttrain-error:0.147632\n",
      "[24]\teval-error:0.215909\ttrain-error:0.147632\n",
      "[25]\teval-error:0.193182\ttrain-error:0.147632\n",
      "[26]\teval-error:0.181818\ttrain-error:0.147632\n",
      "[27]\teval-error:0.193182\ttrain-error:0.147632\n",
      "[28]\teval-error:0.193182\ttrain-error:0.147632\n",
      "[29]\teval-error:0.193182\ttrain-error:0.147632\n",
      "[30]\teval-error:0.204545\ttrain-error:0.147632\n",
      "[31]\teval-error:0.193182\ttrain-error:0.147632\n",
      "[32]\teval-error:0.193182\ttrain-error:0.147632\n",
      "[33]\teval-error:0.193182\ttrain-error:0.147632\n",
      "[34]\teval-error:0.193182\ttrain-error:0.147632\n",
      "[35]\teval-error:0.193182\ttrain-error:0.147632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36]\teval-error:0.204545\ttrain-error:0.147632\n",
      "[37]\teval-error:0.193182\ttrain-error:0.147632\n",
      "[38]\teval-error:0.181818\ttrain-error:0.147632\n",
      "[39]\teval-error:0.181818\ttrain-error:0.147632\n",
      "[40]\teval-error:0.193182\ttrain-error:0.147632\n",
      "[41]\teval-error:0.193182\ttrain-error:0.147632\n",
      "[42]\teval-error:0.181818\ttrain-error:0.147632\n",
      "[43]\teval-error:0.181818\ttrain-error:0.147632\n",
      "[44]\teval-error:0.193182\ttrain-error:0.147632\n",
      "[45]\teval-error:0.215909\ttrain-error:0.147632\n",
      "[46]\teval-error:0.193182\ttrain-error:0.147632\n",
      "[47]\teval-error:0.181818\ttrain-error:0.147632\n",
      "[48]\teval-error:0.193182\ttrain-error:0.147632\n",
      "[49]\teval-error:0.215909\ttrain-error:0.147632\n",
      "[50]\teval-error:0.181818\ttrain-error:0.147632\n",
      "[51]\teval-error:0.181818\ttrain-error:0.147632\n",
      "[52]\teval-error:0.204545\ttrain-error:0.147632\n",
      "[53]\teval-error:0.215909\ttrain-error:0.147632\n",
      "[54]\teval-error:0.193182\ttrain-error:0.147632\n",
      "[55]\teval-error:0.215909\ttrain-error:0.147632\n",
      "[56]\teval-error:0.193182\ttrain-error:0.147632\n",
      "[57]\teval-error:0.204545\ttrain-error:0.147632\n",
      "[58]\teval-error:0.204545\ttrain-error:0.147632\n",
      "[59]\teval-error:0.181818\ttrain-error:0.147632\n",
      "[60]\teval-error:0.204545\ttrain-error:0.147632\n",
      "[61]\teval-error:0.181818\ttrain-error:0.147632\n",
      "[62]\teval-error:0.193182\ttrain-error:0.147632\n",
      "[63]\teval-error:0.181818\ttrain-error:0.147632\n",
      "[64]\teval-error:0.181818\ttrain-error:0.147632\n",
      "[65]\teval-error:0.181818\ttrain-error:0.147632\n",
      "[66]\teval-error:0.204545\ttrain-error:0.147632\n",
      "[67]\teval-error:0.181818\ttrain-error:0.147632\n",
      "[68]\teval-error:0.193182\ttrain-error:0.147632\n",
      "[69]\teval-error:0.193182\ttrain-error:0.147632\n",
      "[0]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[1]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[2]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[3]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[4]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[5]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[6]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[7]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[8]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[9]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[10]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[11]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[12]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[13]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[14]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[15]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[16]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[17]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[18]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[19]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[20]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[21]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[22]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[23]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[24]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[25]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[26]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[27]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[28]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[29]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[30]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[31]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[32]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[33]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[34]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[35]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[36]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[37]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[38]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[39]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[40]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[41]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[42]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[43]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[44]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[45]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[46]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[47]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[48]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[49]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[50]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[51]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[52]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[53]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[54]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[55]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[56]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[57]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[58]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[59]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[60]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[61]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[62]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[63]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[64]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[65]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[66]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[67]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[68]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[69]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[0]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[1]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[2]\teval-error:0.159091\ttrain-error:0.188022\n",
      "[3]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[4]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[5]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[6]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[7]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[8]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[9]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[10]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[11]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[12]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[13]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[14]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[15]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[16]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[17]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[18]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[19]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[20]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[21]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[22]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[23]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[24]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[25]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[26]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[27]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[28]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[29]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[30]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[31]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[32]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[33]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[34]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[35]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[36]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[37]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[38]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[39]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[40]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[41]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[42]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[43]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[44]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[45]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[46]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[47]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[48]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[49]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[50]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[51]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[52]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[53]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[54]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[55]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[56]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[57]\teval-error:0.159091\ttrain-error:0.18663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[59]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[60]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[61]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[62]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[63]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[64]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[65]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[66]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[67]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[68]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[69]\teval-error:0.159091\ttrain-error:0.18663\n",
      "[0]\teval-error:0.193182\ttrain-error:0.167131\n",
      "[1]\teval-error:0.227273\ttrain-error:0.151811\n",
      "[2]\teval-error:0.204545\ttrain-error:0.116992\n",
      "[3]\teval-error:0.227273\ttrain-error:0.097493\n",
      "[4]\teval-error:0.227273\ttrain-error:0.089136\n",
      "[5]\teval-error:0.238636\ttrain-error:0.077994\n",
      "[6]\teval-error:0.25\ttrain-error:0.069638\n",
      "[7]\teval-error:0.284091\ttrain-error:0.061281\n",
      "[8]\teval-error:0.272727\ttrain-error:0.05571\n",
      "[9]\teval-error:0.261364\ttrain-error:0.054318\n",
      "[10]\teval-error:0.272727\ttrain-error:0.048747\n",
      "[11]\teval-error:0.261364\ttrain-error:0.045961\n",
      "[12]\teval-error:0.272727\ttrain-error:0.043175\n",
      "[13]\teval-error:0.272727\ttrain-error:0.041783\n",
      "[14]\teval-error:0.272727\ttrain-error:0.043175\n",
      "[15]\teval-error:0.272727\ttrain-error:0.041783\n",
      "[16]\teval-error:0.261364\ttrain-error:0.041783\n",
      "[17]\teval-error:0.261364\ttrain-error:0.041783\n",
      "[18]\teval-error:0.284091\ttrain-error:0.041783\n",
      "[19]\teval-error:0.284091\ttrain-error:0.041783\n",
      "[20]\teval-error:0.272727\ttrain-error:0.041783\n",
      "[21]\teval-error:0.272727\ttrain-error:0.041783\n",
      "[22]\teval-error:0.261364\ttrain-error:0.041783\n",
      "[23]\teval-error:0.272727\ttrain-error:0.041783\n",
      "[24]\teval-error:0.272727\ttrain-error:0.041783\n",
      "[25]\teval-error:0.272727\ttrain-error:0.041783\n",
      "[26]\teval-error:0.272727\ttrain-error:0.041783\n",
      "[27]\teval-error:0.272727\ttrain-error:0.041783\n",
      "[28]\teval-error:0.261364\ttrain-error:0.041783\n",
      "[29]\teval-error:0.261364\ttrain-error:0.041783\n",
      "[30]\teval-error:0.261364\ttrain-error:0.041783\n",
      "[31]\teval-error:0.261364\ttrain-error:0.041783\n",
      "[32]\teval-error:0.25\ttrain-error:0.041783\n",
      "[33]\teval-error:0.261364\ttrain-error:0.041783\n",
      "[34]\teval-error:0.261364\ttrain-error:0.041783\n",
      "[35]\teval-error:0.25\ttrain-error:0.04039\n",
      "[36]\teval-error:0.25\ttrain-error:0.041783\n",
      "[37]\teval-error:0.272727\ttrain-error:0.041783\n",
      "[38]\teval-error:0.272727\ttrain-error:0.04039\n",
      "[39]\teval-error:0.261364\ttrain-error:0.041783\n",
      "[40]\teval-error:0.25\ttrain-error:0.041783\n",
      "[41]\teval-error:0.261364\ttrain-error:0.041783\n",
      "[42]\teval-error:0.272727\ttrain-error:0.04039\n",
      "[43]\teval-error:0.261364\ttrain-error:0.041783\n",
      "[44]\teval-error:0.261364\ttrain-error:0.041783\n",
      "[45]\teval-error:0.25\ttrain-error:0.041783\n",
      "[46]\teval-error:0.25\ttrain-error:0.04039\n",
      "[47]\teval-error:0.25\ttrain-error:0.04039\n",
      "[48]\teval-error:0.25\ttrain-error:0.04039\n",
      "[49]\teval-error:0.25\ttrain-error:0.04039\n",
      "[50]\teval-error:0.261364\ttrain-error:0.04039\n",
      "[51]\teval-error:0.25\ttrain-error:0.04039\n",
      "[52]\teval-error:0.261364\ttrain-error:0.04039\n",
      "[53]\teval-error:0.25\ttrain-error:0.04039\n",
      "[54]\teval-error:0.261364\ttrain-error:0.04039\n",
      "[55]\teval-error:0.261364\ttrain-error:0.04039\n",
      "[56]\teval-error:0.261364\ttrain-error:0.04039\n",
      "[57]\teval-error:0.261364\ttrain-error:0.04039\n",
      "[58]\teval-error:0.261364\ttrain-error:0.04039\n",
      "[59]\teval-error:0.261364\ttrain-error:0.04039\n",
      "[60]\teval-error:0.25\ttrain-error:0.04039\n",
      "[61]\teval-error:0.25\ttrain-error:0.04039\n",
      "[62]\teval-error:0.261364\ttrain-error:0.04039\n",
      "[63]\teval-error:0.261364\ttrain-error:0.04039\n",
      "[64]\teval-error:0.261364\ttrain-error:0.04039\n",
      "[65]\teval-error:0.261364\ttrain-error:0.04039\n",
      "[66]\teval-error:0.261364\ttrain-error:0.04039\n",
      "[67]\teval-error:0.261364\ttrain-error:0.04039\n",
      "[68]\teval-error:0.261364\ttrain-error:0.04039\n",
      "[69]\teval-error:0.261364\ttrain-error:0.04039\n",
      "[0]\teval-error:0.308511\ttrain-error:0.224405\n",
      "[1]\teval-error:0.308511\ttrain-error:0.21303\n",
      "[2]\teval-error:0.265957\ttrain-error:0.203723\n",
      "[3]\teval-error:0.265957\ttrain-error:0.202689\n",
      "[4]\teval-error:0.287234\ttrain-error:0.20062\n",
      "[5]\teval-error:0.297872\ttrain-error:0.199586\n",
      "[6]\teval-error:0.244681\ttrain-error:0.196484\n",
      "[7]\teval-error:0.265957\ttrain-error:0.196484\n",
      "[8]\teval-error:0.255319\ttrain-error:0.19545\n",
      "[9]\teval-error:0.244681\ttrain-error:0.194416\n",
      "[10]\teval-error:0.255319\ttrain-error:0.193382\n",
      "[11]\teval-error:0.265957\ttrain-error:0.194416\n",
      "[12]\teval-error:0.265957\ttrain-error:0.193382\n",
      "[13]\teval-error:0.255319\ttrain-error:0.192347\n",
      "[14]\teval-error:0.276596\ttrain-error:0.192347\n",
      "[15]\teval-error:0.276596\ttrain-error:0.192347\n",
      "[16]\teval-error:0.287234\ttrain-error:0.192347\n",
      "[17]\teval-error:0.255319\ttrain-error:0.192347\n",
      "[18]\teval-error:0.265957\ttrain-error:0.192347\n",
      "[19]\teval-error:0.276596\ttrain-error:0.192347\n",
      "[20]\teval-error:0.265957\ttrain-error:0.192347\n",
      "[21]\teval-error:0.234043\ttrain-error:0.192347\n",
      "[22]\teval-error:0.265957\ttrain-error:0.192347\n",
      "[23]\teval-error:0.265957\ttrain-error:0.191313\n",
      "[24]\teval-error:0.244681\ttrain-error:0.191313\n",
      "[25]\teval-error:0.265957\ttrain-error:0.191313\n",
      "[26]\teval-error:0.244681\ttrain-error:0.191313\n",
      "[27]\teval-error:0.255319\ttrain-error:0.191313\n",
      "[28]\teval-error:0.255319\ttrain-error:0.191313\n",
      "[29]\teval-error:0.265957\ttrain-error:0.191313\n",
      "[30]\teval-error:0.234043\ttrain-error:0.191313\n",
      "[31]\teval-error:0.265957\ttrain-error:0.191313\n",
      "[32]\teval-error:0.255319\ttrain-error:0.191313\n",
      "[33]\teval-error:0.255319\ttrain-error:0.191313\n",
      "[34]\teval-error:0.234043\ttrain-error:0.191313\n",
      "[35]\teval-error:0.244681\ttrain-error:0.191313\n",
      "[36]\teval-error:0.255319\ttrain-error:0.191313\n",
      "[37]\teval-error:0.255319\ttrain-error:0.191313\n",
      "[38]\teval-error:0.255319\ttrain-error:0.191313\n",
      "[39]\teval-error:0.223404\ttrain-error:0.191313\n",
      "[40]\teval-error:0.255319\ttrain-error:0.191313\n",
      "[41]\teval-error:0.255319\ttrain-error:0.191313\n",
      "[42]\teval-error:0.223404\ttrain-error:0.191313\n",
      "[43]\teval-error:0.265957\ttrain-error:0.191313\n",
      "[44]\teval-error:0.255319\ttrain-error:0.191313\n",
      "[45]\teval-error:0.244681\ttrain-error:0.191313\n",
      "[46]\teval-error:0.234043\ttrain-error:0.191313\n",
      "[47]\teval-error:0.265957\ttrain-error:0.191313\n",
      "[48]\teval-error:0.265957\ttrain-error:0.191313\n",
      "[49]\teval-error:0.276596\ttrain-error:0.191313\n",
      "[50]\teval-error:0.244681\ttrain-error:0.191313\n",
      "[51]\teval-error:0.244681\ttrain-error:0.191313\n",
      "[52]\teval-error:0.244681\ttrain-error:0.191313\n",
      "[53]\teval-error:0.255319\ttrain-error:0.191313\n",
      "[54]\teval-error:0.265957\ttrain-error:0.191313\n",
      "[55]\teval-error:0.255319\ttrain-error:0.191313\n",
      "[56]\teval-error:0.244681\ttrain-error:0.191313\n",
      "[57]\teval-error:0.244681\ttrain-error:0.191313\n",
      "[58]\teval-error:0.234043\ttrain-error:0.191313\n",
      "[59]\teval-error:0.234043\ttrain-error:0.191313\n",
      "[60]\teval-error:0.244681\ttrain-error:0.191313\n",
      "[61]\teval-error:0.265957\ttrain-error:0.191313\n",
      "[62]\teval-error:0.244681\ttrain-error:0.191313\n",
      "[63]\teval-error:0.244681\ttrain-error:0.191313\n",
      "[64]\teval-error:0.265957\ttrain-error:0.191313\n",
      "[65]\teval-error:0.244681\ttrain-error:0.191313\n",
      "[66]\teval-error:0.244681\ttrain-error:0.191313\n",
      "[67]\teval-error:0.234043\ttrain-error:0.191313\n",
      "[68]\teval-error:0.255319\ttrain-error:0.191313\n",
      "[69]\teval-error:0.244681\ttrain-error:0.191313\n",
      "[0]\teval-error:0.234043\ttrain-error:0.251293\n",
      "[1]\teval-error:0.234043\ttrain-error:0.251293\n",
      "[2]\teval-error:0.234043\ttrain-error:0.251293\n",
      "[3]\teval-error:0.234043\ttrain-error:0.251293\n",
      "[4]\teval-error:0.234043\ttrain-error:0.251293\n",
      "[5]\teval-error:0.234043\ttrain-error:0.251293\n",
      "[6]\teval-error:0.234043\ttrain-error:0.251293\n",
      "[7]\teval-error:0.234043\ttrain-error:0.251293\n",
      "[8]\teval-error:0.234043\ttrain-error:0.251293\n",
      "[9]\teval-error:0.234043\ttrain-error:0.251293\n",
      "[10]\teval-error:0.234043\ttrain-error:0.251293\n",
      "[11]\teval-error:0.234043\ttrain-error:0.251293\n",
      "[12]\teval-error:0.234043\ttrain-error:0.251293\n",
      "[13]\teval-error:0.234043\ttrain-error:0.251293\n",
      "[14]\teval-error:0.234043\ttrain-error:0.251293\n",
      "[15]\teval-error:0.234043\ttrain-error:0.251293\n",
      "[16]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[17]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[18]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[19]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[20]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[21]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[22]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[23]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[24]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[25]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[26]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[27]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[28]\teval-error:0.234043\ttrain-error:0.250259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[30]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[31]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[32]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[33]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[34]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[35]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[36]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[37]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[38]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[39]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[40]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[41]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[42]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[43]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[44]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[45]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[46]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[47]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[48]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[49]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[50]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[51]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[52]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[53]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[54]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[55]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[56]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[57]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[58]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[59]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[60]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[61]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[62]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[63]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[64]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[65]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[66]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[67]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[68]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[69]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[0]\teval-error:0.234043\ttrain-error:0.251293\n",
      "[1]\teval-error:0.234043\ttrain-error:0.251293\n",
      "[2]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[3]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[4]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[5]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[6]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[7]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[8]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[9]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[10]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[11]\teval-error:0.234043\ttrain-error:0.250259\n",
      "[12]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[13]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[14]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[15]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[16]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[17]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[18]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[19]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[20]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[21]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[22]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[23]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[24]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[25]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[26]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[27]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[28]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[29]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[30]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[31]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[32]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[33]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[34]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[35]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[36]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[37]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[38]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[39]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[40]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[41]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[42]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[43]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[44]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[45]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[46]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[47]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[48]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[49]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[50]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[51]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[52]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[53]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[54]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[55]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[56]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[57]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[58]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[59]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[60]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[61]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[62]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[63]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[64]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[65]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[66]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[67]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[68]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[69]\teval-error:0.234043\ttrain-error:0.249224\n",
      "[0]\teval-error:0.287234\ttrain-error:0.185109\n",
      "[1]\teval-error:0.276596\ttrain-error:0.169597\n",
      "[2]\teval-error:0.287234\ttrain-error:0.143744\n",
      "[3]\teval-error:0.276596\ttrain-error:0.124095\n",
      "[4]\teval-error:0.276596\ttrain-error:0.110652\n",
      "[5]\teval-error:0.287234\ttrain-error:0.099276\n",
      "[6]\teval-error:0.297872\ttrain-error:0.093071\n",
      "[7]\teval-error:0.265957\ttrain-error:0.091003\n",
      "[8]\teval-error:0.265957\ttrain-error:0.083764\n",
      "[9]\teval-error:0.255319\ttrain-error:0.08273\n",
      "[10]\teval-error:0.287234\ttrain-error:0.081696\n",
      "[11]\teval-error:0.287234\ttrain-error:0.076525\n",
      "[12]\teval-error:0.287234\ttrain-error:0.076525\n",
      "[13]\teval-error:0.287234\ttrain-error:0.074457\n",
      "[14]\teval-error:0.287234\ttrain-error:0.075491\n",
      "[15]\teval-error:0.276596\ttrain-error:0.074457\n",
      "[16]\teval-error:0.287234\ttrain-error:0.074457\n",
      "[17]\teval-error:0.297872\ttrain-error:0.075491\n",
      "[18]\teval-error:0.276596\ttrain-error:0.074457\n",
      "[19]\teval-error:0.297872\ttrain-error:0.074457\n",
      "[20]\teval-error:0.297872\ttrain-error:0.074457\n",
      "[21]\teval-error:0.297872\ttrain-error:0.074457\n",
      "[22]\teval-error:0.297872\ttrain-error:0.074457\n",
      "[23]\teval-error:0.297872\ttrain-error:0.074457\n",
      "[24]\teval-error:0.297872\ttrain-error:0.074457\n",
      "[25]\teval-error:0.297872\ttrain-error:0.074457\n",
      "[26]\teval-error:0.297872\ttrain-error:0.074457\n",
      "[27]\teval-error:0.297872\ttrain-error:0.074457\n",
      "[28]\teval-error:0.297872\ttrain-error:0.074457\n",
      "[29]\teval-error:0.297872\ttrain-error:0.074457\n",
      "[30]\teval-error:0.297872\ttrain-error:0.074457\n",
      "[31]\teval-error:0.297872\ttrain-error:0.074457\n",
      "[32]\teval-error:0.297872\ttrain-error:0.074457\n",
      "[33]\teval-error:0.297872\ttrain-error:0.074457\n",
      "[34]\teval-error:0.297872\ttrain-error:0.074457\n",
      "[35]\teval-error:0.297872\ttrain-error:0.074457\n",
      "[36]\teval-error:0.287234\ttrain-error:0.074457\n",
      "[37]\teval-error:0.297872\ttrain-error:0.074457\n",
      "[38]\teval-error:0.287234\ttrain-error:0.074457\n",
      "[39]\teval-error:0.287234\ttrain-error:0.074457\n",
      "[40]\teval-error:0.287234\ttrain-error:0.074457\n",
      "[41]\teval-error:0.287234\ttrain-error:0.074457\n",
      "[42]\teval-error:0.287234\ttrain-error:0.074457\n",
      "[43]\teval-error:0.287234\ttrain-error:0.074457\n",
      "[44]\teval-error:0.287234\ttrain-error:0.074457\n",
      "[45]\teval-error:0.287234\ttrain-error:0.074457\n",
      "[46]\teval-error:0.287234\ttrain-error:0.074457\n",
      "[47]\teval-error:0.287234\ttrain-error:0.074457\n",
      "[48]\teval-error:0.287234\ttrain-error:0.074457\n",
      "[49]\teval-error:0.287234\ttrain-error:0.074457\n",
      "[50]\teval-error:0.287234\ttrain-error:0.074457\n",
      "[51]\teval-error:0.287234\ttrain-error:0.074457\n",
      "[52]\teval-error:0.287234\ttrain-error:0.074457\n",
      "[53]\teval-error:0.287234\ttrain-error:0.074457\n",
      "[54]\teval-error:0.287234\ttrain-error:0.074457\n",
      "[55]\teval-error:0.287234\ttrain-error:0.074457\n",
      "[56]\teval-error:0.287234\ttrain-error:0.074457\n",
      "[57]\teval-error:0.287234\ttrain-error:0.074457\n",
      "[58]\teval-error:0.297872\ttrain-error:0.074457\n",
      "[59]\teval-error:0.297872\ttrain-error:0.074457\n",
      "[60]\teval-error:0.287234\ttrain-error:0.074457\n",
      "[61]\teval-error:0.287234\ttrain-error:0.074457\n",
      "[62]\teval-error:0.297872\ttrain-error:0.074457\n",
      "[63]\teval-error:0.297872\ttrain-error:0.074457\n",
      "[64]\teval-error:0.297872\ttrain-error:0.074457\n",
      "[65]\teval-error:0.297872\ttrain-error:0.074457\n",
      "[66]\teval-error:0.297872\ttrain-error:0.074457\n",
      "[67]\teval-error:0.297872\ttrain-error:0.074457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[68]\teval-error:0.297872\ttrain-error:0.074457\n",
      "[69]\teval-error:0.297872\ttrain-error:0.074457\n",
      "[0]\teval-error:0.194656\ttrain-error:0.170554\n",
      "[1]\teval-error:0.198473\ttrain-error:0.166667\n",
      "[2]\teval-error:0.198473\ttrain-error:0.161322\n",
      "[3]\teval-error:0.198473\ttrain-error:0.160836\n",
      "[4]\teval-error:0.20229\ttrain-error:0.158892\n",
      "[5]\teval-error:0.20229\ttrain-error:0.15792\n",
      "[6]\teval-error:0.20229\ttrain-error:0.156463\n",
      "[7]\teval-error:0.209924\ttrain-error:0.155977\n",
      "[8]\teval-error:0.21374\ttrain-error:0.153547\n",
      "[9]\teval-error:0.21374\ttrain-error:0.153547\n",
      "[10]\teval-error:0.21374\ttrain-error:0.153547\n",
      "[11]\teval-error:0.20229\ttrain-error:0.152575\n",
      "[12]\teval-error:0.206107\ttrain-error:0.152575\n",
      "[13]\teval-error:0.206107\ttrain-error:0.152089\n",
      "[14]\teval-error:0.206107\ttrain-error:0.151604\n",
      "[15]\teval-error:0.206107\ttrain-error:0.152089\n",
      "[16]\teval-error:0.209924\ttrain-error:0.151604\n",
      "[17]\teval-error:0.206107\ttrain-error:0.151604\n",
      "[18]\teval-error:0.20229\ttrain-error:0.151604\n",
      "[19]\teval-error:0.20229\ttrain-error:0.151604\n",
      "[20]\teval-error:0.206107\ttrain-error:0.151604\n",
      "[21]\teval-error:0.206107\ttrain-error:0.151604\n",
      "[22]\teval-error:0.20229\ttrain-error:0.151604\n",
      "[23]\teval-error:0.206107\ttrain-error:0.151604\n",
      "[24]\teval-error:0.209924\ttrain-error:0.151604\n",
      "[25]\teval-error:0.206107\ttrain-error:0.151604\n",
      "[26]\teval-error:0.21374\ttrain-error:0.151604\n",
      "[27]\teval-error:0.217557\ttrain-error:0.151604\n",
      "[28]\teval-error:0.21374\ttrain-error:0.151604\n",
      "[29]\teval-error:0.209924\ttrain-error:0.151604\n",
      "[30]\teval-error:0.21374\ttrain-error:0.151604\n",
      "[31]\teval-error:0.21374\ttrain-error:0.151604\n",
      "[32]\teval-error:0.221374\ttrain-error:0.151604\n",
      "[33]\teval-error:0.21374\ttrain-error:0.151604\n",
      "[34]\teval-error:0.217557\ttrain-error:0.151604\n",
      "[35]\teval-error:0.21374\ttrain-error:0.151604\n",
      "[36]\teval-error:0.21374\ttrain-error:0.151604\n",
      "[37]\teval-error:0.21374\ttrain-error:0.151604\n",
      "[38]\teval-error:0.209924\ttrain-error:0.151604\n",
      "[39]\teval-error:0.21374\ttrain-error:0.151604\n",
      "[40]\teval-error:0.206107\ttrain-error:0.151604\n",
      "[41]\teval-error:0.21374\ttrain-error:0.151604\n",
      "[42]\teval-error:0.217557\ttrain-error:0.151604\n",
      "[43]\teval-error:0.21374\ttrain-error:0.151604\n",
      "[44]\teval-error:0.21374\ttrain-error:0.151604\n",
      "[45]\teval-error:0.217557\ttrain-error:0.151604\n",
      "[46]\teval-error:0.209924\ttrain-error:0.151604\n",
      "[47]\teval-error:0.209924\ttrain-error:0.151604\n",
      "[48]\teval-error:0.209924\ttrain-error:0.151604\n",
      "[49]\teval-error:0.206107\ttrain-error:0.151604\n",
      "[50]\teval-error:0.20229\ttrain-error:0.151604\n",
      "[51]\teval-error:0.20229\ttrain-error:0.151604\n",
      "[52]\teval-error:0.217557\ttrain-error:0.151604\n",
      "[53]\teval-error:0.209924\ttrain-error:0.151604\n",
      "[54]\teval-error:0.221374\ttrain-error:0.151604\n",
      "[55]\teval-error:0.217557\ttrain-error:0.151604\n",
      "[56]\teval-error:0.21374\ttrain-error:0.151604\n",
      "[57]\teval-error:0.21374\ttrain-error:0.151604\n",
      "[58]\teval-error:0.217557\ttrain-error:0.151604\n",
      "[59]\teval-error:0.21374\ttrain-error:0.151604\n",
      "[60]\teval-error:0.221374\ttrain-error:0.151604\n",
      "[61]\teval-error:0.209924\ttrain-error:0.151604\n",
      "[62]\teval-error:0.209924\ttrain-error:0.151604\n",
      "[63]\teval-error:0.21374\ttrain-error:0.151604\n",
      "[64]\teval-error:0.217557\ttrain-error:0.151604\n",
      "[65]\teval-error:0.206107\ttrain-error:0.151604\n",
      "[66]\teval-error:0.21374\ttrain-error:0.151604\n",
      "[67]\teval-error:0.217557\ttrain-error:0.151604\n",
      "[68]\teval-error:0.21374\ttrain-error:0.151604\n",
      "[69]\teval-error:0.209924\ttrain-error:0.151604\n",
      "[0]\teval-error:0.221374\ttrain-error:0.195335\n",
      "[1]\teval-error:0.221374\ttrain-error:0.195335\n",
      "[2]\teval-error:0.221374\ttrain-error:0.195335\n",
      "[3]\teval-error:0.221374\ttrain-error:0.195335\n",
      "[4]\teval-error:0.221374\ttrain-error:0.195335\n",
      "[5]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[6]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[7]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[8]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[9]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[10]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[11]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[12]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[13]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[14]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[15]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[16]\teval-error:0.221374\ttrain-error:0.195335\n",
      "[17]\teval-error:0.221374\ttrain-error:0.195335\n",
      "[18]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[19]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[20]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[21]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[22]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[23]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[24]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[25]\teval-error:0.221374\ttrain-error:0.195335\n",
      "[26]\teval-error:0.221374\ttrain-error:0.195335\n",
      "[27]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[28]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[29]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[30]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[31]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[32]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[33]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[34]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[35]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[36]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[37]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[38]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[39]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[40]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[41]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[42]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[43]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[44]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[45]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[46]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[47]\teval-error:0.221374\ttrain-error:0.195335\n",
      "[48]\teval-error:0.221374\ttrain-error:0.195335\n",
      "[49]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[50]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[51]\teval-error:0.221374\ttrain-error:0.195335\n",
      "[52]\teval-error:0.221374\ttrain-error:0.195335\n",
      "[53]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[54]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[55]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[56]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[57]\teval-error:0.221374\ttrain-error:0.195335\n",
      "[58]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[59]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[60]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[61]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[62]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[63]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[64]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[65]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[66]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[67]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[68]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[69]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[0]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[1]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[2]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[3]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[4]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[5]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[6]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[7]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[8]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[9]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[10]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[11]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[12]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[13]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[14]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[15]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[16]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[17]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[18]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[19]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[20]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[21]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[22]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[23]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[24]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[25]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[26]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[27]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[28]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[29]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[30]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[31]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[32]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[33]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[34]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[35]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[36]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[37]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[38]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[39]\teval-error:0.21374\ttrain-error:0.195335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[41]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[42]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[43]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[44]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[45]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[46]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[47]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[48]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[49]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[50]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[51]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[52]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[53]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[54]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[55]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[56]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[57]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[58]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[59]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[60]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[61]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[62]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[63]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[64]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[65]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[66]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[67]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[68]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[69]\teval-error:0.21374\ttrain-error:0.195335\n",
      "[0]\teval-error:0.274809\ttrain-error:0.143343\n",
      "[1]\teval-error:0.240458\ttrain-error:0.116618\n",
      "[2]\teval-error:0.251908\ttrain-error:0.102041\n",
      "[3]\teval-error:0.229008\ttrain-error:0.091351\n",
      "[4]\teval-error:0.229008\ttrain-error:0.078717\n",
      "[5]\teval-error:0.217557\ttrain-error:0.075316\n",
      "[6]\teval-error:0.225191\ttrain-error:0.066084\n",
      "[7]\teval-error:0.221374\ttrain-error:0.065112\n",
      "[8]\teval-error:0.225191\ttrain-error:0.060253\n",
      "[9]\teval-error:0.232824\ttrain-error:0.057823\n",
      "[10]\teval-error:0.225191\ttrain-error:0.058795\n",
      "[11]\teval-error:0.232824\ttrain-error:0.054422\n",
      "[12]\teval-error:0.229008\ttrain-error:0.054422\n",
      "[13]\teval-error:0.229008\ttrain-error:0.051506\n",
      "[14]\teval-error:0.225191\ttrain-error:0.051506\n",
      "[15]\teval-error:0.225191\ttrain-error:0.05102\n",
      "[16]\teval-error:0.232824\ttrain-error:0.05102\n",
      "[17]\teval-error:0.232824\ttrain-error:0.050534\n",
      "[18]\teval-error:0.232824\ttrain-error:0.050534\n",
      "[19]\teval-error:0.232824\ttrain-error:0.050534\n",
      "[20]\teval-error:0.232824\ttrain-error:0.050534\n",
      "[21]\teval-error:0.221374\ttrain-error:0.049563\n",
      "[22]\teval-error:0.232824\ttrain-error:0.050049\n",
      "[23]\teval-error:0.229008\ttrain-error:0.049563\n",
      "[24]\teval-error:0.229008\ttrain-error:0.049563\n",
      "[25]\teval-error:0.232824\ttrain-error:0.049563\n",
      "[26]\teval-error:0.229008\ttrain-error:0.049563\n",
      "[27]\teval-error:0.229008\ttrain-error:0.049563\n",
      "[28]\teval-error:0.225191\ttrain-error:0.049563\n",
      "[29]\teval-error:0.232824\ttrain-error:0.049563\n",
      "[30]\teval-error:0.236641\ttrain-error:0.049563\n",
      "[31]\teval-error:0.225191\ttrain-error:0.049563\n",
      "[32]\teval-error:0.225191\ttrain-error:0.049563\n",
      "[33]\teval-error:0.229008\ttrain-error:0.049563\n",
      "[34]\teval-error:0.236641\ttrain-error:0.049563\n",
      "[35]\teval-error:0.225191\ttrain-error:0.049563\n",
      "[36]\teval-error:0.236641\ttrain-error:0.049563\n",
      "[37]\teval-error:0.244275\ttrain-error:0.049563\n",
      "[38]\teval-error:0.236641\ttrain-error:0.049563\n",
      "[39]\teval-error:0.240458\ttrain-error:0.049563\n",
      "[40]\teval-error:0.240458\ttrain-error:0.049563\n",
      "[41]\teval-error:0.236641\ttrain-error:0.049563\n",
      "[42]\teval-error:0.236641\ttrain-error:0.049563\n",
      "[43]\teval-error:0.236641\ttrain-error:0.049563\n",
      "[44]\teval-error:0.236641\ttrain-error:0.049563\n",
      "[45]\teval-error:0.236641\ttrain-error:0.049563\n",
      "[46]\teval-error:0.236641\ttrain-error:0.049563\n",
      "[47]\teval-error:0.240458\ttrain-error:0.049563\n",
      "[48]\teval-error:0.236641\ttrain-error:0.049563\n",
      "[49]\teval-error:0.236641\ttrain-error:0.049563\n",
      "[50]\teval-error:0.240458\ttrain-error:0.049563\n",
      "[51]\teval-error:0.236641\ttrain-error:0.049563\n",
      "[52]\teval-error:0.240458\ttrain-error:0.049563\n",
      "[53]\teval-error:0.236641\ttrain-error:0.049563\n",
      "[54]\teval-error:0.236641\ttrain-error:0.049563\n",
      "[55]\teval-error:0.240458\ttrain-error:0.049563\n",
      "[56]\teval-error:0.236641\ttrain-error:0.049563\n",
      "[57]\teval-error:0.240458\ttrain-error:0.049563\n",
      "[58]\teval-error:0.236641\ttrain-error:0.049563\n",
      "[59]\teval-error:0.232824\ttrain-error:0.049563\n",
      "[60]\teval-error:0.236641\ttrain-error:0.049563\n",
      "[61]\teval-error:0.236641\ttrain-error:0.049563\n",
      "[62]\teval-error:0.236641\ttrain-error:0.049563\n",
      "[63]\teval-error:0.232824\ttrain-error:0.049563\n",
      "[64]\teval-error:0.236641\ttrain-error:0.049563\n",
      "[65]\teval-error:0.240458\ttrain-error:0.049563\n",
      "[66]\teval-error:0.232824\ttrain-error:0.049563\n",
      "[67]\teval-error:0.236641\ttrain-error:0.049563\n",
      "[68]\teval-error:0.229008\ttrain-error:0.049563\n",
      "[69]\teval-error:0.232824\ttrain-error:0.049563\n",
      "[0]\teval-error:0.306818\ttrain-error:0.075209\n",
      "[1]\teval-error:0.261364\ttrain-error:0.026462\n",
      "[2]\teval-error:0.261364\ttrain-error:0.012535\n",
      "[3]\teval-error:0.227273\ttrain-error:0.002786\n",
      "[4]\teval-error:0.227273\ttrain-error:0.002786\n",
      "[5]\teval-error:0.227273\ttrain-error:0.001393\n",
      "[6]\teval-error:0.227273\ttrain-error:0.001393\n",
      "[7]\teval-error:0.227273\ttrain-error:0.001393\n",
      "[8]\teval-error:0.215909\ttrain-error:0.001393\n",
      "[9]\teval-error:0.215909\ttrain-error:0.001393\n",
      "[10]\teval-error:0.215909\ttrain-error:0.001393\n",
      "[11]\teval-error:0.227273\ttrain-error:0.001393\n",
      "[12]\teval-error:0.227273\ttrain-error:0.001393\n",
      "[13]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[14]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[15]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[16]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[17]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[18]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[19]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[20]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[21]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[22]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[23]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[24]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[25]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[26]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[27]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[28]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[29]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[30]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[31]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[32]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[33]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[34]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[35]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[36]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[37]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[38]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[39]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[40]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[41]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[42]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[43]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[44]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[45]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[46]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[47]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[48]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[49]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[50]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[51]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[52]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[53]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[54]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[55]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[56]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[57]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[58]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[59]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[60]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[61]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[62]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[63]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[64]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[65]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[66]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[67]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[68]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[69]\teval-error:0.215909\ttrain-error:0.001393\n",
      "[0]\teval-error:0.227273\ttrain-error:0.076602\n",
      "[1]\teval-error:0.238636\ttrain-error:0.027855\n",
      "[2]\teval-error:0.227273\ttrain-error:0.009749\n",
      "[3]\teval-error:0.181818\ttrain-error:0.001393\n",
      "[4]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[5]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[6]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[7]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[8]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[9]\teval-error:0.204545\ttrain-error:0.001393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[11]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[12]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[13]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[14]\teval-error:0.181818\ttrain-error:0.001393\n",
      "[15]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[16]\teval-error:0.181818\ttrain-error:0.001393\n",
      "[17]\teval-error:0.181818\ttrain-error:0.001393\n",
      "[18]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[19]\teval-error:0.181818\ttrain-error:0.001393\n",
      "[20]\teval-error:0.181818\ttrain-error:0.001393\n",
      "[21]\teval-error:0.181818\ttrain-error:0.001393\n",
      "[22]\teval-error:0.181818\ttrain-error:0.001393\n",
      "[23]\teval-error:0.181818\ttrain-error:0.001393\n",
      "[24]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[25]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[26]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[27]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[28]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[29]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[30]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[31]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[32]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[33]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[34]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[35]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[36]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[37]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[38]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[39]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[40]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[41]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[42]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[43]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[44]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[45]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[46]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[47]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[48]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[49]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[50]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[51]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[52]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[53]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[54]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[55]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[56]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[57]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[58]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[59]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[60]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[61]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[62]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[63]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[64]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[65]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[66]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[67]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[68]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[69]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[0]\teval-error:0.227273\ttrain-error:0.093315\n",
      "[1]\teval-error:0.272727\ttrain-error:0.057103\n",
      "[2]\teval-error:0.193182\ttrain-error:0.016713\n",
      "[3]\teval-error:0.181818\ttrain-error:0.002786\n",
      "[4]\teval-error:0.181818\ttrain-error:0.001393\n",
      "[5]\teval-error:0.159091\ttrain-error:0.001393\n",
      "[6]\teval-error:0.170455\ttrain-error:0.001393\n",
      "[7]\teval-error:0.170455\ttrain-error:0.001393\n",
      "[8]\teval-error:0.181818\ttrain-error:0.001393\n",
      "[9]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[10]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[11]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[12]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[13]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[14]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[15]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[16]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[17]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[18]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[19]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[20]\teval-error:0.181818\ttrain-error:0.001393\n",
      "[21]\teval-error:0.181818\ttrain-error:0.001393\n",
      "[22]\teval-error:0.181818\ttrain-error:0.001393\n",
      "[23]\teval-error:0.181818\ttrain-error:0.001393\n",
      "[24]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[25]\teval-error:0.181818\ttrain-error:0.001393\n",
      "[26]\teval-error:0.181818\ttrain-error:0.001393\n",
      "[27]\teval-error:0.181818\ttrain-error:0.001393\n",
      "[28]\teval-error:0.181818\ttrain-error:0.001393\n",
      "[29]\teval-error:0.181818\ttrain-error:0.001393\n",
      "[30]\teval-error:0.181818\ttrain-error:0.001393\n",
      "[31]\teval-error:0.181818\ttrain-error:0.001393\n",
      "[32]\teval-error:0.181818\ttrain-error:0.001393\n",
      "[33]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[34]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[35]\teval-error:0.181818\ttrain-error:0.001393\n",
      "[36]\teval-error:0.181818\ttrain-error:0.001393\n",
      "[37]\teval-error:0.181818\ttrain-error:0.001393\n",
      "[38]\teval-error:0.181818\ttrain-error:0.001393\n",
      "[39]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[40]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[41]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[42]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[43]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[44]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[45]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[46]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[47]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[48]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[49]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[50]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[51]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[52]\teval-error:0.181818\ttrain-error:0.001393\n",
      "[53]\teval-error:0.181818\ttrain-error:0.001393\n",
      "[54]\teval-error:0.181818\ttrain-error:0.001393\n",
      "[55]\teval-error:0.181818\ttrain-error:0.001393\n",
      "[56]\teval-error:0.181818\ttrain-error:0.001393\n",
      "[57]\teval-error:0.181818\ttrain-error:0.001393\n",
      "[58]\teval-error:0.181818\ttrain-error:0.001393\n",
      "[59]\teval-error:0.181818\ttrain-error:0.001393\n",
      "[60]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[61]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[62]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[63]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[64]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[65]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[66]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[67]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[68]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[69]\teval-error:0.193182\ttrain-error:0.001393\n",
      "[0]\teval-error:0.272727\ttrain-error:0.086351\n",
      "[1]\teval-error:0.238636\ttrain-error:0.033426\n",
      "[2]\teval-error:0.215909\ttrain-error:0.008357\n",
      "[3]\teval-error:0.227273\ttrain-error:0.002786\n",
      "[4]\teval-error:0.25\ttrain-error:0.001393\n",
      "[5]\teval-error:0.238636\ttrain-error:0.001393\n",
      "[6]\teval-error:0.238636\ttrain-error:0.001393\n",
      "[7]\teval-error:0.238636\ttrain-error:0.001393\n",
      "[8]\teval-error:0.238636\ttrain-error:0.001393\n",
      "[9]\teval-error:0.238636\ttrain-error:0.001393\n",
      "[10]\teval-error:0.25\ttrain-error:0.001393\n",
      "[11]\teval-error:0.25\ttrain-error:0.001393\n",
      "[12]\teval-error:0.25\ttrain-error:0.001393\n",
      "[13]\teval-error:0.25\ttrain-error:0.001393\n",
      "[14]\teval-error:0.25\ttrain-error:0.001393\n",
      "[15]\teval-error:0.25\ttrain-error:0.001393\n",
      "[16]\teval-error:0.238636\ttrain-error:0.001393\n",
      "[17]\teval-error:0.25\ttrain-error:0.001393\n",
      "[18]\teval-error:0.25\ttrain-error:0.001393\n",
      "[19]\teval-error:0.238636\ttrain-error:0.001393\n",
      "[20]\teval-error:0.238636\ttrain-error:0.001393\n",
      "[21]\teval-error:0.238636\ttrain-error:0.001393\n",
      "[22]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[23]\teval-error:0.25\ttrain-error:0.001393\n",
      "[24]\teval-error:0.238636\ttrain-error:0.001393\n",
      "[25]\teval-error:0.25\ttrain-error:0.001393\n",
      "[26]\teval-error:0.25\ttrain-error:0.001393\n",
      "[27]\teval-error:0.25\ttrain-error:0.001393\n",
      "[28]\teval-error:0.25\ttrain-error:0.001393\n",
      "[29]\teval-error:0.25\ttrain-error:0.001393\n",
      "[30]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[31]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[32]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[33]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[34]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[35]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[36]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[37]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[38]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[39]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[40]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[41]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[42]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[43]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[44]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[45]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[46]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[47]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[48]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[49]\teval-error:0.261364\ttrain-error:0.001393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[51]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[52]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[53]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[54]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[55]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[56]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[57]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[58]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[59]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[60]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[61]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[62]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[63]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[64]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[65]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[66]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[67]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[68]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[69]\teval-error:0.261364\ttrain-error:0.001393\n",
      "[0]\teval-error:0.329787\ttrain-error:0.089969\n",
      "[1]\teval-error:0.265957\ttrain-error:0.028956\n",
      "[2]\teval-error:0.212766\ttrain-error:0.01758\n",
      "[3]\teval-error:0.244681\ttrain-error:0.013444\n",
      "[4]\teval-error:0.223404\ttrain-error:0.01241\n",
      "[5]\teval-error:0.212766\ttrain-error:0.011375\n",
      "[6]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[7]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[8]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[9]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[10]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[11]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[12]\teval-error:0.265957\ttrain-error:0.011375\n",
      "[13]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[14]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[15]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[16]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[17]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[18]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[19]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[20]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[21]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[22]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[23]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[24]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[25]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[26]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[27]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[28]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[29]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[30]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[31]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[32]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[33]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[34]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[35]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[36]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[37]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[38]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[39]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[40]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[41]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[42]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[43]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[44]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[45]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[46]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[47]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[48]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[49]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[50]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[51]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[52]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[53]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[54]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[55]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[56]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[57]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[58]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[59]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[60]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[61]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[62]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[63]\teval-error:0.223404\ttrain-error:0.011375\n",
      "[64]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[65]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[66]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[67]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[68]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[69]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[0]\teval-error:0.361702\ttrain-error:0.091003\n",
      "[1]\teval-error:0.297872\ttrain-error:0.036194\n",
      "[2]\teval-error:0.297872\ttrain-error:0.016546\n",
      "[3]\teval-error:0.276596\ttrain-error:0.01241\n",
      "[4]\teval-error:0.265957\ttrain-error:0.01241\n",
      "[5]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[6]\teval-error:0.276596\ttrain-error:0.011375\n",
      "[7]\teval-error:0.255319\ttrain-error:0.01241\n",
      "[8]\teval-error:0.265957\ttrain-error:0.011375\n",
      "[9]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[10]\teval-error:0.276596\ttrain-error:0.011375\n",
      "[11]\teval-error:0.276596\ttrain-error:0.011375\n",
      "[12]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[13]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[14]\teval-error:0.265957\ttrain-error:0.011375\n",
      "[15]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[16]\teval-error:0.265957\ttrain-error:0.011375\n",
      "[17]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[18]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[19]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[20]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[21]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[22]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[23]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[24]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[25]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[26]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[27]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[28]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[29]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[30]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[31]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[32]\teval-error:0.265957\ttrain-error:0.011375\n",
      "[33]\teval-error:0.265957\ttrain-error:0.011375\n",
      "[34]\teval-error:0.265957\ttrain-error:0.011375\n",
      "[35]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[36]\teval-error:0.265957\ttrain-error:0.011375\n",
      "[37]\teval-error:0.265957\ttrain-error:0.011375\n",
      "[38]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[39]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[40]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[41]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[42]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[43]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[44]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[45]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[46]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[47]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[48]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[49]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[50]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[51]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[52]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[53]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[54]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[55]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[56]\teval-error:0.265957\ttrain-error:0.011375\n",
      "[57]\teval-error:0.265957\ttrain-error:0.011375\n",
      "[58]\teval-error:0.265957\ttrain-error:0.011375\n",
      "[59]\teval-error:0.265957\ttrain-error:0.011375\n",
      "[60]\teval-error:0.265957\ttrain-error:0.011375\n",
      "[61]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[62]\teval-error:0.265957\ttrain-error:0.011375\n",
      "[63]\teval-error:0.265957\ttrain-error:0.011375\n",
      "[64]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[65]\teval-error:0.276596\ttrain-error:0.011375\n",
      "[66]\teval-error:0.265957\ttrain-error:0.011375\n",
      "[67]\teval-error:0.276596\ttrain-error:0.011375\n",
      "[68]\teval-error:0.265957\ttrain-error:0.011375\n",
      "[69]\teval-error:0.265957\ttrain-error:0.011375\n",
      "[0]\teval-error:0.382979\ttrain-error:0.104447\n",
      "[1]\teval-error:0.329787\ttrain-error:0.044467\n",
      "[2]\teval-error:0.329787\ttrain-error:0.024819\n",
      "[3]\teval-error:0.308511\ttrain-error:0.01758\n",
      "[4]\teval-error:0.319149\ttrain-error:0.013444\n",
      "[5]\teval-error:0.308511\ttrain-error:0.01241\n",
      "[6]\teval-error:0.297872\ttrain-error:0.011375\n",
      "[7]\teval-error:0.287234\ttrain-error:0.011375\n",
      "[8]\teval-error:0.276596\ttrain-error:0.011375\n",
      "[9]\teval-error:0.276596\ttrain-error:0.011375\n",
      "[10]\teval-error:0.276596\ttrain-error:0.011375\n",
      "[11]\teval-error:0.287234\ttrain-error:0.011375\n",
      "[12]\teval-error:0.297872\ttrain-error:0.011375\n",
      "[13]\teval-error:0.297872\ttrain-error:0.011375\n",
      "[14]\teval-error:0.297872\ttrain-error:0.011375\n",
      "[15]\teval-error:0.297872\ttrain-error:0.011375\n",
      "[16]\teval-error:0.297872\ttrain-error:0.011375\n",
      "[17]\teval-error:0.297872\ttrain-error:0.011375\n",
      "[18]\teval-error:0.297872\ttrain-error:0.011375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19]\teval-error:0.297872\ttrain-error:0.011375\n",
      "[20]\teval-error:0.297872\ttrain-error:0.011375\n",
      "[21]\teval-error:0.297872\ttrain-error:0.011375\n",
      "[22]\teval-error:0.297872\ttrain-error:0.011375\n",
      "[23]\teval-error:0.308511\ttrain-error:0.011375\n",
      "[24]\teval-error:0.297872\ttrain-error:0.011375\n",
      "[25]\teval-error:0.308511\ttrain-error:0.011375\n",
      "[26]\teval-error:0.308511\ttrain-error:0.011375\n",
      "[27]\teval-error:0.308511\ttrain-error:0.011375\n",
      "[28]\teval-error:0.319149\ttrain-error:0.011375\n",
      "[29]\teval-error:0.308511\ttrain-error:0.011375\n",
      "[30]\teval-error:0.308511\ttrain-error:0.011375\n",
      "[31]\teval-error:0.308511\ttrain-error:0.011375\n",
      "[32]\teval-error:0.308511\ttrain-error:0.011375\n",
      "[33]\teval-error:0.308511\ttrain-error:0.011375\n",
      "[34]\teval-error:0.308511\ttrain-error:0.011375\n",
      "[35]\teval-error:0.308511\ttrain-error:0.011375\n",
      "[36]\teval-error:0.308511\ttrain-error:0.011375\n",
      "[37]\teval-error:0.319149\ttrain-error:0.011375\n",
      "[38]\teval-error:0.319149\ttrain-error:0.011375\n",
      "[39]\teval-error:0.319149\ttrain-error:0.011375\n",
      "[40]\teval-error:0.319149\ttrain-error:0.011375\n",
      "[41]\teval-error:0.319149\ttrain-error:0.011375\n",
      "[42]\teval-error:0.319149\ttrain-error:0.011375\n",
      "[43]\teval-error:0.319149\ttrain-error:0.011375\n",
      "[44]\teval-error:0.319149\ttrain-error:0.011375\n",
      "[45]\teval-error:0.319149\ttrain-error:0.011375\n",
      "[46]\teval-error:0.319149\ttrain-error:0.011375\n",
      "[47]\teval-error:0.319149\ttrain-error:0.011375\n",
      "[48]\teval-error:0.319149\ttrain-error:0.011375\n",
      "[49]\teval-error:0.319149\ttrain-error:0.011375\n",
      "[50]\teval-error:0.319149\ttrain-error:0.011375\n",
      "[51]\teval-error:0.319149\ttrain-error:0.011375\n",
      "[52]\teval-error:0.319149\ttrain-error:0.011375\n",
      "[53]\teval-error:0.319149\ttrain-error:0.011375\n",
      "[54]\teval-error:0.319149\ttrain-error:0.011375\n",
      "[55]\teval-error:0.308511\ttrain-error:0.011375\n",
      "[56]\teval-error:0.319149\ttrain-error:0.011375\n",
      "[57]\teval-error:0.319149\ttrain-error:0.011375\n",
      "[58]\teval-error:0.319149\ttrain-error:0.011375\n",
      "[59]\teval-error:0.308511\ttrain-error:0.011375\n",
      "[60]\teval-error:0.319149\ttrain-error:0.011375\n",
      "[61]\teval-error:0.319149\ttrain-error:0.011375\n",
      "[62]\teval-error:0.319149\ttrain-error:0.011375\n",
      "[63]\teval-error:0.319149\ttrain-error:0.011375\n",
      "[64]\teval-error:0.319149\ttrain-error:0.011375\n",
      "[65]\teval-error:0.308511\ttrain-error:0.011375\n",
      "[66]\teval-error:0.319149\ttrain-error:0.011375\n",
      "[67]\teval-error:0.319149\ttrain-error:0.011375\n",
      "[68]\teval-error:0.319149\ttrain-error:0.011375\n",
      "[69]\teval-error:0.319149\ttrain-error:0.011375\n",
      "[0]\teval-error:0.276596\ttrain-error:0.089969\n",
      "[1]\teval-error:0.223404\ttrain-error:0.034126\n",
      "[2]\teval-error:0.244681\ttrain-error:0.016546\n",
      "[3]\teval-error:0.223404\ttrain-error:0.015512\n",
      "[4]\teval-error:0.223404\ttrain-error:0.01241\n",
      "[5]\teval-error:0.223404\ttrain-error:0.01241\n",
      "[6]\teval-error:0.212766\ttrain-error:0.011375\n",
      "[7]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[8]\teval-error:0.234043\ttrain-error:0.01241\n",
      "[9]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[10]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[11]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[12]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[13]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[14]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[15]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[16]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[17]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[18]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[19]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[20]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[21]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[22]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[23]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[24]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[25]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[26]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[27]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[28]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[29]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[30]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[31]\teval-error:0.255319\ttrain-error:0.011375\n",
      "[32]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[33]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[34]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[35]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[36]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[37]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[38]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[39]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[40]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[41]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[42]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[43]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[44]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[45]\teval-error:0.244681\ttrain-error:0.011375\n",
      "[46]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[47]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[48]\teval-error:0.223404\ttrain-error:0.011375\n",
      "[49]\teval-error:0.223404\ttrain-error:0.011375\n",
      "[50]\teval-error:0.223404\ttrain-error:0.011375\n",
      "[51]\teval-error:0.223404\ttrain-error:0.011375\n",
      "[52]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[53]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[54]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[55]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[56]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[57]\teval-error:0.223404\ttrain-error:0.011375\n",
      "[58]\teval-error:0.223404\ttrain-error:0.011375\n",
      "[59]\teval-error:0.223404\ttrain-error:0.011375\n",
      "[60]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[61]\teval-error:0.223404\ttrain-error:0.011375\n",
      "[62]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[63]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[64]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[65]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[66]\teval-error:0.223404\ttrain-error:0.011375\n",
      "[67]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[68]\teval-error:0.234043\ttrain-error:0.011375\n",
      "[69]\teval-error:0.223404\ttrain-error:0.011375\n",
      "[0]\teval-error:0.229008\ttrain-error:0.085034\n",
      "[1]\teval-error:0.221374\ttrain-error:0.044704\n",
      "[2]\teval-error:0.229008\ttrain-error:0.027697\n",
      "[3]\teval-error:0.217557\ttrain-error:0.023324\n",
      "[4]\teval-error:0.209924\ttrain-error:0.02138\n",
      "[5]\teval-error:0.217557\ttrain-error:0.019922\n",
      "[6]\teval-error:0.21374\ttrain-error:0.018465\n",
      "[7]\teval-error:0.209924\ttrain-error:0.01895\n",
      "[8]\teval-error:0.229008\ttrain-error:0.018465\n",
      "[9]\teval-error:0.221374\ttrain-error:0.018465\n",
      "[10]\teval-error:0.221374\ttrain-error:0.018465\n",
      "[11]\teval-error:0.217557\ttrain-error:0.018465\n",
      "[12]\teval-error:0.21374\ttrain-error:0.018465\n",
      "[13]\teval-error:0.209924\ttrain-error:0.018465\n",
      "[14]\teval-error:0.209924\ttrain-error:0.018465\n",
      "[15]\teval-error:0.206107\ttrain-error:0.018465\n",
      "[16]\teval-error:0.198473\ttrain-error:0.018465\n",
      "[17]\teval-error:0.217557\ttrain-error:0.018465\n",
      "[18]\teval-error:0.221374\ttrain-error:0.018465\n",
      "[19]\teval-error:0.209924\ttrain-error:0.018465\n",
      "[20]\teval-error:0.20229\ttrain-error:0.018465\n",
      "[21]\teval-error:0.206107\ttrain-error:0.018465\n",
      "[22]\teval-error:0.209924\ttrain-error:0.018465\n",
      "[23]\teval-error:0.21374\ttrain-error:0.018465\n",
      "[24]\teval-error:0.221374\ttrain-error:0.018465\n",
      "[25]\teval-error:0.20229\ttrain-error:0.018465\n",
      "[26]\teval-error:0.21374\ttrain-error:0.018465\n",
      "[27]\teval-error:0.221374\ttrain-error:0.018465\n",
      "[28]\teval-error:0.217557\ttrain-error:0.018465\n",
      "[29]\teval-error:0.225191\ttrain-error:0.018465\n",
      "[30]\teval-error:0.209924\ttrain-error:0.018465\n",
      "[31]\teval-error:0.209924\ttrain-error:0.018465\n",
      "[32]\teval-error:0.209924\ttrain-error:0.018465\n",
      "[33]\teval-error:0.217557\ttrain-error:0.018465\n",
      "[34]\teval-error:0.221374\ttrain-error:0.018465\n",
      "[35]\teval-error:0.217557\ttrain-error:0.018465\n",
      "[36]\teval-error:0.221374\ttrain-error:0.018465\n",
      "[37]\teval-error:0.229008\ttrain-error:0.018465\n",
      "[38]\teval-error:0.221374\ttrain-error:0.018465\n",
      "[39]\teval-error:0.217557\ttrain-error:0.018465\n",
      "[40]\teval-error:0.225191\ttrain-error:0.018465\n",
      "[41]\teval-error:0.232824\ttrain-error:0.018465\n",
      "[42]\teval-error:0.217557\ttrain-error:0.018465\n",
      "[43]\teval-error:0.217557\ttrain-error:0.018465\n",
      "[44]\teval-error:0.217557\ttrain-error:0.018465\n",
      "[45]\teval-error:0.221374\ttrain-error:0.018465\n",
      "[46]\teval-error:0.221374\ttrain-error:0.018465\n",
      "[47]\teval-error:0.217557\ttrain-error:0.018465\n",
      "[48]\teval-error:0.209924\ttrain-error:0.018465\n",
      "[49]\teval-error:0.209924\ttrain-error:0.018465\n",
      "[50]\teval-error:0.217557\ttrain-error:0.018465\n",
      "[51]\teval-error:0.221374\ttrain-error:0.018465\n",
      "[52]\teval-error:0.217557\ttrain-error:0.018465\n",
      "[53]\teval-error:0.21374\ttrain-error:0.018465\n",
      "[54]\teval-error:0.21374\ttrain-error:0.018465\n",
      "[55]\teval-error:0.21374\ttrain-error:0.018465\n",
      "[56]\teval-error:0.217557\ttrain-error:0.018465\n",
      "[57]\teval-error:0.21374\ttrain-error:0.018465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58]\teval-error:0.217557\ttrain-error:0.018465\n",
      "[59]\teval-error:0.21374\ttrain-error:0.018465\n",
      "[60]\teval-error:0.221374\ttrain-error:0.018465\n",
      "[61]\teval-error:0.21374\ttrain-error:0.018465\n",
      "[62]\teval-error:0.209924\ttrain-error:0.018465\n",
      "[63]\teval-error:0.209924\ttrain-error:0.018465\n",
      "[64]\teval-error:0.21374\ttrain-error:0.018465\n",
      "[65]\teval-error:0.21374\ttrain-error:0.018465\n",
      "[66]\teval-error:0.21374\ttrain-error:0.018465\n",
      "[67]\teval-error:0.209924\ttrain-error:0.018465\n",
      "[68]\teval-error:0.206107\ttrain-error:0.018465\n",
      "[69]\teval-error:0.21374\ttrain-error:0.018465\n",
      "[0]\teval-error:0.28626\ttrain-error:0.080661\n",
      "[1]\teval-error:0.244275\ttrain-error:0.04276\n",
      "[2]\teval-error:0.244275\ttrain-error:0.029155\n",
      "[3]\teval-error:0.229008\ttrain-error:0.02381\n",
      "[4]\teval-error:0.229008\ttrain-error:0.021866\n",
      "[5]\teval-error:0.225191\ttrain-error:0.019922\n",
      "[6]\teval-error:0.229008\ttrain-error:0.019436\n",
      "[7]\teval-error:0.244275\ttrain-error:0.01895\n",
      "[8]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[9]\teval-error:0.236641\ttrain-error:0.01895\n",
      "[10]\teval-error:0.229008\ttrain-error:0.01895\n",
      "[11]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[12]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[13]\teval-error:0.240458\ttrain-error:0.01895\n",
      "[14]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[15]\teval-error:0.236641\ttrain-error:0.01895\n",
      "[16]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[17]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[18]\teval-error:0.229008\ttrain-error:0.01895\n",
      "[19]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[20]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[21]\teval-error:0.225191\ttrain-error:0.01895\n",
      "[22]\teval-error:0.229008\ttrain-error:0.01895\n",
      "[23]\teval-error:0.229008\ttrain-error:0.01895\n",
      "[24]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[25]\teval-error:0.236641\ttrain-error:0.01895\n",
      "[26]\teval-error:0.236641\ttrain-error:0.01895\n",
      "[27]\teval-error:0.229008\ttrain-error:0.01895\n",
      "[28]\teval-error:0.229008\ttrain-error:0.01895\n",
      "[29]\teval-error:0.236641\ttrain-error:0.01895\n",
      "[30]\teval-error:0.240458\ttrain-error:0.01895\n",
      "[31]\teval-error:0.236641\ttrain-error:0.01895\n",
      "[32]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[33]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[34]\teval-error:0.225191\ttrain-error:0.01895\n",
      "[35]\teval-error:0.225191\ttrain-error:0.01895\n",
      "[36]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[37]\teval-error:0.229008\ttrain-error:0.01895\n",
      "[38]\teval-error:0.229008\ttrain-error:0.01895\n",
      "[39]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[40]\teval-error:0.236641\ttrain-error:0.01895\n",
      "[41]\teval-error:0.236641\ttrain-error:0.01895\n",
      "[42]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[43]\teval-error:0.236641\ttrain-error:0.01895\n",
      "[44]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[45]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[46]\teval-error:0.236641\ttrain-error:0.01895\n",
      "[47]\teval-error:0.240458\ttrain-error:0.01895\n",
      "[48]\teval-error:0.248092\ttrain-error:0.01895\n",
      "[49]\teval-error:0.244275\ttrain-error:0.01895\n",
      "[50]\teval-error:0.236641\ttrain-error:0.01895\n",
      "[51]\teval-error:0.240458\ttrain-error:0.01895\n",
      "[52]\teval-error:0.236641\ttrain-error:0.01895\n",
      "[53]\teval-error:0.240458\ttrain-error:0.01895\n",
      "[54]\teval-error:0.236641\ttrain-error:0.01895\n",
      "[55]\teval-error:0.236641\ttrain-error:0.01895\n",
      "[56]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[57]\teval-error:0.236641\ttrain-error:0.01895\n",
      "[58]\teval-error:0.236641\ttrain-error:0.01895\n",
      "[59]\teval-error:0.240458\ttrain-error:0.01895\n",
      "[60]\teval-error:0.236641\ttrain-error:0.01895\n",
      "[61]\teval-error:0.229008\ttrain-error:0.01895\n",
      "[62]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[63]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[64]\teval-error:0.244275\ttrain-error:0.01895\n",
      "[65]\teval-error:0.240458\ttrain-error:0.01895\n",
      "[66]\teval-error:0.236641\ttrain-error:0.01895\n",
      "[67]\teval-error:0.236641\ttrain-error:0.01895\n",
      "[68]\teval-error:0.236641\ttrain-error:0.01895\n",
      "[69]\teval-error:0.240458\ttrain-error:0.01895\n",
      "[0]\teval-error:0.290076\ttrain-error:0.083576\n",
      "[1]\teval-error:0.240458\ttrain-error:0.050534\n",
      "[2]\teval-error:0.21374\ttrain-error:0.032556\n",
      "[3]\teval-error:0.206107\ttrain-error:0.024781\n",
      "[4]\teval-error:0.21374\ttrain-error:0.02138\n",
      "[5]\teval-error:0.206107\ttrain-error:0.020408\n",
      "[6]\teval-error:0.206107\ttrain-error:0.020408\n",
      "[7]\teval-error:0.209924\ttrain-error:0.020408\n",
      "[8]\teval-error:0.206107\ttrain-error:0.019922\n",
      "[9]\teval-error:0.206107\ttrain-error:0.019922\n",
      "[10]\teval-error:0.209924\ttrain-error:0.019922\n",
      "[11]\teval-error:0.209924\ttrain-error:0.019436\n",
      "[12]\teval-error:0.217557\ttrain-error:0.019436\n",
      "[13]\teval-error:0.217557\ttrain-error:0.019436\n",
      "[14]\teval-error:0.217557\ttrain-error:0.019436\n",
      "[15]\teval-error:0.21374\ttrain-error:0.019436\n",
      "[16]\teval-error:0.209924\ttrain-error:0.019436\n",
      "[17]\teval-error:0.209924\ttrain-error:0.019436\n",
      "[18]\teval-error:0.209924\ttrain-error:0.019436\n",
      "[19]\teval-error:0.209924\ttrain-error:0.019436\n",
      "[20]\teval-error:0.209924\ttrain-error:0.019436\n",
      "[21]\teval-error:0.209924\ttrain-error:0.019436\n",
      "[22]\teval-error:0.21374\ttrain-error:0.019436\n",
      "[23]\teval-error:0.21374\ttrain-error:0.019436\n",
      "[24]\teval-error:0.209924\ttrain-error:0.019436\n",
      "[25]\teval-error:0.206107\ttrain-error:0.019436\n",
      "[26]\teval-error:0.221374\ttrain-error:0.019436\n",
      "[27]\teval-error:0.21374\ttrain-error:0.019436\n",
      "[28]\teval-error:0.221374\ttrain-error:0.019436\n",
      "[29]\teval-error:0.225191\ttrain-error:0.019436\n",
      "[30]\teval-error:0.221374\ttrain-error:0.019436\n",
      "[31]\teval-error:0.229008\ttrain-error:0.019436\n",
      "[32]\teval-error:0.209924\ttrain-error:0.019436\n",
      "[33]\teval-error:0.20229\ttrain-error:0.019436\n",
      "[34]\teval-error:0.20229\ttrain-error:0.019436\n",
      "[35]\teval-error:0.209924\ttrain-error:0.019436\n",
      "[36]\teval-error:0.21374\ttrain-error:0.019436\n",
      "[37]\teval-error:0.217557\ttrain-error:0.019436\n",
      "[38]\teval-error:0.221374\ttrain-error:0.019436\n",
      "[39]\teval-error:0.217557\ttrain-error:0.019436\n",
      "[40]\teval-error:0.217557\ttrain-error:0.019436\n",
      "[41]\teval-error:0.21374\ttrain-error:0.019436\n",
      "[42]\teval-error:0.209924\ttrain-error:0.019436\n",
      "[43]\teval-error:0.206107\ttrain-error:0.019436\n",
      "[44]\teval-error:0.21374\ttrain-error:0.019436\n",
      "[45]\teval-error:0.209924\ttrain-error:0.019436\n",
      "[46]\teval-error:0.21374\ttrain-error:0.019436\n",
      "[47]\teval-error:0.21374\ttrain-error:0.019436\n",
      "[48]\teval-error:0.21374\ttrain-error:0.019436\n",
      "[49]\teval-error:0.206107\ttrain-error:0.019436\n",
      "[50]\teval-error:0.206107\ttrain-error:0.019436\n",
      "[51]\teval-error:0.206107\ttrain-error:0.019436\n",
      "[52]\teval-error:0.20229\ttrain-error:0.019436\n",
      "[53]\teval-error:0.206107\ttrain-error:0.019436\n",
      "[54]\teval-error:0.21374\ttrain-error:0.019436\n",
      "[55]\teval-error:0.217557\ttrain-error:0.019436\n",
      "[56]\teval-error:0.21374\ttrain-error:0.019436\n",
      "[57]\teval-error:0.217557\ttrain-error:0.019436\n",
      "[58]\teval-error:0.221374\ttrain-error:0.019436\n",
      "[59]\teval-error:0.209924\ttrain-error:0.019436\n",
      "[60]\teval-error:0.21374\ttrain-error:0.019436\n",
      "[61]\teval-error:0.21374\ttrain-error:0.019436\n",
      "[62]\teval-error:0.225191\ttrain-error:0.019436\n",
      "[63]\teval-error:0.217557\ttrain-error:0.019436\n",
      "[64]\teval-error:0.221374\ttrain-error:0.019436\n",
      "[65]\teval-error:0.21374\ttrain-error:0.019436\n",
      "[66]\teval-error:0.221374\ttrain-error:0.019436\n",
      "[67]\teval-error:0.217557\ttrain-error:0.019436\n",
      "[68]\teval-error:0.221374\ttrain-error:0.019436\n",
      "[69]\teval-error:0.221374\ttrain-error:0.019436\n",
      "[0]\teval-error:0.312977\ttrain-error:0.076774\n",
      "[1]\teval-error:0.267176\ttrain-error:0.037415\n",
      "[2]\teval-error:0.259542\ttrain-error:0.025753\n",
      "[3]\teval-error:0.248092\ttrain-error:0.022838\n",
      "[4]\teval-error:0.236641\ttrain-error:0.021866\n",
      "[5]\teval-error:0.229008\ttrain-error:0.020408\n",
      "[6]\teval-error:0.225191\ttrain-error:0.019922\n",
      "[7]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[8]\teval-error:0.240458\ttrain-error:0.01895\n",
      "[9]\teval-error:0.240458\ttrain-error:0.01895\n",
      "[10]\teval-error:0.236641\ttrain-error:0.01895\n",
      "[11]\teval-error:0.217557\ttrain-error:0.01895\n",
      "[12]\teval-error:0.221374\ttrain-error:0.01895\n",
      "[13]\teval-error:0.221374\ttrain-error:0.01895\n",
      "[14]\teval-error:0.221374\ttrain-error:0.01895\n",
      "[15]\teval-error:0.229008\ttrain-error:0.01895\n",
      "[16]\teval-error:0.236641\ttrain-error:0.01895\n",
      "[17]\teval-error:0.229008\ttrain-error:0.01895\n",
      "[18]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[19]\teval-error:0.225191\ttrain-error:0.01895\n",
      "[20]\teval-error:0.240458\ttrain-error:0.01895\n",
      "[21]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[22]\teval-error:0.236641\ttrain-error:0.01895\n",
      "[23]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[24]\teval-error:0.240458\ttrain-error:0.01895\n",
      "[25]\teval-error:0.236641\ttrain-error:0.01895\n",
      "[26]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[27]\teval-error:0.236641\ttrain-error:0.01895\n",
      "[28]\teval-error:0.236641\ttrain-error:0.01895\n",
      "[29]\teval-error:0.232824\ttrain-error:0.01895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30]\teval-error:0.240458\ttrain-error:0.01895\n",
      "[31]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[32]\teval-error:0.240458\ttrain-error:0.01895\n",
      "[33]\teval-error:0.236641\ttrain-error:0.01895\n",
      "[34]\teval-error:0.229008\ttrain-error:0.01895\n",
      "[35]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[36]\teval-error:0.236641\ttrain-error:0.01895\n",
      "[37]\teval-error:0.236641\ttrain-error:0.01895\n",
      "[38]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[39]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[40]\teval-error:0.229008\ttrain-error:0.01895\n",
      "[41]\teval-error:0.240458\ttrain-error:0.01895\n",
      "[42]\teval-error:0.236641\ttrain-error:0.01895\n",
      "[43]\teval-error:0.240458\ttrain-error:0.01895\n",
      "[44]\teval-error:0.244275\ttrain-error:0.01895\n",
      "[45]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[46]\teval-error:0.240458\ttrain-error:0.01895\n",
      "[47]\teval-error:0.236641\ttrain-error:0.01895\n",
      "[48]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[49]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[50]\teval-error:0.244275\ttrain-error:0.01895\n",
      "[51]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[52]\teval-error:0.240458\ttrain-error:0.01895\n",
      "[53]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[54]\teval-error:0.236641\ttrain-error:0.01895\n",
      "[55]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[56]\teval-error:0.229008\ttrain-error:0.01895\n",
      "[57]\teval-error:0.229008\ttrain-error:0.01895\n",
      "[58]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[59]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[60]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[61]\teval-error:0.229008\ttrain-error:0.01895\n",
      "[62]\teval-error:0.236641\ttrain-error:0.01895\n",
      "[63]\teval-error:0.229008\ttrain-error:0.01895\n",
      "[64]\teval-error:0.240458\ttrain-error:0.01895\n",
      "[65]\teval-error:0.244275\ttrain-error:0.01895\n",
      "[66]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[67]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[68]\teval-error:0.236641\ttrain-error:0.01895\n",
      "[69]\teval-error:0.232824\ttrain-error:0.01895\n",
      "[0]\teval-error:0.306818\ttrain-error:0.075209\n",
      "[1]\teval-error:0.227273\ttrain-error:0.023677\n",
      "[2]\teval-error:0.261364\ttrain-error:0.011142\n",
      "[3]\teval-error:0.215909\ttrain-error:0.006964\n",
      "[4]\teval-error:0.193182\ttrain-error:0.005571\n",
      "[5]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[6]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[7]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[8]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[9]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[10]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[11]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[12]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[13]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[14]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[15]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[16]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[17]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[18]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[19]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[20]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[21]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[22]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[23]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[24]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[25]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[26]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[27]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[28]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[29]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[30]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[31]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[32]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[33]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[34]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[35]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[36]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[37]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[38]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[39]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[40]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[41]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[42]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[43]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[44]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[45]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[46]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[47]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[48]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[49]\teval-error:0.238636\ttrain-error:0.004178\n",
      "[50]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[51]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[52]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[53]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[54]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[55]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[56]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[57]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[58]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[59]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[60]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[61]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[62]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[63]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[64]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[65]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[66]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[67]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[68]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[69]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[0]\teval-error:0.272727\ttrain-error:0.098886\n",
      "[1]\teval-error:0.204545\ttrain-error:0.043175\n",
      "[2]\teval-error:0.204545\ttrain-error:0.012535\n",
      "[3]\teval-error:0.204545\ttrain-error:0.008357\n",
      "[4]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[5]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[6]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[7]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[8]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[9]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[10]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[11]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[12]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[13]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[14]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[15]\teval-error:0.181818\ttrain-error:0.004178\n",
      "[16]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[17]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[18]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[19]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[20]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[21]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[22]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[23]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[24]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[25]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[26]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[27]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[28]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[29]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[30]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[31]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[32]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[33]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[34]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[35]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[36]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[37]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[38]\teval-error:0.193182\ttrain-error:0.004178\n",
      "[39]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[40]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[41]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[42]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[43]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[44]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[45]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[46]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[47]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[48]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[49]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[50]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[51]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[52]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[53]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[54]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[55]\teval-error:0.227273\ttrain-error:0.004178\n",
      "[56]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[57]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[58]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[59]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[60]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[61]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[62]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[63]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[64]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[65]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[66]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[67]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[68]\teval-error:0.204545\ttrain-error:0.004178\n",
      "[69]\teval-error:0.215909\ttrain-error:0.004178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-error:0.284091\ttrain-error:0.123955\n",
      "[1]\teval-error:0.204545\ttrain-error:0.071031\n",
      "[2]\teval-error:0.204545\ttrain-error:0.038997\n",
      "[3]\teval-error:0.204545\ttrain-error:0.019499\n",
      "[4]\teval-error:0.204545\ttrain-error:0.009749\n",
      "[5]\teval-error:0.25\ttrain-error:0.006964\n",
      "[6]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[7]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[8]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[9]\teval-error:0.227273\ttrain-error:0.005571\n",
      "[10]\teval-error:0.227273\ttrain-error:0.005571\n",
      "[11]\teval-error:0.227273\ttrain-error:0.005571\n",
      "[12]\teval-error:0.227273\ttrain-error:0.005571\n",
      "[13]\teval-error:0.227273\ttrain-error:0.005571\n",
      "[14]\teval-error:0.215909\ttrain-error:0.005571\n",
      "[15]\teval-error:0.215909\ttrain-error:0.005571\n",
      "[16]\teval-error:0.215909\ttrain-error:0.005571\n",
      "[17]\teval-error:0.204545\ttrain-error:0.005571\n",
      "[18]\teval-error:0.204545\ttrain-error:0.005571\n",
      "[19]\teval-error:0.204545\ttrain-error:0.005571\n",
      "[20]\teval-error:0.193182\ttrain-error:0.005571\n",
      "[21]\teval-error:0.193182\ttrain-error:0.005571\n",
      "[22]\teval-error:0.193182\ttrain-error:0.005571\n",
      "[23]\teval-error:0.204545\ttrain-error:0.005571\n",
      "[24]\teval-error:0.204545\ttrain-error:0.005571\n",
      "[25]\teval-error:0.193182\ttrain-error:0.005571\n",
      "[26]\teval-error:0.215909\ttrain-error:0.005571\n",
      "[27]\teval-error:0.215909\ttrain-error:0.005571\n",
      "[28]\teval-error:0.193182\ttrain-error:0.005571\n",
      "[29]\teval-error:0.193182\ttrain-error:0.005571\n",
      "[30]\teval-error:0.193182\ttrain-error:0.005571\n",
      "[31]\teval-error:0.193182\ttrain-error:0.005571\n",
      "[32]\teval-error:0.193182\ttrain-error:0.005571\n",
      "[33]\teval-error:0.193182\ttrain-error:0.005571\n",
      "[34]\teval-error:0.193182\ttrain-error:0.005571\n",
      "[35]\teval-error:0.193182\ttrain-error:0.005571\n",
      "[36]\teval-error:0.193182\ttrain-error:0.005571\n",
      "[37]\teval-error:0.193182\ttrain-error:0.005571\n",
      "[38]\teval-error:0.193182\ttrain-error:0.005571\n",
      "[39]\teval-error:0.193182\ttrain-error:0.005571\n",
      "[40]\teval-error:0.193182\ttrain-error:0.005571\n",
      "[41]\teval-error:0.204545\ttrain-error:0.005571\n",
      "[42]\teval-error:0.204545\ttrain-error:0.005571\n",
      "[43]\teval-error:0.204545\ttrain-error:0.005571\n",
      "[44]\teval-error:0.204545\ttrain-error:0.005571\n",
      "[45]\teval-error:0.204545\ttrain-error:0.005571\n",
      "[46]\teval-error:0.204545\ttrain-error:0.005571\n",
      "[47]\teval-error:0.204545\ttrain-error:0.005571\n",
      "[48]\teval-error:0.204545\ttrain-error:0.005571\n",
      "[49]\teval-error:0.204545\ttrain-error:0.005571\n",
      "[50]\teval-error:0.204545\ttrain-error:0.005571\n",
      "[51]\teval-error:0.204545\ttrain-error:0.005571\n",
      "[52]\teval-error:0.204545\ttrain-error:0.005571\n",
      "[53]\teval-error:0.204545\ttrain-error:0.005571\n",
      "[54]\teval-error:0.204545\ttrain-error:0.005571\n",
      "[55]\teval-error:0.204545\ttrain-error:0.005571\n",
      "[56]\teval-error:0.204545\ttrain-error:0.005571\n",
      "[57]\teval-error:0.204545\ttrain-error:0.005571\n",
      "[58]\teval-error:0.204545\ttrain-error:0.005571\n",
      "[59]\teval-error:0.204545\ttrain-error:0.005571\n",
      "[60]\teval-error:0.215909\ttrain-error:0.005571\n",
      "[61]\teval-error:0.215909\ttrain-error:0.005571\n",
      "[62]\teval-error:0.215909\ttrain-error:0.005571\n",
      "[63]\teval-error:0.215909\ttrain-error:0.005571\n",
      "[64]\teval-error:0.215909\ttrain-error:0.005571\n",
      "[65]\teval-error:0.215909\ttrain-error:0.005571\n",
      "[66]\teval-error:0.215909\ttrain-error:0.005571\n",
      "[67]\teval-error:0.215909\ttrain-error:0.005571\n",
      "[68]\teval-error:0.204545\ttrain-error:0.005571\n",
      "[69]\teval-error:0.204545\ttrain-error:0.005571\n",
      "[0]\teval-error:0.181818\ttrain-error:0.093315\n",
      "[1]\teval-error:0.170455\ttrain-error:0.019499\n",
      "[2]\teval-error:0.170455\ttrain-error:0.006964\n",
      "[3]\teval-error:0.159091\ttrain-error:0.004178\n",
      "[4]\teval-error:0.136364\ttrain-error:0.004178\n",
      "[5]\teval-error:0.136364\ttrain-error:0.004178\n",
      "[6]\teval-error:0.136364\ttrain-error:0.004178\n",
      "[7]\teval-error:0.159091\ttrain-error:0.004178\n",
      "[8]\teval-error:0.147727\ttrain-error:0.004178\n",
      "[9]\teval-error:0.159091\ttrain-error:0.004178\n",
      "[10]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[11]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[12]\teval-error:0.147727\ttrain-error:0.004178\n",
      "[13]\teval-error:0.159091\ttrain-error:0.004178\n",
      "[14]\teval-error:0.159091\ttrain-error:0.004178\n",
      "[15]\teval-error:0.159091\ttrain-error:0.004178\n",
      "[16]\teval-error:0.159091\ttrain-error:0.004178\n",
      "[17]\teval-error:0.147727\ttrain-error:0.004178\n",
      "[18]\teval-error:0.147727\ttrain-error:0.004178\n",
      "[19]\teval-error:0.147727\ttrain-error:0.004178\n",
      "[20]\teval-error:0.147727\ttrain-error:0.004178\n",
      "[21]\teval-error:0.147727\ttrain-error:0.004178\n",
      "[22]\teval-error:0.147727\ttrain-error:0.004178\n",
      "[23]\teval-error:0.147727\ttrain-error:0.004178\n",
      "[24]\teval-error:0.147727\ttrain-error:0.004178\n",
      "[25]\teval-error:0.147727\ttrain-error:0.004178\n",
      "[26]\teval-error:0.147727\ttrain-error:0.004178\n",
      "[27]\teval-error:0.159091\ttrain-error:0.004178\n",
      "[28]\teval-error:0.159091\ttrain-error:0.004178\n",
      "[29]\teval-error:0.159091\ttrain-error:0.004178\n",
      "[30]\teval-error:0.159091\ttrain-error:0.004178\n",
      "[31]\teval-error:0.159091\ttrain-error:0.004178\n",
      "[32]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[33]\teval-error:0.159091\ttrain-error:0.004178\n",
      "[34]\teval-error:0.159091\ttrain-error:0.004178\n",
      "[35]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[36]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[37]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[38]\teval-error:0.159091\ttrain-error:0.004178\n",
      "[39]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[40]\teval-error:0.159091\ttrain-error:0.004178\n",
      "[41]\teval-error:0.159091\ttrain-error:0.004178\n",
      "[42]\teval-error:0.159091\ttrain-error:0.004178\n",
      "[43]\teval-error:0.159091\ttrain-error:0.004178\n",
      "[44]\teval-error:0.159091\ttrain-error:0.004178\n",
      "[45]\teval-error:0.159091\ttrain-error:0.004178\n",
      "[46]\teval-error:0.159091\ttrain-error:0.004178\n",
      "[47]\teval-error:0.159091\ttrain-error:0.004178\n",
      "[48]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[49]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[50]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[51]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[52]\teval-error:0.159091\ttrain-error:0.004178\n",
      "[53]\teval-error:0.159091\ttrain-error:0.004178\n",
      "[54]\teval-error:0.159091\ttrain-error:0.004178\n",
      "[55]\teval-error:0.159091\ttrain-error:0.004178\n",
      "[56]\teval-error:0.159091\ttrain-error:0.004178\n",
      "[57]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[58]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[59]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[60]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[61]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[62]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[63]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[64]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[65]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[66]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[67]\teval-error:0.159091\ttrain-error:0.004178\n",
      "[68]\teval-error:0.170455\ttrain-error:0.004178\n",
      "[69]\teval-error:0.159091\ttrain-error:0.004178\n",
      "[0]\teval-error:0.234043\ttrain-error:0.106515\n",
      "[1]\teval-error:0.255319\ttrain-error:0.05274\n",
      "[2]\teval-error:0.244681\ttrain-error:0.025853\n",
      "[3]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[4]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[5]\teval-error:0.265957\ttrain-error:0.022751\n",
      "[6]\teval-error:0.255319\ttrain-error:0.022751\n",
      "[7]\teval-error:0.234043\ttrain-error:0.022751\n",
      "[8]\teval-error:0.234043\ttrain-error:0.022751\n",
      "[9]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[10]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[11]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[12]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[13]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[14]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[15]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[16]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[17]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[18]\teval-error:0.191489\ttrain-error:0.022751\n",
      "[19]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[20]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[21]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[22]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[23]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[24]\teval-error:0.191489\ttrain-error:0.022751\n",
      "[25]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[26]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[27]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[28]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[29]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[30]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[31]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[32]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[33]\teval-error:0.202128\ttrain-error:0.022751\n",
      "[34]\teval-error:0.202128\ttrain-error:0.022751\n",
      "[35]\teval-error:0.202128\ttrain-error:0.022751\n",
      "[36]\teval-error:0.202128\ttrain-error:0.022751\n",
      "[37]\teval-error:0.191489\ttrain-error:0.022751\n",
      "[38]\teval-error:0.202128\ttrain-error:0.022751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[40]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[41]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[42]\teval-error:0.202128\ttrain-error:0.022751\n",
      "[43]\teval-error:0.191489\ttrain-error:0.022751\n",
      "[44]\teval-error:0.202128\ttrain-error:0.022751\n",
      "[45]\teval-error:0.180851\ttrain-error:0.022751\n",
      "[46]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[47]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[48]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[49]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[50]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[51]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[52]\teval-error:0.202128\ttrain-error:0.022751\n",
      "[53]\teval-error:0.223404\ttrain-error:0.022751\n",
      "[54]\teval-error:0.223404\ttrain-error:0.022751\n",
      "[55]\teval-error:0.223404\ttrain-error:0.022751\n",
      "[56]\teval-error:0.223404\ttrain-error:0.022751\n",
      "[57]\teval-error:0.223404\ttrain-error:0.022751\n",
      "[58]\teval-error:0.223404\ttrain-error:0.022751\n",
      "[59]\teval-error:0.202128\ttrain-error:0.022751\n",
      "[60]\teval-error:0.223404\ttrain-error:0.022751\n",
      "[61]\teval-error:0.202128\ttrain-error:0.022751\n",
      "[62]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[63]\teval-error:0.223404\ttrain-error:0.022751\n",
      "[64]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[65]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[66]\teval-error:0.191489\ttrain-error:0.022751\n",
      "[67]\teval-error:0.191489\ttrain-error:0.022751\n",
      "[68]\teval-error:0.191489\ttrain-error:0.022751\n",
      "[69]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[0]\teval-error:0.276596\ttrain-error:0.110652\n",
      "[1]\teval-error:0.265957\ttrain-error:0.061013\n",
      "[2]\teval-error:0.191489\ttrain-error:0.040331\n",
      "[3]\teval-error:0.223404\ttrain-error:0.036194\n",
      "[4]\teval-error:0.212766\ttrain-error:0.025853\n",
      "[5]\teval-error:0.255319\ttrain-error:0.025853\n",
      "[6]\teval-error:0.234043\ttrain-error:0.024819\n",
      "[7]\teval-error:0.223404\ttrain-error:0.023785\n",
      "[8]\teval-error:0.223404\ttrain-error:0.023785\n",
      "[9]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[10]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[11]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[12]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[13]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[14]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[15]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[16]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[17]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[18]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[19]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[20]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[21]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[22]\teval-error:0.223404\ttrain-error:0.023785\n",
      "[23]\teval-error:0.223404\ttrain-error:0.023785\n",
      "[24]\teval-error:0.223404\ttrain-error:0.023785\n",
      "[25]\teval-error:0.223404\ttrain-error:0.023785\n",
      "[26]\teval-error:0.223404\ttrain-error:0.023785\n",
      "[27]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[28]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[29]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[30]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[31]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[32]\teval-error:0.223404\ttrain-error:0.023785\n",
      "[33]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[34]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[35]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[36]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[37]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[38]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[39]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[40]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[41]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[42]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[43]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[44]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[45]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[46]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[47]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[48]\teval-error:0.223404\ttrain-error:0.023785\n",
      "[49]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[50]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[51]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[52]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[53]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[54]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[55]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[56]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[57]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[58]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[59]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[60]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[61]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[62]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[63]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[64]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[65]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[66]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[67]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[68]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[69]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[0]\teval-error:0.329787\ttrain-error:0.139607\n",
      "[1]\teval-error:0.308511\ttrain-error:0.088935\n",
      "[2]\teval-error:0.340426\ttrain-error:0.05274\n",
      "[3]\teval-error:0.340426\ttrain-error:0.040331\n",
      "[4]\teval-error:0.340426\ttrain-error:0.032058\n",
      "[5]\teval-error:0.340426\ttrain-error:0.031024\n",
      "[6]\teval-error:0.340426\ttrain-error:0.028956\n",
      "[7]\teval-error:0.340426\ttrain-error:0.027921\n",
      "[8]\teval-error:0.329787\ttrain-error:0.027921\n",
      "[9]\teval-error:0.329787\ttrain-error:0.028956\n",
      "[10]\teval-error:0.308511\ttrain-error:0.027921\n",
      "[11]\teval-error:0.319149\ttrain-error:0.028956\n",
      "[12]\teval-error:0.308511\ttrain-error:0.027921\n",
      "[13]\teval-error:0.329787\ttrain-error:0.027921\n",
      "[14]\teval-error:0.319149\ttrain-error:0.027921\n",
      "[15]\teval-error:0.319149\ttrain-error:0.027921\n",
      "[16]\teval-error:0.319149\ttrain-error:0.026887\n",
      "[17]\teval-error:0.297872\ttrain-error:0.026887\n",
      "[18]\teval-error:0.308511\ttrain-error:0.025853\n",
      "[19]\teval-error:0.287234\ttrain-error:0.026887\n",
      "[20]\teval-error:0.308511\ttrain-error:0.025853\n",
      "[21]\teval-error:0.308511\ttrain-error:0.025853\n",
      "[22]\teval-error:0.308511\ttrain-error:0.025853\n",
      "[23]\teval-error:0.297872\ttrain-error:0.025853\n",
      "[24]\teval-error:0.297872\ttrain-error:0.025853\n",
      "[25]\teval-error:0.297872\ttrain-error:0.025853\n",
      "[26]\teval-error:0.308511\ttrain-error:0.025853\n",
      "[27]\teval-error:0.329787\ttrain-error:0.025853\n",
      "[28]\teval-error:0.319149\ttrain-error:0.025853\n",
      "[29]\teval-error:0.308511\ttrain-error:0.025853\n",
      "[30]\teval-error:0.308511\ttrain-error:0.025853\n",
      "[31]\teval-error:0.308511\ttrain-error:0.025853\n",
      "[32]\teval-error:0.308511\ttrain-error:0.025853\n",
      "[33]\teval-error:0.287234\ttrain-error:0.025853\n",
      "[34]\teval-error:0.287234\ttrain-error:0.025853\n",
      "[35]\teval-error:0.297872\ttrain-error:0.025853\n",
      "[36]\teval-error:0.297872\ttrain-error:0.025853\n",
      "[37]\teval-error:0.319149\ttrain-error:0.025853\n",
      "[38]\teval-error:0.308511\ttrain-error:0.025853\n",
      "[39]\teval-error:0.308511\ttrain-error:0.025853\n",
      "[40]\teval-error:0.308511\ttrain-error:0.025853\n",
      "[41]\teval-error:0.308511\ttrain-error:0.025853\n",
      "[42]\teval-error:0.308511\ttrain-error:0.025853\n",
      "[43]\teval-error:0.287234\ttrain-error:0.025853\n",
      "[44]\teval-error:0.287234\ttrain-error:0.025853\n",
      "[45]\teval-error:0.297872\ttrain-error:0.025853\n",
      "[46]\teval-error:0.287234\ttrain-error:0.025853\n",
      "[47]\teval-error:0.287234\ttrain-error:0.025853\n",
      "[48]\teval-error:0.287234\ttrain-error:0.025853\n",
      "[49]\teval-error:0.287234\ttrain-error:0.025853\n",
      "[50]\teval-error:0.287234\ttrain-error:0.025853\n",
      "[51]\teval-error:0.287234\ttrain-error:0.025853\n",
      "[52]\teval-error:0.287234\ttrain-error:0.025853\n",
      "[53]\teval-error:0.287234\ttrain-error:0.025853\n",
      "[54]\teval-error:0.287234\ttrain-error:0.025853\n",
      "[55]\teval-error:0.287234\ttrain-error:0.025853\n",
      "[56]\teval-error:0.287234\ttrain-error:0.025853\n",
      "[57]\teval-error:0.287234\ttrain-error:0.025853\n",
      "[58]\teval-error:0.287234\ttrain-error:0.025853\n",
      "[59]\teval-error:0.287234\ttrain-error:0.025853\n",
      "[60]\teval-error:0.287234\ttrain-error:0.025853\n",
      "[61]\teval-error:0.287234\ttrain-error:0.025853\n",
      "[62]\teval-error:0.287234\ttrain-error:0.025853\n",
      "[63]\teval-error:0.287234\ttrain-error:0.025853\n",
      "[64]\teval-error:0.287234\ttrain-error:0.025853\n",
      "[65]\teval-error:0.287234\ttrain-error:0.025853\n",
      "[66]\teval-error:0.287234\ttrain-error:0.025853\n",
      "[67]\teval-error:0.287234\ttrain-error:0.025853\n",
      "[68]\teval-error:0.287234\ttrain-error:0.025853\n",
      "[69]\teval-error:0.287234\ttrain-error:0.025853\n",
      "[0]\teval-error:0.276596\ttrain-error:0.111686\n",
      "[1]\teval-error:0.319149\ttrain-error:0.062048\n",
      "[2]\teval-error:0.276596\ttrain-error:0.026887\n",
      "[3]\teval-error:0.234043\ttrain-error:0.024819\n",
      "[4]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[5]\teval-error:0.255319\ttrain-error:0.023785\n",
      "[6]\teval-error:0.244681\ttrain-error:0.023785\n",
      "[7]\teval-error:0.244681\ttrain-error:0.023785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8]\teval-error:0.255319\ttrain-error:0.022751\n",
      "[9]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[10]\teval-error:0.191489\ttrain-error:0.022751\n",
      "[11]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[12]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[13]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[14]\teval-error:0.234043\ttrain-error:0.022751\n",
      "[15]\teval-error:0.223404\ttrain-error:0.022751\n",
      "[16]\teval-error:0.223404\ttrain-error:0.022751\n",
      "[17]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[18]\teval-error:0.223404\ttrain-error:0.022751\n",
      "[19]\teval-error:0.202128\ttrain-error:0.022751\n",
      "[20]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[21]\teval-error:0.223404\ttrain-error:0.022751\n",
      "[22]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[23]\teval-error:0.223404\ttrain-error:0.022751\n",
      "[24]\teval-error:0.223404\ttrain-error:0.022751\n",
      "[25]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[26]\teval-error:0.223404\ttrain-error:0.022751\n",
      "[27]\teval-error:0.223404\ttrain-error:0.022751\n",
      "[28]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[29]\teval-error:0.223404\ttrain-error:0.022751\n",
      "[30]\teval-error:0.234043\ttrain-error:0.022751\n",
      "[31]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[32]\teval-error:0.223404\ttrain-error:0.022751\n",
      "[33]\teval-error:0.223404\ttrain-error:0.022751\n",
      "[34]\teval-error:0.191489\ttrain-error:0.022751\n",
      "[35]\teval-error:0.223404\ttrain-error:0.022751\n",
      "[36]\teval-error:0.223404\ttrain-error:0.022751\n",
      "[37]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[38]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[39]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[40]\teval-error:0.191489\ttrain-error:0.022751\n",
      "[41]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[42]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[43]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[44]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[45]\teval-error:0.223404\ttrain-error:0.022751\n",
      "[46]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[47]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[48]\teval-error:0.202128\ttrain-error:0.022751\n",
      "[49]\teval-error:0.223404\ttrain-error:0.022751\n",
      "[50]\teval-error:0.223404\ttrain-error:0.022751\n",
      "[51]\teval-error:0.223404\ttrain-error:0.022751\n",
      "[52]\teval-error:0.223404\ttrain-error:0.022751\n",
      "[53]\teval-error:0.223404\ttrain-error:0.022751\n",
      "[54]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[55]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[56]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[57]\teval-error:0.191489\ttrain-error:0.022751\n",
      "[58]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[59]\teval-error:0.191489\ttrain-error:0.022751\n",
      "[60]\teval-error:0.212766\ttrain-error:0.022751\n",
      "[61]\teval-error:0.191489\ttrain-error:0.022751\n",
      "[62]\teval-error:0.202128\ttrain-error:0.022751\n",
      "[63]\teval-error:0.191489\ttrain-error:0.022751\n",
      "[64]\teval-error:0.202128\ttrain-error:0.022751\n",
      "[65]\teval-error:0.202128\ttrain-error:0.022751\n",
      "[66]\teval-error:0.180851\ttrain-error:0.022751\n",
      "[67]\teval-error:0.191489\ttrain-error:0.022751\n",
      "[68]\teval-error:0.180851\ttrain-error:0.022751\n",
      "[69]\teval-error:0.191489\ttrain-error:0.022751\n",
      "[0]\teval-error:0.244275\ttrain-error:0.092323\n",
      "[1]\teval-error:0.225191\ttrain-error:0.049077\n",
      "[2]\teval-error:0.217557\ttrain-error:0.033528\n",
      "[3]\teval-error:0.229008\ttrain-error:0.026725\n",
      "[4]\teval-error:0.225191\ttrain-error:0.022838\n",
      "[5]\teval-error:0.236641\ttrain-error:0.021866\n",
      "[6]\teval-error:0.232824\ttrain-error:0.021866\n",
      "[7]\teval-error:0.236641\ttrain-error:0.021866\n",
      "[8]\teval-error:0.236641\ttrain-error:0.021866\n",
      "[9]\teval-error:0.244275\ttrain-error:0.021866\n",
      "[10]\teval-error:0.244275\ttrain-error:0.021866\n",
      "[11]\teval-error:0.229008\ttrain-error:0.021866\n",
      "[12]\teval-error:0.229008\ttrain-error:0.021866\n",
      "[13]\teval-error:0.225191\ttrain-error:0.021866\n",
      "[14]\teval-error:0.229008\ttrain-error:0.021866\n",
      "[15]\teval-error:0.229008\ttrain-error:0.021866\n",
      "[16]\teval-error:0.221374\ttrain-error:0.021866\n",
      "[17]\teval-error:0.225191\ttrain-error:0.021866\n",
      "[18]\teval-error:0.232824\ttrain-error:0.021866\n",
      "[19]\teval-error:0.236641\ttrain-error:0.021866\n",
      "[20]\teval-error:0.225191\ttrain-error:0.021866\n",
      "[21]\teval-error:0.232824\ttrain-error:0.021866\n",
      "[22]\teval-error:0.232824\ttrain-error:0.021866\n",
      "[23]\teval-error:0.232824\ttrain-error:0.021866\n",
      "[24]\teval-error:0.236641\ttrain-error:0.021866\n",
      "[25]\teval-error:0.236641\ttrain-error:0.021866\n",
      "[26]\teval-error:0.225191\ttrain-error:0.021866\n",
      "[27]\teval-error:0.229008\ttrain-error:0.021866\n",
      "[28]\teval-error:0.232824\ttrain-error:0.021866\n",
      "[29]\teval-error:0.236641\ttrain-error:0.021866\n",
      "[30]\teval-error:0.232824\ttrain-error:0.021866\n",
      "[31]\teval-error:0.225191\ttrain-error:0.021866\n",
      "[32]\teval-error:0.225191\ttrain-error:0.021866\n",
      "[33]\teval-error:0.232824\ttrain-error:0.021866\n",
      "[34]\teval-error:0.225191\ttrain-error:0.021866\n",
      "[35]\teval-error:0.232824\ttrain-error:0.021866\n",
      "[36]\teval-error:0.232824\ttrain-error:0.021866\n",
      "[37]\teval-error:0.236641\ttrain-error:0.021866\n",
      "[38]\teval-error:0.221374\ttrain-error:0.021866\n",
      "[39]\teval-error:0.236641\ttrain-error:0.021866\n",
      "[40]\teval-error:0.225191\ttrain-error:0.021866\n",
      "[41]\teval-error:0.229008\ttrain-error:0.021866\n",
      "[42]\teval-error:0.229008\ttrain-error:0.021866\n",
      "[43]\teval-error:0.229008\ttrain-error:0.021866\n",
      "[44]\teval-error:0.232824\ttrain-error:0.021866\n",
      "[45]\teval-error:0.236641\ttrain-error:0.021866\n",
      "[46]\teval-error:0.232824\ttrain-error:0.021866\n",
      "[47]\teval-error:0.236641\ttrain-error:0.021866\n",
      "[48]\teval-error:0.232824\ttrain-error:0.021866\n",
      "[49]\teval-error:0.236641\ttrain-error:0.021866\n",
      "[50]\teval-error:0.232824\ttrain-error:0.021866\n",
      "[51]\teval-error:0.225191\ttrain-error:0.021866\n",
      "[52]\teval-error:0.225191\ttrain-error:0.021866\n",
      "[53]\teval-error:0.225191\ttrain-error:0.021866\n",
      "[54]\teval-error:0.225191\ttrain-error:0.021866\n",
      "[55]\teval-error:0.229008\ttrain-error:0.021866\n",
      "[56]\teval-error:0.225191\ttrain-error:0.021866\n",
      "[57]\teval-error:0.221374\ttrain-error:0.021866\n",
      "[58]\teval-error:0.229008\ttrain-error:0.021866\n",
      "[59]\teval-error:0.236641\ttrain-error:0.021866\n",
      "[60]\teval-error:0.229008\ttrain-error:0.021866\n",
      "[61]\teval-error:0.221374\ttrain-error:0.021866\n",
      "[62]\teval-error:0.229008\ttrain-error:0.021866\n",
      "[63]\teval-error:0.221374\ttrain-error:0.021866\n",
      "[64]\teval-error:0.229008\ttrain-error:0.021866\n",
      "[65]\teval-error:0.229008\ttrain-error:0.021866\n",
      "[66]\teval-error:0.229008\ttrain-error:0.021866\n",
      "[67]\teval-error:0.229008\ttrain-error:0.021866\n",
      "[68]\teval-error:0.221374\ttrain-error:0.021866\n",
      "[69]\teval-error:0.232824\ttrain-error:0.021866\n",
      "[0]\teval-error:0.217557\ttrain-error:0.103013\n",
      "[1]\teval-error:0.21374\ttrain-error:0.057337\n",
      "[2]\teval-error:0.225191\ttrain-error:0.036929\n",
      "[3]\teval-error:0.217557\ttrain-error:0.030126\n",
      "[4]\teval-error:0.194656\ttrain-error:0.025753\n",
      "[5]\teval-error:0.20229\ttrain-error:0.025267\n",
      "[6]\teval-error:0.21374\ttrain-error:0.024295\n",
      "[7]\teval-error:0.206107\ttrain-error:0.02381\n",
      "[8]\teval-error:0.198473\ttrain-error:0.024295\n",
      "[9]\teval-error:0.21374\ttrain-error:0.024781\n",
      "[10]\teval-error:0.21374\ttrain-error:0.02381\n",
      "[11]\teval-error:0.20229\ttrain-error:0.02381\n",
      "[12]\teval-error:0.194656\ttrain-error:0.02381\n",
      "[13]\teval-error:0.198473\ttrain-error:0.02381\n",
      "[14]\teval-error:0.194656\ttrain-error:0.02381\n",
      "[15]\teval-error:0.187023\ttrain-error:0.02381\n",
      "[16]\teval-error:0.19084\ttrain-error:0.02381\n",
      "[17]\teval-error:0.20229\ttrain-error:0.02381\n",
      "[18]\teval-error:0.198473\ttrain-error:0.02381\n",
      "[19]\teval-error:0.198473\ttrain-error:0.02381\n",
      "[20]\teval-error:0.206107\ttrain-error:0.02381\n",
      "[21]\teval-error:0.194656\ttrain-error:0.02381\n",
      "[22]\teval-error:0.19084\ttrain-error:0.02381\n",
      "[23]\teval-error:0.20229\ttrain-error:0.02381\n",
      "[24]\teval-error:0.20229\ttrain-error:0.02381\n",
      "[25]\teval-error:0.194656\ttrain-error:0.02381\n",
      "[26]\teval-error:0.198473\ttrain-error:0.02381\n",
      "[27]\teval-error:0.198473\ttrain-error:0.02381\n",
      "[28]\teval-error:0.19084\ttrain-error:0.02381\n",
      "[29]\teval-error:0.19084\ttrain-error:0.02381\n",
      "[30]\teval-error:0.194656\ttrain-error:0.02381\n",
      "[31]\teval-error:0.198473\ttrain-error:0.02381\n",
      "[32]\teval-error:0.194656\ttrain-error:0.02381\n",
      "[33]\teval-error:0.194656\ttrain-error:0.02381\n",
      "[34]\teval-error:0.194656\ttrain-error:0.02381\n",
      "[35]\teval-error:0.19084\ttrain-error:0.02381\n",
      "[36]\teval-error:0.194656\ttrain-error:0.02381\n",
      "[37]\teval-error:0.19084\ttrain-error:0.02381\n",
      "[38]\teval-error:0.20229\ttrain-error:0.02381\n",
      "[39]\teval-error:0.20229\ttrain-error:0.02381\n",
      "[40]\teval-error:0.194656\ttrain-error:0.02381\n",
      "[41]\teval-error:0.20229\ttrain-error:0.02381\n",
      "[42]\teval-error:0.20229\ttrain-error:0.02381\n",
      "[43]\teval-error:0.20229\ttrain-error:0.02381\n",
      "[44]\teval-error:0.20229\ttrain-error:0.02381\n",
      "[45]\teval-error:0.20229\ttrain-error:0.02381\n",
      "[46]\teval-error:0.198473\ttrain-error:0.02381\n",
      "[47]\teval-error:0.19084\ttrain-error:0.02381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48]\teval-error:0.194656\ttrain-error:0.02381\n",
      "[49]\teval-error:0.194656\ttrain-error:0.02381\n",
      "[50]\teval-error:0.194656\ttrain-error:0.02381\n",
      "[51]\teval-error:0.198473\ttrain-error:0.02381\n",
      "[52]\teval-error:0.198473\ttrain-error:0.02381\n",
      "[53]\teval-error:0.198473\ttrain-error:0.02381\n",
      "[54]\teval-error:0.194656\ttrain-error:0.02381\n",
      "[55]\teval-error:0.194656\ttrain-error:0.02381\n",
      "[56]\teval-error:0.20229\ttrain-error:0.02381\n",
      "[57]\teval-error:0.198473\ttrain-error:0.02381\n",
      "[58]\teval-error:0.20229\ttrain-error:0.02381\n",
      "[59]\teval-error:0.194656\ttrain-error:0.02381\n",
      "[60]\teval-error:0.194656\ttrain-error:0.02381\n",
      "[61]\teval-error:0.194656\ttrain-error:0.02381\n",
      "[62]\teval-error:0.198473\ttrain-error:0.02381\n",
      "[63]\teval-error:0.20229\ttrain-error:0.02381\n",
      "[64]\teval-error:0.198473\ttrain-error:0.02381\n",
      "[65]\teval-error:0.194656\ttrain-error:0.02381\n",
      "[66]\teval-error:0.194656\ttrain-error:0.02381\n",
      "[67]\teval-error:0.194656\ttrain-error:0.02381\n",
      "[68]\teval-error:0.198473\ttrain-error:0.02381\n",
      "[69]\teval-error:0.198473\ttrain-error:0.02381\n",
      "[0]\teval-error:0.270992\ttrain-error:0.103499\n",
      "[1]\teval-error:0.236641\ttrain-error:0.069485\n",
      "[2]\teval-error:0.248092\ttrain-error:0.046161\n",
      "[3]\teval-error:0.232824\ttrain-error:0.036443\n",
      "[4]\teval-error:0.240458\ttrain-error:0.030612\n",
      "[5]\teval-error:0.232824\ttrain-error:0.029155\n",
      "[6]\teval-error:0.251908\ttrain-error:0.027211\n",
      "[7]\teval-error:0.240458\ttrain-error:0.027211\n",
      "[8]\teval-error:0.240458\ttrain-error:0.027211\n",
      "[9]\teval-error:0.244275\ttrain-error:0.027211\n",
      "[10]\teval-error:0.240458\ttrain-error:0.026725\n",
      "[11]\teval-error:0.236641\ttrain-error:0.026725\n",
      "[12]\teval-error:0.236641\ttrain-error:0.026725\n",
      "[13]\teval-error:0.225191\ttrain-error:0.026725\n",
      "[14]\teval-error:0.244275\ttrain-error:0.026725\n",
      "[15]\teval-error:0.236641\ttrain-error:0.026725\n",
      "[16]\teval-error:0.232824\ttrain-error:0.026725\n",
      "[17]\teval-error:0.236641\ttrain-error:0.026725\n",
      "[18]\teval-error:0.248092\ttrain-error:0.026725\n",
      "[19]\teval-error:0.232824\ttrain-error:0.026725\n",
      "[20]\teval-error:0.229008\ttrain-error:0.026725\n",
      "[21]\teval-error:0.232824\ttrain-error:0.026725\n",
      "[22]\teval-error:0.232824\ttrain-error:0.026725\n",
      "[23]\teval-error:0.225191\ttrain-error:0.026725\n",
      "[24]\teval-error:0.229008\ttrain-error:0.026725\n",
      "[25]\teval-error:0.225191\ttrain-error:0.026725\n",
      "[26]\teval-error:0.221374\ttrain-error:0.026725\n",
      "[27]\teval-error:0.229008\ttrain-error:0.026725\n",
      "[28]\teval-error:0.232824\ttrain-error:0.026725\n",
      "[29]\teval-error:0.229008\ttrain-error:0.026725\n",
      "[30]\teval-error:0.229008\ttrain-error:0.026725\n",
      "[31]\teval-error:0.217557\ttrain-error:0.026725\n",
      "[32]\teval-error:0.229008\ttrain-error:0.026725\n",
      "[33]\teval-error:0.225191\ttrain-error:0.026725\n",
      "[34]\teval-error:0.225191\ttrain-error:0.026725\n",
      "[35]\teval-error:0.217557\ttrain-error:0.026725\n",
      "[36]\teval-error:0.221374\ttrain-error:0.026725\n",
      "[37]\teval-error:0.221374\ttrain-error:0.026725\n",
      "[38]\teval-error:0.232824\ttrain-error:0.026725\n",
      "[39]\teval-error:0.225191\ttrain-error:0.026725\n",
      "[40]\teval-error:0.221374\ttrain-error:0.026725\n",
      "[41]\teval-error:0.221374\ttrain-error:0.026725\n",
      "[42]\teval-error:0.225191\ttrain-error:0.026725\n",
      "[43]\teval-error:0.225191\ttrain-error:0.026725\n",
      "[44]\teval-error:0.225191\ttrain-error:0.026725\n",
      "[45]\teval-error:0.221374\ttrain-error:0.026725\n",
      "[46]\teval-error:0.217557\ttrain-error:0.026725\n",
      "[47]\teval-error:0.217557\ttrain-error:0.026725\n",
      "[48]\teval-error:0.225191\ttrain-error:0.026725\n",
      "[49]\teval-error:0.221374\ttrain-error:0.026725\n",
      "[50]\teval-error:0.209924\ttrain-error:0.026725\n",
      "[51]\teval-error:0.217557\ttrain-error:0.026725\n",
      "[52]\teval-error:0.217557\ttrain-error:0.026725\n",
      "[53]\teval-error:0.221374\ttrain-error:0.026725\n",
      "[54]\teval-error:0.217557\ttrain-error:0.026725\n",
      "[55]\teval-error:0.217557\ttrain-error:0.026725\n",
      "[56]\teval-error:0.221374\ttrain-error:0.026725\n",
      "[57]\teval-error:0.221374\ttrain-error:0.026725\n",
      "[58]\teval-error:0.221374\ttrain-error:0.026725\n",
      "[59]\teval-error:0.225191\ttrain-error:0.026725\n",
      "[60]\teval-error:0.209924\ttrain-error:0.026725\n",
      "[61]\teval-error:0.232824\ttrain-error:0.026725\n",
      "[62]\teval-error:0.21374\ttrain-error:0.026725\n",
      "[63]\teval-error:0.221374\ttrain-error:0.026725\n",
      "[64]\teval-error:0.225191\ttrain-error:0.026725\n",
      "[65]\teval-error:0.217557\ttrain-error:0.026725\n",
      "[66]\teval-error:0.229008\ttrain-error:0.026725\n",
      "[67]\teval-error:0.225191\ttrain-error:0.026725\n",
      "[68]\teval-error:0.221374\ttrain-error:0.026725\n",
      "[69]\teval-error:0.21374\ttrain-error:0.026725\n",
      "[0]\teval-error:0.232824\ttrain-error:0.094266\n",
      "[1]\teval-error:0.236641\ttrain-error:0.053936\n",
      "[2]\teval-error:0.259542\ttrain-error:0.031584\n",
      "[3]\teval-error:0.274809\ttrain-error:0.025753\n",
      "[4]\teval-error:0.259542\ttrain-error:0.023324\n",
      "[5]\teval-error:0.251908\ttrain-error:0.023324\n",
      "[6]\teval-error:0.244275\ttrain-error:0.022838\n",
      "[7]\teval-error:0.240458\ttrain-error:0.021866\n",
      "[8]\teval-error:0.240458\ttrain-error:0.021866\n",
      "[9]\teval-error:0.236641\ttrain-error:0.021866\n",
      "[10]\teval-error:0.240458\ttrain-error:0.021866\n",
      "[11]\teval-error:0.248092\ttrain-error:0.021866\n",
      "[12]\teval-error:0.251908\ttrain-error:0.021866\n",
      "[13]\teval-error:0.244275\ttrain-error:0.021866\n",
      "[14]\teval-error:0.259542\ttrain-error:0.021866\n",
      "[15]\teval-error:0.248092\ttrain-error:0.021866\n",
      "[16]\teval-error:0.244275\ttrain-error:0.021866\n",
      "[17]\teval-error:0.244275\ttrain-error:0.021866\n",
      "[18]\teval-error:0.248092\ttrain-error:0.021866\n",
      "[19]\teval-error:0.251908\ttrain-error:0.021866\n",
      "[20]\teval-error:0.248092\ttrain-error:0.021866\n",
      "[21]\teval-error:0.240458\ttrain-error:0.021866\n",
      "[22]\teval-error:0.248092\ttrain-error:0.021866\n",
      "[23]\teval-error:0.244275\ttrain-error:0.021866\n",
      "[24]\teval-error:0.232824\ttrain-error:0.021866\n",
      "[25]\teval-error:0.244275\ttrain-error:0.021866\n",
      "[26]\teval-error:0.248092\ttrain-error:0.021866\n",
      "[27]\teval-error:0.240458\ttrain-error:0.021866\n",
      "[28]\teval-error:0.244275\ttrain-error:0.021866\n",
      "[29]\teval-error:0.240458\ttrain-error:0.021866\n",
      "[30]\teval-error:0.251908\ttrain-error:0.021866\n",
      "[31]\teval-error:0.244275\ttrain-error:0.021866\n",
      "[32]\teval-error:0.244275\ttrain-error:0.021866\n",
      "[33]\teval-error:0.244275\ttrain-error:0.021866\n",
      "[34]\teval-error:0.232824\ttrain-error:0.021866\n",
      "[35]\teval-error:0.236641\ttrain-error:0.021866\n",
      "[36]\teval-error:0.232824\ttrain-error:0.021866\n",
      "[37]\teval-error:0.236641\ttrain-error:0.021866\n",
      "[38]\teval-error:0.236641\ttrain-error:0.021866\n",
      "[39]\teval-error:0.248092\ttrain-error:0.021866\n",
      "[40]\teval-error:0.244275\ttrain-error:0.021866\n",
      "[41]\teval-error:0.240458\ttrain-error:0.021866\n",
      "[42]\teval-error:0.240458\ttrain-error:0.021866\n",
      "[43]\teval-error:0.248092\ttrain-error:0.021866\n",
      "[44]\teval-error:0.244275\ttrain-error:0.021866\n",
      "[45]\teval-error:0.240458\ttrain-error:0.021866\n",
      "[46]\teval-error:0.244275\ttrain-error:0.021866\n",
      "[47]\teval-error:0.248092\ttrain-error:0.021866\n",
      "[48]\teval-error:0.244275\ttrain-error:0.021866\n",
      "[49]\teval-error:0.232824\ttrain-error:0.021866\n",
      "[50]\teval-error:0.236641\ttrain-error:0.021866\n",
      "[51]\teval-error:0.236641\ttrain-error:0.021866\n",
      "[52]\teval-error:0.232824\ttrain-error:0.021866\n",
      "[53]\teval-error:0.232824\ttrain-error:0.021866\n",
      "[54]\teval-error:0.236641\ttrain-error:0.021866\n",
      "[55]\teval-error:0.236641\ttrain-error:0.021866\n",
      "[56]\teval-error:0.229008\ttrain-error:0.021866\n",
      "[57]\teval-error:0.232824\ttrain-error:0.021866\n",
      "[58]\teval-error:0.232824\ttrain-error:0.021866\n",
      "[59]\teval-error:0.244275\ttrain-error:0.021866\n",
      "[60]\teval-error:0.251908\ttrain-error:0.021866\n",
      "[61]\teval-error:0.244275\ttrain-error:0.021866\n",
      "[62]\teval-error:0.244275\ttrain-error:0.021866\n",
      "[63]\teval-error:0.236641\ttrain-error:0.021866\n",
      "[64]\teval-error:0.244275\ttrain-error:0.021866\n",
      "[65]\teval-error:0.240458\ttrain-error:0.021866\n",
      "[66]\teval-error:0.232824\ttrain-error:0.021866\n",
      "[67]\teval-error:0.240458\ttrain-error:0.021866\n",
      "[68]\teval-error:0.240458\ttrain-error:0.021866\n",
      "[69]\teval-error:0.236641\ttrain-error:0.021866\n",
      "[0]\teval-error:0.329545\ttrain-error:0.069638\n",
      "[1]\teval-error:0.227273\ttrain-error:0.019499\n",
      "[2]\teval-error:0.238636\ttrain-error:0.005571\n",
      "[3]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[4]\teval-error:0.227273\ttrain-error:0\n",
      "[5]\teval-error:0.227273\ttrain-error:0\n",
      "[6]\teval-error:0.215909\ttrain-error:0\n",
      "[7]\teval-error:0.181818\ttrain-error:0\n",
      "[8]\teval-error:0.170455\ttrain-error:0\n",
      "[9]\teval-error:0.181818\ttrain-error:0\n",
      "[10]\teval-error:0.193182\ttrain-error:0\n",
      "[11]\teval-error:0.181818\ttrain-error:0\n",
      "[12]\teval-error:0.181818\ttrain-error:0\n",
      "[13]\teval-error:0.181818\ttrain-error:0\n",
      "[14]\teval-error:0.181818\ttrain-error:0\n",
      "[15]\teval-error:0.181818\ttrain-error:0\n",
      "[16]\teval-error:0.181818\ttrain-error:0\n",
      "[17]\teval-error:0.181818\ttrain-error:0\n",
      "[18]\teval-error:0.193182\ttrain-error:0\n",
      "[19]\teval-error:0.204545\ttrain-error:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20]\teval-error:0.193182\ttrain-error:0\n",
      "[21]\teval-error:0.181818\ttrain-error:0\n",
      "[22]\teval-error:0.181818\ttrain-error:0\n",
      "[23]\teval-error:0.181818\ttrain-error:0\n",
      "[24]\teval-error:0.181818\ttrain-error:0\n",
      "[25]\teval-error:0.181818\ttrain-error:0\n",
      "[26]\teval-error:0.193182\ttrain-error:0\n",
      "[27]\teval-error:0.181818\ttrain-error:0\n",
      "[28]\teval-error:0.181818\ttrain-error:0\n",
      "[29]\teval-error:0.193182\ttrain-error:0\n",
      "[30]\teval-error:0.193182\ttrain-error:0\n",
      "[31]\teval-error:0.181818\ttrain-error:0\n",
      "[32]\teval-error:0.181818\ttrain-error:0\n",
      "[33]\teval-error:0.193182\ttrain-error:0\n",
      "[34]\teval-error:0.193182\ttrain-error:0\n",
      "[35]\teval-error:0.181818\ttrain-error:0\n",
      "[36]\teval-error:0.193182\ttrain-error:0\n",
      "[37]\teval-error:0.193182\ttrain-error:0\n",
      "[38]\teval-error:0.181818\ttrain-error:0\n",
      "[39]\teval-error:0.193182\ttrain-error:0\n",
      "[40]\teval-error:0.193182\ttrain-error:0\n",
      "[41]\teval-error:0.193182\ttrain-error:0\n",
      "[42]\teval-error:0.193182\ttrain-error:0\n",
      "[43]\teval-error:0.193182\ttrain-error:0\n",
      "[44]\teval-error:0.193182\ttrain-error:0\n",
      "[45]\teval-error:0.193182\ttrain-error:0\n",
      "[46]\teval-error:0.193182\ttrain-error:0\n",
      "[47]\teval-error:0.193182\ttrain-error:0\n",
      "[48]\teval-error:0.193182\ttrain-error:0\n",
      "[49]\teval-error:0.193182\ttrain-error:0\n",
      "[50]\teval-error:0.193182\ttrain-error:0\n",
      "[51]\teval-error:0.193182\ttrain-error:0\n",
      "[52]\teval-error:0.193182\ttrain-error:0\n",
      "[53]\teval-error:0.193182\ttrain-error:0\n",
      "[54]\teval-error:0.193182\ttrain-error:0\n",
      "[55]\teval-error:0.193182\ttrain-error:0\n",
      "[56]\teval-error:0.193182\ttrain-error:0\n",
      "[57]\teval-error:0.193182\ttrain-error:0\n",
      "[58]\teval-error:0.193182\ttrain-error:0\n",
      "[59]\teval-error:0.193182\ttrain-error:0\n",
      "[60]\teval-error:0.193182\ttrain-error:0\n",
      "[61]\teval-error:0.193182\ttrain-error:0\n",
      "[62]\teval-error:0.193182\ttrain-error:0\n",
      "[63]\teval-error:0.193182\ttrain-error:0\n",
      "[64]\teval-error:0.193182\ttrain-error:0\n",
      "[65]\teval-error:0.193182\ttrain-error:0\n",
      "[66]\teval-error:0.193182\ttrain-error:0\n",
      "[67]\teval-error:0.193182\ttrain-error:0\n",
      "[68]\teval-error:0.193182\ttrain-error:0\n",
      "[69]\teval-error:0.193182\ttrain-error:0\n",
      "[0]\teval-error:0.284091\ttrain-error:0.075209\n",
      "[1]\teval-error:0.227273\ttrain-error:0.016713\n",
      "[2]\teval-error:0.215909\ttrain-error:0.002786\n",
      "[3]\teval-error:0.204545\ttrain-error:0.001393\n",
      "[4]\teval-error:0.193182\ttrain-error:0\n",
      "[5]\teval-error:0.181818\ttrain-error:0\n",
      "[6]\teval-error:0.170455\ttrain-error:0\n",
      "[7]\teval-error:0.181818\ttrain-error:0\n",
      "[8]\teval-error:0.181818\ttrain-error:0\n",
      "[9]\teval-error:0.193182\ttrain-error:0\n",
      "[10]\teval-error:0.170455\ttrain-error:0\n",
      "[11]\teval-error:0.181818\ttrain-error:0\n",
      "[12]\teval-error:0.170455\ttrain-error:0\n",
      "[13]\teval-error:0.170455\ttrain-error:0\n",
      "[14]\teval-error:0.170455\ttrain-error:0\n",
      "[15]\teval-error:0.170455\ttrain-error:0\n",
      "[16]\teval-error:0.170455\ttrain-error:0\n",
      "[17]\teval-error:0.170455\ttrain-error:0\n",
      "[18]\teval-error:0.170455\ttrain-error:0\n",
      "[19]\teval-error:0.170455\ttrain-error:0\n",
      "[20]\teval-error:0.170455\ttrain-error:0\n",
      "[21]\teval-error:0.170455\ttrain-error:0\n",
      "[22]\teval-error:0.159091\ttrain-error:0\n",
      "[23]\teval-error:0.159091\ttrain-error:0\n",
      "[24]\teval-error:0.159091\ttrain-error:0\n",
      "[25]\teval-error:0.159091\ttrain-error:0\n",
      "[26]\teval-error:0.159091\ttrain-error:0\n",
      "[27]\teval-error:0.147727\ttrain-error:0\n",
      "[28]\teval-error:0.147727\ttrain-error:0\n",
      "[29]\teval-error:0.147727\ttrain-error:0\n",
      "[30]\teval-error:0.159091\ttrain-error:0\n",
      "[31]\teval-error:0.136364\ttrain-error:0\n",
      "[32]\teval-error:0.147727\ttrain-error:0\n",
      "[33]\teval-error:0.147727\ttrain-error:0\n",
      "[34]\teval-error:0.147727\ttrain-error:0\n",
      "[35]\teval-error:0.159091\ttrain-error:0\n",
      "[36]\teval-error:0.147727\ttrain-error:0\n",
      "[37]\teval-error:0.147727\ttrain-error:0\n",
      "[38]\teval-error:0.147727\ttrain-error:0\n",
      "[39]\teval-error:0.147727\ttrain-error:0\n",
      "[40]\teval-error:0.147727\ttrain-error:0\n",
      "[41]\teval-error:0.147727\ttrain-error:0\n",
      "[42]\teval-error:0.159091\ttrain-error:0\n",
      "[43]\teval-error:0.147727\ttrain-error:0\n",
      "[44]\teval-error:0.147727\ttrain-error:0\n",
      "[45]\teval-error:0.159091\ttrain-error:0\n",
      "[46]\teval-error:0.147727\ttrain-error:0\n",
      "[47]\teval-error:0.147727\ttrain-error:0\n",
      "[48]\teval-error:0.147727\ttrain-error:0\n",
      "[49]\teval-error:0.147727\ttrain-error:0\n",
      "[50]\teval-error:0.147727\ttrain-error:0\n",
      "[51]\teval-error:0.159091\ttrain-error:0\n",
      "[52]\teval-error:0.159091\ttrain-error:0\n",
      "[53]\teval-error:0.159091\ttrain-error:0\n",
      "[54]\teval-error:0.159091\ttrain-error:0\n",
      "[55]\teval-error:0.159091\ttrain-error:0\n",
      "[56]\teval-error:0.159091\ttrain-error:0\n",
      "[57]\teval-error:0.159091\ttrain-error:0\n",
      "[58]\teval-error:0.147727\ttrain-error:0\n",
      "[59]\teval-error:0.147727\ttrain-error:0\n",
      "[60]\teval-error:0.159091\ttrain-error:0\n",
      "[61]\teval-error:0.147727\ttrain-error:0\n",
      "[62]\teval-error:0.159091\ttrain-error:0\n",
      "[63]\teval-error:0.159091\ttrain-error:0\n",
      "[64]\teval-error:0.159091\ttrain-error:0\n",
      "[65]\teval-error:0.159091\ttrain-error:0\n",
      "[66]\teval-error:0.159091\ttrain-error:0\n",
      "[67]\teval-error:0.159091\ttrain-error:0\n",
      "[68]\teval-error:0.159091\ttrain-error:0\n",
      "[69]\teval-error:0.159091\ttrain-error:0\n",
      "[0]\teval-error:0.272727\ttrain-error:0.083565\n",
      "[1]\teval-error:0.215909\ttrain-error:0.032033\n",
      "[2]\teval-error:0.238636\ttrain-error:0.008357\n",
      "[3]\teval-error:0.227273\ttrain-error:0.002786\n",
      "[4]\teval-error:0.227273\ttrain-error:0\n",
      "[5]\teval-error:0.227273\ttrain-error:0\n",
      "[6]\teval-error:0.238636\ttrain-error:0\n",
      "[7]\teval-error:0.215909\ttrain-error:0\n",
      "[8]\teval-error:0.227273\ttrain-error:0\n",
      "[9]\teval-error:0.215909\ttrain-error:0\n",
      "[10]\teval-error:0.238636\ttrain-error:0\n",
      "[11]\teval-error:0.227273\ttrain-error:0\n",
      "[12]\teval-error:0.238636\ttrain-error:0\n",
      "[13]\teval-error:0.215909\ttrain-error:0\n",
      "[14]\teval-error:0.227273\ttrain-error:0\n",
      "[15]\teval-error:0.227273\ttrain-error:0\n",
      "[16]\teval-error:0.227273\ttrain-error:0\n",
      "[17]\teval-error:0.238636\ttrain-error:0\n",
      "[18]\teval-error:0.215909\ttrain-error:0\n",
      "[19]\teval-error:0.215909\ttrain-error:0\n",
      "[20]\teval-error:0.215909\ttrain-error:0\n",
      "[21]\teval-error:0.204545\ttrain-error:0\n",
      "[22]\teval-error:0.215909\ttrain-error:0\n",
      "[23]\teval-error:0.215909\ttrain-error:0\n",
      "[24]\teval-error:0.215909\ttrain-error:0\n",
      "[25]\teval-error:0.204545\ttrain-error:0\n",
      "[26]\teval-error:0.204545\ttrain-error:0\n",
      "[27]\teval-error:0.204545\ttrain-error:0\n",
      "[28]\teval-error:0.204545\ttrain-error:0\n",
      "[29]\teval-error:0.204545\ttrain-error:0\n",
      "[30]\teval-error:0.193182\ttrain-error:0\n",
      "[31]\teval-error:0.193182\ttrain-error:0\n",
      "[32]\teval-error:0.193182\ttrain-error:0\n",
      "[33]\teval-error:0.193182\ttrain-error:0\n",
      "[34]\teval-error:0.193182\ttrain-error:0\n",
      "[35]\teval-error:0.193182\ttrain-error:0\n",
      "[36]\teval-error:0.193182\ttrain-error:0\n",
      "[37]\teval-error:0.193182\ttrain-error:0\n",
      "[38]\teval-error:0.181818\ttrain-error:0\n",
      "[39]\teval-error:0.193182\ttrain-error:0\n",
      "[40]\teval-error:0.193182\ttrain-error:0\n",
      "[41]\teval-error:0.193182\ttrain-error:0\n",
      "[42]\teval-error:0.204545\ttrain-error:0\n",
      "[43]\teval-error:0.204545\ttrain-error:0\n",
      "[44]\teval-error:0.204545\ttrain-error:0\n",
      "[45]\teval-error:0.193182\ttrain-error:0\n",
      "[46]\teval-error:0.193182\ttrain-error:0\n",
      "[47]\teval-error:0.204545\ttrain-error:0\n",
      "[48]\teval-error:0.204545\ttrain-error:0\n",
      "[49]\teval-error:0.204545\ttrain-error:0\n",
      "[50]\teval-error:0.193182\ttrain-error:0\n",
      "[51]\teval-error:0.193182\ttrain-error:0\n",
      "[52]\teval-error:0.193182\ttrain-error:0\n",
      "[53]\teval-error:0.193182\ttrain-error:0\n",
      "[54]\teval-error:0.181818\ttrain-error:0\n",
      "[55]\teval-error:0.193182\ttrain-error:0\n",
      "[56]\teval-error:0.193182\ttrain-error:0\n",
      "[57]\teval-error:0.193182\ttrain-error:0\n",
      "[58]\teval-error:0.181818\ttrain-error:0\n",
      "[59]\teval-error:0.193182\ttrain-error:0\n",
      "[60]\teval-error:0.193182\ttrain-error:0\n",
      "[61]\teval-error:0.193182\ttrain-error:0\n",
      "[62]\teval-error:0.193182\ttrain-error:0\n",
      "[63]\teval-error:0.193182\ttrain-error:0\n",
      "[64]\teval-error:0.193182\ttrain-error:0\n",
      "[65]\teval-error:0.193182\ttrain-error:0\n",
      "[66]\teval-error:0.193182\ttrain-error:0\n",
      "[67]\teval-error:0.193182\ttrain-error:0\n",
      "[68]\teval-error:0.193182\ttrain-error:0\n",
      "[69]\teval-error:0.193182\ttrain-error:0\n",
      "[0]\teval-error:0.329545\ttrain-error:0.083565\n",
      "[1]\teval-error:0.284091\ttrain-error:0.02507\n",
      "[2]\teval-error:0.215909\ttrain-error:0.004178\n",
      "[3]\teval-error:0.284091\ttrain-error:0.001393\n",
      "[4]\teval-error:0.227273\ttrain-error:0\n",
      "[5]\teval-error:0.25\ttrain-error:0\n",
      "[6]\teval-error:0.238636\ttrain-error:0\n",
      "[7]\teval-error:0.227273\ttrain-error:0\n",
      "[8]\teval-error:0.215909\ttrain-error:0\n",
      "[9]\teval-error:0.204545\ttrain-error:0\n",
      "[10]\teval-error:0.215909\ttrain-error:0\n",
      "[11]\teval-error:0.215909\ttrain-error:0\n",
      "[12]\teval-error:0.25\ttrain-error:0\n",
      "[13]\teval-error:0.25\ttrain-error:0\n",
      "[14]\teval-error:0.238636\ttrain-error:0\n",
      "[15]\teval-error:0.227273\ttrain-error:0\n",
      "[16]\teval-error:0.25\ttrain-error:0\n",
      "[17]\teval-error:0.25\ttrain-error:0\n",
      "[18]\teval-error:0.227273\ttrain-error:0\n",
      "[19]\teval-error:0.227273\ttrain-error:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20]\teval-error:0.227273\ttrain-error:0\n",
      "[21]\teval-error:0.227273\ttrain-error:0\n",
      "[22]\teval-error:0.227273\ttrain-error:0\n",
      "[23]\teval-error:0.227273\ttrain-error:0\n",
      "[24]\teval-error:0.227273\ttrain-error:0\n",
      "[25]\teval-error:0.25\ttrain-error:0\n",
      "[26]\teval-error:0.238636\ttrain-error:0\n",
      "[27]\teval-error:0.25\ttrain-error:0\n",
      "[28]\teval-error:0.25\ttrain-error:0\n",
      "[29]\teval-error:0.238636\ttrain-error:0\n",
      "[30]\teval-error:0.25\ttrain-error:0\n",
      "[31]\teval-error:0.25\ttrain-error:0\n",
      "[32]\teval-error:0.238636\ttrain-error:0\n",
      "[33]\teval-error:0.238636\ttrain-error:0\n",
      "[34]\teval-error:0.227273\ttrain-error:0\n",
      "[35]\teval-error:0.238636\ttrain-error:0\n",
      "[36]\teval-error:0.238636\ttrain-error:0\n",
      "[37]\teval-error:0.227273\ttrain-error:0\n",
      "[38]\teval-error:0.238636\ttrain-error:0\n",
      "[39]\teval-error:0.261364\ttrain-error:0\n",
      "[40]\teval-error:0.25\ttrain-error:0\n",
      "[41]\teval-error:0.25\ttrain-error:0\n",
      "[42]\teval-error:0.25\ttrain-error:0\n",
      "[43]\teval-error:0.238636\ttrain-error:0\n",
      "[44]\teval-error:0.25\ttrain-error:0\n",
      "[45]\teval-error:0.238636\ttrain-error:0\n",
      "[46]\teval-error:0.238636\ttrain-error:0\n",
      "[47]\teval-error:0.238636\ttrain-error:0\n",
      "[48]\teval-error:0.25\ttrain-error:0\n",
      "[49]\teval-error:0.238636\ttrain-error:0\n",
      "[50]\teval-error:0.215909\ttrain-error:0\n",
      "[51]\teval-error:0.238636\ttrain-error:0\n",
      "[52]\teval-error:0.238636\ttrain-error:0\n",
      "[53]\teval-error:0.227273\ttrain-error:0\n",
      "[54]\teval-error:0.25\ttrain-error:0\n",
      "[55]\teval-error:0.261364\ttrain-error:0\n",
      "[56]\teval-error:0.25\ttrain-error:0\n",
      "[57]\teval-error:0.238636\ttrain-error:0\n",
      "[58]\teval-error:0.238636\ttrain-error:0\n",
      "[59]\teval-error:0.238636\ttrain-error:0\n",
      "[60]\teval-error:0.238636\ttrain-error:0\n",
      "[61]\teval-error:0.238636\ttrain-error:0\n",
      "[62]\teval-error:0.25\ttrain-error:0\n",
      "[63]\teval-error:0.238636\ttrain-error:0\n",
      "[64]\teval-error:0.238636\ttrain-error:0\n",
      "[65]\teval-error:0.238636\ttrain-error:0\n",
      "[66]\teval-error:0.238636\ttrain-error:0\n",
      "[67]\teval-error:0.238636\ttrain-error:0\n",
      "[68]\teval-error:0.227273\ttrain-error:0\n",
      "[69]\teval-error:0.227273\ttrain-error:0\n",
      "[0]\teval-error:0.297872\ttrain-error:0.085832\n",
      "[1]\teval-error:0.255319\ttrain-error:0.033092\n",
      "[2]\teval-error:0.287234\ttrain-error:0.01241\n",
      "[3]\teval-error:0.255319\ttrain-error:0.007239\n",
      "[4]\teval-error:0.223404\ttrain-error:0.006205\n",
      "[5]\teval-error:0.234043\ttrain-error:0.006205\n",
      "[6]\teval-error:0.244681\ttrain-error:0.005171\n",
      "[7]\teval-error:0.234043\ttrain-error:0.004137\n",
      "[8]\teval-error:0.234043\ttrain-error:0.004137\n",
      "[9]\teval-error:0.255319\ttrain-error:0.003102\n",
      "[10]\teval-error:0.255319\ttrain-error:0.002068\n",
      "[11]\teval-error:0.244681\ttrain-error:0.002068\n",
      "[12]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[13]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[14]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[15]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[16]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[17]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[18]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[19]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[20]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[21]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[22]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[23]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[24]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[25]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[26]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[27]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[28]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[29]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[30]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[31]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[32]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[33]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[34]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[35]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[36]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[37]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[38]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[39]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[40]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[41]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[42]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[43]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[44]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[45]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[46]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[47]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[48]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[49]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[50]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[51]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[52]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[53]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[54]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[55]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[56]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[57]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[58]\teval-error:0.212766\ttrain-error:0.001034\n",
      "[59]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[60]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[61]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[62]\teval-error:0.223404\ttrain-error:0.001034\n",
      "[63]\teval-error:0.212766\ttrain-error:0.001034\n",
      "[64]\teval-error:0.212766\ttrain-error:0.001034\n",
      "[65]\teval-error:0.212766\ttrain-error:0.001034\n",
      "[66]\teval-error:0.212766\ttrain-error:0.001034\n",
      "[67]\teval-error:0.212766\ttrain-error:0.001034\n",
      "[68]\teval-error:0.212766\ttrain-error:0.001034\n",
      "[69]\teval-error:0.212766\ttrain-error:0.001034\n",
      "[0]\teval-error:0.329787\ttrain-error:0.09514\n",
      "[1]\teval-error:0.276596\ttrain-error:0.036194\n",
      "[2]\teval-error:0.265957\ttrain-error:0.010341\n",
      "[3]\teval-error:0.297872\ttrain-error:0.006205\n",
      "[4]\teval-error:0.308511\ttrain-error:0.005171\n",
      "[5]\teval-error:0.276596\ttrain-error:0.002068\n",
      "[6]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[7]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[8]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[9]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[10]\teval-error:0.234043\ttrain-error:0.001034\n",
      "[11]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[12]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[13]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[14]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[15]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[16]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[17]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[18]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[19]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[20]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[21]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[22]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[23]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[24]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[25]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[26]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[27]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[28]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[29]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[30]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[31]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[32]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[33]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[34]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[35]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[36]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[37]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[38]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[39]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[40]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[41]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[42]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[43]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[44]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[45]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[46]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[47]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[48]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[49]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[50]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[51]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[52]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[53]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[54]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[55]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[56]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[57]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[58]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[59]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[60]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[61]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[62]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[63]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[64]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[65]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[66]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[67]\teval-error:0.255319\ttrain-error:0.001034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[68]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[69]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[0]\teval-error:0.329787\ttrain-error:0.086867\n",
      "[1]\teval-error:0.329787\ttrain-error:0.031024\n",
      "[2]\teval-error:0.297872\ttrain-error:0.010341\n",
      "[3]\teval-error:0.287234\ttrain-error:0.003102\n",
      "[4]\teval-error:0.287234\ttrain-error:0.001034\n",
      "[5]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[6]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[7]\teval-error:0.287234\ttrain-error:0.001034\n",
      "[8]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[9]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[10]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[11]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[12]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[13]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[14]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[15]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[16]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[17]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[18]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[19]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[20]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[21]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[22]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[23]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[24]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[25]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[26]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[27]\teval-error:0.287234\ttrain-error:0.001034\n",
      "[28]\teval-error:0.297872\ttrain-error:0.001034\n",
      "[29]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[30]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[31]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[32]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[33]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[34]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[35]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[36]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[37]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[38]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[39]\teval-error:0.276596\ttrain-error:0.001034\n",
      "[40]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[41]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[42]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[43]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[44]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[45]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[46]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[47]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[48]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[49]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[50]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[51]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[52]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[53]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[54]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[55]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[56]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[57]\teval-error:0.265957\ttrain-error:0.001034\n",
      "[58]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[59]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[60]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[61]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[62]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[63]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[64]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[65]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[66]\teval-error:0.244681\ttrain-error:0.001034\n",
      "[67]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[68]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[69]\teval-error:0.255319\ttrain-error:0.001034\n",
      "[0]\teval-error:0.297872\ttrain-error:0.091003\n",
      "[1]\teval-error:0.234043\ttrain-error:0.023785\n",
      "[2]\teval-error:0.244681\ttrain-error:0.01241\n",
      "[3]\teval-error:0.234043\ttrain-error:0.008273\n",
      "[4]\teval-error:0.255319\ttrain-error:0.007239\n",
      "[5]\teval-error:0.234043\ttrain-error:0.006205\n",
      "[6]\teval-error:0.202128\ttrain-error:0.005171\n",
      "[7]\teval-error:0.180851\ttrain-error:0.005171\n",
      "[8]\teval-error:0.159574\ttrain-error:0.003102\n",
      "[9]\teval-error:0.180851\ttrain-error:0.001034\n",
      "[10]\teval-error:0.180851\ttrain-error:0.001034\n",
      "[11]\teval-error:0.191489\ttrain-error:0.001034\n",
      "[12]\teval-error:0.191489\ttrain-error:0.001034\n",
      "[13]\teval-error:0.180851\ttrain-error:0.001034\n",
      "[14]\teval-error:0.159574\ttrain-error:0.001034\n",
      "[15]\teval-error:0.170213\ttrain-error:0.001034\n",
      "[16]\teval-error:0.180851\ttrain-error:0.001034\n",
      "[17]\teval-error:0.180851\ttrain-error:0.001034\n",
      "[18]\teval-error:0.180851\ttrain-error:0.001034\n",
      "[19]\teval-error:0.170213\ttrain-error:0.001034\n",
      "[20]\teval-error:0.170213\ttrain-error:0.001034\n",
      "[21]\teval-error:0.170213\ttrain-error:0.001034\n",
      "[22]\teval-error:0.191489\ttrain-error:0.001034\n",
      "[23]\teval-error:0.202128\ttrain-error:0.001034\n",
      "[24]\teval-error:0.202128\ttrain-error:0.001034\n",
      "[25]\teval-error:0.202128\ttrain-error:0.001034\n",
      "[26]\teval-error:0.180851\ttrain-error:0.001034\n",
      "[27]\teval-error:0.180851\ttrain-error:0.001034\n",
      "[28]\teval-error:0.180851\ttrain-error:0.001034\n",
      "[29]\teval-error:0.180851\ttrain-error:0.001034\n",
      "[30]\teval-error:0.180851\ttrain-error:0.001034\n",
      "[31]\teval-error:0.180851\ttrain-error:0.001034\n",
      "[32]\teval-error:0.170213\ttrain-error:0.001034\n",
      "[33]\teval-error:0.170213\ttrain-error:0.001034\n",
      "[34]\teval-error:0.170213\ttrain-error:0.001034\n",
      "[35]\teval-error:0.180851\ttrain-error:0.001034\n",
      "[36]\teval-error:0.170213\ttrain-error:0.001034\n",
      "[37]\teval-error:0.180851\ttrain-error:0.001034\n",
      "[38]\teval-error:0.170213\ttrain-error:0.001034\n",
      "[39]\teval-error:0.180851\ttrain-error:0.001034\n",
      "[40]\teval-error:0.191489\ttrain-error:0.001034\n",
      "[41]\teval-error:0.180851\ttrain-error:0.001034\n",
      "[42]\teval-error:0.180851\ttrain-error:0.001034\n",
      "[43]\teval-error:0.180851\ttrain-error:0.001034\n",
      "[44]\teval-error:0.180851\ttrain-error:0.001034\n",
      "[45]\teval-error:0.191489\ttrain-error:0.001034\n",
      "[46]\teval-error:0.180851\ttrain-error:0.001034\n",
      "[47]\teval-error:0.202128\ttrain-error:0.001034\n",
      "[48]\teval-error:0.180851\ttrain-error:0.001034\n",
      "[49]\teval-error:0.202128\ttrain-error:0.001034\n",
      "[50]\teval-error:0.191489\ttrain-error:0.001034\n",
      "[51]\teval-error:0.180851\ttrain-error:0.001034\n",
      "[52]\teval-error:0.191489\ttrain-error:0.001034\n",
      "[53]\teval-error:0.191489\ttrain-error:0.001034\n",
      "[54]\teval-error:0.170213\ttrain-error:0.001034\n",
      "[55]\teval-error:0.191489\ttrain-error:0.001034\n",
      "[56]\teval-error:0.191489\ttrain-error:0.001034\n",
      "[57]\teval-error:0.191489\ttrain-error:0.001034\n",
      "[58]\teval-error:0.191489\ttrain-error:0.001034\n",
      "[59]\teval-error:0.191489\ttrain-error:0.001034\n",
      "[60]\teval-error:0.180851\ttrain-error:0.001034\n",
      "[61]\teval-error:0.180851\ttrain-error:0.001034\n",
      "[62]\teval-error:0.170213\ttrain-error:0.001034\n",
      "[63]\teval-error:0.180851\ttrain-error:0.001034\n",
      "[64]\teval-error:0.170213\ttrain-error:0.001034\n",
      "[65]\teval-error:0.180851\ttrain-error:0.001034\n",
      "[66]\teval-error:0.180851\ttrain-error:0.001034\n",
      "[67]\teval-error:0.180851\ttrain-error:0.001034\n",
      "[68]\teval-error:0.180851\ttrain-error:0.001034\n",
      "[69]\teval-error:0.170213\ttrain-error:0.001034\n",
      "[0]\teval-error:0.270992\ttrain-error:0.073372\n",
      "[1]\teval-error:0.225191\ttrain-error:0.036443\n",
      "[2]\teval-error:0.221374\ttrain-error:0.017979\n",
      "[3]\teval-error:0.209924\ttrain-error:0.012148\n",
      "[4]\teval-error:0.221374\ttrain-error:0.009718\n",
      "[5]\teval-error:0.236641\ttrain-error:0.007289\n",
      "[6]\teval-error:0.232824\ttrain-error:0.007289\n",
      "[7]\teval-error:0.221374\ttrain-error:0.006317\n",
      "[8]\teval-error:0.236641\ttrain-error:0.006317\n",
      "[9]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[10]\teval-error:0.225191\ttrain-error:0.005831\n",
      "[11]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[12]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[13]\teval-error:0.225191\ttrain-error:0.005831\n",
      "[14]\teval-error:0.236641\ttrain-error:0.005831\n",
      "[15]\teval-error:0.236641\ttrain-error:0.005831\n",
      "[16]\teval-error:0.232824\ttrain-error:0.005831\n",
      "[17]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[18]\teval-error:0.236641\ttrain-error:0.005831\n",
      "[19]\teval-error:0.232824\ttrain-error:0.005831\n",
      "[20]\teval-error:0.232824\ttrain-error:0.005831\n",
      "[21]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[22]\teval-error:0.240458\ttrain-error:0.005831\n",
      "[23]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[24]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[25]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[26]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[27]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[28]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[29]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[30]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[31]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[32]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[33]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[34]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[35]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[36]\teval-error:0.217557\ttrain-error:0.005831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[38]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[39]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[40]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[41]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[42]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[43]\teval-error:0.225191\ttrain-error:0.005831\n",
      "[44]\teval-error:0.225191\ttrain-error:0.005831\n",
      "[45]\teval-error:0.225191\ttrain-error:0.005831\n",
      "[46]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[47]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[48]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[49]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[50]\teval-error:0.225191\ttrain-error:0.005831\n",
      "[51]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[52]\teval-error:0.225191\ttrain-error:0.005831\n",
      "[53]\teval-error:0.225191\ttrain-error:0.005831\n",
      "[54]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[55]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[56]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[57]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[58]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[59]\teval-error:0.225191\ttrain-error:0.005831\n",
      "[60]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[61]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[62]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[63]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[64]\teval-error:0.225191\ttrain-error:0.005831\n",
      "[65]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[66]\teval-error:0.225191\ttrain-error:0.005831\n",
      "[67]\teval-error:0.225191\ttrain-error:0.005831\n",
      "[68]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[69]\teval-error:0.225191\ttrain-error:0.005831\n",
      "[0]\teval-error:0.232824\ttrain-error:0.079689\n",
      "[1]\teval-error:0.194656\ttrain-error:0.037415\n",
      "[2]\teval-error:0.209924\ttrain-error:0.01895\n",
      "[3]\teval-error:0.206107\ttrain-error:0.012148\n",
      "[4]\teval-error:0.194656\ttrain-error:0.007775\n",
      "[5]\teval-error:0.21374\ttrain-error:0.006803\n",
      "[6]\teval-error:0.21374\ttrain-error:0.006317\n",
      "[7]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[8]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[9]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[10]\teval-error:0.225191\ttrain-error:0.005831\n",
      "[11]\teval-error:0.225191\ttrain-error:0.005831\n",
      "[12]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[13]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[14]\teval-error:0.225191\ttrain-error:0.005831\n",
      "[15]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[16]\teval-error:0.225191\ttrain-error:0.005831\n",
      "[17]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[18]\teval-error:0.225191\ttrain-error:0.005831\n",
      "[19]\teval-error:0.229008\ttrain-error:0.005831\n",
      "[20]\teval-error:0.225191\ttrain-error:0.005831\n",
      "[21]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[22]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[23]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[24]\teval-error:0.225191\ttrain-error:0.005831\n",
      "[25]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[26]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[27]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[28]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[29]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[30]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[31]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[32]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[33]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[34]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[35]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[36]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[37]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[38]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[39]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[40]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[41]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[42]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[43]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[44]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[45]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[46]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[47]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[48]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[49]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[50]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[51]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[52]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[53]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[54]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[55]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[56]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[57]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[58]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[59]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[60]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[61]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[62]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[63]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[64]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[65]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[66]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[67]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[68]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[69]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[0]\teval-error:0.232824\ttrain-error:0.083576\n",
      "[1]\teval-error:0.225191\ttrain-error:0.042274\n",
      "[2]\teval-error:0.221374\ttrain-error:0.02381\n",
      "[3]\teval-error:0.217557\ttrain-error:0.012148\n",
      "[4]\teval-error:0.217557\ttrain-error:0.006803\n",
      "[5]\teval-error:0.20229\ttrain-error:0.006803\n",
      "[6]\teval-error:0.209924\ttrain-error:0.006317\n",
      "[7]\teval-error:0.229008\ttrain-error:0.006317\n",
      "[8]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[9]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[10]\teval-error:0.20229\ttrain-error:0.005831\n",
      "[11]\teval-error:0.20229\ttrain-error:0.005831\n",
      "[12]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[13]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[14]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[15]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[16]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[17]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[18]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[19]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[20]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[21]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[22]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[23]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[24]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[25]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[26]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[27]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[28]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[29]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[30]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[31]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[32]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[33]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[34]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[35]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[36]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[37]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[38]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[39]\teval-error:0.221374\ttrain-error:0.005831\n",
      "[40]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[41]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[42]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[43]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[44]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[45]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[46]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[47]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[48]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[49]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[50]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[51]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[52]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[53]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[54]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[55]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[56]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[57]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[58]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[59]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[60]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[61]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[62]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[63]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[64]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[65]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[66]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[67]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[68]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[69]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[0]\teval-error:0.251908\ttrain-error:0.076774\n",
      "[1]\teval-error:0.221374\ttrain-error:0.03207\n",
      "[2]\teval-error:0.209924\ttrain-error:0.015549\n",
      "[3]\teval-error:0.206107\ttrain-error:0.009718\n",
      "[4]\teval-error:0.217557\ttrain-error:0.00826\n",
      "[5]\teval-error:0.217557\ttrain-error:0.006317\n",
      "[6]\teval-error:0.21374\ttrain-error:0.006803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[8]\teval-error:0.217557\ttrain-error:0.005831\n",
      "[9]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[10]\teval-error:0.20229\ttrain-error:0.005831\n",
      "[11]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[12]\teval-error:0.198473\ttrain-error:0.005831\n",
      "[13]\teval-error:0.20229\ttrain-error:0.005831\n",
      "[14]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[15]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[16]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[17]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[18]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[19]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[20]\teval-error:0.198473\ttrain-error:0.005831\n",
      "[21]\teval-error:0.198473\ttrain-error:0.005831\n",
      "[22]\teval-error:0.198473\ttrain-error:0.005831\n",
      "[23]\teval-error:0.198473\ttrain-error:0.005831\n",
      "[24]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[25]\teval-error:0.194656\ttrain-error:0.005831\n",
      "[26]\teval-error:0.20229\ttrain-error:0.005831\n",
      "[27]\teval-error:0.20229\ttrain-error:0.005831\n",
      "[28]\teval-error:0.20229\ttrain-error:0.005831\n",
      "[29]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[30]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[31]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[32]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[33]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[34]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[35]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[36]\teval-error:0.198473\ttrain-error:0.005831\n",
      "[37]\teval-error:0.194656\ttrain-error:0.005831\n",
      "[38]\teval-error:0.198473\ttrain-error:0.005831\n",
      "[39]\teval-error:0.198473\ttrain-error:0.005831\n",
      "[40]\teval-error:0.198473\ttrain-error:0.005831\n",
      "[41]\teval-error:0.20229\ttrain-error:0.005831\n",
      "[42]\teval-error:0.198473\ttrain-error:0.005831\n",
      "[43]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[44]\teval-error:0.198473\ttrain-error:0.005831\n",
      "[45]\teval-error:0.198473\ttrain-error:0.005831\n",
      "[46]\teval-error:0.20229\ttrain-error:0.005831\n",
      "[47]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[48]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[49]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[50]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[51]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[52]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[53]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[54]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[55]\teval-error:0.21374\ttrain-error:0.005831\n",
      "[56]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[57]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[58]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[59]\teval-error:0.209924\ttrain-error:0.005831\n",
      "[60]\teval-error:0.20229\ttrain-error:0.005831\n",
      "[61]\teval-error:0.206107\ttrain-error:0.005831\n",
      "[62]\teval-error:0.20229\ttrain-error:0.005831\n",
      "[63]\teval-error:0.20229\ttrain-error:0.005831\n",
      "[64]\teval-error:0.20229\ttrain-error:0.005831\n",
      "[65]\teval-error:0.20229\ttrain-error:0.005831\n",
      "[66]\teval-error:0.20229\ttrain-error:0.005831\n",
      "[67]\teval-error:0.20229\ttrain-error:0.005831\n",
      "[68]\teval-error:0.20229\ttrain-error:0.005831\n",
      "[69]\teval-error:0.20229\ttrain-error:0.005831\n"
     ]
    }
   ],
   "source": [
    "results = [Result(fs, fs.fc, fs.agg,\n",
    "        precision_recall_fscore_support(*xgboost(*transform_feat_to_num(remove_labels_for_binary_df(fs.train), \n",
    "        remove_labels_for_binary_df(fs.test))), average='macro')) for fs in all_fc_datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.474851</td>\n",
       "      <td>0.473277</td>\n",
       "      <td>0.481660</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.574879</td>\n",
       "      <td>0.597046</td>\n",
       "      <td>0.566602</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.439490</td>\n",
       "      <td>0.415663</td>\n",
       "      <td>0.466216</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.540399</td>\n",
       "      <td>0.568783</td>\n",
       "      <td>0.537645</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.571228</td>\n",
       "      <td>0.584936</td>\n",
       "      <td>0.566919</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.646680</td>\n",
       "      <td>0.678059</td>\n",
       "      <td>0.633207</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.533231</td>\n",
       "      <td>0.556911</td>\n",
       "      <td>0.535354</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.680329</td>\n",
       "      <td>0.717722</td>\n",
       "      <td>0.662879</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.577685</td>\n",
       "      <td>0.628605</td>\n",
       "      <td>0.570735</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.634292</td>\n",
       "      <td>0.673442</td>\n",
       "      <td>0.619452</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.566273</td>\n",
       "      <td>0.616199</td>\n",
       "      <td>0.561807</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.669999</td>\n",
       "      <td>0.748187</td>\n",
       "      <td>0.645371</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.486329</td>\n",
       "      <td>0.491182</td>\n",
       "      <td>0.495174</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.463841</td>\n",
       "      <td>0.461039</td>\n",
       "      <td>0.468147</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.503590</td>\n",
       "      <td>0.504386</td>\n",
       "      <td>0.503861</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.503590</td>\n",
       "      <td>0.504386</td>\n",
       "      <td>0.503861</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.563713</td>\n",
       "      <td>0.645349</td>\n",
       "      <td>0.563131</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.490349</td>\n",
       "      <td>0.527915</td>\n",
       "      <td>0.510732</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.509459</td>\n",
       "      <td>0.510135</td>\n",
       "      <td>0.509470</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.670175</td>\n",
       "      <td>0.697917</td>\n",
       "      <td>0.655934</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.624373</td>\n",
       "      <td>0.664474</td>\n",
       "      <td>0.610524</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.610958</td>\n",
       "      <td>0.647575</td>\n",
       "      <td>0.599168</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.584127</td>\n",
       "      <td>0.613777</td>\n",
       "      <td>0.576456</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.662327</td>\n",
       "      <td>0.726365</td>\n",
       "      <td>0.640517</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.480519</td>\n",
       "      <td>0.481250</td>\n",
       "      <td>0.488417</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.566502</td>\n",
       "      <td>0.564840</td>\n",
       "      <td>0.568533</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.566502</td>\n",
       "      <td>0.564840</td>\n",
       "      <td>0.568533</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.417219</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.425676</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.503812</td>\n",
       "      <td>0.521906</td>\n",
       "      <td>0.512626</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.439901</td>\n",
       "      <td>0.432026</td>\n",
       "      <td>0.467172</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.553363</td>\n",
       "      <td>0.572321</td>\n",
       "      <td>0.551136</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.490943</td>\n",
       "      <td>0.498101</td>\n",
       "      <td>0.498737</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.617598</td>\n",
       "      <td>0.663043</td>\n",
       "      <td>0.604022</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.581098</td>\n",
       "      <td>0.607489</td>\n",
       "      <td>0.574029</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.620993</td>\n",
       "      <td>0.656954</td>\n",
       "      <td>0.608096</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.614258</td>\n",
       "      <td>0.655088</td>\n",
       "      <td>0.601595</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.565789</td>\n",
       "      <td>0.579487</td>\n",
       "      <td>0.559846</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.532468</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.530888</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.540641</td>\n",
       "      <td>0.542051</td>\n",
       "      <td>0.539575</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.463841</td>\n",
       "      <td>0.461039</td>\n",
       "      <td>0.468147</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.478529</td>\n",
       "      <td>0.493464</td>\n",
       "      <td>0.496843</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.478529</td>\n",
       "      <td>0.493464</td>\n",
       "      <td>0.496843</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.484649</td>\n",
       "      <td>0.488393</td>\n",
       "      <td>0.491793</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.613030</td>\n",
       "      <td>0.638397</td>\n",
       "      <td>0.603535</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.598417</td>\n",
       "      <td>0.669283</td>\n",
       "      <td>0.586945</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.560119</td>\n",
       "      <td>0.585748</td>\n",
       "      <td>0.556172</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.607196</td>\n",
       "      <td>0.653191</td>\n",
       "      <td>0.595094</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.653569</td>\n",
       "      <td>0.690270</td>\n",
       "      <td>0.637309</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.474851</td>\n",
       "      <td>0.473277</td>\n",
       "      <td>0.481660</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.463841</td>\n",
       "      <td>0.461039</td>\n",
       "      <td>0.468147</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.565789</td>\n",
       "      <td>0.579487</td>\n",
       "      <td>0.559846</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.616493</td>\n",
       "      <td>0.733735</td>\n",
       "      <td>0.593629</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.579381</td>\n",
       "      <td>0.598734</td>\n",
       "      <td>0.573864</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.553656</td>\n",
       "      <td>0.769444</td>\n",
       "      <td>0.561237</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.605042</td>\n",
       "      <td>0.652439</td>\n",
       "      <td>0.594697</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.505263</td>\n",
       "      <td>0.509615</td>\n",
       "      <td>0.507576</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.570712</td>\n",
       "      <td>0.596878</td>\n",
       "      <td>0.565101</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.601277</td>\n",
       "      <td>0.627327</td>\n",
       "      <td>0.591886</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.576626</td>\n",
       "      <td>0.609647</td>\n",
       "      <td>0.569955</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.557296</td>\n",
       "      <td>0.579979</td>\n",
       "      <td>0.553745</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.498491</td>\n",
       "      <td>0.521687</td>\n",
       "      <td>0.508687</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.548718</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.546332</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.432258</td>\n",
       "      <td>0.413580</td>\n",
       "      <td>0.452703</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.517544</td>\n",
       "      <td>0.523077</td>\n",
       "      <td>0.517375</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.548077</td>\n",
       "      <td>0.592857</td>\n",
       "      <td>0.549242</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.553363</td>\n",
       "      <td>0.572321</td>\n",
       "      <td>0.551136</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.563713</td>\n",
       "      <td>0.645349</td>\n",
       "      <td>0.563131</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.646680</td>\n",
       "      <td>0.678059</td>\n",
       "      <td>0.633207</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.577685</td>\n",
       "      <td>0.628605</td>\n",
       "      <td>0.570735</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.647590</td>\n",
       "      <td>0.689931</td>\n",
       "      <td>0.630808</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.627423</td>\n",
       "      <td>0.658799</td>\n",
       "      <td>0.614598</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.496196</td>\n",
       "      <td>0.514515</td>\n",
       "      <td>0.507455</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.472973</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.472973</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.472973</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.472973</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.571228</td>\n",
       "      <td>0.584936</td>\n",
       "      <td>0.566919</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.571228</td>\n",
       "      <td>0.584936</td>\n",
       "      <td>0.566919</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.571228</td>\n",
       "      <td>0.584936</td>\n",
       "      <td>0.566919</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.571228</td>\n",
       "      <td>0.584936</td>\n",
       "      <td>0.566919</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.588204</td>\n",
       "      <td>0.611592</td>\n",
       "      <td>0.580531</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.588204</td>\n",
       "      <td>0.611592</td>\n",
       "      <td>0.580531</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.588204</td>\n",
       "      <td>0.611592</td>\n",
       "      <td>0.580531</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.588204</td>\n",
       "      <td>0.611592</td>\n",
       "      <td>0.580531</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.480519</td>\n",
       "      <td>0.481250</td>\n",
       "      <td>0.488417</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.548718</td>\n",
       "      <td>0.593496</td>\n",
       "      <td>0.544402</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.532468</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.530888</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.479730</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.545731</td>\n",
       "      <td>0.559072</td>\n",
       "      <td>0.544192</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.503812</td>\n",
       "      <td>0.521906</td>\n",
       "      <td>0.512626</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.503812</td>\n",
       "      <td>0.521906</td>\n",
       "      <td>0.512626</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.484649</td>\n",
       "      <td>0.488393</td>\n",
       "      <td>0.491793</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.546264</td>\n",
       "      <td>0.559680</td>\n",
       "      <td>0.544036</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.546584</td>\n",
       "      <td>0.568413</td>\n",
       "      <td>0.544816</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.549312</td>\n",
       "      <td>0.574049</td>\n",
       "      <td>0.547243</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.527904</td>\n",
       "      <td>0.541536</td>\n",
       "      <td>0.528606</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.498491</td>\n",
       "      <td>0.521687</td>\n",
       "      <td>0.508687</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.594470</td>\n",
       "      <td>0.646384</td>\n",
       "      <td>0.580116</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.424837</td>\n",
       "      <td>0.411392</td>\n",
       "      <td>0.439189</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.648541</td>\n",
       "      <td>0.723986</td>\n",
       "      <td>0.622587</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.623397</td>\n",
       "      <td>0.704762</td>\n",
       "      <td>0.608586</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.596265</td>\n",
       "      <td>0.632004</td>\n",
       "      <td>0.587753</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.627947</td>\n",
       "      <td>0.644385</td>\n",
       "      <td>0.619318</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.614076</td>\n",
       "      <td>0.676342</td>\n",
       "      <td>0.601641</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.546584</td>\n",
       "      <td>0.568413</td>\n",
       "      <td>0.544816</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.585743</td>\n",
       "      <td>0.631863</td>\n",
       "      <td>0.577236</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.540005</td>\n",
       "      <td>0.580844</td>\n",
       "      <td>0.541522</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.599786</td>\n",
       "      <td>0.651251</td>\n",
       "      <td>0.588592</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.498491</td>\n",
       "      <td>0.521687</td>\n",
       "      <td>0.508687</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.496893</td>\n",
       "      <td>0.496923</td>\n",
       "      <td>0.497104</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.531121</td>\n",
       "      <td>0.605090</td>\n",
       "      <td>0.540404</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.538246</td>\n",
       "      <td>0.547276</td>\n",
       "      <td>0.537247</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.541010</td>\n",
       "      <td>0.670837</td>\n",
       "      <td>0.547937</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.594396</td>\n",
       "      <td>0.623977</td>\n",
       "      <td>0.585385</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.486329</td>\n",
       "      <td>0.491182</td>\n",
       "      <td>0.495174</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.540399</td>\n",
       "      <td>0.568783</td>\n",
       "      <td>0.537645</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.498491</td>\n",
       "      <td>0.521687</td>\n",
       "      <td>0.508687</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.424837</td>\n",
       "      <td>0.411392</td>\n",
       "      <td>0.439189</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.613030</td>\n",
       "      <td>0.638397</td>\n",
       "      <td>0.603535</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.490349</td>\n",
       "      <td>0.527915</td>\n",
       "      <td>0.510732</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.461421</td>\n",
       "      <td>0.461382</td>\n",
       "      <td>0.476010</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.594411</td>\n",
       "      <td>0.677778</td>\n",
       "      <td>0.585859</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.603030</td>\n",
       "      <td>0.660256</td>\n",
       "      <td>0.591019</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.613651</td>\n",
       "      <td>0.625858</td>\n",
       "      <td>0.606536</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.603941</td>\n",
       "      <td>0.645245</td>\n",
       "      <td>0.592666</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.607852</td>\n",
       "      <td>0.630505</td>\n",
       "      <td>0.598388</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.486329</td>\n",
       "      <td>0.491182</td>\n",
       "      <td>0.495174</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.524865</td>\n",
       "      <td>0.535162</td>\n",
       "      <td>0.524131</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.472973</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.566807</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.557915</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.656433</td>\n",
       "      <td>0.698214</td>\n",
       "      <td>0.640152</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.563713</td>\n",
       "      <td>0.645349</td>\n",
       "      <td>0.563131</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.526050</td>\n",
       "      <td>0.542735</td>\n",
       "      <td>0.528409</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.676853</td>\n",
       "      <td>0.747967</td>\n",
       "      <td>0.654040</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.587188</td>\n",
       "      <td>0.620418</td>\n",
       "      <td>0.578883</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.651203</td>\n",
       "      <td>0.698271</td>\n",
       "      <td>0.633235</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.603030</td>\n",
       "      <td>0.660256</td>\n",
       "      <td>0.591019</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.560498</td>\n",
       "      <td>0.600275</td>\n",
       "      <td>0.556952</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.498491</td>\n",
       "      <td>0.521687</td>\n",
       "      <td>0.508687</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.566807</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.557915</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.540399</td>\n",
       "      <td>0.568783</td>\n",
       "      <td>0.537645</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.480519</td>\n",
       "      <td>0.481250</td>\n",
       "      <td>0.488417</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.640947</td>\n",
       "      <td>0.700203</td>\n",
       "      <td>0.624369</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.548077</td>\n",
       "      <td>0.592857</td>\n",
       "      <td>0.549242</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.569137</td>\n",
       "      <td>0.604675</td>\n",
       "      <td>0.565025</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.712758</td>\n",
       "      <td>0.795732</td>\n",
       "      <td>0.683712</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.607695</td>\n",
       "      <td>0.640466</td>\n",
       "      <td>0.596741</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.640496</td>\n",
       "      <td>0.674535</td>\n",
       "      <td>0.625954</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.620979</td>\n",
       "      <td>0.671484</td>\n",
       "      <td>0.606449</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.653569</td>\n",
       "      <td>0.690270</td>\n",
       "      <td>0.637309</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                agg  \\\n",
       "0                                                                                                                              mean   \n",
       "1                                                                                                                               max   \n",
       "2                                                                                                                               min   \n",
       "3                                                                                                                     weighted_mean   \n",
       "4                                                                                                                              mean   \n",
       "5                                                                                                                               max   \n",
       "6                                                                                                                               min   \n",
       "7                                                                                                                     weighted_mean   \n",
       "8                                                                                                                              mean   \n",
       "9                                                                                                                               max   \n",
       "10                                                                                                                              min   \n",
       "11                                                                                                                    weighted_mean   \n",
       "12                                                                                                                             mean   \n",
       "13                                                                                                                              max   \n",
       "14                                                                                                                              min   \n",
       "15                                                                                                                    weighted_mean   \n",
       "16                                                                                                                             mean   \n",
       "17                                                                                                                              max   \n",
       "18                                                                                                                              min   \n",
       "19                                                                                                                    weighted_mean   \n",
       "20                                                                                                                             mean   \n",
       "21                                                                                                                              max   \n",
       "22                                                                                                                              min   \n",
       "23                                                                                                                    weighted_mean   \n",
       "24                                                                                                                             mean   \n",
       "25                                                                                                                              max   \n",
       "26                                                                                                                              min   \n",
       "27                                                                                                                    weighted_mean   \n",
       "28                                                                                                                             mean   \n",
       "29                                                                                                                              max   \n",
       "30                                                                                                                              min   \n",
       "31                                                                                                                    weighted_mean   \n",
       "32                                                                                                                             mean   \n",
       "33                                                                                                                              max   \n",
       "34                                                                                                                              min   \n",
       "35                                                                                                                    weighted_mean   \n",
       "36                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "37                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "38                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "39                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "40                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "41                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "42                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "43                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "44                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "45                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "46                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "47                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "48                                                                                                                             mean   \n",
       "49                                                                                                                              max   \n",
       "50                                                                                                                              min   \n",
       "51                                                                                                                    weighted_mean   \n",
       "52                                                                                                                             mean   \n",
       "53                                                                                                                              max   \n",
       "54                                                                                                                              min   \n",
       "55                                                                                                                    weighted_mean   \n",
       "56                                                                                                                             mean   \n",
       "57                                                                                                                              max   \n",
       "58                                                                                                                              min   \n",
       "59                                                                                                                    weighted_mean   \n",
       "60                                                                                                                             mean   \n",
       "61                                                                                                                              max   \n",
       "62                                                                                                                              min   \n",
       "63                                                                                                                    weighted_mean   \n",
       "64                                                                                                                             mean   \n",
       "65                                                                                                                              max   \n",
       "66                                                                                                                              min   \n",
       "67                                                                                                                    weighted_mean   \n",
       "68                                                                                                                             mean   \n",
       "69                                                                                                                              max   \n",
       "70                                                                                                                              min   \n",
       "71                                                                                                                    weighted_mean   \n",
       "72                                                                                                                             mean   \n",
       "73                                                                                                                              max   \n",
       "74                                                                                                                              min   \n",
       "75                                                                                                                    weighted_mean   \n",
       "76                                                                                                                             mean   \n",
       "77                                                                                                                              max   \n",
       "78                                                                                                                              min   \n",
       "79                                                                                                                    weighted_mean   \n",
       "80                                                                                                                             mean   \n",
       "81                                                                                                                              max   \n",
       "82                                                                                                                              min   \n",
       "83                                                                                                                    weighted_mean   \n",
       "84                                                                                                                             mean   \n",
       "85                                                                                                                              max   \n",
       "86                                                                                                                              min   \n",
       "87                                                                                                                    weighted_mean   \n",
       "88                                                                                                                             mean   \n",
       "89                                                                                                                              max   \n",
       "90                                                                                                                              min   \n",
       "91                                                                                                                    weighted_mean   \n",
       "92                                                                                                                             mean   \n",
       "93                                                                                                                              max   \n",
       "94                                                                                                                              min   \n",
       "95                                                                                                                    weighted_mean   \n",
       "96                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "97                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "98                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "99                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "100                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "101                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "102                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "103                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "104                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "105                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "106                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "107                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "108                                                                                                                            mean   \n",
       "109                                                                                                                             max   \n",
       "110                                                                                                                             min   \n",
       "111                                                                                                                   weighted_mean   \n",
       "112                                                                                                                            mean   \n",
       "113                                                                                                                             max   \n",
       "114                                                                                                                             min   \n",
       "115                                                                                                                   weighted_mean   \n",
       "116                                                                                                                            mean   \n",
       "117                                                                                                                             max   \n",
       "118                                                                                                                             min   \n",
       "119                                                                                                                   weighted_mean   \n",
       "120  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "121            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "122            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "123        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "124  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "125            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "126            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "127        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "128  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "129            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "130            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "131        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "132                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "133                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "134                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "135                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "136                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "137                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "138                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "139                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "140                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "141                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "142                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "143                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "144                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "145                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "146                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "147                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "148                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "149                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "150                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "151                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "152                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "153                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "154                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "155                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "\n",
       "       dataset        f1      prec       rec                        zc  \n",
       "0    Wikipedia  0.474851  0.473277  0.481660                linguistic  \n",
       "1    Wikipedia  0.574879  0.597046  0.566602                linguistic  \n",
       "2    Wikipedia  0.439490  0.415663  0.466216                linguistic  \n",
       "3    Wikipedia  0.540399  0.568783  0.537645                linguistic  \n",
       "4     WikiNews  0.571228  0.584936  0.566919                linguistic  \n",
       "5     WikiNews  0.646680  0.678059  0.633207                linguistic  \n",
       "6     WikiNews  0.533231  0.556911  0.535354                linguistic  \n",
       "7     WikiNews  0.680329  0.717722  0.662879                linguistic  \n",
       "8         News  0.577685  0.628605  0.570735                linguistic  \n",
       "9         News  0.634292  0.673442  0.619452                linguistic  \n",
       "10        News  0.566273  0.616199  0.561807                linguistic  \n",
       "11        News  0.669999  0.748187  0.645371                linguistic  \n",
       "12   Wikipedia  0.486329  0.491182  0.495174                 frequency  \n",
       "13   Wikipedia  0.463841  0.461039  0.468147                 frequency  \n",
       "14   Wikipedia  0.503590  0.504386  0.503861                 frequency  \n",
       "15   Wikipedia  0.503590  0.504386  0.503861                 frequency  \n",
       "16    WikiNews  0.563713  0.645349  0.563131                 frequency  \n",
       "17    WikiNews  0.490349  0.527915  0.510732                 frequency  \n",
       "18    WikiNews  0.509459  0.510135  0.509470                 frequency  \n",
       "19    WikiNews  0.670175  0.697917  0.655934                 frequency  \n",
       "20        News  0.624373  0.664474  0.610524                 frequency  \n",
       "21        News  0.610958  0.647575  0.599168                 frequency  \n",
       "22        News  0.584127  0.613777  0.576456                 frequency  \n",
       "23        News  0.662327  0.726365  0.640517                 frequency  \n",
       "24   Wikipedia  0.480519  0.481250  0.488417            language_model  \n",
       "25   Wikipedia  0.566502  0.564840  0.568533            language_model  \n",
       "26   Wikipedia  0.566502  0.564840  0.568533            language_model  \n",
       "27   Wikipedia  0.417219  0.409091  0.425676            language_model  \n",
       "28    WikiNews  0.503812  0.521906  0.512626            language_model  \n",
       "29    WikiNews  0.439901  0.432026  0.467172            language_model  \n",
       "30    WikiNews  0.553363  0.572321  0.551136            language_model  \n",
       "31    WikiNews  0.490943  0.498101  0.498737            language_model  \n",
       "32        News  0.617598  0.663043  0.604022            language_model  \n",
       "33        News  0.581098  0.607489  0.574029            language_model  \n",
       "34        News  0.620993  0.656954  0.608096            language_model  \n",
       "35        News  0.614258  0.655088  0.601595            language_model  \n",
       "36   Wikipedia  0.565789  0.579487  0.559846                    corpus  \n",
       "37   Wikipedia  0.532468  0.550000  0.530888                    corpus  \n",
       "38   Wikipedia  0.540641  0.542051  0.539575                    corpus  \n",
       "39   Wikipedia  0.463841  0.461039  0.468147                    corpus  \n",
       "40    WikiNews  0.478529  0.493464  0.496843                    corpus  \n",
       "41    WikiNews  0.478529  0.493464  0.496843                    corpus  \n",
       "42    WikiNews  0.484649  0.488393  0.491793                    corpus  \n",
       "43    WikiNews  0.613030  0.638397  0.603535                    corpus  \n",
       "44        News  0.598417  0.669283  0.586945                    corpus  \n",
       "45        News  0.560119  0.585748  0.556172                    corpus  \n",
       "46        News  0.607196  0.653191  0.595094                    corpus  \n",
       "47        News  0.653569  0.690270  0.637309                    corpus  \n",
       "48   Wikipedia  0.474851  0.473277  0.481660          psycholinguistic  \n",
       "49   Wikipedia  0.463841  0.461039  0.468147          psycholinguistic  \n",
       "50   Wikipedia  0.565789  0.579487  0.559846          psycholinguistic  \n",
       "51   Wikipedia  0.616493  0.733735  0.593629          psycholinguistic  \n",
       "52    WikiNews  0.579381  0.598734  0.573864          psycholinguistic  \n",
       "53    WikiNews  0.553656  0.769444  0.561237          psycholinguistic  \n",
       "54    WikiNews  0.605042  0.652439  0.594697          psycholinguistic  \n",
       "55    WikiNews  0.505263  0.509615  0.507576          psycholinguistic  \n",
       "56        News  0.570712  0.596878  0.565101          psycholinguistic  \n",
       "57        News  0.601277  0.627327  0.591886          psycholinguistic  \n",
       "58        News  0.576626  0.609647  0.569955          psycholinguistic  \n",
       "59        News  0.557296  0.579979  0.553745          psycholinguistic  \n",
       "60   Wikipedia  0.498491  0.521687  0.508687                   wordnet  \n",
       "61   Wikipedia  0.548718  0.552632  0.546332                   wordnet  \n",
       "62   Wikipedia  0.432258  0.413580  0.452703                   wordnet  \n",
       "63   Wikipedia  0.517544  0.523077  0.517375                   wordnet  \n",
       "64    WikiNews  0.548077  0.592857  0.549242                   wordnet  \n",
       "65    WikiNews  0.553363  0.572321  0.551136                   wordnet  \n",
       "66    WikiNews  0.563713  0.645349  0.563131                   wordnet  \n",
       "67    WikiNews  0.646680  0.678059  0.633207                   wordnet  \n",
       "68        News  0.577685  0.628605  0.570735                   wordnet  \n",
       "69        News  0.647590  0.689931  0.630808                   wordnet  \n",
       "70        News  0.627423  0.658799  0.614598                   wordnet  \n",
       "71        News  0.496196  0.514515  0.507455                   wordnet  \n",
       "72   Wikipedia  0.443038  0.416667  0.472973                   dbpedia  \n",
       "73   Wikipedia  0.443038  0.416667  0.472973                   dbpedia  \n",
       "74   Wikipedia  0.443038  0.416667  0.472973                   dbpedia  \n",
       "75   Wikipedia  0.443038  0.416667  0.472973                   dbpedia  \n",
       "76    WikiNews  0.571228  0.584936  0.566919                   dbpedia  \n",
       "77    WikiNews  0.571228  0.584936  0.566919                   dbpedia  \n",
       "78    WikiNews  0.571228  0.584936  0.566919                   dbpedia  \n",
       "79    WikiNews  0.571228  0.584936  0.566919                   dbpedia  \n",
       "80        News  0.588204  0.611592  0.580531                   dbpedia  \n",
       "81        News  0.588204  0.611592  0.580531                   dbpedia  \n",
       "82        News  0.588204  0.611592  0.580531                   dbpedia  \n",
       "83        News  0.588204  0.611592  0.580531                   dbpedia  \n",
       "84   Wikipedia  0.480519  0.481250  0.488417          brown_clustering  \n",
       "85   Wikipedia  0.548718  0.593496  0.544402          brown_clustering  \n",
       "86   Wikipedia  0.532468  0.550000  0.530888          brown_clustering  \n",
       "87   Wikipedia  0.446541  0.417647  0.479730          brown_clustering  \n",
       "88    WikiNews  0.545731  0.559072  0.544192          brown_clustering  \n",
       "89    WikiNews  0.503812  0.521906  0.512626          brown_clustering  \n",
       "90    WikiNews  0.503812  0.521906  0.512626          brown_clustering  \n",
       "91    WikiNews  0.484649  0.488393  0.491793          brown_clustering  \n",
       "92        News  0.546264  0.559680  0.544036          brown_clustering  \n",
       "93        News  0.546584  0.568413  0.544816          brown_clustering  \n",
       "94        News  0.549312  0.574049  0.547243          brown_clustering  \n",
       "95        News  0.527904  0.541536  0.528606          brown_clustering  \n",
       "96   Wikipedia  0.498491  0.521687  0.508687                  semantic  \n",
       "97   Wikipedia  0.594470  0.646384  0.580116                  semantic  \n",
       "98   Wikipedia  0.424837  0.411392  0.439189                  semantic  \n",
       "99   Wikipedia  0.648541  0.723986  0.622587                  semantic  \n",
       "100   WikiNews  0.623397  0.704762  0.608586                  semantic  \n",
       "101   WikiNews  0.596265  0.632004  0.587753                  semantic  \n",
       "102   WikiNews  0.627947  0.644385  0.619318                  semantic  \n",
       "103   WikiNews  0.614076  0.676342  0.601641                  semantic  \n",
       "104       News  0.546584  0.568413  0.544816                  semantic  \n",
       "105       News  0.585743  0.631863  0.577236                  semantic  \n",
       "106       News  0.540005  0.580844  0.541522                  semantic  \n",
       "107       News  0.599786  0.651251  0.588592                  semantic  \n",
       "108  Wikipedia  0.498491  0.521687  0.508687                dictionary  \n",
       "109  Wikipedia  0.456790  0.420455  0.500000                dictionary  \n",
       "110  Wikipedia  0.456790  0.420455  0.500000                dictionary  \n",
       "111  Wikipedia  0.496893  0.496923  0.497104                dictionary  \n",
       "112   WikiNews  0.531121  0.605090  0.540404                dictionary  \n",
       "113   WikiNews  0.433735  0.382979  0.500000                dictionary  \n",
       "114   WikiNews  0.433735  0.382979  0.500000                dictionary  \n",
       "115   WikiNews  0.538246  0.547276  0.537247                dictionary  \n",
       "116       News  0.541010  0.670837  0.547937                dictionary  \n",
       "117       News  0.440171  0.393130  0.500000                dictionary  \n",
       "118       News  0.440171  0.393130  0.500000                dictionary  \n",
       "119       News  0.594396  0.623977  0.585385                dictionary  \n",
       "120  Wikipedia  0.486329  0.491182  0.495174           corpus+semantic  \n",
       "121  Wikipedia  0.540399  0.568783  0.537645           corpus+semantic  \n",
       "122  Wikipedia  0.498491  0.521687  0.508687           corpus+semantic  \n",
       "123  Wikipedia  0.424837  0.411392  0.439189           corpus+semantic  \n",
       "124   WikiNews  0.613030  0.638397  0.603535           corpus+semantic  \n",
       "125   WikiNews  0.490349  0.527915  0.510732           corpus+semantic  \n",
       "126   WikiNews  0.461421  0.461382  0.476010           corpus+semantic  \n",
       "127   WikiNews  0.594411  0.677778  0.585859           corpus+semantic  \n",
       "128       News  0.603030  0.660256  0.591019           corpus+semantic  \n",
       "129       News  0.613651  0.625858  0.606536           corpus+semantic  \n",
       "130       News  0.603941  0.645245  0.592666           corpus+semantic  \n",
       "131       News  0.607852  0.630505  0.598388           corpus+semantic  \n",
       "132  Wikipedia  0.486329  0.491182  0.495174  wordnet+psycholinguistic  \n",
       "133  Wikipedia  0.524865  0.535162  0.524131  wordnet+psycholinguistic  \n",
       "134  Wikipedia  0.443038  0.416667  0.472973  wordnet+psycholinguistic  \n",
       "135  Wikipedia  0.566807  0.678571  0.557915  wordnet+psycholinguistic  \n",
       "136   WikiNews  0.656433  0.698214  0.640152  wordnet+psycholinguistic  \n",
       "137   WikiNews  0.563713  0.645349  0.563131  wordnet+psycholinguistic  \n",
       "138   WikiNews  0.526050  0.542735  0.528409  wordnet+psycholinguistic  \n",
       "139   WikiNews  0.676853  0.747967  0.654040  wordnet+psycholinguistic  \n",
       "140       News  0.587188  0.620418  0.578883  wordnet+psycholinguistic  \n",
       "141       News  0.651203  0.698271  0.633235  wordnet+psycholinguistic  \n",
       "142       News  0.603030  0.660256  0.591019  wordnet+psycholinguistic  \n",
       "143       News  0.560498  0.600275  0.556952  wordnet+psycholinguistic  \n",
       "144  Wikipedia  0.498491  0.521687  0.508687                       all  \n",
       "145  Wikipedia  0.566807  0.678571  0.557915                       all  \n",
       "146  Wikipedia  0.540399  0.568783  0.537645                       all  \n",
       "147  Wikipedia  0.480519  0.481250  0.488417                       all  \n",
       "148   WikiNews  0.640947  0.700203  0.624369                       all  \n",
       "149   WikiNews  0.548077  0.592857  0.549242                       all  \n",
       "150   WikiNews  0.569137  0.604675  0.565025                       all  \n",
       "151   WikiNews  0.712758  0.795732  0.683712                       all  \n",
       "152       News  0.607695  0.640466  0.596741                       all  \n",
       "153       News  0.640496  0.674535  0.625954                       all  \n",
       "154       News  0.620979  0.671484  0.606449                       all  \n",
       "155       News  0.653569  0.690270  0.637309                       all  "
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_eval_data_xg = create_eval_df_from_results_macro(results, False)\n",
    "feature_eval_data_xg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.669999</td>\n",
       "      <td>0.748187</td>\n",
       "      <td>0.645371</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.648541</td>\n",
       "      <td>0.723986</td>\n",
       "      <td>0.622587</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.712758</td>\n",
       "      <td>0.795732</td>\n",
       "      <td>0.683712</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            agg    dataset  \\\n",
       "11                                                weighted_mean       News   \n",
       "99   (weighted_mean, <function <lambda> at 0x000000FF52232E18>)  Wikipedia   \n",
       "151  (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   WikiNews   \n",
       "\n",
       "           f1      prec       rec          zc  \n",
       "11   0.669999  0.748187  0.645371  linguistic  \n",
       "99   0.648541  0.723986  0.622587    semantic  \n",
       "151  0.712758  0.795732  0.683712         all  "
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = feature_eval_data_xg.groupby(['dataset'])['f1'].transform(max) == feature_eval_data_xg['f1']\n",
    "feature_eval_data_xg[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-error:0.295455\ttrain-error:0.115655\n",
      "[1]\teval-error:0.25\ttrain-error:0.062151\n",
      "[2]\teval-error:0.227273\ttrain-error:0.026842\n",
      "[3]\teval-error:0.238636\ttrain-error:0.008647\n",
      "[4]\teval-error:0.227273\ttrain-error:0.002882\n",
      "[5]\teval-error:0.215909\ttrain-error:0.001081\n",
      "[6]\teval-error:0.215909\ttrain-error:0.00036\n",
      "[7]\teval-error:0.227273\ttrain-error:0\n",
      "[8]\teval-error:0.215909\ttrain-error:0\n",
      "[9]\teval-error:0.215909\ttrain-error:0\n",
      "[10]\teval-error:0.238636\ttrain-error:0\n",
      "[11]\teval-error:0.238636\ttrain-error:0\n",
      "[12]\teval-error:0.215909\ttrain-error:0\n",
      "[13]\teval-error:0.238636\ttrain-error:0\n",
      "[14]\teval-error:0.25\ttrain-error:0\n",
      "[15]\teval-error:0.227273\ttrain-error:0\n",
      "[16]\teval-error:0.227273\ttrain-error:0\n",
      "[17]\teval-error:0.238636\ttrain-error:0\n",
      "[18]\teval-error:0.193182\ttrain-error:0\n",
      "[19]\teval-error:0.215909\ttrain-error:0\n",
      "[20]\teval-error:0.215909\ttrain-error:0\n",
      "[21]\teval-error:0.227273\ttrain-error:0\n",
      "[22]\teval-error:0.215909\ttrain-error:0\n",
      "[23]\teval-error:0.215909\ttrain-error:0\n",
      "[24]\teval-error:0.204545\ttrain-error:0\n",
      "[25]\teval-error:0.215909\ttrain-error:0\n",
      "[26]\teval-error:0.215909\ttrain-error:0\n",
      "[27]\teval-error:0.215909\ttrain-error:0\n",
      "[28]\teval-error:0.227273\ttrain-error:0\n",
      "[29]\teval-error:0.204545\ttrain-error:0\n",
      "[30]\teval-error:0.204545\ttrain-error:0\n",
      "[31]\teval-error:0.204545\ttrain-error:0\n",
      "[32]\teval-error:0.204545\ttrain-error:0\n",
      "[33]\teval-error:0.193182\ttrain-error:0\n",
      "[34]\teval-error:0.204545\ttrain-error:0\n",
      "[35]\teval-error:0.204545\ttrain-error:0\n",
      "[36]\teval-error:0.227273\ttrain-error:0\n",
      "[37]\teval-error:0.227273\ttrain-error:0\n",
      "[38]\teval-error:0.227273\ttrain-error:0\n",
      "[39]\teval-error:0.227273\ttrain-error:0\n",
      "[40]\teval-error:0.227273\ttrain-error:0\n",
      "[41]\teval-error:0.227273\ttrain-error:0\n",
      "[42]\teval-error:0.227273\ttrain-error:0\n",
      "[43]\teval-error:0.227273\ttrain-error:0\n",
      "[44]\teval-error:0.227273\ttrain-error:0\n",
      "[45]\teval-error:0.227273\ttrain-error:0\n",
      "[46]\teval-error:0.227273\ttrain-error:0\n",
      "[47]\teval-error:0.227273\ttrain-error:0\n",
      "[48]\teval-error:0.227273\ttrain-error:0\n",
      "[49]\teval-error:0.227273\ttrain-error:0\n",
      "[50]\teval-error:0.215909\ttrain-error:0\n",
      "[51]\teval-error:0.215909\ttrain-error:0\n",
      "[52]\teval-error:0.215909\ttrain-error:0\n",
      "[53]\teval-error:0.215909\ttrain-error:0\n",
      "[54]\teval-error:0.215909\ttrain-error:0\n",
      "[55]\teval-error:0.227273\ttrain-error:0\n",
      "[56]\teval-error:0.215909\ttrain-error:0\n",
      "[57]\teval-error:0.227273\ttrain-error:0\n",
      "[58]\teval-error:0.215909\ttrain-error:0\n",
      "[59]\teval-error:0.215909\ttrain-error:0\n",
      "[60]\teval-error:0.215909\ttrain-error:0\n",
      "[61]\teval-error:0.204545\ttrain-error:0\n",
      "[62]\teval-error:0.204545\ttrain-error:0\n",
      "[63]\teval-error:0.193182\ttrain-error:0\n",
      "[64]\teval-error:0.193182\ttrain-error:0\n",
      "[65]\teval-error:0.193182\ttrain-error:0\n",
      "[66]\teval-error:0.193182\ttrain-error:0\n",
      "[67]\teval-error:0.204545\ttrain-error:0\n",
      "[68]\teval-error:0.204545\ttrain-error:0\n",
      "[69]\teval-error:0.204545\ttrain-error:0\n",
      "[0]\teval-error:0.25\ttrain-error:0.12214\n",
      "[1]\teval-error:0.25\ttrain-error:0.065394\n",
      "[2]\teval-error:0.238636\ttrain-error:0.028824\n",
      "[3]\teval-error:0.25\ttrain-error:0.010268\n",
      "[4]\teval-error:0.227273\ttrain-error:0.001261\n",
      "[5]\teval-error:0.25\ttrain-error:0.00054\n",
      "[6]\teval-error:0.227273\ttrain-error:0\n",
      "[7]\teval-error:0.215909\ttrain-error:0\n",
      "[8]\teval-error:0.215909\ttrain-error:0\n",
      "[9]\teval-error:0.215909\ttrain-error:0\n",
      "[10]\teval-error:0.215909\ttrain-error:0\n",
      "[11]\teval-error:0.215909\ttrain-error:0\n",
      "[12]\teval-error:0.204545\ttrain-error:0\n",
      "[13]\teval-error:0.204545\ttrain-error:0\n",
      "[14]\teval-error:0.215909\ttrain-error:0\n",
      "[15]\teval-error:0.227273\ttrain-error:0\n",
      "[16]\teval-error:0.238636\ttrain-error:0\n",
      "[17]\teval-error:0.238636\ttrain-error:0\n",
      "[18]\teval-error:0.227273\ttrain-error:0\n",
      "[19]\teval-error:0.227273\ttrain-error:0\n",
      "[20]\teval-error:0.227273\ttrain-error:0\n",
      "[21]\teval-error:0.238636\ttrain-error:0\n",
      "[22]\teval-error:0.227273\ttrain-error:0\n",
      "[23]\teval-error:0.227273\ttrain-error:0\n",
      "[24]\teval-error:0.215909\ttrain-error:0\n",
      "[25]\teval-error:0.215909\ttrain-error:0\n",
      "[26]\teval-error:0.215909\ttrain-error:0\n",
      "[27]\teval-error:0.227273\ttrain-error:0\n",
      "[28]\teval-error:0.227273\ttrain-error:0\n",
      "[29]\teval-error:0.227273\ttrain-error:0\n",
      "[30]\teval-error:0.227273\ttrain-error:0\n",
      "[31]\teval-error:0.227273\ttrain-error:0\n",
      "[32]\teval-error:0.238636\ttrain-error:0\n",
      "[33]\teval-error:0.227273\ttrain-error:0\n",
      "[34]\teval-error:0.227273\ttrain-error:0\n",
      "[35]\teval-error:0.227273\ttrain-error:0\n",
      "[36]\teval-error:0.227273\ttrain-error:0\n",
      "[37]\teval-error:0.227273\ttrain-error:0\n",
      "[38]\teval-error:0.227273\ttrain-error:0\n",
      "[39]\teval-error:0.215909\ttrain-error:0\n",
      "[40]\teval-error:0.215909\ttrain-error:0\n",
      "[41]\teval-error:0.215909\ttrain-error:0\n",
      "[42]\teval-error:0.215909\ttrain-error:0\n",
      "[43]\teval-error:0.215909\ttrain-error:0\n",
      "[44]\teval-error:0.215909\ttrain-error:0\n",
      "[45]\teval-error:0.215909\ttrain-error:0\n",
      "[46]\teval-error:0.227273\ttrain-error:0\n",
      "[47]\teval-error:0.215909\ttrain-error:0\n",
      "[48]\teval-error:0.227273\ttrain-error:0\n",
      "[49]\teval-error:0.227273\ttrain-error:0\n",
      "[50]\teval-error:0.215909\ttrain-error:0\n",
      "[51]\teval-error:0.238636\ttrain-error:0\n",
      "[52]\teval-error:0.227273\ttrain-error:0\n",
      "[53]\teval-error:0.215909\ttrain-error:0\n",
      "[54]\teval-error:0.227273\ttrain-error:0\n",
      "[55]\teval-error:0.227273\ttrain-error:0\n",
      "[56]\teval-error:0.227273\ttrain-error:0\n",
      "[57]\teval-error:0.227273\ttrain-error:0\n",
      "[58]\teval-error:0.227273\ttrain-error:0\n",
      "[59]\teval-error:0.227273\ttrain-error:0\n",
      "[60]\teval-error:0.227273\ttrain-error:0\n",
      "[61]\teval-error:0.227273\ttrain-error:0\n",
      "[62]\teval-error:0.227273\ttrain-error:0\n",
      "[63]\teval-error:0.227273\ttrain-error:0\n",
      "[64]\teval-error:0.227273\ttrain-error:0\n",
      "[65]\teval-error:0.227273\ttrain-error:0\n",
      "[66]\teval-error:0.227273\ttrain-error:0\n",
      "[67]\teval-error:0.227273\ttrain-error:0\n",
      "[68]\teval-error:0.227273\ttrain-error:0\n",
      "[69]\teval-error:0.227273\ttrain-error:0\n",
      "[0]\teval-error:0.340909\ttrain-error:0.114394\n",
      "[1]\teval-error:0.284091\ttrain-error:0.058548\n",
      "[2]\teval-error:0.25\ttrain-error:0.020537\n",
      "[3]\teval-error:0.25\ttrain-error:0.008647\n",
      "[4]\teval-error:0.261364\ttrain-error:0.002882\n",
      "[5]\teval-error:0.261364\ttrain-error:0.000721\n",
      "[6]\teval-error:0.272727\ttrain-error:0.00018\n",
      "[7]\teval-error:0.261364\ttrain-error:0\n",
      "[8]\teval-error:0.272727\ttrain-error:0\n",
      "[9]\teval-error:0.261364\ttrain-error:0\n",
      "[10]\teval-error:0.261364\ttrain-error:0\n",
      "[11]\teval-error:0.25\ttrain-error:0\n",
      "[12]\teval-error:0.261364\ttrain-error:0\n",
      "[13]\teval-error:0.261364\ttrain-error:0\n",
      "[14]\teval-error:0.25\ttrain-error:0\n",
      "[15]\teval-error:0.261364\ttrain-error:0\n",
      "[16]\teval-error:0.261364\ttrain-error:0\n",
      "[17]\teval-error:0.25\ttrain-error:0\n",
      "[18]\teval-error:0.25\ttrain-error:0\n",
      "[19]\teval-error:0.261364\ttrain-error:0\n",
      "[20]\teval-error:0.261364\ttrain-error:0\n",
      "[21]\teval-error:0.261364\ttrain-error:0\n",
      "[22]\teval-error:0.261364\ttrain-error:0\n",
      "[23]\teval-error:0.261364\ttrain-error:0\n",
      "[24]\teval-error:0.261364\ttrain-error:0\n",
      "[25]\teval-error:0.272727\ttrain-error:0\n",
      "[26]\teval-error:0.272727\ttrain-error:0\n",
      "[27]\teval-error:0.272727\ttrain-error:0\n",
      "[28]\teval-error:0.272727\ttrain-error:0\n",
      "[29]\teval-error:0.272727\ttrain-error:0\n",
      "[30]\teval-error:0.272727\ttrain-error:0\n",
      "[31]\teval-error:0.272727\ttrain-error:0\n",
      "[32]\teval-error:0.272727\ttrain-error:0\n",
      "[33]\teval-error:0.261364\ttrain-error:0\n",
      "[34]\teval-error:0.261364\ttrain-error:0\n",
      "[35]\teval-error:0.25\ttrain-error:0\n",
      "[36]\teval-error:0.25\ttrain-error:0\n",
      "[37]\teval-error:0.25\ttrain-error:0\n",
      "[38]\teval-error:0.261364\ttrain-error:0\n",
      "[39]\teval-error:0.25\ttrain-error:0\n",
      "[40]\teval-error:0.238636\ttrain-error:0\n",
      "[41]\teval-error:0.238636\ttrain-error:0\n",
      "[42]\teval-error:0.238636\ttrain-error:0\n",
      "[43]\teval-error:0.238636\ttrain-error:0\n",
      "[44]\teval-error:0.238636\ttrain-error:0\n",
      "[45]\teval-error:0.238636\ttrain-error:0\n",
      "[46]\teval-error:0.238636\ttrain-error:0\n",
      "[47]\teval-error:0.238636\ttrain-error:0\n",
      "[48]\teval-error:0.25\ttrain-error:0\n",
      "[49]\teval-error:0.25\ttrain-error:0\n",
      "[50]\teval-error:0.25\ttrain-error:0\n",
      "[51]\teval-error:0.25\ttrain-error:0\n",
      "[52]\teval-error:0.25\ttrain-error:0\n",
      "[53]\teval-error:0.25\ttrain-error:0\n",
      "[54]\teval-error:0.25\ttrain-error:0\n",
      "[55]\teval-error:0.25\ttrain-error:0\n",
      "[56]\teval-error:0.25\ttrain-error:0\n",
      "[57]\teval-error:0.25\ttrain-error:0\n",
      "[58]\teval-error:0.25\ttrain-error:0\n",
      "[59]\teval-error:0.25\ttrain-error:0\n",
      "[60]\teval-error:0.25\ttrain-error:0\n",
      "[61]\teval-error:0.25\ttrain-error:0\n",
      "[62]\teval-error:0.25\ttrain-error:0\n",
      "[63]\teval-error:0.238636\ttrain-error:0\n",
      "[64]\teval-error:0.238636\ttrain-error:0\n",
      "[65]\teval-error:0.238636\ttrain-error:0\n",
      "[66]\teval-error:0.238636\ttrain-error:0\n",
      "[67]\teval-error:0.238636\ttrain-error:0\n",
      "[68]\teval-error:0.238636\ttrain-error:0\n",
      "[69]\teval-error:0.25\ttrain-error:0\n",
      "[0]\teval-error:0.363636\ttrain-error:0.115835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\teval-error:0.227273\ttrain-error:0.058368\n",
      "[2]\teval-error:0.25\ttrain-error:0.026482\n",
      "[3]\teval-error:0.227273\ttrain-error:0.008647\n",
      "[4]\teval-error:0.25\ttrain-error:0.002882\n",
      "[5]\teval-error:0.215909\ttrain-error:0.00054\n",
      "[6]\teval-error:0.204545\ttrain-error:0.00018\n",
      "[7]\teval-error:0.193182\ttrain-error:0\n",
      "[8]\teval-error:0.204545\ttrain-error:0\n",
      "[9]\teval-error:0.204545\ttrain-error:0\n",
      "[10]\teval-error:0.204545\ttrain-error:0\n",
      "[11]\teval-error:0.215909\ttrain-error:0\n",
      "[12]\teval-error:0.193182\ttrain-error:0\n",
      "[13]\teval-error:0.204545\ttrain-error:0\n",
      "[14]\teval-error:0.204545\ttrain-error:0\n",
      "[15]\teval-error:0.204545\ttrain-error:0\n",
      "[16]\teval-error:0.204545\ttrain-error:0\n",
      "[17]\teval-error:0.215909\ttrain-error:0\n",
      "[18]\teval-error:0.204545\ttrain-error:0\n",
      "[19]\teval-error:0.215909\ttrain-error:0\n",
      "[20]\teval-error:0.204545\ttrain-error:0\n",
      "[21]\teval-error:0.193182\ttrain-error:0\n",
      "[22]\teval-error:0.193182\ttrain-error:0\n",
      "[23]\teval-error:0.193182\ttrain-error:0\n",
      "[24]\teval-error:0.193182\ttrain-error:0\n",
      "[25]\teval-error:0.193182\ttrain-error:0\n",
      "[26]\teval-error:0.193182\ttrain-error:0\n",
      "[27]\teval-error:0.193182\ttrain-error:0\n",
      "[28]\teval-error:0.193182\ttrain-error:0\n",
      "[29]\teval-error:0.193182\ttrain-error:0\n",
      "[30]\teval-error:0.193182\ttrain-error:0\n",
      "[31]\teval-error:0.193182\ttrain-error:0\n",
      "[32]\teval-error:0.193182\ttrain-error:0\n",
      "[33]\teval-error:0.204545\ttrain-error:0\n",
      "[34]\teval-error:0.204545\ttrain-error:0\n",
      "[35]\teval-error:0.204545\ttrain-error:0\n",
      "[36]\teval-error:0.204545\ttrain-error:0\n",
      "[37]\teval-error:0.193182\ttrain-error:0\n",
      "[38]\teval-error:0.204545\ttrain-error:0\n",
      "[39]\teval-error:0.215909\ttrain-error:0\n",
      "[40]\teval-error:0.204545\ttrain-error:0\n",
      "[41]\teval-error:0.204545\ttrain-error:0\n",
      "[42]\teval-error:0.215909\ttrain-error:0\n",
      "[43]\teval-error:0.193182\ttrain-error:0\n",
      "[44]\teval-error:0.215909\ttrain-error:0\n",
      "[45]\teval-error:0.227273\ttrain-error:0\n",
      "[46]\teval-error:0.215909\ttrain-error:0\n",
      "[47]\teval-error:0.227273\ttrain-error:0\n",
      "[48]\teval-error:0.227273\ttrain-error:0\n",
      "[49]\teval-error:0.215909\ttrain-error:0\n",
      "[50]\teval-error:0.204545\ttrain-error:0\n",
      "[51]\teval-error:0.204545\ttrain-error:0\n",
      "[52]\teval-error:0.215909\ttrain-error:0\n",
      "[53]\teval-error:0.227273\ttrain-error:0\n",
      "[54]\teval-error:0.227273\ttrain-error:0\n",
      "[55]\teval-error:0.215909\ttrain-error:0\n",
      "[56]\teval-error:0.204545\ttrain-error:0\n",
      "[57]\teval-error:0.204545\ttrain-error:0\n",
      "[58]\teval-error:0.204545\ttrain-error:0\n",
      "[59]\teval-error:0.204545\ttrain-error:0\n",
      "[60]\teval-error:0.204545\ttrain-error:0\n",
      "[61]\teval-error:0.215909\ttrain-error:0\n",
      "[62]\teval-error:0.215909\ttrain-error:0\n",
      "[63]\teval-error:0.215909\ttrain-error:0\n",
      "[64]\teval-error:0.215909\ttrain-error:0\n",
      "[65]\teval-error:0.204545\ttrain-error:0\n",
      "[66]\teval-error:0.215909\ttrain-error:0\n",
      "[67]\teval-error:0.204545\ttrain-error:0\n",
      "[68]\teval-error:0.215909\ttrain-error:0\n",
      "[69]\teval-error:0.215909\ttrain-error:0\n",
      "[0]\teval-error:0.351064\ttrain-error:0.106894\n",
      "[1]\teval-error:0.287234\ttrain-error:0.060418\n",
      "[2]\teval-error:0.319149\ttrain-error:0.026594\n",
      "[3]\teval-error:0.308511\ttrain-error:0.01433\n",
      "[4]\teval-error:0.287234\ttrain-error:0.00723\n",
      "[5]\teval-error:0.329787\ttrain-error:0.004906\n",
      "[6]\teval-error:0.297872\ttrain-error:0.004131\n",
      "[7]\teval-error:0.308511\ttrain-error:0.003744\n",
      "[8]\teval-error:0.287234\ttrain-error:0.003486\n",
      "[9]\teval-error:0.319149\ttrain-error:0.003098\n",
      "[10]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[11]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[12]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[13]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[14]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[15]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[16]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[17]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[18]\teval-error:0.255319\ttrain-error:0.003098\n",
      "[19]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[20]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[21]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[22]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[23]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[24]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[25]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[26]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[27]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[28]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[29]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[30]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[31]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[32]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[33]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[34]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[35]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[36]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[37]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[38]\teval-error:0.255319\ttrain-error:0.003098\n",
      "[39]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[40]\teval-error:0.244681\ttrain-error:0.003098\n",
      "[41]\teval-error:0.244681\ttrain-error:0.003098\n",
      "[42]\teval-error:0.255319\ttrain-error:0.003098\n",
      "[43]\teval-error:0.244681\ttrain-error:0.003098\n",
      "[44]\teval-error:0.244681\ttrain-error:0.003098\n",
      "[45]\teval-error:0.244681\ttrain-error:0.003098\n",
      "[46]\teval-error:0.255319\ttrain-error:0.003098\n",
      "[47]\teval-error:0.255319\ttrain-error:0.003098\n",
      "[48]\teval-error:0.255319\ttrain-error:0.003098\n",
      "[49]\teval-error:0.244681\ttrain-error:0.003098\n",
      "[50]\teval-error:0.244681\ttrain-error:0.003098\n",
      "[51]\teval-error:0.244681\ttrain-error:0.003098\n",
      "[52]\teval-error:0.244681\ttrain-error:0.003098\n",
      "[53]\teval-error:0.244681\ttrain-error:0.003098\n",
      "[54]\teval-error:0.244681\ttrain-error:0.003098\n",
      "[55]\teval-error:0.255319\ttrain-error:0.003098\n",
      "[56]\teval-error:0.244681\ttrain-error:0.003098\n",
      "[57]\teval-error:0.255319\ttrain-error:0.003098\n",
      "[58]\teval-error:0.244681\ttrain-error:0.003098\n",
      "[59]\teval-error:0.255319\ttrain-error:0.003098\n",
      "[60]\teval-error:0.244681\ttrain-error:0.003098\n",
      "[61]\teval-error:0.244681\ttrain-error:0.003098\n",
      "[62]\teval-error:0.234043\ttrain-error:0.003098\n",
      "[63]\teval-error:0.234043\ttrain-error:0.003098\n",
      "[64]\teval-error:0.234043\ttrain-error:0.003098\n",
      "[65]\teval-error:0.234043\ttrain-error:0.003098\n",
      "[66]\teval-error:0.234043\ttrain-error:0.003098\n",
      "[67]\teval-error:0.234043\ttrain-error:0.003098\n",
      "[68]\teval-error:0.234043\ttrain-error:0.003098\n",
      "[69]\teval-error:0.234043\ttrain-error:0.003098\n",
      "[0]\teval-error:0.329787\ttrain-error:0.107023\n",
      "[1]\teval-error:0.319149\ttrain-error:0.057191\n",
      "[2]\teval-error:0.255319\ttrain-error:0.02724\n",
      "[3]\teval-error:0.276596\ttrain-error:0.013555\n",
      "[4]\teval-error:0.265957\ttrain-error:0.006971\n",
      "[5]\teval-error:0.287234\ttrain-error:0.004648\n",
      "[6]\teval-error:0.265957\ttrain-error:0.003744\n",
      "[7]\teval-error:0.308511\ttrain-error:0.003227\n",
      "[8]\teval-error:0.297872\ttrain-error:0.003227\n",
      "[9]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[10]\teval-error:0.297872\ttrain-error:0.003227\n",
      "[11]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[12]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[13]\teval-error:0.308511\ttrain-error:0.003098\n",
      "[14]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[15]\teval-error:0.308511\ttrain-error:0.003098\n",
      "[16]\teval-error:0.329787\ttrain-error:0.003098\n",
      "[17]\teval-error:0.329787\ttrain-error:0.003098\n",
      "[18]\teval-error:0.329787\ttrain-error:0.003098\n",
      "[19]\teval-error:0.319149\ttrain-error:0.003098\n",
      "[20]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[21]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[22]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[23]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[24]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[25]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[26]\teval-error:0.255319\ttrain-error:0.003098\n",
      "[27]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[28]\teval-error:0.244681\ttrain-error:0.003098\n",
      "[29]\teval-error:0.255319\ttrain-error:0.003098\n",
      "[30]\teval-error:0.255319\ttrain-error:0.003098\n",
      "[31]\teval-error:0.244681\ttrain-error:0.003098\n",
      "[32]\teval-error:0.223404\ttrain-error:0.003098\n",
      "[33]\teval-error:0.223404\ttrain-error:0.003098\n",
      "[34]\teval-error:0.244681\ttrain-error:0.003098\n",
      "[35]\teval-error:0.223404\ttrain-error:0.003098\n",
      "[36]\teval-error:0.212766\ttrain-error:0.003098\n",
      "[37]\teval-error:0.234043\ttrain-error:0.003098\n",
      "[38]\teval-error:0.212766\ttrain-error:0.003098\n",
      "[39]\teval-error:0.212766\ttrain-error:0.003098\n",
      "[40]\teval-error:0.212766\ttrain-error:0.003098\n",
      "[41]\teval-error:0.212766\ttrain-error:0.003098\n",
      "[42]\teval-error:0.212766\ttrain-error:0.003098\n",
      "[43]\teval-error:0.223404\ttrain-error:0.003098\n",
      "[44]\teval-error:0.223404\ttrain-error:0.003098\n",
      "[45]\teval-error:0.223404\ttrain-error:0.003098\n",
      "[46]\teval-error:0.234043\ttrain-error:0.003098\n",
      "[47]\teval-error:0.223404\ttrain-error:0.003098\n",
      "[48]\teval-error:0.234043\ttrain-error:0.003098\n",
      "[49]\teval-error:0.255319\ttrain-error:0.003098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\teval-error:0.244681\ttrain-error:0.003098\n",
      "[51]\teval-error:0.244681\ttrain-error:0.003098\n",
      "[52]\teval-error:0.244681\ttrain-error:0.003098\n",
      "[53]\teval-error:0.244681\ttrain-error:0.003098\n",
      "[54]\teval-error:0.223404\ttrain-error:0.003098\n",
      "[55]\teval-error:0.234043\ttrain-error:0.003098\n",
      "[56]\teval-error:0.234043\ttrain-error:0.003098\n",
      "[57]\teval-error:0.234043\ttrain-error:0.003098\n",
      "[58]\teval-error:0.223404\ttrain-error:0.003098\n",
      "[59]\teval-error:0.234043\ttrain-error:0.003098\n",
      "[60]\teval-error:0.234043\ttrain-error:0.003098\n",
      "[61]\teval-error:0.234043\ttrain-error:0.003098\n",
      "[62]\teval-error:0.234043\ttrain-error:0.003098\n",
      "[63]\teval-error:0.223404\ttrain-error:0.003098\n",
      "[64]\teval-error:0.234043\ttrain-error:0.003098\n",
      "[65]\teval-error:0.234043\ttrain-error:0.003098\n",
      "[66]\teval-error:0.223404\ttrain-error:0.003098\n",
      "[67]\teval-error:0.223404\ttrain-error:0.003098\n",
      "[68]\teval-error:0.223404\ttrain-error:0.003098\n",
      "[69]\teval-error:0.234043\ttrain-error:0.003098\n",
      "[0]\teval-error:0.393617\ttrain-error:0.108185\n",
      "[1]\teval-error:0.351064\ttrain-error:0.056933\n",
      "[2]\teval-error:0.351064\ttrain-error:0.026594\n",
      "[3]\teval-error:0.351064\ttrain-error:0.011877\n",
      "[4]\teval-error:0.351064\ttrain-error:0.006713\n",
      "[5]\teval-error:0.329787\ttrain-error:0.004518\n",
      "[6]\teval-error:0.308511\ttrain-error:0.003486\n",
      "[7]\teval-error:0.319149\ttrain-error:0.003357\n",
      "[8]\teval-error:0.329787\ttrain-error:0.003227\n",
      "[9]\teval-error:0.319149\ttrain-error:0.003098\n",
      "[10]\teval-error:0.319149\ttrain-error:0.003098\n",
      "[11]\teval-error:0.308511\ttrain-error:0.003098\n",
      "[12]\teval-error:0.351064\ttrain-error:0.003098\n",
      "[13]\teval-error:0.329787\ttrain-error:0.003098\n",
      "[14]\teval-error:0.329787\ttrain-error:0.003098\n",
      "[15]\teval-error:0.329787\ttrain-error:0.003098\n",
      "[16]\teval-error:0.329787\ttrain-error:0.003098\n",
      "[17]\teval-error:0.329787\ttrain-error:0.003098\n",
      "[18]\teval-error:0.319149\ttrain-error:0.003098\n",
      "[19]\teval-error:0.329787\ttrain-error:0.003098\n",
      "[20]\teval-error:0.329787\ttrain-error:0.003098\n",
      "[21]\teval-error:0.329787\ttrain-error:0.003098\n",
      "[22]\teval-error:0.329787\ttrain-error:0.003098\n",
      "[23]\teval-error:0.308511\ttrain-error:0.003098\n",
      "[24]\teval-error:0.308511\ttrain-error:0.003098\n",
      "[25]\teval-error:0.308511\ttrain-error:0.003098\n",
      "[26]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[27]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[28]\teval-error:0.308511\ttrain-error:0.003098\n",
      "[29]\teval-error:0.308511\ttrain-error:0.003098\n",
      "[30]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[31]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[32]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[33]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[34]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[35]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[36]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[37]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[38]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[39]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[40]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[41]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[42]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[43]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[44]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[45]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[46]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[47]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[48]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[49]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[50]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[51]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[52]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[53]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[54]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[55]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[56]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[57]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[58]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[59]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[60]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[61]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[62]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[63]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[64]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[65]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[66]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[67]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[68]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[69]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[0]\teval-error:0.329787\ttrain-error:0.104957\n",
      "[1]\teval-error:0.265957\ttrain-error:0.055125\n",
      "[2]\teval-error:0.276596\ttrain-error:0.026594\n",
      "[3]\teval-error:0.297872\ttrain-error:0.012652\n",
      "[4]\teval-error:0.276596\ttrain-error:0.0071\n",
      "[5]\teval-error:0.265957\ttrain-error:0.004648\n",
      "[6]\teval-error:0.276596\ttrain-error:0.003744\n",
      "[7]\teval-error:0.265957\ttrain-error:0.003744\n",
      "[8]\teval-error:0.287234\ttrain-error:0.003357\n",
      "[9]\teval-error:0.287234\ttrain-error:0.003357\n",
      "[10]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[11]\teval-error:0.255319\ttrain-error:0.003098\n",
      "[12]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[13]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[14]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[15]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[16]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[17]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[18]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[19]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[20]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[21]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[22]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[23]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[24]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[25]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[26]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[27]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[28]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[29]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[30]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[31]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[32]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[33]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[34]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[35]\teval-error:0.255319\ttrain-error:0.003098\n",
      "[36]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[37]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[38]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[39]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[40]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[41]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[42]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[43]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[44]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[45]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[46]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[47]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[48]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[49]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[50]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[51]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[52]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[53]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[54]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[55]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[56]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[57]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[58]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[59]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[60]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[61]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[62]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[63]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[64]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[65]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[66]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[67]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[68]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[69]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[0]\teval-error:0.244275\ttrain-error:0.090344\n",
      "[1]\teval-error:0.236641\ttrain-error:0.058634\n",
      "[2]\teval-error:0.244275\ttrain-error:0.032567\n",
      "[3]\teval-error:0.221374\ttrain-error:0.018569\n",
      "[4]\teval-error:0.21374\ttrain-error:0.012927\n",
      "[5]\teval-error:0.225191\ttrain-error:0.010141\n",
      "[6]\teval-error:0.229008\ttrain-error:0.008999\n",
      "[7]\teval-error:0.229008\ttrain-error:0.008499\n",
      "[8]\teval-error:0.21374\ttrain-error:0.008142\n",
      "[9]\teval-error:0.229008\ttrain-error:0.008142\n",
      "[10]\teval-error:0.221374\ttrain-error:0.007999\n",
      "[11]\teval-error:0.217557\ttrain-error:0.007999\n",
      "[12]\teval-error:0.225191\ttrain-error:0.007856\n",
      "[13]\teval-error:0.221374\ttrain-error:0.007927\n",
      "[14]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[15]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[16]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[17]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[18]\teval-error:0.221374\ttrain-error:0.007785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[20]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[21]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[22]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[23]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[24]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[25]\teval-error:0.229008\ttrain-error:0.007785\n",
      "[26]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[27]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[28]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[29]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[30]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[31]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[32]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[33]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[34]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[35]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[36]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[37]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[38]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[39]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[40]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[41]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[42]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[43]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[44]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[45]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[46]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[47]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[48]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[49]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[50]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[51]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[52]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[53]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[54]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[55]\teval-error:0.206107\ttrain-error:0.007785\n",
      "[56]\teval-error:0.20229\ttrain-error:0.007785\n",
      "[57]\teval-error:0.206107\ttrain-error:0.007785\n",
      "[58]\teval-error:0.206107\ttrain-error:0.007785\n",
      "[59]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[60]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[61]\teval-error:0.206107\ttrain-error:0.007785\n",
      "[62]\teval-error:0.206107\ttrain-error:0.007785\n",
      "[63]\teval-error:0.206107\ttrain-error:0.007785\n",
      "[64]\teval-error:0.20229\ttrain-error:0.007785\n",
      "[65]\teval-error:0.20229\ttrain-error:0.007785\n",
      "[66]\teval-error:0.206107\ttrain-error:0.007785\n",
      "[67]\teval-error:0.20229\ttrain-error:0.007785\n",
      "[68]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[69]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[0]\teval-error:0.270992\ttrain-error:0.090987\n",
      "[1]\teval-error:0.232824\ttrain-error:0.056206\n",
      "[2]\teval-error:0.248092\ttrain-error:0.030924\n",
      "[3]\teval-error:0.229008\ttrain-error:0.018569\n",
      "[4]\teval-error:0.229008\ttrain-error:0.012784\n",
      "[5]\teval-error:0.229008\ttrain-error:0.01007\n",
      "[6]\teval-error:0.244275\ttrain-error:0.008927\n",
      "[7]\teval-error:0.232824\ttrain-error:0.008427\n",
      "[8]\teval-error:0.232824\ttrain-error:0.007999\n",
      "[9]\teval-error:0.232824\ttrain-error:0.007927\n",
      "[10]\teval-error:0.232824\ttrain-error:0.007999\n",
      "[11]\teval-error:0.217557\ttrain-error:0.007856\n",
      "[12]\teval-error:0.229008\ttrain-error:0.007785\n",
      "[13]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[14]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[15]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[16]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[17]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[18]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[19]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[20]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[21]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[22]\teval-error:0.229008\ttrain-error:0.007785\n",
      "[23]\teval-error:0.229008\ttrain-error:0.007785\n",
      "[24]\teval-error:0.232824\ttrain-error:0.007785\n",
      "[25]\teval-error:0.232824\ttrain-error:0.007785\n",
      "[26]\teval-error:0.232824\ttrain-error:0.007785\n",
      "[27]\teval-error:0.229008\ttrain-error:0.007785\n",
      "[28]\teval-error:0.232824\ttrain-error:0.007785\n",
      "[29]\teval-error:0.232824\ttrain-error:0.007785\n",
      "[30]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[31]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[32]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[33]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[34]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[35]\teval-error:0.232824\ttrain-error:0.007785\n",
      "[36]\teval-error:0.229008\ttrain-error:0.007785\n",
      "[37]\teval-error:0.229008\ttrain-error:0.007785\n",
      "[38]\teval-error:0.229008\ttrain-error:0.007785\n",
      "[39]\teval-error:0.232824\ttrain-error:0.007785\n",
      "[40]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[41]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[42]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[43]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[44]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[45]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[46]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[47]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[48]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[49]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[50]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[51]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[52]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[53]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[54]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[55]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[56]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[57]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[58]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[59]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[60]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[61]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[62]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[63]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[64]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[65]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[66]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[67]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[68]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[69]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[0]\teval-error:0.229008\ttrain-error:0.09213\n",
      "[1]\teval-error:0.236641\ttrain-error:0.055349\n",
      "[2]\teval-error:0.248092\ttrain-error:0.03171\n",
      "[3]\teval-error:0.251908\ttrain-error:0.020283\n",
      "[4]\teval-error:0.259542\ttrain-error:0.013641\n",
      "[5]\teval-error:0.244275\ttrain-error:0.010713\n",
      "[6]\teval-error:0.244275\ttrain-error:0.009427\n",
      "[7]\teval-error:0.232824\ttrain-error:0.008999\n",
      "[8]\teval-error:0.229008\ttrain-error:0.008285\n",
      "[9]\teval-error:0.229008\ttrain-error:0.007927\n",
      "[10]\teval-error:0.232824\ttrain-error:0.007856\n",
      "[11]\teval-error:0.244275\ttrain-error:0.007856\n",
      "[12]\teval-error:0.232824\ttrain-error:0.007927\n",
      "[13]\teval-error:0.232824\ttrain-error:0.007856\n",
      "[14]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[15]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[16]\teval-error:0.229008\ttrain-error:0.007785\n",
      "[17]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[18]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[19]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[20]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[21]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[22]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[23]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[24]\teval-error:0.232824\ttrain-error:0.007785\n",
      "[25]\teval-error:0.229008\ttrain-error:0.007785\n",
      "[26]\teval-error:0.229008\ttrain-error:0.007785\n",
      "[27]\teval-error:0.229008\ttrain-error:0.007785\n",
      "[28]\teval-error:0.229008\ttrain-error:0.007785\n",
      "[29]\teval-error:0.232824\ttrain-error:0.007785\n",
      "[30]\teval-error:0.232824\ttrain-error:0.007785\n",
      "[31]\teval-error:0.232824\ttrain-error:0.007785\n",
      "[32]\teval-error:0.232824\ttrain-error:0.007785\n",
      "[33]\teval-error:0.236641\ttrain-error:0.007785\n",
      "[34]\teval-error:0.229008\ttrain-error:0.007785\n",
      "[35]\teval-error:0.229008\ttrain-error:0.007785\n",
      "[36]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[37]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[38]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[39]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[40]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[41]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[42]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[43]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[44]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[45]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[46]\teval-error:0.229008\ttrain-error:0.007785\n",
      "[47]\teval-error:0.229008\ttrain-error:0.007785\n",
      "[48]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[49]\teval-error:0.229008\ttrain-error:0.007785\n",
      "[50]\teval-error:0.229008\ttrain-error:0.007785\n",
      "[51]\teval-error:0.229008\ttrain-error:0.007785\n",
      "[52]\teval-error:0.229008\ttrain-error:0.007785\n",
      "[53]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[54]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[55]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[56]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[57]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[58]\teval-error:0.21374\ttrain-error:0.007785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[59]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[60]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[61]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[62]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[63]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[64]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[65]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[66]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[67]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[68]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[69]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[0]\teval-error:0.259542\ttrain-error:0.088916\n",
      "[1]\teval-error:0.229008\ttrain-error:0.054278\n",
      "[2]\teval-error:0.209924\ttrain-error:0.030353\n",
      "[3]\teval-error:0.209924\ttrain-error:0.019997\n",
      "[4]\teval-error:0.225191\ttrain-error:0.013498\n",
      "[5]\teval-error:0.236641\ttrain-error:0.010713\n",
      "[6]\teval-error:0.240458\ttrain-error:0.00907\n",
      "[7]\teval-error:0.251908\ttrain-error:0.00807\n",
      "[8]\teval-error:0.236641\ttrain-error:0.00807\n",
      "[9]\teval-error:0.229008\ttrain-error:0.007927\n",
      "[10]\teval-error:0.225191\ttrain-error:0.007856\n",
      "[11]\teval-error:0.217557\ttrain-error:0.007927\n",
      "[12]\teval-error:0.225191\ttrain-error:0.007856\n",
      "[13]\teval-error:0.221374\ttrain-error:0.007856\n",
      "[14]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[15]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[16]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[17]\teval-error:0.229008\ttrain-error:0.007785\n",
      "[18]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[19]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[20]\teval-error:0.232824\ttrain-error:0.007785\n",
      "[21]\teval-error:0.232824\ttrain-error:0.007785\n",
      "[22]\teval-error:0.232824\ttrain-error:0.007785\n",
      "[23]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[24]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[25]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[26]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[27]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[28]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[29]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[30]\teval-error:0.229008\ttrain-error:0.007785\n",
      "[31]\teval-error:0.232824\ttrain-error:0.007785\n",
      "[32]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[33]\teval-error:0.229008\ttrain-error:0.007785\n",
      "[34]\teval-error:0.229008\ttrain-error:0.007785\n",
      "[35]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[36]\teval-error:0.229008\ttrain-error:0.007785\n",
      "[37]\teval-error:0.229008\ttrain-error:0.007785\n",
      "[38]\teval-error:0.229008\ttrain-error:0.007785\n",
      "[39]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[40]\teval-error:0.229008\ttrain-error:0.007785\n",
      "[41]\teval-error:0.232824\ttrain-error:0.007785\n",
      "[42]\teval-error:0.229008\ttrain-error:0.007785\n",
      "[43]\teval-error:0.232824\ttrain-error:0.007785\n",
      "[44]\teval-error:0.232824\ttrain-error:0.007785\n",
      "[45]\teval-error:0.236641\ttrain-error:0.007785\n",
      "[46]\teval-error:0.232824\ttrain-error:0.007785\n",
      "[47]\teval-error:0.232824\ttrain-error:0.007785\n",
      "[48]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[49]\teval-error:0.232824\ttrain-error:0.007785\n",
      "[50]\teval-error:0.232824\ttrain-error:0.007785\n",
      "[51]\teval-error:0.229008\ttrain-error:0.007785\n",
      "[52]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[53]\teval-error:0.229008\ttrain-error:0.007785\n",
      "[54]\teval-error:0.229008\ttrain-error:0.007785\n",
      "[55]\teval-error:0.232824\ttrain-error:0.007785\n",
      "[56]\teval-error:0.232824\ttrain-error:0.007785\n",
      "[57]\teval-error:0.229008\ttrain-error:0.007785\n",
      "[58]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[59]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[60]\teval-error:0.229008\ttrain-error:0.007785\n",
      "[61]\teval-error:0.229008\ttrain-error:0.007785\n",
      "[62]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[63]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[64]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[65]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[66]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[67]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[68]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[69]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[0]\teval-error:0.386364\ttrain-error:0.10989\n",
      "[1]\teval-error:0.272727\ttrain-error:0.081607\n",
      "[2]\teval-error:0.261364\ttrain-error:0.066835\n",
      "[3]\teval-error:0.25\ttrain-error:0.059629\n",
      "[4]\teval-error:0.272727\ttrain-error:0.057827\n",
      "[5]\teval-error:0.261364\ttrain-error:0.057647\n",
      "[6]\teval-error:0.25\ttrain-error:0.056927\n",
      "[7]\teval-error:0.25\ttrain-error:0.056927\n",
      "[8]\teval-error:0.261364\ttrain-error:0.056747\n",
      "[9]\teval-error:0.261364\ttrain-error:0.056747\n",
      "[10]\teval-error:0.261364\ttrain-error:0.056747\n",
      "[11]\teval-error:0.238636\ttrain-error:0.056566\n",
      "[12]\teval-error:0.261364\ttrain-error:0.056747\n",
      "[13]\teval-error:0.261364\ttrain-error:0.056566\n",
      "[14]\teval-error:0.261364\ttrain-error:0.056566\n",
      "[15]\teval-error:0.25\ttrain-error:0.056566\n",
      "[16]\teval-error:0.25\ttrain-error:0.056566\n",
      "[17]\teval-error:0.25\ttrain-error:0.056566\n",
      "[18]\teval-error:0.272727\ttrain-error:0.056566\n",
      "[19]\teval-error:0.261364\ttrain-error:0.056566\n",
      "[20]\teval-error:0.25\ttrain-error:0.056566\n",
      "[21]\teval-error:0.238636\ttrain-error:0.056566\n",
      "[22]\teval-error:0.215909\ttrain-error:0.056566\n",
      "[23]\teval-error:0.215909\ttrain-error:0.056566\n",
      "[24]\teval-error:0.227273\ttrain-error:0.056566\n",
      "[25]\teval-error:0.227273\ttrain-error:0.056566\n",
      "[26]\teval-error:0.238636\ttrain-error:0.056566\n",
      "[27]\teval-error:0.238636\ttrain-error:0.056566\n",
      "[28]\teval-error:0.238636\ttrain-error:0.056566\n",
      "[29]\teval-error:0.238636\ttrain-error:0.056566\n",
      "[30]\teval-error:0.238636\ttrain-error:0.056566\n",
      "[31]\teval-error:0.238636\ttrain-error:0.056566\n",
      "[32]\teval-error:0.25\ttrain-error:0.056566\n",
      "[33]\teval-error:0.238636\ttrain-error:0.056566\n",
      "[34]\teval-error:0.238636\ttrain-error:0.056566\n",
      "[35]\teval-error:0.238636\ttrain-error:0.056566\n",
      "[36]\teval-error:0.25\ttrain-error:0.056566\n",
      "[37]\teval-error:0.25\ttrain-error:0.056566\n",
      "[38]\teval-error:0.261364\ttrain-error:0.056566\n",
      "[39]\teval-error:0.25\ttrain-error:0.056566\n",
      "[40]\teval-error:0.261364\ttrain-error:0.056566\n",
      "[41]\teval-error:0.25\ttrain-error:0.056566\n",
      "[42]\teval-error:0.261364\ttrain-error:0.056566\n",
      "[43]\teval-error:0.25\ttrain-error:0.056566\n",
      "[44]\teval-error:0.261364\ttrain-error:0.056566\n",
      "[45]\teval-error:0.261364\ttrain-error:0.056566\n",
      "[46]\teval-error:0.261364\ttrain-error:0.056566\n",
      "[47]\teval-error:0.25\ttrain-error:0.056566\n",
      "[48]\teval-error:0.25\ttrain-error:0.056566\n",
      "[49]\teval-error:0.25\ttrain-error:0.056566\n",
      "[50]\teval-error:0.25\ttrain-error:0.056566\n",
      "[51]\teval-error:0.25\ttrain-error:0.056566\n",
      "[52]\teval-error:0.25\ttrain-error:0.056566\n",
      "[53]\teval-error:0.25\ttrain-error:0.056566\n",
      "[54]\teval-error:0.25\ttrain-error:0.056566\n",
      "[55]\teval-error:0.25\ttrain-error:0.056566\n",
      "[56]\teval-error:0.25\ttrain-error:0.056566\n",
      "[57]\teval-error:0.238636\ttrain-error:0.056566\n",
      "[58]\teval-error:0.238636\ttrain-error:0.056566\n",
      "[59]\teval-error:0.238636\ttrain-error:0.056566\n",
      "[60]\teval-error:0.238636\ttrain-error:0.056566\n",
      "[61]\teval-error:0.238636\ttrain-error:0.056566\n",
      "[62]\teval-error:0.238636\ttrain-error:0.056566\n",
      "[63]\teval-error:0.238636\ttrain-error:0.056566\n",
      "[64]\teval-error:0.238636\ttrain-error:0.056566\n",
      "[65]\teval-error:0.238636\ttrain-error:0.056566\n",
      "[66]\teval-error:0.25\ttrain-error:0.056566\n",
      "[67]\teval-error:0.238636\ttrain-error:0.056566\n",
      "[68]\teval-error:0.25\ttrain-error:0.056566\n",
      "[69]\teval-error:0.25\ttrain-error:0.056566\n",
      "[0]\teval-error:0.306818\ttrain-error:0.115835\n",
      "[1]\teval-error:0.306818\ttrain-error:0.08485\n",
      "[2]\teval-error:0.306818\ttrain-error:0.068456\n",
      "[3]\teval-error:0.306818\ttrain-error:0.061611\n",
      "[4]\teval-error:0.329545\ttrain-error:0.06053\n",
      "[5]\teval-error:0.306818\ttrain-error:0.058908\n",
      "[6]\teval-error:0.284091\ttrain-error:0.058908\n",
      "[7]\teval-error:0.295455\ttrain-error:0.058908\n",
      "[8]\teval-error:0.295455\ttrain-error:0.058908\n",
      "[9]\teval-error:0.284091\ttrain-error:0.058908\n",
      "[10]\teval-error:0.272727\ttrain-error:0.058908\n",
      "[11]\teval-error:0.284091\ttrain-error:0.058908\n",
      "[12]\teval-error:0.284091\ttrain-error:0.058908\n",
      "[13]\teval-error:0.284091\ttrain-error:0.058908\n",
      "[14]\teval-error:0.284091\ttrain-error:0.058908\n",
      "[15]\teval-error:0.284091\ttrain-error:0.058908\n",
      "[16]\teval-error:0.284091\ttrain-error:0.058908\n",
      "[17]\teval-error:0.284091\ttrain-error:0.058908\n",
      "[18]\teval-error:0.295455\ttrain-error:0.058908\n",
      "[19]\teval-error:0.284091\ttrain-error:0.058908\n",
      "[20]\teval-error:0.295455\ttrain-error:0.058908\n",
      "[21]\teval-error:0.306818\ttrain-error:0.058908\n",
      "[22]\teval-error:0.284091\ttrain-error:0.058908\n",
      "[23]\teval-error:0.284091\ttrain-error:0.058908\n",
      "[24]\teval-error:0.284091\ttrain-error:0.058908\n",
      "[25]\teval-error:0.284091\ttrain-error:0.058908\n",
      "[26]\teval-error:0.295455\ttrain-error:0.058908\n",
      "[27]\teval-error:0.284091\ttrain-error:0.058908\n",
      "[28]\teval-error:0.284091\ttrain-error:0.058908\n",
      "[29]\teval-error:0.295455\ttrain-error:0.058908\n",
      "[30]\teval-error:0.284091\ttrain-error:0.058908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31]\teval-error:0.295455\ttrain-error:0.058908\n",
      "[32]\teval-error:0.295455\ttrain-error:0.058908\n",
      "[33]\teval-error:0.295455\ttrain-error:0.058908\n",
      "[34]\teval-error:0.295455\ttrain-error:0.058908\n",
      "[35]\teval-error:0.306818\ttrain-error:0.058908\n",
      "[36]\teval-error:0.284091\ttrain-error:0.058908\n",
      "[37]\teval-error:0.295455\ttrain-error:0.058908\n",
      "[38]\teval-error:0.295455\ttrain-error:0.058908\n",
      "[39]\teval-error:0.295455\ttrain-error:0.058908\n",
      "[40]\teval-error:0.295455\ttrain-error:0.058908\n",
      "[41]\teval-error:0.295455\ttrain-error:0.058908\n",
      "[42]\teval-error:0.284091\ttrain-error:0.058908\n",
      "[43]\teval-error:0.295455\ttrain-error:0.058908\n",
      "[44]\teval-error:0.284091\ttrain-error:0.058908\n",
      "[45]\teval-error:0.284091\ttrain-error:0.058908\n",
      "[46]\teval-error:0.284091\ttrain-error:0.058908\n",
      "[47]\teval-error:0.295455\ttrain-error:0.058908\n",
      "[48]\teval-error:0.284091\ttrain-error:0.058908\n",
      "[49]\teval-error:0.295455\ttrain-error:0.058908\n",
      "[50]\teval-error:0.295455\ttrain-error:0.058908\n",
      "[51]\teval-error:0.295455\ttrain-error:0.058908\n",
      "[52]\teval-error:0.284091\ttrain-error:0.058908\n",
      "[53]\teval-error:0.295455\ttrain-error:0.058908\n",
      "[54]\teval-error:0.295455\ttrain-error:0.058908\n",
      "[55]\teval-error:0.295455\ttrain-error:0.058908\n",
      "[56]\teval-error:0.295455\ttrain-error:0.058908\n",
      "[57]\teval-error:0.295455\ttrain-error:0.058908\n",
      "[58]\teval-error:0.295455\ttrain-error:0.058908\n",
      "[59]\teval-error:0.295455\ttrain-error:0.058908\n",
      "[60]\teval-error:0.295455\ttrain-error:0.058908\n",
      "[61]\teval-error:0.295455\ttrain-error:0.058908\n",
      "[62]\teval-error:0.295455\ttrain-error:0.058908\n",
      "[63]\teval-error:0.295455\ttrain-error:0.058908\n",
      "[64]\teval-error:0.284091\ttrain-error:0.058908\n",
      "[65]\teval-error:0.284091\ttrain-error:0.058908\n",
      "[66]\teval-error:0.295455\ttrain-error:0.058908\n",
      "[67]\teval-error:0.284091\ttrain-error:0.058908\n",
      "[68]\teval-error:0.295455\ttrain-error:0.058908\n",
      "[69]\teval-error:0.284091\ttrain-error:0.058908\n",
      "[0]\teval-error:0.409091\ttrain-error:0.119438\n",
      "[1]\teval-error:0.375\ttrain-error:0.089714\n",
      "[2]\teval-error:0.409091\ttrain-error:0.07314\n",
      "[3]\teval-error:0.409091\ttrain-error:0.064853\n",
      "[4]\teval-error:0.386364\ttrain-error:0.063232\n",
      "[5]\teval-error:0.397727\ttrain-error:0.062331\n",
      "[6]\teval-error:0.420455\ttrain-error:0.061971\n",
      "[7]\teval-error:0.431818\ttrain-error:0.061791\n",
      "[8]\teval-error:0.409091\ttrain-error:0.06125\n",
      "[9]\teval-error:0.409091\ttrain-error:0.06107\n",
      "[10]\teval-error:0.420455\ttrain-error:0.06107\n",
      "[11]\teval-error:0.409091\ttrain-error:0.06107\n",
      "[12]\teval-error:0.420455\ttrain-error:0.06107\n",
      "[13]\teval-error:0.397727\ttrain-error:0.06107\n",
      "[14]\teval-error:0.397727\ttrain-error:0.06107\n",
      "[15]\teval-error:0.397727\ttrain-error:0.06107\n",
      "[16]\teval-error:0.386364\ttrain-error:0.06107\n",
      "[17]\teval-error:0.397727\ttrain-error:0.06107\n",
      "[18]\teval-error:0.386364\ttrain-error:0.06107\n",
      "[19]\teval-error:0.386364\ttrain-error:0.06107\n",
      "[20]\teval-error:0.386364\ttrain-error:0.06107\n",
      "[21]\teval-error:0.386364\ttrain-error:0.06107\n",
      "[22]\teval-error:0.375\ttrain-error:0.06107\n",
      "[23]\teval-error:0.375\ttrain-error:0.06107\n",
      "[24]\teval-error:0.375\ttrain-error:0.06107\n",
      "[25]\teval-error:0.363636\ttrain-error:0.06107\n",
      "[26]\teval-error:0.363636\ttrain-error:0.06107\n",
      "[27]\teval-error:0.375\ttrain-error:0.06107\n",
      "[28]\teval-error:0.375\ttrain-error:0.06107\n",
      "[29]\teval-error:0.386364\ttrain-error:0.06107\n",
      "[30]\teval-error:0.386364\ttrain-error:0.06107\n",
      "[31]\teval-error:0.386364\ttrain-error:0.06107\n",
      "[32]\teval-error:0.386364\ttrain-error:0.06107\n",
      "[33]\teval-error:0.386364\ttrain-error:0.06107\n",
      "[34]\teval-error:0.386364\ttrain-error:0.06107\n",
      "[35]\teval-error:0.409091\ttrain-error:0.06107\n",
      "[36]\teval-error:0.397727\ttrain-error:0.06107\n",
      "[37]\teval-error:0.409091\ttrain-error:0.06107\n",
      "[38]\teval-error:0.409091\ttrain-error:0.06107\n",
      "[39]\teval-error:0.409091\ttrain-error:0.06107\n",
      "[40]\teval-error:0.397727\ttrain-error:0.06107\n",
      "[41]\teval-error:0.397727\ttrain-error:0.06107\n",
      "[42]\teval-error:0.397727\ttrain-error:0.06107\n",
      "[43]\teval-error:0.397727\ttrain-error:0.06107\n",
      "[44]\teval-error:0.409091\ttrain-error:0.06107\n",
      "[45]\teval-error:0.397727\ttrain-error:0.06107\n",
      "[46]\teval-error:0.397727\ttrain-error:0.06107\n",
      "[47]\teval-error:0.397727\ttrain-error:0.06107\n",
      "[48]\teval-error:0.409091\ttrain-error:0.06107\n",
      "[49]\teval-error:0.420455\ttrain-error:0.06107\n",
      "[50]\teval-error:0.420455\ttrain-error:0.06107\n",
      "[51]\teval-error:0.420455\ttrain-error:0.06107\n",
      "[52]\teval-error:0.409091\ttrain-error:0.06107\n",
      "[53]\teval-error:0.409091\ttrain-error:0.06107\n",
      "[54]\teval-error:0.409091\ttrain-error:0.06107\n",
      "[55]\teval-error:0.409091\ttrain-error:0.06107\n",
      "[56]\teval-error:0.409091\ttrain-error:0.06107\n",
      "[57]\teval-error:0.409091\ttrain-error:0.06107\n",
      "[58]\teval-error:0.409091\ttrain-error:0.06107\n",
      "[59]\teval-error:0.409091\ttrain-error:0.06107\n",
      "[60]\teval-error:0.409091\ttrain-error:0.06107\n",
      "[61]\teval-error:0.409091\ttrain-error:0.06107\n",
      "[62]\teval-error:0.409091\ttrain-error:0.06107\n",
      "[63]\teval-error:0.409091\ttrain-error:0.06107\n",
      "[64]\teval-error:0.397727\ttrain-error:0.06107\n",
      "[65]\teval-error:0.409091\ttrain-error:0.06107\n",
      "[66]\teval-error:0.409091\ttrain-error:0.06107\n",
      "[67]\teval-error:0.397727\ttrain-error:0.06107\n",
      "[68]\teval-error:0.397727\ttrain-error:0.06107\n",
      "[69]\teval-error:0.397727\ttrain-error:0.06107\n",
      "[0]\teval-error:0.375\ttrain-error:0.115475\n",
      "[1]\teval-error:0.352273\ttrain-error:0.082147\n",
      "[2]\teval-error:0.306818\ttrain-error:0.067555\n",
      "[3]\teval-error:0.340909\ttrain-error:0.06107\n",
      "[4]\teval-error:0.318182\ttrain-error:0.058728\n",
      "[5]\teval-error:0.306818\ttrain-error:0.057647\n",
      "[6]\teval-error:0.318182\ttrain-error:0.056927\n",
      "[7]\teval-error:0.306818\ttrain-error:0.056927\n",
      "[8]\teval-error:0.318182\ttrain-error:0.056927\n",
      "[9]\teval-error:0.306818\ttrain-error:0.056927\n",
      "[10]\teval-error:0.318182\ttrain-error:0.056747\n",
      "[11]\teval-error:0.318182\ttrain-error:0.056747\n",
      "[12]\teval-error:0.284091\ttrain-error:0.056747\n",
      "[13]\teval-error:0.295455\ttrain-error:0.056747\n",
      "[14]\teval-error:0.318182\ttrain-error:0.056747\n",
      "[15]\teval-error:0.306818\ttrain-error:0.056747\n",
      "[16]\teval-error:0.318182\ttrain-error:0.056747\n",
      "[17]\teval-error:0.318182\ttrain-error:0.056747\n",
      "[18]\teval-error:0.318182\ttrain-error:0.056747\n",
      "[19]\teval-error:0.284091\ttrain-error:0.056747\n",
      "[20]\teval-error:0.284091\ttrain-error:0.056566\n",
      "[21]\teval-error:0.306818\ttrain-error:0.056566\n",
      "[22]\teval-error:0.318182\ttrain-error:0.056566\n",
      "[23]\teval-error:0.318182\ttrain-error:0.056566\n",
      "[24]\teval-error:0.306818\ttrain-error:0.056566\n",
      "[25]\teval-error:0.318182\ttrain-error:0.056566\n",
      "[26]\teval-error:0.318182\ttrain-error:0.056566\n",
      "[27]\teval-error:0.340909\ttrain-error:0.056566\n",
      "[28]\teval-error:0.329545\ttrain-error:0.056566\n",
      "[29]\teval-error:0.329545\ttrain-error:0.056566\n",
      "[30]\teval-error:0.318182\ttrain-error:0.056566\n",
      "[31]\teval-error:0.318182\ttrain-error:0.056566\n",
      "[32]\teval-error:0.318182\ttrain-error:0.056566\n",
      "[33]\teval-error:0.318182\ttrain-error:0.056566\n",
      "[34]\teval-error:0.318182\ttrain-error:0.056566\n",
      "[35]\teval-error:0.318182\ttrain-error:0.056566\n",
      "[36]\teval-error:0.329545\ttrain-error:0.056566\n",
      "[37]\teval-error:0.318182\ttrain-error:0.056566\n",
      "[38]\teval-error:0.318182\ttrain-error:0.056566\n",
      "[39]\teval-error:0.306818\ttrain-error:0.056566\n",
      "[40]\teval-error:0.306818\ttrain-error:0.056566\n",
      "[41]\teval-error:0.306818\ttrain-error:0.056566\n",
      "[42]\teval-error:0.318182\ttrain-error:0.056566\n",
      "[43]\teval-error:0.318182\ttrain-error:0.056566\n",
      "[44]\teval-error:0.318182\ttrain-error:0.056566\n",
      "[45]\teval-error:0.318182\ttrain-error:0.056566\n",
      "[46]\teval-error:0.318182\ttrain-error:0.056566\n",
      "[47]\teval-error:0.318182\ttrain-error:0.056566\n",
      "[48]\teval-error:0.318182\ttrain-error:0.056566\n",
      "[49]\teval-error:0.306818\ttrain-error:0.056566\n",
      "[50]\teval-error:0.318182\ttrain-error:0.056566\n",
      "[51]\teval-error:0.318182\ttrain-error:0.056566\n",
      "[52]\teval-error:0.329545\ttrain-error:0.056566\n",
      "[53]\teval-error:0.329545\ttrain-error:0.056566\n",
      "[54]\teval-error:0.318182\ttrain-error:0.056566\n",
      "[55]\teval-error:0.329545\ttrain-error:0.056566\n",
      "[56]\teval-error:0.318182\ttrain-error:0.056566\n",
      "[57]\teval-error:0.340909\ttrain-error:0.056566\n",
      "[58]\teval-error:0.340909\ttrain-error:0.056566\n",
      "[59]\teval-error:0.340909\ttrain-error:0.056566\n",
      "[60]\teval-error:0.340909\ttrain-error:0.056566\n",
      "[61]\teval-error:0.318182\ttrain-error:0.056566\n",
      "[62]\teval-error:0.329545\ttrain-error:0.056566\n",
      "[63]\teval-error:0.318182\ttrain-error:0.056566\n",
      "[64]\teval-error:0.329545\ttrain-error:0.056566\n",
      "[65]\teval-error:0.329545\ttrain-error:0.056566\n",
      "[66]\teval-error:0.318182\ttrain-error:0.056566\n",
      "[67]\teval-error:0.329545\ttrain-error:0.056566\n",
      "[68]\teval-error:0.318182\ttrain-error:0.056566\n",
      "[69]\teval-error:0.318182\ttrain-error:0.056566\n",
      "[0]\teval-error:0.319149\ttrain-error:0.111154\n",
      "[1]\teval-error:0.287234\ttrain-error:0.089853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\teval-error:0.234043\ttrain-error:0.076168\n",
      "[3]\teval-error:0.244681\ttrain-error:0.071263\n",
      "[4]\teval-error:0.265957\ttrain-error:0.068939\n",
      "[5]\teval-error:0.244681\ttrain-error:0.068422\n",
      "[6]\teval-error:0.244681\ttrain-error:0.068035\n",
      "[7]\teval-error:0.255319\ttrain-error:0.067648\n",
      "[8]\teval-error:0.276596\ttrain-error:0.067648\n",
      "[9]\teval-error:0.265957\ttrain-error:0.067519\n",
      "[10]\teval-error:0.265957\ttrain-error:0.067519\n",
      "[11]\teval-error:0.265957\ttrain-error:0.067519\n",
      "[12]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[13]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[14]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[15]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[16]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[17]\teval-error:0.287234\ttrain-error:0.067519\n",
      "[18]\teval-error:0.287234\ttrain-error:0.067519\n",
      "[19]\teval-error:0.287234\ttrain-error:0.067519\n",
      "[20]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[21]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[22]\teval-error:0.287234\ttrain-error:0.067519\n",
      "[23]\teval-error:0.287234\ttrain-error:0.067519\n",
      "[24]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[25]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[26]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[27]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[28]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[29]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[30]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[31]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[32]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[33]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[34]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[35]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[36]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[37]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[38]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[39]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[40]\teval-error:0.287234\ttrain-error:0.067519\n",
      "[41]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[42]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[43]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[44]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[45]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[46]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[47]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[48]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[49]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[50]\teval-error:0.265957\ttrain-error:0.067519\n",
      "[51]\teval-error:0.265957\ttrain-error:0.067519\n",
      "[52]\teval-error:0.265957\ttrain-error:0.067519\n",
      "[53]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[54]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[55]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[56]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[57]\teval-error:0.287234\ttrain-error:0.067519\n",
      "[58]\teval-error:0.287234\ttrain-error:0.067519\n",
      "[59]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[60]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[61]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[62]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[63]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[64]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[65]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[66]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[67]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[68]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[69]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[0]\teval-error:0.297872\ttrain-error:0.113736\n",
      "[1]\teval-error:0.276596\ttrain-error:0.091789\n",
      "[2]\teval-error:0.276596\ttrain-error:0.079267\n",
      "[3]\teval-error:0.255319\ttrain-error:0.073586\n",
      "[4]\teval-error:0.255319\ttrain-error:0.070359\n",
      "[5]\teval-error:0.255319\ttrain-error:0.070101\n",
      "[6]\teval-error:0.255319\ttrain-error:0.069713\n",
      "[7]\teval-error:0.255319\ttrain-error:0.069455\n",
      "[8]\teval-error:0.255319\ttrain-error:0.069326\n",
      "[9]\teval-error:0.244681\ttrain-error:0.069326\n",
      "[10]\teval-error:0.244681\ttrain-error:0.069197\n",
      "[11]\teval-error:0.255319\ttrain-error:0.069197\n",
      "[12]\teval-error:0.255319\ttrain-error:0.069197\n",
      "[13]\teval-error:0.244681\ttrain-error:0.069197\n",
      "[14]\teval-error:0.255319\ttrain-error:0.069197\n",
      "[15]\teval-error:0.244681\ttrain-error:0.069197\n",
      "[16]\teval-error:0.244681\ttrain-error:0.069197\n",
      "[17]\teval-error:0.244681\ttrain-error:0.069197\n",
      "[18]\teval-error:0.244681\ttrain-error:0.069197\n",
      "[19]\teval-error:0.244681\ttrain-error:0.069197\n",
      "[20]\teval-error:0.234043\ttrain-error:0.069197\n",
      "[21]\teval-error:0.234043\ttrain-error:0.069197\n",
      "[22]\teval-error:0.234043\ttrain-error:0.069197\n",
      "[23]\teval-error:0.255319\ttrain-error:0.069197\n",
      "[24]\teval-error:0.244681\ttrain-error:0.069197\n",
      "[25]\teval-error:0.255319\ttrain-error:0.069197\n",
      "[26]\teval-error:0.255319\ttrain-error:0.069197\n",
      "[27]\teval-error:0.234043\ttrain-error:0.069197\n",
      "[28]\teval-error:0.244681\ttrain-error:0.069197\n",
      "[29]\teval-error:0.255319\ttrain-error:0.069197\n",
      "[30]\teval-error:0.255319\ttrain-error:0.069197\n",
      "[31]\teval-error:0.244681\ttrain-error:0.069197\n",
      "[32]\teval-error:0.255319\ttrain-error:0.069197\n",
      "[33]\teval-error:0.255319\ttrain-error:0.069197\n",
      "[34]\teval-error:0.255319\ttrain-error:0.069197\n",
      "[35]\teval-error:0.255319\ttrain-error:0.069197\n",
      "[36]\teval-error:0.255319\ttrain-error:0.069197\n",
      "[37]\teval-error:0.255319\ttrain-error:0.069197\n",
      "[38]\teval-error:0.244681\ttrain-error:0.069197\n",
      "[39]\teval-error:0.244681\ttrain-error:0.069197\n",
      "[40]\teval-error:0.244681\ttrain-error:0.069197\n",
      "[41]\teval-error:0.244681\ttrain-error:0.069197\n",
      "[42]\teval-error:0.244681\ttrain-error:0.069197\n",
      "[43]\teval-error:0.244681\ttrain-error:0.069197\n",
      "[44]\teval-error:0.244681\ttrain-error:0.069197\n",
      "[45]\teval-error:0.244681\ttrain-error:0.069197\n",
      "[46]\teval-error:0.255319\ttrain-error:0.069197\n",
      "[47]\teval-error:0.255319\ttrain-error:0.069197\n",
      "[48]\teval-error:0.255319\ttrain-error:0.069197\n",
      "[49]\teval-error:0.255319\ttrain-error:0.069197\n",
      "[50]\teval-error:0.255319\ttrain-error:0.069197\n",
      "[51]\teval-error:0.255319\ttrain-error:0.069197\n",
      "[52]\teval-error:0.255319\ttrain-error:0.069197\n",
      "[53]\teval-error:0.255319\ttrain-error:0.069197\n",
      "[54]\teval-error:0.255319\ttrain-error:0.069197\n",
      "[55]\teval-error:0.255319\ttrain-error:0.069197\n",
      "[56]\teval-error:0.255319\ttrain-error:0.069197\n",
      "[57]\teval-error:0.255319\ttrain-error:0.069197\n",
      "[58]\teval-error:0.255319\ttrain-error:0.069197\n",
      "[59]\teval-error:0.255319\ttrain-error:0.069197\n",
      "[60]\teval-error:0.255319\ttrain-error:0.069197\n",
      "[61]\teval-error:0.255319\ttrain-error:0.069197\n",
      "[62]\teval-error:0.255319\ttrain-error:0.069197\n",
      "[63]\teval-error:0.255319\ttrain-error:0.069197\n",
      "[64]\teval-error:0.255319\ttrain-error:0.069197\n",
      "[65]\teval-error:0.255319\ttrain-error:0.069197\n",
      "[66]\teval-error:0.255319\ttrain-error:0.069197\n",
      "[67]\teval-error:0.244681\ttrain-error:0.069197\n",
      "[68]\teval-error:0.244681\ttrain-error:0.069197\n",
      "[69]\teval-error:0.244681\ttrain-error:0.069197\n",
      "[0]\teval-error:0.43617\ttrain-error:0.116834\n",
      "[1]\teval-error:0.382979\ttrain-error:0.092822\n",
      "[2]\teval-error:0.382979\ttrain-error:0.079654\n",
      "[3]\teval-error:0.382979\ttrain-error:0.076427\n",
      "[4]\teval-error:0.37234\ttrain-error:0.073457\n",
      "[5]\teval-error:0.382979\ttrain-error:0.072037\n",
      "[6]\teval-error:0.361702\ttrain-error:0.072037\n",
      "[7]\teval-error:0.351064\ttrain-error:0.071392\n",
      "[8]\teval-error:0.37234\ttrain-error:0.071263\n",
      "[9]\teval-error:0.340426\ttrain-error:0.071263\n",
      "[10]\teval-error:0.340426\ttrain-error:0.071263\n",
      "[11]\teval-error:0.340426\ttrain-error:0.071263\n",
      "[12]\teval-error:0.340426\ttrain-error:0.071004\n",
      "[13]\teval-error:0.340426\ttrain-error:0.071004\n",
      "[14]\teval-error:0.319149\ttrain-error:0.071004\n",
      "[15]\teval-error:0.319149\ttrain-error:0.071004\n",
      "[16]\teval-error:0.308511\ttrain-error:0.071004\n",
      "[17]\teval-error:0.308511\ttrain-error:0.071004\n",
      "[18]\teval-error:0.308511\ttrain-error:0.071004\n",
      "[19]\teval-error:0.308511\ttrain-error:0.071004\n",
      "[20]\teval-error:0.308511\ttrain-error:0.071004\n",
      "[21]\teval-error:0.308511\ttrain-error:0.071004\n",
      "[22]\teval-error:0.308511\ttrain-error:0.071004\n",
      "[23]\teval-error:0.308511\ttrain-error:0.071004\n",
      "[24]\teval-error:0.308511\ttrain-error:0.071004\n",
      "[25]\teval-error:0.308511\ttrain-error:0.071004\n",
      "[26]\teval-error:0.308511\ttrain-error:0.071004\n",
      "[27]\teval-error:0.308511\ttrain-error:0.071004\n",
      "[28]\teval-error:0.308511\ttrain-error:0.071004\n",
      "[29]\teval-error:0.297872\ttrain-error:0.071004\n",
      "[30]\teval-error:0.297872\ttrain-error:0.071004\n",
      "[31]\teval-error:0.297872\ttrain-error:0.071004\n",
      "[32]\teval-error:0.297872\ttrain-error:0.071004\n",
      "[33]\teval-error:0.297872\ttrain-error:0.071004\n",
      "[34]\teval-error:0.297872\ttrain-error:0.071004\n",
      "[35]\teval-error:0.308511\ttrain-error:0.071004\n",
      "[36]\teval-error:0.319149\ttrain-error:0.071004\n",
      "[37]\teval-error:0.308511\ttrain-error:0.071004\n",
      "[38]\teval-error:0.297872\ttrain-error:0.071004\n",
      "[39]\teval-error:0.297872\ttrain-error:0.071004\n",
      "[40]\teval-error:0.297872\ttrain-error:0.071004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41]\teval-error:0.297872\ttrain-error:0.071004\n",
      "[42]\teval-error:0.297872\ttrain-error:0.071004\n",
      "[43]\teval-error:0.308511\ttrain-error:0.071004\n",
      "[44]\teval-error:0.308511\ttrain-error:0.071004\n",
      "[45]\teval-error:0.308511\ttrain-error:0.071004\n",
      "[46]\teval-error:0.319149\ttrain-error:0.071004\n",
      "[47]\teval-error:0.319149\ttrain-error:0.071004\n",
      "[48]\teval-error:0.308511\ttrain-error:0.071004\n",
      "[49]\teval-error:0.329787\ttrain-error:0.071004\n",
      "[50]\teval-error:0.319149\ttrain-error:0.071004\n",
      "[51]\teval-error:0.319149\ttrain-error:0.071004\n",
      "[52]\teval-error:0.308511\ttrain-error:0.071004\n",
      "[53]\teval-error:0.308511\ttrain-error:0.071004\n",
      "[54]\teval-error:0.308511\ttrain-error:0.071004\n",
      "[55]\teval-error:0.308511\ttrain-error:0.071004\n",
      "[56]\teval-error:0.308511\ttrain-error:0.071004\n",
      "[57]\teval-error:0.308511\ttrain-error:0.071004\n",
      "[58]\teval-error:0.308511\ttrain-error:0.071004\n",
      "[59]\teval-error:0.308511\ttrain-error:0.071004\n",
      "[60]\teval-error:0.308511\ttrain-error:0.071004\n",
      "[61]\teval-error:0.308511\ttrain-error:0.071004\n",
      "[62]\teval-error:0.308511\ttrain-error:0.071004\n",
      "[63]\teval-error:0.308511\ttrain-error:0.071004\n",
      "[64]\teval-error:0.308511\ttrain-error:0.071004\n",
      "[65]\teval-error:0.308511\ttrain-error:0.071004\n",
      "[66]\teval-error:0.308511\ttrain-error:0.071004\n",
      "[67]\teval-error:0.297872\ttrain-error:0.071004\n",
      "[68]\teval-error:0.308511\ttrain-error:0.071004\n",
      "[69]\teval-error:0.308511\ttrain-error:0.071004\n",
      "[0]\teval-error:0.287234\ttrain-error:0.109992\n",
      "[1]\teval-error:0.265957\ttrain-error:0.087787\n",
      "[2]\teval-error:0.255319\ttrain-error:0.076685\n",
      "[3]\teval-error:0.287234\ttrain-error:0.070875\n",
      "[4]\teval-error:0.276596\ttrain-error:0.068939\n",
      "[5]\teval-error:0.276596\ttrain-error:0.067906\n",
      "[6]\teval-error:0.244681\ttrain-error:0.067777\n",
      "[7]\teval-error:0.265957\ttrain-error:0.067648\n",
      "[8]\teval-error:0.265957\ttrain-error:0.067519\n",
      "[9]\teval-error:0.255319\ttrain-error:0.067519\n",
      "[10]\teval-error:0.265957\ttrain-error:0.067519\n",
      "[11]\teval-error:0.265957\ttrain-error:0.067519\n",
      "[12]\teval-error:0.287234\ttrain-error:0.067519\n",
      "[13]\teval-error:0.265957\ttrain-error:0.067519\n",
      "[14]\teval-error:0.265957\ttrain-error:0.067519\n",
      "[15]\teval-error:0.265957\ttrain-error:0.067519\n",
      "[16]\teval-error:0.265957\ttrain-error:0.067519\n",
      "[17]\teval-error:0.265957\ttrain-error:0.067519\n",
      "[18]\teval-error:0.255319\ttrain-error:0.067519\n",
      "[19]\teval-error:0.265957\ttrain-error:0.067519\n",
      "[20]\teval-error:0.265957\ttrain-error:0.067519\n",
      "[21]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[22]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[23]\teval-error:0.287234\ttrain-error:0.067519\n",
      "[24]\teval-error:0.287234\ttrain-error:0.067519\n",
      "[25]\teval-error:0.287234\ttrain-error:0.067519\n",
      "[26]\teval-error:0.287234\ttrain-error:0.067519\n",
      "[27]\teval-error:0.287234\ttrain-error:0.067519\n",
      "[28]\teval-error:0.287234\ttrain-error:0.067519\n",
      "[29]\teval-error:0.287234\ttrain-error:0.067519\n",
      "[30]\teval-error:0.287234\ttrain-error:0.067519\n",
      "[31]\teval-error:0.287234\ttrain-error:0.067519\n",
      "[32]\teval-error:0.287234\ttrain-error:0.067519\n",
      "[33]\teval-error:0.287234\ttrain-error:0.067519\n",
      "[34]\teval-error:0.287234\ttrain-error:0.067519\n",
      "[35]\teval-error:0.297872\ttrain-error:0.067519\n",
      "[36]\teval-error:0.297872\ttrain-error:0.067519\n",
      "[37]\teval-error:0.287234\ttrain-error:0.067519\n",
      "[38]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[39]\teval-error:0.287234\ttrain-error:0.067519\n",
      "[40]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[41]\teval-error:0.297872\ttrain-error:0.067519\n",
      "[42]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[43]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[44]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[45]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[46]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[47]\teval-error:0.265957\ttrain-error:0.067519\n",
      "[48]\teval-error:0.265957\ttrain-error:0.067519\n",
      "[49]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[50]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[51]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[52]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[53]\teval-error:0.265957\ttrain-error:0.067519\n",
      "[54]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[55]\teval-error:0.265957\ttrain-error:0.067519\n",
      "[56]\teval-error:0.265957\ttrain-error:0.067519\n",
      "[57]\teval-error:0.265957\ttrain-error:0.067519\n",
      "[58]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[59]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[60]\teval-error:0.276596\ttrain-error:0.067519\n",
      "[61]\teval-error:0.287234\ttrain-error:0.067519\n",
      "[62]\teval-error:0.255319\ttrain-error:0.067519\n",
      "[63]\teval-error:0.287234\ttrain-error:0.067519\n",
      "[64]\teval-error:0.265957\ttrain-error:0.067519\n",
      "[65]\teval-error:0.265957\ttrain-error:0.067519\n",
      "[66]\teval-error:0.265957\ttrain-error:0.067519\n",
      "[67]\teval-error:0.287234\ttrain-error:0.067519\n",
      "[68]\teval-error:0.255319\ttrain-error:0.067519\n",
      "[69]\teval-error:0.265957\ttrain-error:0.067519\n",
      "[0]\teval-error:0.229008\ttrain-error:0.095058\n",
      "[1]\teval-error:0.221374\ttrain-error:0.081631\n",
      "[2]\teval-error:0.221374\ttrain-error:0.072775\n",
      "[3]\teval-error:0.229008\ttrain-error:0.06949\n",
      "[4]\teval-error:0.217557\ttrain-error:0.067705\n",
      "[5]\teval-error:0.217557\ttrain-error:0.067205\n",
      "[6]\teval-error:0.221374\ttrain-error:0.06699\n",
      "[7]\teval-error:0.225191\ttrain-error:0.067062\n",
      "[8]\teval-error:0.21374\ttrain-error:0.06699\n",
      "[9]\teval-error:0.209924\ttrain-error:0.06699\n",
      "[10]\teval-error:0.209924\ttrain-error:0.066848\n",
      "[11]\teval-error:0.209924\ttrain-error:0.066848\n",
      "[12]\teval-error:0.217557\ttrain-error:0.066848\n",
      "[13]\teval-error:0.221374\ttrain-error:0.066848\n",
      "[14]\teval-error:0.21374\ttrain-error:0.066848\n",
      "[15]\teval-error:0.209924\ttrain-error:0.066848\n",
      "[16]\teval-error:0.209924\ttrain-error:0.066848\n",
      "[17]\teval-error:0.21374\ttrain-error:0.066848\n",
      "[18]\teval-error:0.225191\ttrain-error:0.066848\n",
      "[19]\teval-error:0.217557\ttrain-error:0.066848\n",
      "[20]\teval-error:0.209924\ttrain-error:0.066848\n",
      "[21]\teval-error:0.206107\ttrain-error:0.066848\n",
      "[22]\teval-error:0.206107\ttrain-error:0.066848\n",
      "[23]\teval-error:0.209924\ttrain-error:0.066848\n",
      "[24]\teval-error:0.209924\ttrain-error:0.066848\n",
      "[25]\teval-error:0.21374\ttrain-error:0.066848\n",
      "[26]\teval-error:0.21374\ttrain-error:0.066848\n",
      "[27]\teval-error:0.217557\ttrain-error:0.066848\n",
      "[28]\teval-error:0.217557\ttrain-error:0.066848\n",
      "[29]\teval-error:0.206107\ttrain-error:0.066848\n",
      "[30]\teval-error:0.209924\ttrain-error:0.066848\n",
      "[31]\teval-error:0.206107\ttrain-error:0.066848\n",
      "[32]\teval-error:0.206107\ttrain-error:0.066848\n",
      "[33]\teval-error:0.209924\ttrain-error:0.066848\n",
      "[34]\teval-error:0.217557\ttrain-error:0.066848\n",
      "[35]\teval-error:0.209924\ttrain-error:0.066848\n",
      "[36]\teval-error:0.209924\ttrain-error:0.066848\n",
      "[37]\teval-error:0.206107\ttrain-error:0.066848\n",
      "[38]\teval-error:0.206107\ttrain-error:0.066848\n",
      "[39]\teval-error:0.209924\ttrain-error:0.066848\n",
      "[40]\teval-error:0.209924\ttrain-error:0.066848\n",
      "[41]\teval-error:0.206107\ttrain-error:0.066848\n",
      "[42]\teval-error:0.209924\ttrain-error:0.066848\n",
      "[43]\teval-error:0.20229\ttrain-error:0.066848\n",
      "[44]\teval-error:0.20229\ttrain-error:0.066848\n",
      "[45]\teval-error:0.206107\ttrain-error:0.066848\n",
      "[46]\teval-error:0.20229\ttrain-error:0.066848\n",
      "[47]\teval-error:0.198473\ttrain-error:0.066848\n",
      "[48]\teval-error:0.198473\ttrain-error:0.066848\n",
      "[49]\teval-error:0.20229\ttrain-error:0.066848\n",
      "[50]\teval-error:0.198473\ttrain-error:0.066848\n",
      "[51]\teval-error:0.21374\ttrain-error:0.066848\n",
      "[52]\teval-error:0.206107\ttrain-error:0.066848\n",
      "[53]\teval-error:0.20229\ttrain-error:0.066848\n",
      "[54]\teval-error:0.206107\ttrain-error:0.066848\n",
      "[55]\teval-error:0.206107\ttrain-error:0.066848\n",
      "[56]\teval-error:0.20229\ttrain-error:0.066848\n",
      "[57]\teval-error:0.206107\ttrain-error:0.066848\n",
      "[58]\teval-error:0.206107\ttrain-error:0.066848\n",
      "[59]\teval-error:0.206107\ttrain-error:0.066848\n",
      "[60]\teval-error:0.206107\ttrain-error:0.066848\n",
      "[61]\teval-error:0.206107\ttrain-error:0.066848\n",
      "[62]\teval-error:0.20229\ttrain-error:0.066848\n",
      "[63]\teval-error:0.20229\ttrain-error:0.066848\n",
      "[64]\teval-error:0.20229\ttrain-error:0.066848\n",
      "[65]\teval-error:0.20229\ttrain-error:0.066848\n",
      "[66]\teval-error:0.20229\ttrain-error:0.066848\n",
      "[67]\teval-error:0.198473\ttrain-error:0.066848\n",
      "[68]\teval-error:0.206107\ttrain-error:0.066848\n",
      "[69]\teval-error:0.20229\ttrain-error:0.066848\n",
      "[0]\teval-error:0.236641\ttrain-error:0.095415\n",
      "[1]\teval-error:0.251908\ttrain-error:0.082631\n",
      "[2]\teval-error:0.251908\ttrain-error:0.075132\n",
      "[3]\teval-error:0.263359\ttrain-error:0.070776\n",
      "[4]\teval-error:0.251908\ttrain-error:0.06899\n",
      "[5]\teval-error:0.248092\ttrain-error:0.068347\n",
      "[6]\teval-error:0.259542\ttrain-error:0.06749\n",
      "[7]\teval-error:0.251908\ttrain-error:0.067348\n",
      "[8]\teval-error:0.259542\ttrain-error:0.067348\n",
      "[9]\teval-error:0.251908\ttrain-error:0.067348\n",
      "[10]\teval-error:0.255725\ttrain-error:0.067348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11]\teval-error:0.263359\ttrain-error:0.067348\n",
      "[12]\teval-error:0.240458\ttrain-error:0.067276\n",
      "[13]\teval-error:0.240458\ttrain-error:0.067276\n",
      "[14]\teval-error:0.251908\ttrain-error:0.067276\n",
      "[15]\teval-error:0.240458\ttrain-error:0.067276\n",
      "[16]\teval-error:0.236641\ttrain-error:0.067276\n",
      "[17]\teval-error:0.248092\ttrain-error:0.067276\n",
      "[18]\teval-error:0.267176\ttrain-error:0.067276\n",
      "[19]\teval-error:0.263359\ttrain-error:0.067276\n",
      "[20]\teval-error:0.259542\ttrain-error:0.067276\n",
      "[21]\teval-error:0.255725\ttrain-error:0.067276\n",
      "[22]\teval-error:0.248092\ttrain-error:0.067276\n",
      "[23]\teval-error:0.248092\ttrain-error:0.067276\n",
      "[24]\teval-error:0.248092\ttrain-error:0.067276\n",
      "[25]\teval-error:0.251908\ttrain-error:0.067276\n",
      "[26]\teval-error:0.248092\ttrain-error:0.067276\n",
      "[27]\teval-error:0.251908\ttrain-error:0.067276\n",
      "[28]\teval-error:0.255725\ttrain-error:0.067276\n",
      "[29]\teval-error:0.251908\ttrain-error:0.067276\n",
      "[30]\teval-error:0.248092\ttrain-error:0.067276\n",
      "[31]\teval-error:0.248092\ttrain-error:0.067276\n",
      "[32]\teval-error:0.248092\ttrain-error:0.067276\n",
      "[33]\teval-error:0.244275\ttrain-error:0.067276\n",
      "[34]\teval-error:0.244275\ttrain-error:0.067276\n",
      "[35]\teval-error:0.244275\ttrain-error:0.067276\n",
      "[36]\teval-error:0.240458\ttrain-error:0.067276\n",
      "[37]\teval-error:0.244275\ttrain-error:0.067276\n",
      "[38]\teval-error:0.248092\ttrain-error:0.067276\n",
      "[39]\teval-error:0.244275\ttrain-error:0.067276\n",
      "[40]\teval-error:0.248092\ttrain-error:0.067276\n",
      "[41]\teval-error:0.251908\ttrain-error:0.067276\n",
      "[42]\teval-error:0.255725\ttrain-error:0.067276\n",
      "[43]\teval-error:0.251908\ttrain-error:0.067276\n",
      "[44]\teval-error:0.248092\ttrain-error:0.067276\n",
      "[45]\teval-error:0.251908\ttrain-error:0.067276\n",
      "[46]\teval-error:0.251908\ttrain-error:0.067276\n",
      "[47]\teval-error:0.255725\ttrain-error:0.067276\n",
      "[48]\teval-error:0.251908\ttrain-error:0.067276\n",
      "[49]\teval-error:0.248092\ttrain-error:0.067276\n",
      "[50]\teval-error:0.251908\ttrain-error:0.067276\n",
      "[51]\teval-error:0.259542\ttrain-error:0.067276\n",
      "[52]\teval-error:0.255725\ttrain-error:0.067276\n",
      "[53]\teval-error:0.251908\ttrain-error:0.067276\n",
      "[54]\teval-error:0.255725\ttrain-error:0.067276\n",
      "[55]\teval-error:0.251908\ttrain-error:0.067276\n",
      "[56]\teval-error:0.251908\ttrain-error:0.067276\n",
      "[57]\teval-error:0.251908\ttrain-error:0.067276\n",
      "[58]\teval-error:0.255725\ttrain-error:0.067276\n",
      "[59]\teval-error:0.255725\ttrain-error:0.067276\n",
      "[60]\teval-error:0.259542\ttrain-error:0.067276\n",
      "[61]\teval-error:0.259542\ttrain-error:0.067276\n",
      "[62]\teval-error:0.251908\ttrain-error:0.067276\n",
      "[63]\teval-error:0.255725\ttrain-error:0.067276\n",
      "[64]\teval-error:0.255725\ttrain-error:0.067276\n",
      "[65]\teval-error:0.255725\ttrain-error:0.067276\n",
      "[66]\teval-error:0.259542\ttrain-error:0.067276\n",
      "[67]\teval-error:0.251908\ttrain-error:0.067276\n",
      "[68]\teval-error:0.259542\ttrain-error:0.067276\n",
      "[69]\teval-error:0.259542\ttrain-error:0.067276\n",
      "[0]\teval-error:0.293893\ttrain-error:0.098129\n",
      "[1]\teval-error:0.267176\ttrain-error:0.086059\n",
      "[2]\teval-error:0.263359\ttrain-error:0.076989\n",
      "[3]\teval-error:0.267176\ttrain-error:0.072204\n",
      "[4]\teval-error:0.278626\ttrain-error:0.070133\n",
      "[5]\teval-error:0.267176\ttrain-error:0.069204\n",
      "[6]\teval-error:0.270992\ttrain-error:0.069204\n",
      "[7]\teval-error:0.278626\ttrain-error:0.06899\n",
      "[8]\teval-error:0.267176\ttrain-error:0.068776\n",
      "[9]\teval-error:0.270992\ttrain-error:0.068776\n",
      "[10]\teval-error:0.267176\ttrain-error:0.068847\n",
      "[11]\teval-error:0.267176\ttrain-error:0.068776\n",
      "[12]\teval-error:0.255725\ttrain-error:0.068776\n",
      "[13]\teval-error:0.255725\ttrain-error:0.068776\n",
      "[14]\teval-error:0.267176\ttrain-error:0.068776\n",
      "[15]\teval-error:0.263359\ttrain-error:0.068776\n",
      "[16]\teval-error:0.259542\ttrain-error:0.068776\n",
      "[17]\teval-error:0.248092\ttrain-error:0.068776\n",
      "[18]\teval-error:0.255725\ttrain-error:0.068776\n",
      "[19]\teval-error:0.251908\ttrain-error:0.068776\n",
      "[20]\teval-error:0.259542\ttrain-error:0.068776\n",
      "[21]\teval-error:0.263359\ttrain-error:0.068776\n",
      "[22]\teval-error:0.255725\ttrain-error:0.068776\n",
      "[23]\teval-error:0.274809\ttrain-error:0.068776\n",
      "[24]\teval-error:0.263359\ttrain-error:0.068776\n",
      "[25]\teval-error:0.267176\ttrain-error:0.068776\n",
      "[26]\teval-error:0.263359\ttrain-error:0.068776\n",
      "[27]\teval-error:0.263359\ttrain-error:0.068776\n",
      "[28]\teval-error:0.251908\ttrain-error:0.068776\n",
      "[29]\teval-error:0.259542\ttrain-error:0.068776\n",
      "[30]\teval-error:0.259542\ttrain-error:0.068776\n",
      "[31]\teval-error:0.255725\ttrain-error:0.068776\n",
      "[32]\teval-error:0.259542\ttrain-error:0.068776\n",
      "[33]\teval-error:0.259542\ttrain-error:0.068776\n",
      "[34]\teval-error:0.259542\ttrain-error:0.068776\n",
      "[35]\teval-error:0.267176\ttrain-error:0.068776\n",
      "[36]\teval-error:0.263359\ttrain-error:0.068776\n",
      "[37]\teval-error:0.263359\ttrain-error:0.068776\n",
      "[38]\teval-error:0.263359\ttrain-error:0.068776\n",
      "[39]\teval-error:0.259542\ttrain-error:0.068776\n",
      "[40]\teval-error:0.263359\ttrain-error:0.068776\n",
      "[41]\teval-error:0.255725\ttrain-error:0.068776\n",
      "[42]\teval-error:0.267176\ttrain-error:0.068776\n",
      "[43]\teval-error:0.270992\ttrain-error:0.068776\n",
      "[44]\teval-error:0.267176\ttrain-error:0.068776\n",
      "[45]\teval-error:0.248092\ttrain-error:0.068776\n",
      "[46]\teval-error:0.251908\ttrain-error:0.068776\n",
      "[47]\teval-error:0.251908\ttrain-error:0.068776\n",
      "[48]\teval-error:0.259542\ttrain-error:0.068776\n",
      "[49]\teval-error:0.251908\ttrain-error:0.068776\n",
      "[50]\teval-error:0.255725\ttrain-error:0.068776\n",
      "[51]\teval-error:0.248092\ttrain-error:0.068776\n",
      "[52]\teval-error:0.255725\ttrain-error:0.068776\n",
      "[53]\teval-error:0.259542\ttrain-error:0.068776\n",
      "[54]\teval-error:0.263359\ttrain-error:0.068776\n",
      "[55]\teval-error:0.263359\ttrain-error:0.068776\n",
      "[56]\teval-error:0.259542\ttrain-error:0.068776\n",
      "[57]\teval-error:0.251908\ttrain-error:0.068776\n",
      "[58]\teval-error:0.255725\ttrain-error:0.068776\n",
      "[59]\teval-error:0.263359\ttrain-error:0.068776\n",
      "[60]\teval-error:0.259542\ttrain-error:0.068776\n",
      "[61]\teval-error:0.251908\ttrain-error:0.068776\n",
      "[62]\teval-error:0.259542\ttrain-error:0.068776\n",
      "[63]\teval-error:0.255725\ttrain-error:0.068776\n",
      "[64]\teval-error:0.263359\ttrain-error:0.068776\n",
      "[65]\teval-error:0.259542\ttrain-error:0.068776\n",
      "[66]\teval-error:0.255725\ttrain-error:0.068776\n",
      "[67]\teval-error:0.263359\ttrain-error:0.068776\n",
      "[68]\teval-error:0.263359\ttrain-error:0.068776\n",
      "[69]\teval-error:0.255725\ttrain-error:0.068776\n",
      "[0]\teval-error:0.240458\ttrain-error:0.094629\n",
      "[1]\teval-error:0.225191\ttrain-error:0.080631\n",
      "[2]\teval-error:0.244275\ttrain-error:0.073204\n",
      "[3]\teval-error:0.248092\ttrain-error:0.070061\n",
      "[4]\teval-error:0.248092\ttrain-error:0.068133\n",
      "[5]\teval-error:0.248092\ttrain-error:0.067276\n",
      "[6]\teval-error:0.240458\ttrain-error:0.067062\n",
      "[7]\teval-error:0.244275\ttrain-error:0.066848\n",
      "[8]\teval-error:0.251908\ttrain-error:0.066848\n",
      "[9]\teval-error:0.244275\ttrain-error:0.066776\n",
      "[10]\teval-error:0.255725\ttrain-error:0.066776\n",
      "[11]\teval-error:0.251908\ttrain-error:0.066776\n",
      "[12]\teval-error:0.263359\ttrain-error:0.066776\n",
      "[13]\teval-error:0.244275\ttrain-error:0.066776\n",
      "[14]\teval-error:0.236641\ttrain-error:0.066776\n",
      "[15]\teval-error:0.236641\ttrain-error:0.066776\n",
      "[16]\teval-error:0.232824\ttrain-error:0.066776\n",
      "[17]\teval-error:0.244275\ttrain-error:0.066776\n",
      "[18]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[19]\teval-error:0.240458\ttrain-error:0.066776\n",
      "[20]\teval-error:0.244275\ttrain-error:0.066776\n",
      "[21]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[22]\teval-error:0.236641\ttrain-error:0.066776\n",
      "[23]\teval-error:0.236641\ttrain-error:0.066776\n",
      "[24]\teval-error:0.236641\ttrain-error:0.066776\n",
      "[25]\teval-error:0.236641\ttrain-error:0.066776\n",
      "[26]\teval-error:0.244275\ttrain-error:0.066776\n",
      "[27]\teval-error:0.240458\ttrain-error:0.066776\n",
      "[28]\teval-error:0.244275\ttrain-error:0.066776\n",
      "[29]\teval-error:0.240458\ttrain-error:0.066776\n",
      "[30]\teval-error:0.232824\ttrain-error:0.066776\n",
      "[31]\teval-error:0.244275\ttrain-error:0.066776\n",
      "[32]\teval-error:0.240458\ttrain-error:0.066776\n",
      "[33]\teval-error:0.244275\ttrain-error:0.066776\n",
      "[34]\teval-error:0.240458\ttrain-error:0.066776\n",
      "[35]\teval-error:0.244275\ttrain-error:0.066776\n",
      "[36]\teval-error:0.244275\ttrain-error:0.066776\n",
      "[37]\teval-error:0.244275\ttrain-error:0.066776\n",
      "[38]\teval-error:0.240458\ttrain-error:0.066776\n",
      "[39]\teval-error:0.240458\ttrain-error:0.066776\n",
      "[40]\teval-error:0.251908\ttrain-error:0.066776\n",
      "[41]\teval-error:0.244275\ttrain-error:0.066776\n",
      "[42]\teval-error:0.240458\ttrain-error:0.066776\n",
      "[43]\teval-error:0.240458\ttrain-error:0.066776\n",
      "[44]\teval-error:0.244275\ttrain-error:0.066776\n",
      "[45]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[46]\teval-error:0.240458\ttrain-error:0.066776\n",
      "[47]\teval-error:0.240458\ttrain-error:0.066776\n",
      "[48]\teval-error:0.240458\ttrain-error:0.066776\n",
      "[49]\teval-error:0.244275\ttrain-error:0.066776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[51]\teval-error:0.244275\ttrain-error:0.066776\n",
      "[52]\teval-error:0.251908\ttrain-error:0.066776\n",
      "[53]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[54]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[55]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[56]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[57]\teval-error:0.240458\ttrain-error:0.066776\n",
      "[58]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[59]\teval-error:0.236641\ttrain-error:0.066776\n",
      "[60]\teval-error:0.244275\ttrain-error:0.066776\n",
      "[61]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[62]\teval-error:0.244275\ttrain-error:0.066776\n",
      "[63]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[64]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[65]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[66]\teval-error:0.251908\ttrain-error:0.066776\n",
      "[67]\teval-error:0.244275\ttrain-error:0.066776\n",
      "[68]\teval-error:0.251908\ttrain-error:0.066776\n",
      "[69]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[0]\teval-error:0.409091\ttrain-error:0.124482\n",
      "[1]\teval-error:0.284091\ttrain-error:0.087912\n",
      "[2]\teval-error:0.284091\ttrain-error:0.067555\n",
      "[3]\teval-error:0.306818\ttrain-error:0.058008\n",
      "[4]\teval-error:0.284091\ttrain-error:0.054765\n",
      "[5]\teval-error:0.284091\ttrain-error:0.054405\n",
      "[6]\teval-error:0.284091\ttrain-error:0.054405\n",
      "[7]\teval-error:0.238636\ttrain-error:0.054405\n",
      "[8]\teval-error:0.261364\ttrain-error:0.054405\n",
      "[9]\teval-error:0.272727\ttrain-error:0.054405\n",
      "[10]\teval-error:0.25\ttrain-error:0.054405\n",
      "[11]\teval-error:0.272727\ttrain-error:0.054405\n",
      "[12]\teval-error:0.25\ttrain-error:0.054405\n",
      "[13]\teval-error:0.272727\ttrain-error:0.054405\n",
      "[14]\teval-error:0.261364\ttrain-error:0.054405\n",
      "[15]\teval-error:0.25\ttrain-error:0.054405\n",
      "[16]\teval-error:0.272727\ttrain-error:0.054405\n",
      "[17]\teval-error:0.284091\ttrain-error:0.054405\n",
      "[18]\teval-error:0.272727\ttrain-error:0.054405\n",
      "[19]\teval-error:0.261364\ttrain-error:0.054405\n",
      "[20]\teval-error:0.261364\ttrain-error:0.054405\n",
      "[21]\teval-error:0.261364\ttrain-error:0.054405\n",
      "[22]\teval-error:0.261364\ttrain-error:0.054405\n",
      "[23]\teval-error:0.261364\ttrain-error:0.054405\n",
      "[24]\teval-error:0.261364\ttrain-error:0.054405\n",
      "[25]\teval-error:0.261364\ttrain-error:0.054405\n",
      "[26]\teval-error:0.25\ttrain-error:0.054405\n",
      "[27]\teval-error:0.261364\ttrain-error:0.054405\n",
      "[28]\teval-error:0.261364\ttrain-error:0.054405\n",
      "[29]\teval-error:0.261364\ttrain-error:0.054405\n",
      "[30]\teval-error:0.25\ttrain-error:0.054405\n",
      "[31]\teval-error:0.25\ttrain-error:0.054405\n",
      "[32]\teval-error:0.238636\ttrain-error:0.054405\n",
      "[33]\teval-error:0.238636\ttrain-error:0.054405\n",
      "[34]\teval-error:0.25\ttrain-error:0.054405\n",
      "[35]\teval-error:0.238636\ttrain-error:0.054405\n",
      "[36]\teval-error:0.238636\ttrain-error:0.054405\n",
      "[37]\teval-error:0.238636\ttrain-error:0.054405\n",
      "[38]\teval-error:0.238636\ttrain-error:0.054405\n",
      "[39]\teval-error:0.227273\ttrain-error:0.054405\n",
      "[40]\teval-error:0.25\ttrain-error:0.054405\n",
      "[41]\teval-error:0.25\ttrain-error:0.054405\n",
      "[42]\teval-error:0.25\ttrain-error:0.054405\n",
      "[43]\teval-error:0.238636\ttrain-error:0.054405\n",
      "[44]\teval-error:0.238636\ttrain-error:0.054405\n",
      "[45]\teval-error:0.238636\ttrain-error:0.054405\n",
      "[46]\teval-error:0.238636\ttrain-error:0.054405\n",
      "[47]\teval-error:0.238636\ttrain-error:0.054405\n",
      "[48]\teval-error:0.238636\ttrain-error:0.054405\n",
      "[49]\teval-error:0.238636\ttrain-error:0.054405\n",
      "[50]\teval-error:0.238636\ttrain-error:0.054405\n",
      "[51]\teval-error:0.238636\ttrain-error:0.054405\n",
      "[52]\teval-error:0.238636\ttrain-error:0.054405\n",
      "[53]\teval-error:0.238636\ttrain-error:0.054405\n",
      "[54]\teval-error:0.238636\ttrain-error:0.054405\n",
      "[55]\teval-error:0.238636\ttrain-error:0.054405\n",
      "[56]\teval-error:0.227273\ttrain-error:0.054405\n",
      "[57]\teval-error:0.227273\ttrain-error:0.054405\n",
      "[58]\teval-error:0.227273\ttrain-error:0.054405\n",
      "[59]\teval-error:0.227273\ttrain-error:0.054405\n",
      "[60]\teval-error:0.227273\ttrain-error:0.054405\n",
      "[61]\teval-error:0.227273\ttrain-error:0.054405\n",
      "[62]\teval-error:0.227273\ttrain-error:0.054405\n",
      "[63]\teval-error:0.227273\ttrain-error:0.054405\n",
      "[64]\teval-error:0.227273\ttrain-error:0.054405\n",
      "[65]\teval-error:0.227273\ttrain-error:0.054405\n",
      "[66]\teval-error:0.227273\ttrain-error:0.054405\n",
      "[67]\teval-error:0.227273\ttrain-error:0.054405\n",
      "[68]\teval-error:0.238636\ttrain-error:0.054405\n",
      "[69]\teval-error:0.238636\ttrain-error:0.054405\n",
      "[0]\teval-error:0.340909\ttrain-error:0.125923\n",
      "[1]\teval-error:0.329545\ttrain-error:0.089533\n",
      "[2]\teval-error:0.329545\ttrain-error:0.066835\n",
      "[3]\teval-error:0.306818\ttrain-error:0.059629\n",
      "[4]\teval-error:0.340909\ttrain-error:0.056386\n",
      "[5]\teval-error:0.352273\ttrain-error:0.055485\n",
      "[6]\teval-error:0.340909\ttrain-error:0.054765\n",
      "[7]\teval-error:0.318182\ttrain-error:0.054765\n",
      "[8]\teval-error:0.295455\ttrain-error:0.054585\n",
      "[9]\teval-error:0.306818\ttrain-error:0.054585\n",
      "[10]\teval-error:0.306818\ttrain-error:0.054585\n",
      "[11]\teval-error:0.340909\ttrain-error:0.054585\n",
      "[12]\teval-error:0.329545\ttrain-error:0.054585\n",
      "[13]\teval-error:0.329545\ttrain-error:0.054585\n",
      "[14]\teval-error:0.318182\ttrain-error:0.054585\n",
      "[15]\teval-error:0.329545\ttrain-error:0.054585\n",
      "[16]\teval-error:0.318182\ttrain-error:0.054585\n",
      "[17]\teval-error:0.329545\ttrain-error:0.054585\n",
      "[18]\teval-error:0.352273\ttrain-error:0.054585\n",
      "[19]\teval-error:0.329545\ttrain-error:0.054585\n",
      "[20]\teval-error:0.340909\ttrain-error:0.054585\n",
      "[21]\teval-error:0.340909\ttrain-error:0.054585\n",
      "[22]\teval-error:0.340909\ttrain-error:0.054585\n",
      "[23]\teval-error:0.329545\ttrain-error:0.054585\n",
      "[24]\teval-error:0.329545\ttrain-error:0.054585\n",
      "[25]\teval-error:0.318182\ttrain-error:0.054585\n",
      "[26]\teval-error:0.318182\ttrain-error:0.054585\n",
      "[27]\teval-error:0.329545\ttrain-error:0.054585\n",
      "[28]\teval-error:0.340909\ttrain-error:0.054585\n",
      "[29]\teval-error:0.340909\ttrain-error:0.054585\n",
      "[30]\teval-error:0.329545\ttrain-error:0.054585\n",
      "[31]\teval-error:0.340909\ttrain-error:0.054585\n",
      "[32]\teval-error:0.340909\ttrain-error:0.054585\n",
      "[33]\teval-error:0.329545\ttrain-error:0.054585\n",
      "[34]\teval-error:0.329545\ttrain-error:0.054585\n",
      "[35]\teval-error:0.329545\ttrain-error:0.054585\n",
      "[36]\teval-error:0.329545\ttrain-error:0.054585\n",
      "[37]\teval-error:0.329545\ttrain-error:0.054585\n",
      "[38]\teval-error:0.329545\ttrain-error:0.054585\n",
      "[39]\teval-error:0.329545\ttrain-error:0.054585\n",
      "[40]\teval-error:0.340909\ttrain-error:0.054585\n",
      "[41]\teval-error:0.329545\ttrain-error:0.054585\n",
      "[42]\teval-error:0.340909\ttrain-error:0.054585\n",
      "[43]\teval-error:0.340909\ttrain-error:0.054585\n",
      "[44]\teval-error:0.340909\ttrain-error:0.054585\n",
      "[45]\teval-error:0.340909\ttrain-error:0.054585\n",
      "[46]\teval-error:0.340909\ttrain-error:0.054585\n",
      "[47]\teval-error:0.329545\ttrain-error:0.054585\n",
      "[48]\teval-error:0.318182\ttrain-error:0.054585\n",
      "[49]\teval-error:0.318182\ttrain-error:0.054585\n",
      "[50]\teval-error:0.318182\ttrain-error:0.054585\n",
      "[51]\teval-error:0.329545\ttrain-error:0.054585\n",
      "[52]\teval-error:0.340909\ttrain-error:0.054585\n",
      "[53]\teval-error:0.340909\ttrain-error:0.054585\n",
      "[54]\teval-error:0.329545\ttrain-error:0.054585\n",
      "[55]\teval-error:0.340909\ttrain-error:0.054585\n",
      "[56]\teval-error:0.329545\ttrain-error:0.054585\n",
      "[57]\teval-error:0.329545\ttrain-error:0.054585\n",
      "[58]\teval-error:0.329545\ttrain-error:0.054585\n",
      "[59]\teval-error:0.340909\ttrain-error:0.054585\n",
      "[60]\teval-error:0.318182\ttrain-error:0.054585\n",
      "[61]\teval-error:0.340909\ttrain-error:0.054585\n",
      "[62]\teval-error:0.340909\ttrain-error:0.054585\n",
      "[63]\teval-error:0.340909\ttrain-error:0.054585\n",
      "[64]\teval-error:0.329545\ttrain-error:0.054585\n",
      "[65]\teval-error:0.340909\ttrain-error:0.054585\n",
      "[66]\teval-error:0.318182\ttrain-error:0.054585\n",
      "[67]\teval-error:0.340909\ttrain-error:0.054585\n",
      "[68]\teval-error:0.340909\ttrain-error:0.054585\n",
      "[69]\teval-error:0.340909\ttrain-error:0.054585\n",
      "[0]\teval-error:0.431818\ttrain-error:0.1225\n",
      "[1]\teval-error:0.386364\ttrain-error:0.090614\n",
      "[2]\teval-error:0.386364\ttrain-error:0.065754\n",
      "[3]\teval-error:0.340909\ttrain-error:0.058908\n",
      "[4]\teval-error:0.363636\ttrain-error:0.056386\n",
      "[5]\teval-error:0.340909\ttrain-error:0.054765\n",
      "[6]\teval-error:0.386364\ttrain-error:0.054585\n",
      "[7]\teval-error:0.352273\ttrain-error:0.054585\n",
      "[8]\teval-error:0.352273\ttrain-error:0.054585\n",
      "[9]\teval-error:0.375\ttrain-error:0.054405\n",
      "[10]\teval-error:0.363636\ttrain-error:0.054405\n",
      "[11]\teval-error:0.386364\ttrain-error:0.054405\n",
      "[12]\teval-error:0.363636\ttrain-error:0.054405\n",
      "[13]\teval-error:0.340909\ttrain-error:0.054405\n",
      "[14]\teval-error:0.340909\ttrain-error:0.054405\n",
      "[15]\teval-error:0.340909\ttrain-error:0.054405\n",
      "[16]\teval-error:0.340909\ttrain-error:0.054405\n",
      "[17]\teval-error:0.352273\ttrain-error:0.054405\n",
      "[18]\teval-error:0.352273\ttrain-error:0.054405\n",
      "[19]\teval-error:0.329545\ttrain-error:0.054405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20]\teval-error:0.329545\ttrain-error:0.054405\n",
      "[21]\teval-error:0.318182\ttrain-error:0.054405\n",
      "[22]\teval-error:0.318182\ttrain-error:0.054405\n",
      "[23]\teval-error:0.329545\ttrain-error:0.054405\n",
      "[24]\teval-error:0.329545\ttrain-error:0.054405\n",
      "[25]\teval-error:0.306818\ttrain-error:0.054405\n",
      "[26]\teval-error:0.306818\ttrain-error:0.054405\n",
      "[27]\teval-error:0.318182\ttrain-error:0.054405\n",
      "[28]\teval-error:0.329545\ttrain-error:0.054405\n",
      "[29]\teval-error:0.329545\ttrain-error:0.054405\n",
      "[30]\teval-error:0.329545\ttrain-error:0.054405\n",
      "[31]\teval-error:0.318182\ttrain-error:0.054405\n",
      "[32]\teval-error:0.329545\ttrain-error:0.054405\n",
      "[33]\teval-error:0.329545\ttrain-error:0.054405\n",
      "[34]\teval-error:0.306818\ttrain-error:0.054405\n",
      "[35]\teval-error:0.306818\ttrain-error:0.054405\n",
      "[36]\teval-error:0.318182\ttrain-error:0.054405\n",
      "[37]\teval-error:0.306818\ttrain-error:0.054405\n",
      "[38]\teval-error:0.318182\ttrain-error:0.054405\n",
      "[39]\teval-error:0.318182\ttrain-error:0.054405\n",
      "[40]\teval-error:0.306818\ttrain-error:0.054405\n",
      "[41]\teval-error:0.318182\ttrain-error:0.054405\n",
      "[42]\teval-error:0.318182\ttrain-error:0.054405\n",
      "[43]\teval-error:0.318182\ttrain-error:0.054405\n",
      "[44]\teval-error:0.329545\ttrain-error:0.054405\n",
      "[45]\teval-error:0.329545\ttrain-error:0.054405\n",
      "[46]\teval-error:0.318182\ttrain-error:0.054405\n",
      "[47]\teval-error:0.306818\ttrain-error:0.054405\n",
      "[48]\teval-error:0.306818\ttrain-error:0.054405\n",
      "[49]\teval-error:0.329545\ttrain-error:0.054405\n",
      "[50]\teval-error:0.329545\ttrain-error:0.054405\n",
      "[51]\teval-error:0.329545\ttrain-error:0.054405\n",
      "[52]\teval-error:0.329545\ttrain-error:0.054405\n",
      "[53]\teval-error:0.329545\ttrain-error:0.054405\n",
      "[54]\teval-error:0.329545\ttrain-error:0.054405\n",
      "[55]\teval-error:0.329545\ttrain-error:0.054405\n",
      "[56]\teval-error:0.329545\ttrain-error:0.054405\n",
      "[57]\teval-error:0.329545\ttrain-error:0.054405\n",
      "[58]\teval-error:0.329545\ttrain-error:0.054405\n",
      "[59]\teval-error:0.329545\ttrain-error:0.054405\n",
      "[60]\teval-error:0.329545\ttrain-error:0.054405\n",
      "[61]\teval-error:0.318182\ttrain-error:0.054405\n",
      "[62]\teval-error:0.318182\ttrain-error:0.054405\n",
      "[63]\teval-error:0.318182\ttrain-error:0.054405\n",
      "[64]\teval-error:0.318182\ttrain-error:0.054405\n",
      "[65]\teval-error:0.329545\ttrain-error:0.054405\n",
      "[66]\teval-error:0.318182\ttrain-error:0.054405\n",
      "[67]\teval-error:0.318182\ttrain-error:0.054405\n",
      "[68]\teval-error:0.318182\ttrain-error:0.054405\n",
      "[69]\teval-error:0.329545\ttrain-error:0.054405\n",
      "[0]\teval-error:0.375\ttrain-error:0.124122\n",
      "[1]\teval-error:0.306818\ttrain-error:0.08485\n",
      "[2]\teval-error:0.306818\ttrain-error:0.065934\n",
      "[3]\teval-error:0.306818\ttrain-error:0.057467\n",
      "[4]\teval-error:0.284091\ttrain-error:0.054945\n",
      "[5]\teval-error:0.306818\ttrain-error:0.054585\n",
      "[6]\teval-error:0.340909\ttrain-error:0.054405\n",
      "[7]\teval-error:0.306818\ttrain-error:0.054585\n",
      "[8]\teval-error:0.318182\ttrain-error:0.054405\n",
      "[9]\teval-error:0.329545\ttrain-error:0.054405\n",
      "[10]\teval-error:0.295455\ttrain-error:0.054405\n",
      "[11]\teval-error:0.318182\ttrain-error:0.054405\n",
      "[12]\teval-error:0.340909\ttrain-error:0.054405\n",
      "[13]\teval-error:0.306818\ttrain-error:0.054405\n",
      "[14]\teval-error:0.318182\ttrain-error:0.054405\n",
      "[15]\teval-error:0.318182\ttrain-error:0.054405\n",
      "[16]\teval-error:0.318182\ttrain-error:0.054405\n",
      "[17]\teval-error:0.306818\ttrain-error:0.054405\n",
      "[18]\teval-error:0.329545\ttrain-error:0.054405\n",
      "[19]\teval-error:0.329545\ttrain-error:0.054405\n",
      "[20]\teval-error:0.318182\ttrain-error:0.054405\n",
      "[21]\teval-error:0.306818\ttrain-error:0.054405\n",
      "[22]\teval-error:0.318182\ttrain-error:0.054405\n",
      "[23]\teval-error:0.318182\ttrain-error:0.054405\n",
      "[24]\teval-error:0.318182\ttrain-error:0.054405\n",
      "[25]\teval-error:0.306818\ttrain-error:0.054405\n",
      "[26]\teval-error:0.306818\ttrain-error:0.054405\n",
      "[27]\teval-error:0.306818\ttrain-error:0.054405\n",
      "[28]\teval-error:0.295455\ttrain-error:0.054405\n",
      "[29]\teval-error:0.295455\ttrain-error:0.054405\n",
      "[30]\teval-error:0.295455\ttrain-error:0.054405\n",
      "[31]\teval-error:0.284091\ttrain-error:0.054405\n",
      "[32]\teval-error:0.284091\ttrain-error:0.054405\n",
      "[33]\teval-error:0.295455\ttrain-error:0.054405\n",
      "[34]\teval-error:0.284091\ttrain-error:0.054405\n",
      "[35]\teval-error:0.295455\ttrain-error:0.054405\n",
      "[36]\teval-error:0.295455\ttrain-error:0.054405\n",
      "[37]\teval-error:0.306818\ttrain-error:0.054405\n",
      "[38]\teval-error:0.295455\ttrain-error:0.054405\n",
      "[39]\teval-error:0.295455\ttrain-error:0.054405\n",
      "[40]\teval-error:0.318182\ttrain-error:0.054405\n",
      "[41]\teval-error:0.306818\ttrain-error:0.054405\n",
      "[42]\teval-error:0.306818\ttrain-error:0.054405\n",
      "[43]\teval-error:0.295455\ttrain-error:0.054405\n",
      "[44]\teval-error:0.295455\ttrain-error:0.054405\n",
      "[45]\teval-error:0.295455\ttrain-error:0.054405\n",
      "[46]\teval-error:0.295455\ttrain-error:0.054405\n",
      "[47]\teval-error:0.295455\ttrain-error:0.054405\n",
      "[48]\teval-error:0.295455\ttrain-error:0.054405\n",
      "[49]\teval-error:0.295455\ttrain-error:0.054405\n",
      "[50]\teval-error:0.295455\ttrain-error:0.054405\n",
      "[51]\teval-error:0.295455\ttrain-error:0.054405\n",
      "[52]\teval-error:0.295455\ttrain-error:0.054405\n",
      "[53]\teval-error:0.295455\ttrain-error:0.054405\n",
      "[54]\teval-error:0.306818\ttrain-error:0.054405\n",
      "[55]\teval-error:0.295455\ttrain-error:0.054405\n",
      "[56]\teval-error:0.306818\ttrain-error:0.054405\n",
      "[57]\teval-error:0.306818\ttrain-error:0.054405\n",
      "[58]\teval-error:0.295455\ttrain-error:0.054405\n",
      "[59]\teval-error:0.306818\ttrain-error:0.054405\n",
      "[60]\teval-error:0.306818\ttrain-error:0.054405\n",
      "[61]\teval-error:0.306818\ttrain-error:0.054405\n",
      "[62]\teval-error:0.318182\ttrain-error:0.054405\n",
      "[63]\teval-error:0.318182\ttrain-error:0.054405\n",
      "[64]\teval-error:0.306818\ttrain-error:0.054405\n",
      "[65]\teval-error:0.295455\ttrain-error:0.054405\n",
      "[66]\teval-error:0.306818\ttrain-error:0.054405\n",
      "[67]\teval-error:0.318182\ttrain-error:0.054405\n",
      "[68]\teval-error:0.306818\ttrain-error:0.054405\n",
      "[69]\teval-error:0.295455\ttrain-error:0.054405\n",
      "[0]\teval-error:0.425532\ttrain-error:0.11748\n",
      "[1]\teval-error:0.329787\ttrain-error:0.091531\n",
      "[2]\teval-error:0.255319\ttrain-error:0.073586\n",
      "[3]\teval-error:0.255319\ttrain-error:0.069068\n",
      "[4]\teval-error:0.265957\ttrain-error:0.067131\n",
      "[5]\teval-error:0.276596\ttrain-error:0.066744\n",
      "[6]\teval-error:0.276596\ttrain-error:0.066486\n",
      "[7]\teval-error:0.265957\ttrain-error:0.066486\n",
      "[8]\teval-error:0.265957\ttrain-error:0.066357\n",
      "[9]\teval-error:0.265957\ttrain-error:0.066357\n",
      "[10]\teval-error:0.234043\ttrain-error:0.066357\n",
      "[11]\teval-error:0.244681\ttrain-error:0.066357\n",
      "[12]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[13]\teval-error:0.265957\ttrain-error:0.066357\n",
      "[14]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[15]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[16]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[17]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[18]\teval-error:0.244681\ttrain-error:0.066357\n",
      "[19]\teval-error:0.244681\ttrain-error:0.066357\n",
      "[20]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[21]\teval-error:0.244681\ttrain-error:0.066357\n",
      "[22]\teval-error:0.244681\ttrain-error:0.066357\n",
      "[23]\teval-error:0.244681\ttrain-error:0.066357\n",
      "[24]\teval-error:0.244681\ttrain-error:0.066357\n",
      "[25]\teval-error:0.244681\ttrain-error:0.066357\n",
      "[26]\teval-error:0.244681\ttrain-error:0.066357\n",
      "[27]\teval-error:0.244681\ttrain-error:0.066357\n",
      "[28]\teval-error:0.244681\ttrain-error:0.066357\n",
      "[29]\teval-error:0.244681\ttrain-error:0.066357\n",
      "[30]\teval-error:0.244681\ttrain-error:0.066357\n",
      "[31]\teval-error:0.244681\ttrain-error:0.066357\n",
      "[32]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[33]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[34]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[35]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[36]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[37]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[38]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[39]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[40]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[41]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[42]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[43]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[44]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[45]\teval-error:0.265957\ttrain-error:0.066357\n",
      "[46]\teval-error:0.265957\ttrain-error:0.066357\n",
      "[47]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[48]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[49]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[50]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[51]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[52]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[53]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[54]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[55]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[56]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[57]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[58]\teval-error:0.255319\ttrain-error:0.066357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[59]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[60]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[61]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[62]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[63]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[64]\teval-error:0.244681\ttrain-error:0.066357\n",
      "[65]\teval-error:0.244681\ttrain-error:0.066357\n",
      "[66]\teval-error:0.244681\ttrain-error:0.066357\n",
      "[67]\teval-error:0.244681\ttrain-error:0.066357\n",
      "[68]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[69]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[0]\teval-error:0.244681\ttrain-error:0.12174\n",
      "[1]\teval-error:0.244681\ttrain-error:0.095275\n",
      "[2]\teval-error:0.244681\ttrain-error:0.078879\n",
      "[3]\teval-error:0.212766\ttrain-error:0.071521\n",
      "[4]\teval-error:0.234043\ttrain-error:0.06881\n",
      "[5]\teval-error:0.234043\ttrain-error:0.067777\n",
      "[6]\teval-error:0.244681\ttrain-error:0.067261\n",
      "[7]\teval-error:0.255319\ttrain-error:0.066873\n",
      "[8]\teval-error:0.255319\ttrain-error:0.066744\n",
      "[9]\teval-error:0.265957\ttrain-error:0.066615\n",
      "[10]\teval-error:0.234043\ttrain-error:0.066486\n",
      "[11]\teval-error:0.255319\ttrain-error:0.066486\n",
      "[12]\teval-error:0.244681\ttrain-error:0.066357\n",
      "[13]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[14]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[15]\teval-error:0.255319\ttrain-error:0.066486\n",
      "[16]\teval-error:0.265957\ttrain-error:0.066357\n",
      "[17]\teval-error:0.265957\ttrain-error:0.066357\n",
      "[18]\teval-error:0.276596\ttrain-error:0.066357\n",
      "[19]\teval-error:0.276596\ttrain-error:0.066357\n",
      "[20]\teval-error:0.265957\ttrain-error:0.066357\n",
      "[21]\teval-error:0.265957\ttrain-error:0.066357\n",
      "[22]\teval-error:0.276596\ttrain-error:0.066357\n",
      "[23]\teval-error:0.276596\ttrain-error:0.066357\n",
      "[24]\teval-error:0.276596\ttrain-error:0.066357\n",
      "[25]\teval-error:0.265957\ttrain-error:0.066357\n",
      "[26]\teval-error:0.276596\ttrain-error:0.066357\n",
      "[27]\teval-error:0.265957\ttrain-error:0.066357\n",
      "[28]\teval-error:0.265957\ttrain-error:0.066357\n",
      "[29]\teval-error:0.265957\ttrain-error:0.066357\n",
      "[30]\teval-error:0.265957\ttrain-error:0.066357\n",
      "[31]\teval-error:0.265957\ttrain-error:0.066357\n",
      "[32]\teval-error:0.265957\ttrain-error:0.066357\n",
      "[33]\teval-error:0.265957\ttrain-error:0.066357\n",
      "[34]\teval-error:0.265957\ttrain-error:0.066357\n",
      "[35]\teval-error:0.276596\ttrain-error:0.066357\n",
      "[36]\teval-error:0.276596\ttrain-error:0.066357\n",
      "[37]\teval-error:0.276596\ttrain-error:0.066357\n",
      "[38]\teval-error:0.276596\ttrain-error:0.066357\n",
      "[39]\teval-error:0.276596\ttrain-error:0.066357\n",
      "[40]\teval-error:0.276596\ttrain-error:0.066357\n",
      "[41]\teval-error:0.308511\ttrain-error:0.066357\n",
      "[42]\teval-error:0.276596\ttrain-error:0.066357\n",
      "[43]\teval-error:0.308511\ttrain-error:0.066357\n",
      "[44]\teval-error:0.276596\ttrain-error:0.066357\n",
      "[45]\teval-error:0.276596\ttrain-error:0.066357\n",
      "[46]\teval-error:0.265957\ttrain-error:0.066357\n",
      "[47]\teval-error:0.265957\ttrain-error:0.066357\n",
      "[48]\teval-error:0.287234\ttrain-error:0.066357\n",
      "[49]\teval-error:0.287234\ttrain-error:0.066357\n",
      "[50]\teval-error:0.287234\ttrain-error:0.066357\n",
      "[51]\teval-error:0.287234\ttrain-error:0.066357\n",
      "[52]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[53]\teval-error:0.287234\ttrain-error:0.066357\n",
      "[54]\teval-error:0.287234\ttrain-error:0.066357\n",
      "[55]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[56]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[57]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[58]\teval-error:0.287234\ttrain-error:0.066357\n",
      "[59]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[60]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[61]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[62]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[63]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[64]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[65]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[66]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[67]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[68]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[69]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[0]\teval-error:0.287234\ttrain-error:0.119287\n",
      "[1]\teval-error:0.276596\ttrain-error:0.092306\n",
      "[2]\teval-error:0.308511\ttrain-error:0.076168\n",
      "[3]\teval-error:0.319149\ttrain-error:0.069584\n",
      "[4]\teval-error:0.265957\ttrain-error:0.067906\n",
      "[5]\teval-error:0.265957\ttrain-error:0.066873\n",
      "[6]\teval-error:0.287234\ttrain-error:0.066744\n",
      "[7]\teval-error:0.265957\ttrain-error:0.066615\n",
      "[8]\teval-error:0.255319\ttrain-error:0.066486\n",
      "[9]\teval-error:0.265957\ttrain-error:0.066615\n",
      "[10]\teval-error:0.276596\ttrain-error:0.066486\n",
      "[11]\teval-error:0.276596\ttrain-error:0.066486\n",
      "[12]\teval-error:0.287234\ttrain-error:0.066486\n",
      "[13]\teval-error:0.287234\ttrain-error:0.066357\n",
      "[14]\teval-error:0.297872\ttrain-error:0.066486\n",
      "[15]\teval-error:0.287234\ttrain-error:0.066357\n",
      "[16]\teval-error:0.287234\ttrain-error:0.066357\n",
      "[17]\teval-error:0.287234\ttrain-error:0.066357\n",
      "[18]\teval-error:0.276596\ttrain-error:0.066357\n",
      "[19]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[20]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[21]\teval-error:0.329787\ttrain-error:0.066357\n",
      "[22]\teval-error:0.308511\ttrain-error:0.066357\n",
      "[23]\teval-error:0.308511\ttrain-error:0.066357\n",
      "[24]\teval-error:0.308511\ttrain-error:0.066357\n",
      "[25]\teval-error:0.308511\ttrain-error:0.066357\n",
      "[26]\teval-error:0.308511\ttrain-error:0.066357\n",
      "[27]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[28]\teval-error:0.287234\ttrain-error:0.066357\n",
      "[29]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[30]\teval-error:0.308511\ttrain-error:0.066357\n",
      "[31]\teval-error:0.308511\ttrain-error:0.066357\n",
      "[32]\teval-error:0.308511\ttrain-error:0.066357\n",
      "[33]\teval-error:0.308511\ttrain-error:0.066357\n",
      "[34]\teval-error:0.308511\ttrain-error:0.066357\n",
      "[35]\teval-error:0.308511\ttrain-error:0.066357\n",
      "[36]\teval-error:0.308511\ttrain-error:0.066357\n",
      "[37]\teval-error:0.308511\ttrain-error:0.066357\n",
      "[38]\teval-error:0.308511\ttrain-error:0.066357\n",
      "[39]\teval-error:0.308511\ttrain-error:0.066357\n",
      "[40]\teval-error:0.308511\ttrain-error:0.066357\n",
      "[41]\teval-error:0.308511\ttrain-error:0.066357\n",
      "[42]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[43]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[44]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[45]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[46]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[47]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[48]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[49]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[50]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[51]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[52]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[53]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[54]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[55]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[56]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[57]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[58]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[59]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[60]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[61]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[62]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[63]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[64]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[65]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[66]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[67]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[68]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[69]\teval-error:0.297872\ttrain-error:0.066357\n",
      "[0]\teval-error:0.287234\ttrain-error:0.114898\n",
      "[1]\teval-error:0.265957\ttrain-error:0.088691\n",
      "[2]\teval-error:0.255319\ttrain-error:0.075781\n",
      "[3]\teval-error:0.255319\ttrain-error:0.069713\n",
      "[4]\teval-error:0.276596\ttrain-error:0.067131\n",
      "[5]\teval-error:0.265957\ttrain-error:0.066615\n",
      "[6]\teval-error:0.287234\ttrain-error:0.066615\n",
      "[7]\teval-error:0.265957\ttrain-error:0.066486\n",
      "[8]\teval-error:0.276596\ttrain-error:0.066486\n",
      "[9]\teval-error:0.276596\ttrain-error:0.066357\n",
      "[10]\teval-error:0.265957\ttrain-error:0.066357\n",
      "[11]\teval-error:0.265957\ttrain-error:0.066357\n",
      "[12]\teval-error:0.265957\ttrain-error:0.066357\n",
      "[13]\teval-error:0.265957\ttrain-error:0.066357\n",
      "[14]\teval-error:0.276596\ttrain-error:0.066357\n",
      "[15]\teval-error:0.287234\ttrain-error:0.066357\n",
      "[16]\teval-error:0.276596\ttrain-error:0.066357\n",
      "[17]\teval-error:0.276596\ttrain-error:0.066357\n",
      "[18]\teval-error:0.265957\ttrain-error:0.066357\n",
      "[19]\teval-error:0.265957\ttrain-error:0.066357\n",
      "[20]\teval-error:0.265957\ttrain-error:0.066357\n",
      "[21]\teval-error:0.265957\ttrain-error:0.066357\n",
      "[22]\teval-error:0.244681\ttrain-error:0.066357\n",
      "[23]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[24]\teval-error:0.244681\ttrain-error:0.066357\n",
      "[25]\teval-error:0.265957\ttrain-error:0.066357\n",
      "[26]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[27]\teval-error:0.265957\ttrain-error:0.066357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[29]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[30]\teval-error:0.244681\ttrain-error:0.066357\n",
      "[31]\teval-error:0.244681\ttrain-error:0.066357\n",
      "[32]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[33]\teval-error:0.244681\ttrain-error:0.066357\n",
      "[34]\teval-error:0.244681\ttrain-error:0.066357\n",
      "[35]\teval-error:0.244681\ttrain-error:0.066357\n",
      "[36]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[37]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[38]\teval-error:0.244681\ttrain-error:0.066357\n",
      "[39]\teval-error:0.244681\ttrain-error:0.066357\n",
      "[40]\teval-error:0.234043\ttrain-error:0.066357\n",
      "[41]\teval-error:0.244681\ttrain-error:0.066357\n",
      "[42]\teval-error:0.234043\ttrain-error:0.066357\n",
      "[43]\teval-error:0.234043\ttrain-error:0.066357\n",
      "[44]\teval-error:0.244681\ttrain-error:0.066357\n",
      "[45]\teval-error:0.244681\ttrain-error:0.066357\n",
      "[46]\teval-error:0.234043\ttrain-error:0.066357\n",
      "[47]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[48]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[49]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[50]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[51]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[52]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[53]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[54]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[55]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[56]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[57]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[58]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[59]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[60]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[61]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[62]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[63]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[64]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[65]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[66]\teval-error:0.265957\ttrain-error:0.066357\n",
      "[67]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[68]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[69]\teval-error:0.255319\ttrain-error:0.066357\n",
      "[0]\teval-error:0.312977\ttrain-error:0.101557\n",
      "[1]\teval-error:0.29771\ttrain-error:0.085274\n",
      "[2]\teval-error:0.278626\ttrain-error:0.075132\n",
      "[3]\teval-error:0.290076\ttrain-error:0.070133\n",
      "[4]\teval-error:0.301527\ttrain-error:0.067919\n",
      "[5]\teval-error:0.290076\ttrain-error:0.067419\n",
      "[6]\teval-error:0.293893\ttrain-error:0.067133\n",
      "[7]\teval-error:0.301527\ttrain-error:0.066919\n",
      "[8]\teval-error:0.301527\ttrain-error:0.066848\n",
      "[9]\teval-error:0.290076\ttrain-error:0.066776\n",
      "[10]\teval-error:0.301527\ttrain-error:0.066776\n",
      "[11]\teval-error:0.282443\ttrain-error:0.066776\n",
      "[12]\teval-error:0.29771\ttrain-error:0.066776\n",
      "[13]\teval-error:0.290076\ttrain-error:0.066776\n",
      "[14]\teval-error:0.28626\ttrain-error:0.066776\n",
      "[15]\teval-error:0.290076\ttrain-error:0.066776\n",
      "[16]\teval-error:0.290076\ttrain-error:0.066776\n",
      "[17]\teval-error:0.290076\ttrain-error:0.066776\n",
      "[18]\teval-error:0.28626\ttrain-error:0.066776\n",
      "[19]\teval-error:0.282443\ttrain-error:0.066776\n",
      "[20]\teval-error:0.293893\ttrain-error:0.066776\n",
      "[21]\teval-error:0.28626\ttrain-error:0.066776\n",
      "[22]\teval-error:0.293893\ttrain-error:0.066776\n",
      "[23]\teval-error:0.290076\ttrain-error:0.066776\n",
      "[24]\teval-error:0.282443\ttrain-error:0.066776\n",
      "[25]\teval-error:0.290076\ttrain-error:0.066776\n",
      "[26]\teval-error:0.290076\ttrain-error:0.066776\n",
      "[27]\teval-error:0.293893\ttrain-error:0.066776\n",
      "[28]\teval-error:0.293893\ttrain-error:0.066776\n",
      "[29]\teval-error:0.293893\ttrain-error:0.066776\n",
      "[30]\teval-error:0.28626\ttrain-error:0.066776\n",
      "[31]\teval-error:0.290076\ttrain-error:0.066776\n",
      "[32]\teval-error:0.290076\ttrain-error:0.066776\n",
      "[33]\teval-error:0.293893\ttrain-error:0.066776\n",
      "[34]\teval-error:0.29771\ttrain-error:0.066776\n",
      "[35]\teval-error:0.29771\ttrain-error:0.066776\n",
      "[36]\teval-error:0.293893\ttrain-error:0.066776\n",
      "[37]\teval-error:0.293893\ttrain-error:0.066776\n",
      "[38]\teval-error:0.293893\ttrain-error:0.066776\n",
      "[39]\teval-error:0.282443\ttrain-error:0.066776\n",
      "[40]\teval-error:0.293893\ttrain-error:0.066776\n",
      "[41]\teval-error:0.282443\ttrain-error:0.066776\n",
      "[42]\teval-error:0.290076\ttrain-error:0.066776\n",
      "[43]\teval-error:0.290076\ttrain-error:0.066776\n",
      "[44]\teval-error:0.293893\ttrain-error:0.066776\n",
      "[45]\teval-error:0.28626\ttrain-error:0.066776\n",
      "[46]\teval-error:0.290076\ttrain-error:0.066776\n",
      "[47]\teval-error:0.290076\ttrain-error:0.066776\n",
      "[48]\teval-error:0.290076\ttrain-error:0.066776\n",
      "[49]\teval-error:0.28626\ttrain-error:0.066776\n",
      "[50]\teval-error:0.290076\ttrain-error:0.066776\n",
      "[51]\teval-error:0.290076\ttrain-error:0.066776\n",
      "[52]\teval-error:0.28626\ttrain-error:0.066776\n",
      "[53]\teval-error:0.282443\ttrain-error:0.066776\n",
      "[54]\teval-error:0.290076\ttrain-error:0.066776\n",
      "[55]\teval-error:0.29771\ttrain-error:0.066776\n",
      "[56]\teval-error:0.290076\ttrain-error:0.066776\n",
      "[57]\teval-error:0.29771\ttrain-error:0.066776\n",
      "[58]\teval-error:0.28626\ttrain-error:0.066776\n",
      "[59]\teval-error:0.290076\ttrain-error:0.066776\n",
      "[60]\teval-error:0.28626\ttrain-error:0.066776\n",
      "[61]\teval-error:0.290076\ttrain-error:0.066776\n",
      "[62]\teval-error:0.290076\ttrain-error:0.066776\n",
      "[63]\teval-error:0.29771\ttrain-error:0.066776\n",
      "[64]\teval-error:0.290076\ttrain-error:0.066776\n",
      "[65]\teval-error:0.28626\ttrain-error:0.066776\n",
      "[66]\teval-error:0.290076\ttrain-error:0.066776\n",
      "[67]\teval-error:0.28626\ttrain-error:0.066776\n",
      "[68]\teval-error:0.28626\ttrain-error:0.066776\n",
      "[69]\teval-error:0.28626\ttrain-error:0.066776\n",
      "[0]\teval-error:0.30916\ttrain-error:0.102343\n",
      "[1]\teval-error:0.28626\ttrain-error:0.088345\n",
      "[2]\teval-error:0.267176\ttrain-error:0.076418\n",
      "[3]\teval-error:0.263359\ttrain-error:0.071847\n",
      "[4]\teval-error:0.290076\ttrain-error:0.069133\n",
      "[5]\teval-error:0.267176\ttrain-error:0.068276\n",
      "[6]\teval-error:0.28626\ttrain-error:0.067847\n",
      "[7]\teval-error:0.282443\ttrain-error:0.06749\n",
      "[8]\teval-error:0.282443\ttrain-error:0.067062\n",
      "[9]\teval-error:0.274809\ttrain-error:0.06699\n",
      "[10]\teval-error:0.267176\ttrain-error:0.067062\n",
      "[11]\teval-error:0.267176\ttrain-error:0.066919\n",
      "[12]\teval-error:0.270992\ttrain-error:0.066848\n",
      "[13]\teval-error:0.270992\ttrain-error:0.066848\n",
      "[14]\teval-error:0.267176\ttrain-error:0.066776\n",
      "[15]\teval-error:0.263359\ttrain-error:0.066776\n",
      "[16]\teval-error:0.270992\ttrain-error:0.066776\n",
      "[17]\teval-error:0.270992\ttrain-error:0.066776\n",
      "[18]\teval-error:0.270992\ttrain-error:0.066776\n",
      "[19]\teval-error:0.259542\ttrain-error:0.066776\n",
      "[20]\teval-error:0.270992\ttrain-error:0.066776\n",
      "[21]\teval-error:0.267176\ttrain-error:0.066776\n",
      "[22]\teval-error:0.274809\ttrain-error:0.066776\n",
      "[23]\teval-error:0.274809\ttrain-error:0.066776\n",
      "[24]\teval-error:0.270992\ttrain-error:0.066776\n",
      "[25]\teval-error:0.274809\ttrain-error:0.066776\n",
      "[26]\teval-error:0.267176\ttrain-error:0.066776\n",
      "[27]\teval-error:0.263359\ttrain-error:0.066776\n",
      "[28]\teval-error:0.259542\ttrain-error:0.066776\n",
      "[29]\teval-error:0.255725\ttrain-error:0.066776\n",
      "[30]\teval-error:0.263359\ttrain-error:0.066776\n",
      "[31]\teval-error:0.263359\ttrain-error:0.066776\n",
      "[32]\teval-error:0.255725\ttrain-error:0.066776\n",
      "[33]\teval-error:0.263359\ttrain-error:0.066776\n",
      "[34]\teval-error:0.255725\ttrain-error:0.066776\n",
      "[35]\teval-error:0.251908\ttrain-error:0.066776\n",
      "[36]\teval-error:0.259542\ttrain-error:0.066776\n",
      "[37]\teval-error:0.255725\ttrain-error:0.066776\n",
      "[38]\teval-error:0.251908\ttrain-error:0.066776\n",
      "[39]\teval-error:0.251908\ttrain-error:0.066776\n",
      "[40]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[41]\teval-error:0.259542\ttrain-error:0.066776\n",
      "[42]\teval-error:0.259542\ttrain-error:0.066776\n",
      "[43]\teval-error:0.259542\ttrain-error:0.066776\n",
      "[44]\teval-error:0.259542\ttrain-error:0.066776\n",
      "[45]\teval-error:0.263359\ttrain-error:0.066776\n",
      "[46]\teval-error:0.255725\ttrain-error:0.066776\n",
      "[47]\teval-error:0.263359\ttrain-error:0.066776\n",
      "[48]\teval-error:0.263359\ttrain-error:0.066776\n",
      "[49]\teval-error:0.263359\ttrain-error:0.066776\n",
      "[50]\teval-error:0.259542\ttrain-error:0.066776\n",
      "[51]\teval-error:0.263359\ttrain-error:0.066776\n",
      "[52]\teval-error:0.267176\ttrain-error:0.066776\n",
      "[53]\teval-error:0.255725\ttrain-error:0.066776\n",
      "[54]\teval-error:0.267176\ttrain-error:0.066776\n",
      "[55]\teval-error:0.270992\ttrain-error:0.066776\n",
      "[56]\teval-error:0.259542\ttrain-error:0.066776\n",
      "[57]\teval-error:0.263359\ttrain-error:0.066776\n",
      "[58]\teval-error:0.259542\ttrain-error:0.066776\n",
      "[59]\teval-error:0.259542\ttrain-error:0.066776\n",
      "[60]\teval-error:0.255725\ttrain-error:0.066776\n",
      "[61]\teval-error:0.259542\ttrain-error:0.066776\n",
      "[62]\teval-error:0.255725\ttrain-error:0.066776\n",
      "[63]\teval-error:0.251908\ttrain-error:0.066776\n",
      "[64]\teval-error:0.263359\ttrain-error:0.066776\n",
      "[65]\teval-error:0.259542\ttrain-error:0.066776\n",
      "[66]\teval-error:0.255725\ttrain-error:0.066776\n",
      "[67]\teval-error:0.259542\ttrain-error:0.066776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[68]\teval-error:0.259542\ttrain-error:0.066776\n",
      "[69]\teval-error:0.259542\ttrain-error:0.066776\n",
      "[0]\teval-error:0.290076\ttrain-error:0.101628\n",
      "[1]\teval-error:0.244275\ttrain-error:0.086845\n",
      "[2]\teval-error:0.251908\ttrain-error:0.075418\n",
      "[3]\teval-error:0.255725\ttrain-error:0.071204\n",
      "[4]\teval-error:0.259542\ttrain-error:0.068919\n",
      "[5]\teval-error:0.251908\ttrain-error:0.067776\n",
      "[6]\teval-error:0.270992\ttrain-error:0.067133\n",
      "[7]\teval-error:0.267176\ttrain-error:0.066848\n",
      "[8]\teval-error:0.282443\ttrain-error:0.066919\n",
      "[9]\teval-error:0.270992\ttrain-error:0.066776\n",
      "[10]\teval-error:0.282443\ttrain-error:0.066776\n",
      "[11]\teval-error:0.278626\ttrain-error:0.066776\n",
      "[12]\teval-error:0.278626\ttrain-error:0.066776\n",
      "[13]\teval-error:0.274809\ttrain-error:0.066776\n",
      "[14]\teval-error:0.278626\ttrain-error:0.066776\n",
      "[15]\teval-error:0.270992\ttrain-error:0.066776\n",
      "[16]\teval-error:0.274809\ttrain-error:0.066776\n",
      "[17]\teval-error:0.270992\ttrain-error:0.066776\n",
      "[18]\teval-error:0.293893\ttrain-error:0.066776\n",
      "[19]\teval-error:0.263359\ttrain-error:0.066776\n",
      "[20]\teval-error:0.263359\ttrain-error:0.066776\n",
      "[21]\teval-error:0.251908\ttrain-error:0.066776\n",
      "[22]\teval-error:0.259542\ttrain-error:0.066776\n",
      "[23]\teval-error:0.263359\ttrain-error:0.066776\n",
      "[24]\teval-error:0.263359\ttrain-error:0.066776\n",
      "[25]\teval-error:0.263359\ttrain-error:0.066776\n",
      "[26]\teval-error:0.267176\ttrain-error:0.066776\n",
      "[27]\teval-error:0.259542\ttrain-error:0.066776\n",
      "[28]\teval-error:0.267176\ttrain-error:0.066776\n",
      "[29]\teval-error:0.267176\ttrain-error:0.066776\n",
      "[30]\teval-error:0.263359\ttrain-error:0.066776\n",
      "[31]\teval-error:0.251908\ttrain-error:0.066776\n",
      "[32]\teval-error:0.263359\ttrain-error:0.066776\n",
      "[33]\teval-error:0.259542\ttrain-error:0.066776\n",
      "[34]\teval-error:0.255725\ttrain-error:0.066776\n",
      "[35]\teval-error:0.244275\ttrain-error:0.066776\n",
      "[36]\teval-error:0.263359\ttrain-error:0.066776\n",
      "[37]\teval-error:0.255725\ttrain-error:0.066776\n",
      "[38]\teval-error:0.255725\ttrain-error:0.066776\n",
      "[39]\teval-error:0.255725\ttrain-error:0.066776\n",
      "[40]\teval-error:0.255725\ttrain-error:0.066776\n",
      "[41]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[42]\teval-error:0.255725\ttrain-error:0.066776\n",
      "[43]\teval-error:0.259542\ttrain-error:0.066776\n",
      "[44]\teval-error:0.259542\ttrain-error:0.066776\n",
      "[45]\teval-error:0.255725\ttrain-error:0.066776\n",
      "[46]\teval-error:0.263359\ttrain-error:0.066776\n",
      "[47]\teval-error:0.259542\ttrain-error:0.066776\n",
      "[48]\teval-error:0.259542\ttrain-error:0.066776\n",
      "[49]\teval-error:0.259542\ttrain-error:0.066776\n",
      "[50]\teval-error:0.255725\ttrain-error:0.066776\n",
      "[51]\teval-error:0.259542\ttrain-error:0.066776\n",
      "[52]\teval-error:0.251908\ttrain-error:0.066776\n",
      "[53]\teval-error:0.255725\ttrain-error:0.066776\n",
      "[54]\teval-error:0.255725\ttrain-error:0.066776\n",
      "[55]\teval-error:0.255725\ttrain-error:0.066776\n",
      "[56]\teval-error:0.251908\ttrain-error:0.066776\n",
      "[57]\teval-error:0.255725\ttrain-error:0.066776\n",
      "[58]\teval-error:0.255725\ttrain-error:0.066776\n",
      "[59]\teval-error:0.259542\ttrain-error:0.066776\n",
      "[60]\teval-error:0.251908\ttrain-error:0.066776\n",
      "[61]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[62]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[63]\teval-error:0.244275\ttrain-error:0.066776\n",
      "[64]\teval-error:0.251908\ttrain-error:0.066776\n",
      "[65]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[66]\teval-error:0.255725\ttrain-error:0.066776\n",
      "[67]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[68]\teval-error:0.259542\ttrain-error:0.066776\n",
      "[69]\teval-error:0.255725\ttrain-error:0.066776\n",
      "[0]\teval-error:0.328244\ttrain-error:0.100771\n",
      "[1]\teval-error:0.282443\ttrain-error:0.084345\n",
      "[2]\teval-error:0.267176\ttrain-error:0.074561\n",
      "[3]\teval-error:0.274809\ttrain-error:0.069062\n",
      "[4]\teval-error:0.270992\ttrain-error:0.067776\n",
      "[5]\teval-error:0.270992\ttrain-error:0.067133\n",
      "[6]\teval-error:0.259542\ttrain-error:0.066919\n",
      "[7]\teval-error:0.251908\ttrain-error:0.066848\n",
      "[8]\teval-error:0.248092\ttrain-error:0.066848\n",
      "[9]\teval-error:0.255725\ttrain-error:0.066848\n",
      "[10]\teval-error:0.240458\ttrain-error:0.066776\n",
      "[11]\teval-error:0.240458\ttrain-error:0.066776\n",
      "[12]\teval-error:0.259542\ttrain-error:0.066776\n",
      "[13]\teval-error:0.244275\ttrain-error:0.066776\n",
      "[14]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[15]\teval-error:0.255725\ttrain-error:0.066776\n",
      "[16]\teval-error:0.240458\ttrain-error:0.066776\n",
      "[17]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[18]\teval-error:0.255725\ttrain-error:0.066776\n",
      "[19]\teval-error:0.251908\ttrain-error:0.066776\n",
      "[20]\teval-error:0.267176\ttrain-error:0.066776\n",
      "[21]\teval-error:0.251908\ttrain-error:0.066776\n",
      "[22]\teval-error:0.259542\ttrain-error:0.066776\n",
      "[23]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[24]\teval-error:0.240458\ttrain-error:0.066776\n",
      "[25]\teval-error:0.236641\ttrain-error:0.066776\n",
      "[26]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[27]\teval-error:0.240458\ttrain-error:0.066776\n",
      "[28]\teval-error:0.244275\ttrain-error:0.066776\n",
      "[29]\teval-error:0.236641\ttrain-error:0.066776\n",
      "[30]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[31]\teval-error:0.244275\ttrain-error:0.066776\n",
      "[32]\teval-error:0.240458\ttrain-error:0.066776\n",
      "[33]\teval-error:0.232824\ttrain-error:0.066776\n",
      "[34]\teval-error:0.232824\ttrain-error:0.066776\n",
      "[35]\teval-error:0.232824\ttrain-error:0.066776\n",
      "[36]\teval-error:0.240458\ttrain-error:0.066776\n",
      "[37]\teval-error:0.236641\ttrain-error:0.066776\n",
      "[38]\teval-error:0.229008\ttrain-error:0.066776\n",
      "[39]\teval-error:0.229008\ttrain-error:0.066776\n",
      "[40]\teval-error:0.229008\ttrain-error:0.066776\n",
      "[41]\teval-error:0.236641\ttrain-error:0.066776\n",
      "[42]\teval-error:0.229008\ttrain-error:0.066776\n",
      "[43]\teval-error:0.240458\ttrain-error:0.066776\n",
      "[44]\teval-error:0.232824\ttrain-error:0.066776\n",
      "[45]\teval-error:0.240458\ttrain-error:0.066776\n",
      "[46]\teval-error:0.236641\ttrain-error:0.066776\n",
      "[47]\teval-error:0.232824\ttrain-error:0.066776\n",
      "[48]\teval-error:0.236641\ttrain-error:0.066776\n",
      "[49]\teval-error:0.236641\ttrain-error:0.066776\n",
      "[50]\teval-error:0.236641\ttrain-error:0.066776\n",
      "[51]\teval-error:0.236641\ttrain-error:0.066776\n",
      "[52]\teval-error:0.225191\ttrain-error:0.066776\n",
      "[53]\teval-error:0.229008\ttrain-error:0.066776\n",
      "[54]\teval-error:0.225191\ttrain-error:0.066776\n",
      "[55]\teval-error:0.236641\ttrain-error:0.066776\n",
      "[56]\teval-error:0.232824\ttrain-error:0.066776\n",
      "[57]\teval-error:0.232824\ttrain-error:0.066776\n",
      "[58]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[59]\teval-error:0.244275\ttrain-error:0.066776\n",
      "[60]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[61]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[62]\teval-error:0.244275\ttrain-error:0.066776\n",
      "[63]\teval-error:0.236641\ttrain-error:0.066776\n",
      "[64]\teval-error:0.240458\ttrain-error:0.066776\n",
      "[65]\teval-error:0.236641\ttrain-error:0.066776\n",
      "[66]\teval-error:0.240458\ttrain-error:0.066776\n",
      "[67]\teval-error:0.236641\ttrain-error:0.066776\n",
      "[68]\teval-error:0.236641\ttrain-error:0.066776\n",
      "[69]\teval-error:0.244275\ttrain-error:0.066776\n",
      "[0]\teval-error:0.454545\ttrain-error:0.098361\n",
      "[1]\teval-error:0.363636\ttrain-error:0.070978\n",
      "[2]\teval-error:0.272727\ttrain-error:0.059269\n",
      "[3]\teval-error:0.284091\ttrain-error:0.055305\n",
      "[4]\teval-error:0.306818\ttrain-error:0.053864\n",
      "[5]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[6]\teval-error:0.261364\ttrain-error:0.053684\n",
      "[7]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[8]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[9]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[10]\teval-error:0.261364\ttrain-error:0.053684\n",
      "[11]\teval-error:0.25\ttrain-error:0.053684\n",
      "[12]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[13]\teval-error:0.261364\ttrain-error:0.053684\n",
      "[14]\teval-error:0.295455\ttrain-error:0.053684\n",
      "[15]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[16]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[17]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[18]\teval-error:0.261364\ttrain-error:0.053684\n",
      "[19]\teval-error:0.261364\ttrain-error:0.053684\n",
      "[20]\teval-error:0.261364\ttrain-error:0.053684\n",
      "[21]\teval-error:0.25\ttrain-error:0.053684\n",
      "[22]\teval-error:0.25\ttrain-error:0.053684\n",
      "[23]\teval-error:0.25\ttrain-error:0.053684\n",
      "[24]\teval-error:0.25\ttrain-error:0.053684\n",
      "[25]\teval-error:0.261364\ttrain-error:0.053684\n",
      "[26]\teval-error:0.261364\ttrain-error:0.053684\n",
      "[27]\teval-error:0.261364\ttrain-error:0.053684\n",
      "[28]\teval-error:0.261364\ttrain-error:0.053684\n",
      "[29]\teval-error:0.261364\ttrain-error:0.053684\n",
      "[30]\teval-error:0.261364\ttrain-error:0.053684\n",
      "[31]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[32]\teval-error:0.25\ttrain-error:0.053684\n",
      "[33]\teval-error:0.261364\ttrain-error:0.053684\n",
      "[34]\teval-error:0.25\ttrain-error:0.053684\n",
      "[35]\teval-error:0.261364\ttrain-error:0.053684\n",
      "[36]\teval-error:0.261364\ttrain-error:0.053684\n",
      "[37]\teval-error:0.261364\ttrain-error:0.053684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38]\teval-error:0.261364\ttrain-error:0.053684\n",
      "[39]\teval-error:0.261364\ttrain-error:0.053684\n",
      "[40]\teval-error:0.25\ttrain-error:0.053684\n",
      "[41]\teval-error:0.261364\ttrain-error:0.053684\n",
      "[42]\teval-error:0.261364\ttrain-error:0.053684\n",
      "[43]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[44]\teval-error:0.261364\ttrain-error:0.053684\n",
      "[45]\teval-error:0.261364\ttrain-error:0.053684\n",
      "[46]\teval-error:0.261364\ttrain-error:0.053684\n",
      "[47]\teval-error:0.261364\ttrain-error:0.053684\n",
      "[48]\teval-error:0.261364\ttrain-error:0.053684\n",
      "[49]\teval-error:0.261364\ttrain-error:0.053684\n",
      "[50]\teval-error:0.261364\ttrain-error:0.053684\n",
      "[51]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[52]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[53]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[54]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[55]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[56]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[57]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[58]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[59]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[60]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[61]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[62]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[63]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[64]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[65]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[66]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[67]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[68]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[69]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[0]\teval-error:0.386364\ttrain-error:0.101603\n",
      "[1]\teval-error:0.329545\ttrain-error:0.070077\n",
      "[2]\teval-error:0.306818\ttrain-error:0.059088\n",
      "[3]\teval-error:0.295455\ttrain-error:0.055666\n",
      "[4]\teval-error:0.295455\ttrain-error:0.053864\n",
      "[5]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[6]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[7]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[8]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[9]\teval-error:0.261364\ttrain-error:0.053684\n",
      "[10]\teval-error:0.25\ttrain-error:0.053684\n",
      "[11]\teval-error:0.261364\ttrain-error:0.053684\n",
      "[12]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[13]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[14]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[15]\teval-error:0.261364\ttrain-error:0.053684\n",
      "[16]\teval-error:0.261364\ttrain-error:0.053684\n",
      "[17]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[18]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[19]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[20]\teval-error:0.261364\ttrain-error:0.053684\n",
      "[21]\teval-error:0.261364\ttrain-error:0.053684\n",
      "[22]\teval-error:0.261364\ttrain-error:0.053684\n",
      "[23]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[24]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[25]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[26]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[27]\teval-error:0.295455\ttrain-error:0.053684\n",
      "[28]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[29]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[30]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[31]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[32]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[33]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[34]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[35]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[36]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[37]\teval-error:0.295455\ttrain-error:0.053684\n",
      "[38]\teval-error:0.295455\ttrain-error:0.053684\n",
      "[39]\teval-error:0.295455\ttrain-error:0.053684\n",
      "[40]\teval-error:0.295455\ttrain-error:0.053684\n",
      "[41]\teval-error:0.295455\ttrain-error:0.053684\n",
      "[42]\teval-error:0.295455\ttrain-error:0.053684\n",
      "[43]\teval-error:0.295455\ttrain-error:0.053684\n",
      "[44]\teval-error:0.295455\ttrain-error:0.053684\n",
      "[45]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[46]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[47]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[48]\teval-error:0.295455\ttrain-error:0.053684\n",
      "[49]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[50]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[51]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[52]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[53]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[54]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[55]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[56]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[57]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[58]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[59]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[60]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[61]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[62]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[63]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[64]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[65]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[66]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[67]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[68]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[69]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[0]\teval-error:0.352273\ttrain-error:0.100342\n",
      "[1]\teval-error:0.306818\ttrain-error:0.07296\n",
      "[2]\teval-error:0.306818\ttrain-error:0.058368\n",
      "[3]\teval-error:0.238636\ttrain-error:0.055125\n",
      "[4]\teval-error:0.25\ttrain-error:0.054405\n",
      "[5]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[6]\teval-error:0.261364\ttrain-error:0.053684\n",
      "[7]\teval-error:0.238636\ttrain-error:0.053684\n",
      "[8]\teval-error:0.227273\ttrain-error:0.053684\n",
      "[9]\teval-error:0.25\ttrain-error:0.053684\n",
      "[10]\teval-error:0.227273\ttrain-error:0.053684\n",
      "[11]\teval-error:0.227273\ttrain-error:0.053684\n",
      "[12]\teval-error:0.227273\ttrain-error:0.053684\n",
      "[13]\teval-error:0.25\ttrain-error:0.053684\n",
      "[14]\teval-error:0.261364\ttrain-error:0.053684\n",
      "[15]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[16]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[17]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[18]\teval-error:0.295455\ttrain-error:0.053684\n",
      "[19]\teval-error:0.306818\ttrain-error:0.053684\n",
      "[20]\teval-error:0.295455\ttrain-error:0.053684\n",
      "[21]\teval-error:0.306818\ttrain-error:0.053684\n",
      "[22]\teval-error:0.295455\ttrain-error:0.053684\n",
      "[23]\teval-error:0.306818\ttrain-error:0.053684\n",
      "[24]\teval-error:0.295455\ttrain-error:0.053684\n",
      "[25]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[26]\teval-error:0.295455\ttrain-error:0.053684\n",
      "[27]\teval-error:0.295455\ttrain-error:0.053684\n",
      "[28]\teval-error:0.295455\ttrain-error:0.053684\n",
      "[29]\teval-error:0.295455\ttrain-error:0.053684\n",
      "[30]\teval-error:0.295455\ttrain-error:0.053684\n",
      "[31]\teval-error:0.295455\ttrain-error:0.053684\n",
      "[32]\teval-error:0.295455\ttrain-error:0.053684\n",
      "[33]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[34]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[35]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[36]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[37]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[38]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[39]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[40]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[41]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[42]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[43]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[44]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[45]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[46]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[47]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[48]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[49]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[50]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[51]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[52]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[53]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[54]\teval-error:0.295455\ttrain-error:0.053684\n",
      "[55]\teval-error:0.295455\ttrain-error:0.053684\n",
      "[56]\teval-error:0.295455\ttrain-error:0.053684\n",
      "[57]\teval-error:0.295455\ttrain-error:0.053684\n",
      "[58]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[59]\teval-error:0.295455\ttrain-error:0.053684\n",
      "[60]\teval-error:0.295455\ttrain-error:0.053684\n",
      "[61]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[62]\teval-error:0.306818\ttrain-error:0.053684\n",
      "[63]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[64]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[65]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[66]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[67]\teval-error:0.272727\ttrain-error:0.053684\n",
      "[68]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[69]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[0]\teval-error:0.363636\ttrain-error:0.102324\n",
      "[1]\teval-error:0.352273\ttrain-error:0.071519\n",
      "[2]\teval-error:0.329545\ttrain-error:0.058728\n",
      "[3]\teval-error:0.329545\ttrain-error:0.054405\n",
      "[4]\teval-error:0.329545\ttrain-error:0.053684\n",
      "[5]\teval-error:0.318182\ttrain-error:0.053684\n",
      "[6]\teval-error:0.284091\ttrain-error:0.053684\n",
      "[7]\teval-error:0.318182\ttrain-error:0.053684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8]\teval-error:0.306818\ttrain-error:0.053684\n",
      "[9]\teval-error:0.318182\ttrain-error:0.053684\n",
      "[10]\teval-error:0.306818\ttrain-error:0.053684\n",
      "[11]\teval-error:0.318182\ttrain-error:0.053684\n",
      "[12]\teval-error:0.329545\ttrain-error:0.053684\n",
      "[13]\teval-error:0.329545\ttrain-error:0.053684\n",
      "[14]\teval-error:0.329545\ttrain-error:0.053684\n",
      "[15]\teval-error:0.318182\ttrain-error:0.053684\n",
      "[16]\teval-error:0.318182\ttrain-error:0.053684\n",
      "[17]\teval-error:0.329545\ttrain-error:0.053684\n",
      "[18]\teval-error:0.318182\ttrain-error:0.053684\n",
      "[19]\teval-error:0.318182\ttrain-error:0.053684\n",
      "[20]\teval-error:0.340909\ttrain-error:0.053684\n",
      "[21]\teval-error:0.329545\ttrain-error:0.053684\n",
      "[22]\teval-error:0.329545\ttrain-error:0.053684\n",
      "[23]\teval-error:0.318182\ttrain-error:0.053684\n",
      "[24]\teval-error:0.329545\ttrain-error:0.053684\n",
      "[25]\teval-error:0.318182\ttrain-error:0.053684\n",
      "[26]\teval-error:0.306818\ttrain-error:0.053684\n",
      "[27]\teval-error:0.306818\ttrain-error:0.053684\n",
      "[28]\teval-error:0.306818\ttrain-error:0.053684\n",
      "[29]\teval-error:0.306818\ttrain-error:0.053684\n",
      "[30]\teval-error:0.329545\ttrain-error:0.053684\n",
      "[31]\teval-error:0.329545\ttrain-error:0.053684\n",
      "[32]\teval-error:0.318182\ttrain-error:0.053684\n",
      "[33]\teval-error:0.318182\ttrain-error:0.053684\n",
      "[34]\teval-error:0.318182\ttrain-error:0.053684\n",
      "[35]\teval-error:0.318182\ttrain-error:0.053684\n",
      "[36]\teval-error:0.318182\ttrain-error:0.053684\n",
      "[37]\teval-error:0.306818\ttrain-error:0.053684\n",
      "[38]\teval-error:0.318182\ttrain-error:0.053684\n",
      "[39]\teval-error:0.318182\ttrain-error:0.053684\n",
      "[40]\teval-error:0.329545\ttrain-error:0.053684\n",
      "[41]\teval-error:0.329545\ttrain-error:0.053684\n",
      "[42]\teval-error:0.318182\ttrain-error:0.053684\n",
      "[43]\teval-error:0.318182\ttrain-error:0.053684\n",
      "[44]\teval-error:0.318182\ttrain-error:0.053684\n",
      "[45]\teval-error:0.329545\ttrain-error:0.053684\n",
      "[46]\teval-error:0.329545\ttrain-error:0.053684\n",
      "[47]\teval-error:0.318182\ttrain-error:0.053684\n",
      "[48]\teval-error:0.318182\ttrain-error:0.053684\n",
      "[49]\teval-error:0.318182\ttrain-error:0.053684\n",
      "[50]\teval-error:0.318182\ttrain-error:0.053684\n",
      "[51]\teval-error:0.295455\ttrain-error:0.053684\n",
      "[52]\teval-error:0.295455\ttrain-error:0.053684\n",
      "[53]\teval-error:0.306818\ttrain-error:0.053684\n",
      "[54]\teval-error:0.295455\ttrain-error:0.053684\n",
      "[55]\teval-error:0.318182\ttrain-error:0.053684\n",
      "[56]\teval-error:0.318182\ttrain-error:0.053684\n",
      "[57]\teval-error:0.306818\ttrain-error:0.053684\n",
      "[58]\teval-error:0.295455\ttrain-error:0.053684\n",
      "[59]\teval-error:0.306818\ttrain-error:0.053684\n",
      "[60]\teval-error:0.318182\ttrain-error:0.053684\n",
      "[61]\teval-error:0.306818\ttrain-error:0.053684\n",
      "[62]\teval-error:0.306818\ttrain-error:0.053684\n",
      "[63]\teval-error:0.306818\ttrain-error:0.053684\n",
      "[64]\teval-error:0.306818\ttrain-error:0.053684\n",
      "[65]\teval-error:0.306818\ttrain-error:0.053684\n",
      "[66]\teval-error:0.306818\ttrain-error:0.053684\n",
      "[67]\teval-error:0.306818\ttrain-error:0.053684\n",
      "[68]\teval-error:0.306818\ttrain-error:0.053684\n",
      "[69]\teval-error:0.306818\ttrain-error:0.053684\n",
      "[0]\teval-error:0.382979\ttrain-error:0.104183\n",
      "[1]\teval-error:0.329787\ttrain-error:0.079138\n",
      "[2]\teval-error:0.37234\ttrain-error:0.068293\n",
      "[3]\teval-error:0.361702\ttrain-error:0.065582\n",
      "[4]\teval-error:0.37234\ttrain-error:0.064549\n",
      "[5]\teval-error:0.329787\ttrain-error:0.064291\n",
      "[6]\teval-error:0.319149\ttrain-error:0.064162\n",
      "[7]\teval-error:0.308511\ttrain-error:0.064162\n",
      "[8]\teval-error:0.319149\ttrain-error:0.064162\n",
      "[9]\teval-error:0.319149\ttrain-error:0.064162\n",
      "[10]\teval-error:0.308511\ttrain-error:0.064162\n",
      "[11]\teval-error:0.308511\ttrain-error:0.064162\n",
      "[12]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[13]\teval-error:0.287234\ttrain-error:0.064162\n",
      "[14]\teval-error:0.287234\ttrain-error:0.064162\n",
      "[15]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[16]\teval-error:0.319149\ttrain-error:0.064162\n",
      "[17]\teval-error:0.319149\ttrain-error:0.064162\n",
      "[18]\teval-error:0.319149\ttrain-error:0.064162\n",
      "[19]\teval-error:0.308511\ttrain-error:0.064162\n",
      "[20]\teval-error:0.308511\ttrain-error:0.064162\n",
      "[21]\teval-error:0.308511\ttrain-error:0.064162\n",
      "[22]\teval-error:0.308511\ttrain-error:0.064162\n",
      "[23]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[24]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[25]\teval-error:0.308511\ttrain-error:0.064162\n",
      "[26]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[27]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[28]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[29]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[30]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[31]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[32]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[33]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[34]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[35]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[36]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[37]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[38]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[39]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[40]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[41]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[42]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[43]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[44]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[45]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[46]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[47]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[48]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[49]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[50]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[51]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[52]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[53]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[54]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[55]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[56]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[57]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[58]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[59]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[60]\teval-error:0.308511\ttrain-error:0.064162\n",
      "[61]\teval-error:0.308511\ttrain-error:0.064162\n",
      "[62]\teval-error:0.308511\ttrain-error:0.064162\n",
      "[63]\teval-error:0.308511\ttrain-error:0.064162\n",
      "[64]\teval-error:0.308511\ttrain-error:0.064162\n",
      "[65]\teval-error:0.319149\ttrain-error:0.064162\n",
      "[66]\teval-error:0.319149\ttrain-error:0.064162\n",
      "[67]\teval-error:0.319149\ttrain-error:0.064162\n",
      "[68]\teval-error:0.308511\ttrain-error:0.064162\n",
      "[69]\teval-error:0.308511\ttrain-error:0.064162\n",
      "[0]\teval-error:0.351064\ttrain-error:0.103666\n",
      "[1]\teval-error:0.319149\ttrain-error:0.080041\n",
      "[2]\teval-error:0.308511\ttrain-error:0.069197\n",
      "[3]\teval-error:0.308511\ttrain-error:0.065453\n",
      "[4]\teval-error:0.287234\ttrain-error:0.064808\n",
      "[5]\teval-error:0.265957\ttrain-error:0.064291\n",
      "[6]\teval-error:0.255319\ttrain-error:0.064162\n",
      "[7]\teval-error:0.244681\ttrain-error:0.064162\n",
      "[8]\teval-error:0.265957\ttrain-error:0.064162\n",
      "[9]\teval-error:0.244681\ttrain-error:0.064162\n",
      "[10]\teval-error:0.244681\ttrain-error:0.064162\n",
      "[11]\teval-error:0.234043\ttrain-error:0.064162\n",
      "[12]\teval-error:0.255319\ttrain-error:0.064162\n",
      "[13]\teval-error:0.255319\ttrain-error:0.064162\n",
      "[14]\teval-error:0.255319\ttrain-error:0.064162\n",
      "[15]\teval-error:0.255319\ttrain-error:0.064162\n",
      "[16]\teval-error:0.255319\ttrain-error:0.064162\n",
      "[17]\teval-error:0.265957\ttrain-error:0.064162\n",
      "[18]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[19]\teval-error:0.265957\ttrain-error:0.064162\n",
      "[20]\teval-error:0.255319\ttrain-error:0.064162\n",
      "[21]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[22]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[23]\teval-error:0.287234\ttrain-error:0.064162\n",
      "[24]\teval-error:0.255319\ttrain-error:0.064162\n",
      "[25]\teval-error:0.265957\ttrain-error:0.064162\n",
      "[26]\teval-error:0.265957\ttrain-error:0.064162\n",
      "[27]\teval-error:0.265957\ttrain-error:0.064162\n",
      "[28]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[29]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[30]\teval-error:0.287234\ttrain-error:0.064162\n",
      "[31]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[32]\teval-error:0.287234\ttrain-error:0.064162\n",
      "[33]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[34]\teval-error:0.287234\ttrain-error:0.064162\n",
      "[35]\teval-error:0.287234\ttrain-error:0.064162\n",
      "[36]\teval-error:0.287234\ttrain-error:0.064162\n",
      "[37]\teval-error:0.287234\ttrain-error:0.064162\n",
      "[38]\teval-error:0.287234\ttrain-error:0.064162\n",
      "[39]\teval-error:0.287234\ttrain-error:0.064162\n",
      "[40]\teval-error:0.287234\ttrain-error:0.064162\n",
      "[41]\teval-error:0.287234\ttrain-error:0.064162\n",
      "[42]\teval-error:0.287234\ttrain-error:0.064162\n",
      "[43]\teval-error:0.287234\ttrain-error:0.064162\n",
      "[44]\teval-error:0.287234\ttrain-error:0.064162\n",
      "[45]\teval-error:0.287234\ttrain-error:0.064162\n",
      "[46]\teval-error:0.287234\ttrain-error:0.064162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47]\teval-error:0.287234\ttrain-error:0.064162\n",
      "[48]\teval-error:0.287234\ttrain-error:0.064162\n",
      "[49]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[50]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[51]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[52]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[53]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[54]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[55]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[56]\teval-error:0.265957\ttrain-error:0.064162\n",
      "[57]\teval-error:0.265957\ttrain-error:0.064162\n",
      "[58]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[59]\teval-error:0.265957\ttrain-error:0.064162\n",
      "[60]\teval-error:0.265957\ttrain-error:0.064162\n",
      "[61]\teval-error:0.265957\ttrain-error:0.064162\n",
      "[62]\teval-error:0.265957\ttrain-error:0.064162\n",
      "[63]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[64]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[65]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[66]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[67]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[68]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[69]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[0]\teval-error:0.382979\ttrain-error:0.102763\n",
      "[1]\teval-error:0.351064\ttrain-error:0.079912\n",
      "[2]\teval-error:0.308511\ttrain-error:0.069972\n",
      "[3]\teval-error:0.319149\ttrain-error:0.066486\n",
      "[4]\teval-error:0.297872\ttrain-error:0.065195\n",
      "[5]\teval-error:0.276596\ttrain-error:0.064679\n",
      "[6]\teval-error:0.287234\ttrain-error:0.06442\n",
      "[7]\teval-error:0.255319\ttrain-error:0.064291\n",
      "[8]\teval-error:0.265957\ttrain-error:0.064162\n",
      "[9]\teval-error:0.265957\ttrain-error:0.064162\n",
      "[10]\teval-error:0.255319\ttrain-error:0.064162\n",
      "[11]\teval-error:0.265957\ttrain-error:0.064162\n",
      "[12]\teval-error:0.265957\ttrain-error:0.064162\n",
      "[13]\teval-error:0.287234\ttrain-error:0.064162\n",
      "[14]\teval-error:0.287234\ttrain-error:0.064162\n",
      "[15]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[16]\teval-error:0.287234\ttrain-error:0.064162\n",
      "[17]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[18]\teval-error:0.287234\ttrain-error:0.064162\n",
      "[19]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[20]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[21]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[22]\teval-error:0.287234\ttrain-error:0.064162\n",
      "[23]\teval-error:0.287234\ttrain-error:0.064162\n",
      "[24]\teval-error:0.287234\ttrain-error:0.064162\n",
      "[25]\teval-error:0.287234\ttrain-error:0.064162\n",
      "[26]\teval-error:0.287234\ttrain-error:0.064162\n",
      "[27]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[28]\teval-error:0.265957\ttrain-error:0.064162\n",
      "[29]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[30]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[31]\teval-error:0.265957\ttrain-error:0.064162\n",
      "[32]\teval-error:0.265957\ttrain-error:0.064162\n",
      "[33]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[34]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[35]\teval-error:0.287234\ttrain-error:0.064162\n",
      "[36]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[37]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[38]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[39]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[40]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[41]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[42]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[43]\teval-error:0.287234\ttrain-error:0.064162\n",
      "[44]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[45]\teval-error:0.287234\ttrain-error:0.064162\n",
      "[46]\teval-error:0.308511\ttrain-error:0.064162\n",
      "[47]\teval-error:0.308511\ttrain-error:0.064162\n",
      "[48]\teval-error:0.287234\ttrain-error:0.064162\n",
      "[49]\teval-error:0.287234\ttrain-error:0.064162\n",
      "[50]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[51]\teval-error:0.308511\ttrain-error:0.064162\n",
      "[52]\teval-error:0.308511\ttrain-error:0.064162\n",
      "[53]\teval-error:0.308511\ttrain-error:0.064162\n",
      "[54]\teval-error:0.308511\ttrain-error:0.064162\n",
      "[55]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[56]\teval-error:0.308511\ttrain-error:0.064162\n",
      "[57]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[58]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[59]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[60]\teval-error:0.308511\ttrain-error:0.064162\n",
      "[61]\teval-error:0.308511\ttrain-error:0.064162\n",
      "[62]\teval-error:0.308511\ttrain-error:0.064162\n",
      "[63]\teval-error:0.308511\ttrain-error:0.064162\n",
      "[64]\teval-error:0.308511\ttrain-error:0.064162\n",
      "[65]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[66]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[67]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[68]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[69]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[0]\teval-error:0.340426\ttrain-error:0.101601\n",
      "[1]\teval-error:0.276596\ttrain-error:0.078105\n",
      "[2]\teval-error:0.276596\ttrain-error:0.068293\n",
      "[3]\teval-error:0.265957\ttrain-error:0.065324\n",
      "[4]\teval-error:0.287234\ttrain-error:0.064937\n",
      "[5]\teval-error:0.308511\ttrain-error:0.064549\n",
      "[6]\teval-error:0.287234\ttrain-error:0.064162\n",
      "[7]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[8]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[9]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[10]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[11]\teval-error:0.287234\ttrain-error:0.064162\n",
      "[12]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[13]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[14]\teval-error:0.265957\ttrain-error:0.064162\n",
      "[15]\teval-error:0.265957\ttrain-error:0.064162\n",
      "[16]\teval-error:0.265957\ttrain-error:0.064162\n",
      "[17]\teval-error:0.255319\ttrain-error:0.064162\n",
      "[18]\teval-error:0.255319\ttrain-error:0.064162\n",
      "[19]\teval-error:0.255319\ttrain-error:0.064162\n",
      "[20]\teval-error:0.265957\ttrain-error:0.064162\n",
      "[21]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[22]\teval-error:0.265957\ttrain-error:0.064162\n",
      "[23]\teval-error:0.255319\ttrain-error:0.064162\n",
      "[24]\teval-error:0.297872\ttrain-error:0.064162\n",
      "[25]\teval-error:0.287234\ttrain-error:0.064162\n",
      "[26]\teval-error:0.287234\ttrain-error:0.064162\n",
      "[27]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[28]\teval-error:0.287234\ttrain-error:0.064162\n",
      "[29]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[30]\teval-error:0.265957\ttrain-error:0.064162\n",
      "[31]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[32]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[33]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[34]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[35]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[36]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[37]\teval-error:0.265957\ttrain-error:0.064162\n",
      "[38]\teval-error:0.265957\ttrain-error:0.064162\n",
      "[39]\teval-error:0.265957\ttrain-error:0.064162\n",
      "[40]\teval-error:0.255319\ttrain-error:0.064162\n",
      "[41]\teval-error:0.255319\ttrain-error:0.064162\n",
      "[42]\teval-error:0.255319\ttrain-error:0.064162\n",
      "[43]\teval-error:0.255319\ttrain-error:0.064162\n",
      "[44]\teval-error:0.255319\ttrain-error:0.064162\n",
      "[45]\teval-error:0.265957\ttrain-error:0.064162\n",
      "[46]\teval-error:0.265957\ttrain-error:0.064162\n",
      "[47]\teval-error:0.265957\ttrain-error:0.064162\n",
      "[48]\teval-error:0.265957\ttrain-error:0.064162\n",
      "[49]\teval-error:0.265957\ttrain-error:0.064162\n",
      "[50]\teval-error:0.265957\ttrain-error:0.064162\n",
      "[51]\teval-error:0.265957\ttrain-error:0.064162\n",
      "[52]\teval-error:0.265957\ttrain-error:0.064162\n",
      "[53]\teval-error:0.265957\ttrain-error:0.064162\n",
      "[54]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[55]\teval-error:0.265957\ttrain-error:0.064162\n",
      "[56]\teval-error:0.265957\ttrain-error:0.064162\n",
      "[57]\teval-error:0.255319\ttrain-error:0.064162\n",
      "[58]\teval-error:0.255319\ttrain-error:0.064162\n",
      "[59]\teval-error:0.255319\ttrain-error:0.064162\n",
      "[60]\teval-error:0.255319\ttrain-error:0.064162\n",
      "[61]\teval-error:0.255319\ttrain-error:0.064162\n",
      "[62]\teval-error:0.255319\ttrain-error:0.064162\n",
      "[63]\teval-error:0.255319\ttrain-error:0.064162\n",
      "[64]\teval-error:0.255319\ttrain-error:0.064162\n",
      "[65]\teval-error:0.255319\ttrain-error:0.064162\n",
      "[66]\teval-error:0.255319\ttrain-error:0.064162\n",
      "[67]\teval-error:0.255319\ttrain-error:0.064162\n",
      "[68]\teval-error:0.255319\ttrain-error:0.064162\n",
      "[69]\teval-error:0.255319\ttrain-error:0.064162\n",
      "[0]\teval-error:0.221374\ttrain-error:0.090987\n",
      "[1]\teval-error:0.217557\ttrain-error:0.077846\n",
      "[2]\teval-error:0.225191\ttrain-error:0.069776\n",
      "[3]\teval-error:0.21374\ttrain-error:0.066919\n",
      "[4]\teval-error:0.221374\ttrain-error:0.065919\n",
      "[5]\teval-error:0.206107\ttrain-error:0.065848\n",
      "[6]\teval-error:0.209924\ttrain-error:0.065776\n",
      "[7]\teval-error:0.209924\ttrain-error:0.065848\n",
      "[8]\teval-error:0.194656\ttrain-error:0.065776\n",
      "[9]\teval-error:0.21374\ttrain-error:0.065776\n",
      "[10]\teval-error:0.217557\ttrain-error:0.065776\n",
      "[11]\teval-error:0.209924\ttrain-error:0.065776\n",
      "[12]\teval-error:0.217557\ttrain-error:0.065776\n",
      "[13]\teval-error:0.198473\ttrain-error:0.065776\n",
      "[14]\teval-error:0.21374\ttrain-error:0.065776\n",
      "[15]\teval-error:0.221374\ttrain-error:0.065776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16]\teval-error:0.217557\ttrain-error:0.065776\n",
      "[17]\teval-error:0.209924\ttrain-error:0.065776\n",
      "[18]\teval-error:0.20229\ttrain-error:0.065776\n",
      "[19]\teval-error:0.21374\ttrain-error:0.065776\n",
      "[20]\teval-error:0.198473\ttrain-error:0.065776\n",
      "[21]\teval-error:0.20229\ttrain-error:0.065776\n",
      "[22]\teval-error:0.21374\ttrain-error:0.065776\n",
      "[23]\teval-error:0.209924\ttrain-error:0.065776\n",
      "[24]\teval-error:0.194656\ttrain-error:0.065776\n",
      "[25]\teval-error:0.198473\ttrain-error:0.065776\n",
      "[26]\teval-error:0.198473\ttrain-error:0.065776\n",
      "[27]\teval-error:0.194656\ttrain-error:0.065776\n",
      "[28]\teval-error:0.194656\ttrain-error:0.065776\n",
      "[29]\teval-error:0.198473\ttrain-error:0.065776\n",
      "[30]\teval-error:0.198473\ttrain-error:0.065776\n",
      "[31]\teval-error:0.194656\ttrain-error:0.065776\n",
      "[32]\teval-error:0.198473\ttrain-error:0.065776\n",
      "[33]\teval-error:0.187023\ttrain-error:0.065776\n",
      "[34]\teval-error:0.194656\ttrain-error:0.065776\n",
      "[35]\teval-error:0.187023\ttrain-error:0.065776\n",
      "[36]\teval-error:0.187023\ttrain-error:0.065776\n",
      "[37]\teval-error:0.194656\ttrain-error:0.065776\n",
      "[38]\teval-error:0.20229\ttrain-error:0.065776\n",
      "[39]\teval-error:0.19084\ttrain-error:0.065776\n",
      "[40]\teval-error:0.194656\ttrain-error:0.065776\n",
      "[41]\teval-error:0.194656\ttrain-error:0.065776\n",
      "[42]\teval-error:0.19084\ttrain-error:0.065776\n",
      "[43]\teval-error:0.19084\ttrain-error:0.065776\n",
      "[44]\teval-error:0.19084\ttrain-error:0.065776\n",
      "[45]\teval-error:0.19084\ttrain-error:0.065776\n",
      "[46]\teval-error:0.19084\ttrain-error:0.065776\n",
      "[47]\teval-error:0.187023\ttrain-error:0.065776\n",
      "[48]\teval-error:0.19084\ttrain-error:0.065776\n",
      "[49]\teval-error:0.194656\ttrain-error:0.065776\n",
      "[50]\teval-error:0.187023\ttrain-error:0.065776\n",
      "[51]\teval-error:0.198473\ttrain-error:0.065776\n",
      "[52]\teval-error:0.194656\ttrain-error:0.065776\n",
      "[53]\teval-error:0.19084\ttrain-error:0.065776\n",
      "[54]\teval-error:0.198473\ttrain-error:0.065776\n",
      "[55]\teval-error:0.194656\ttrain-error:0.065776\n",
      "[56]\teval-error:0.187023\ttrain-error:0.065776\n",
      "[57]\teval-error:0.194656\ttrain-error:0.065776\n",
      "[58]\teval-error:0.194656\ttrain-error:0.065776\n",
      "[59]\teval-error:0.19084\ttrain-error:0.065776\n",
      "[60]\teval-error:0.206107\ttrain-error:0.065776\n",
      "[61]\teval-error:0.198473\ttrain-error:0.065776\n",
      "[62]\teval-error:0.198473\ttrain-error:0.065776\n",
      "[63]\teval-error:0.198473\ttrain-error:0.065776\n",
      "[64]\teval-error:0.194656\ttrain-error:0.065776\n",
      "[65]\teval-error:0.19084\ttrain-error:0.065776\n",
      "[66]\teval-error:0.19084\ttrain-error:0.065776\n",
      "[67]\teval-error:0.20229\ttrain-error:0.065776\n",
      "[68]\teval-error:0.198473\ttrain-error:0.065776\n",
      "[69]\teval-error:0.20229\ttrain-error:0.065776\n",
      "[0]\teval-error:0.274809\ttrain-error:0.087202\n",
      "[1]\teval-error:0.267176\ttrain-error:0.075846\n",
      "[2]\teval-error:0.259542\ttrain-error:0.069204\n",
      "[3]\teval-error:0.236641\ttrain-error:0.067062\n",
      "[4]\teval-error:0.240458\ttrain-error:0.066491\n",
      "[5]\teval-error:0.240458\ttrain-error:0.066133\n",
      "[6]\teval-error:0.248092\ttrain-error:0.065919\n",
      "[7]\teval-error:0.244275\ttrain-error:0.065848\n",
      "[8]\teval-error:0.251908\ttrain-error:0.065776\n",
      "[9]\teval-error:0.251908\ttrain-error:0.065776\n",
      "[10]\teval-error:0.244275\ttrain-error:0.065776\n",
      "[11]\teval-error:0.248092\ttrain-error:0.065776\n",
      "[12]\teval-error:0.251908\ttrain-error:0.065776\n",
      "[13]\teval-error:0.251908\ttrain-error:0.065776\n",
      "[14]\teval-error:0.251908\ttrain-error:0.065776\n",
      "[15]\teval-error:0.244275\ttrain-error:0.065776\n",
      "[16]\teval-error:0.251908\ttrain-error:0.065776\n",
      "[17]\teval-error:0.244275\ttrain-error:0.065776\n",
      "[18]\teval-error:0.248092\ttrain-error:0.065776\n",
      "[19]\teval-error:0.244275\ttrain-error:0.065776\n",
      "[20]\teval-error:0.240458\ttrain-error:0.065776\n",
      "[21]\teval-error:0.232824\ttrain-error:0.065776\n",
      "[22]\teval-error:0.236641\ttrain-error:0.065776\n",
      "[23]\teval-error:0.236641\ttrain-error:0.065776\n",
      "[24]\teval-error:0.240458\ttrain-error:0.065776\n",
      "[25]\teval-error:0.244275\ttrain-error:0.065776\n",
      "[26]\teval-error:0.240458\ttrain-error:0.065776\n",
      "[27]\teval-error:0.240458\ttrain-error:0.065776\n",
      "[28]\teval-error:0.244275\ttrain-error:0.065776\n",
      "[29]\teval-error:0.236641\ttrain-error:0.065776\n",
      "[30]\teval-error:0.244275\ttrain-error:0.065776\n",
      "[31]\teval-error:0.244275\ttrain-error:0.065776\n",
      "[32]\teval-error:0.240458\ttrain-error:0.065776\n",
      "[33]\teval-error:0.248092\ttrain-error:0.065776\n",
      "[34]\teval-error:0.244275\ttrain-error:0.065776\n",
      "[35]\teval-error:0.236641\ttrain-error:0.065776\n",
      "[36]\teval-error:0.248092\ttrain-error:0.065776\n",
      "[37]\teval-error:0.240458\ttrain-error:0.065776\n",
      "[38]\teval-error:0.236641\ttrain-error:0.065776\n",
      "[39]\teval-error:0.236641\ttrain-error:0.065776\n",
      "[40]\teval-error:0.232824\ttrain-error:0.065776\n",
      "[41]\teval-error:0.240458\ttrain-error:0.065776\n",
      "[42]\teval-error:0.244275\ttrain-error:0.065776\n",
      "[43]\teval-error:0.244275\ttrain-error:0.065776\n",
      "[44]\teval-error:0.240458\ttrain-error:0.065776\n",
      "[45]\teval-error:0.240458\ttrain-error:0.065776\n",
      "[46]\teval-error:0.259542\ttrain-error:0.065776\n",
      "[47]\teval-error:0.248092\ttrain-error:0.065776\n",
      "[48]\teval-error:0.244275\ttrain-error:0.065776\n",
      "[49]\teval-error:0.251908\ttrain-error:0.065776\n",
      "[50]\teval-error:0.244275\ttrain-error:0.065776\n",
      "[51]\teval-error:0.244275\ttrain-error:0.065776\n",
      "[52]\teval-error:0.248092\ttrain-error:0.065776\n",
      "[53]\teval-error:0.248092\ttrain-error:0.065776\n",
      "[54]\teval-error:0.240458\ttrain-error:0.065776\n",
      "[55]\teval-error:0.240458\ttrain-error:0.065776\n",
      "[56]\teval-error:0.232824\ttrain-error:0.065776\n",
      "[57]\teval-error:0.236641\ttrain-error:0.065776\n",
      "[58]\teval-error:0.244275\ttrain-error:0.065776\n",
      "[59]\teval-error:0.244275\ttrain-error:0.065776\n",
      "[60]\teval-error:0.232824\ttrain-error:0.065776\n",
      "[61]\teval-error:0.248092\ttrain-error:0.065776\n",
      "[62]\teval-error:0.248092\ttrain-error:0.065776\n",
      "[63]\teval-error:0.244275\ttrain-error:0.065776\n",
      "[64]\teval-error:0.240458\ttrain-error:0.065776\n",
      "[65]\teval-error:0.244275\ttrain-error:0.065776\n",
      "[66]\teval-error:0.240458\ttrain-error:0.065776\n",
      "[67]\teval-error:0.240458\ttrain-error:0.065776\n",
      "[68]\teval-error:0.236641\ttrain-error:0.065776\n",
      "[69]\teval-error:0.244275\ttrain-error:0.065776\n",
      "[0]\teval-error:0.30916\ttrain-error:0.093987\n",
      "[1]\teval-error:0.278626\ttrain-error:0.07856\n",
      "[2]\teval-error:0.263359\ttrain-error:0.070847\n",
      "[3]\teval-error:0.248092\ttrain-error:0.067205\n",
      "[4]\teval-error:0.251908\ttrain-error:0.066276\n",
      "[5]\teval-error:0.259542\ttrain-error:0.065991\n",
      "[6]\teval-error:0.263359\ttrain-error:0.065776\n",
      "[7]\teval-error:0.255725\ttrain-error:0.065776\n",
      "[8]\teval-error:0.244275\ttrain-error:0.065776\n",
      "[9]\teval-error:0.244275\ttrain-error:0.065776\n",
      "[10]\teval-error:0.236641\ttrain-error:0.065776\n",
      "[11]\teval-error:0.248092\ttrain-error:0.065776\n",
      "[12]\teval-error:0.244275\ttrain-error:0.065776\n",
      "[13]\teval-error:0.248092\ttrain-error:0.065776\n",
      "[14]\teval-error:0.236641\ttrain-error:0.065776\n",
      "[15]\teval-error:0.240458\ttrain-error:0.065776\n",
      "[16]\teval-error:0.236641\ttrain-error:0.065776\n",
      "[17]\teval-error:0.232824\ttrain-error:0.065776\n",
      "[18]\teval-error:0.232824\ttrain-error:0.065776\n",
      "[19]\teval-error:0.236641\ttrain-error:0.065776\n",
      "[20]\teval-error:0.232824\ttrain-error:0.065776\n",
      "[21]\teval-error:0.248092\ttrain-error:0.065776\n",
      "[22]\teval-error:0.248092\ttrain-error:0.065776\n",
      "[23]\teval-error:0.244275\ttrain-error:0.065776\n",
      "[24]\teval-error:0.251908\ttrain-error:0.065776\n",
      "[25]\teval-error:0.248092\ttrain-error:0.065776\n",
      "[26]\teval-error:0.255725\ttrain-error:0.065776\n",
      "[27]\teval-error:0.255725\ttrain-error:0.065776\n",
      "[28]\teval-error:0.259542\ttrain-error:0.065776\n",
      "[29]\teval-error:0.255725\ttrain-error:0.065776\n",
      "[30]\teval-error:0.263359\ttrain-error:0.065776\n",
      "[31]\teval-error:0.263359\ttrain-error:0.065776\n",
      "[32]\teval-error:0.255725\ttrain-error:0.065776\n",
      "[33]\teval-error:0.255725\ttrain-error:0.065776\n",
      "[34]\teval-error:0.259542\ttrain-error:0.065776\n",
      "[35]\teval-error:0.255725\ttrain-error:0.065776\n",
      "[36]\teval-error:0.259542\ttrain-error:0.065776\n",
      "[37]\teval-error:0.255725\ttrain-error:0.065776\n",
      "[38]\teval-error:0.267176\ttrain-error:0.065776\n",
      "[39]\teval-error:0.255725\ttrain-error:0.065776\n",
      "[40]\teval-error:0.259542\ttrain-error:0.065776\n",
      "[41]\teval-error:0.248092\ttrain-error:0.065776\n",
      "[42]\teval-error:0.255725\ttrain-error:0.065776\n",
      "[43]\teval-error:0.251908\ttrain-error:0.065776\n",
      "[44]\teval-error:0.251908\ttrain-error:0.065776\n",
      "[45]\teval-error:0.244275\ttrain-error:0.065776\n",
      "[46]\teval-error:0.255725\ttrain-error:0.065776\n",
      "[47]\teval-error:0.248092\ttrain-error:0.065776\n",
      "[48]\teval-error:0.251908\ttrain-error:0.065776\n",
      "[49]\teval-error:0.244275\ttrain-error:0.065776\n",
      "[50]\teval-error:0.248092\ttrain-error:0.065776\n",
      "[51]\teval-error:0.248092\ttrain-error:0.065776\n",
      "[52]\teval-error:0.255725\ttrain-error:0.065776\n",
      "[53]\teval-error:0.251908\ttrain-error:0.065776\n",
      "[54]\teval-error:0.251908\ttrain-error:0.065776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55]\teval-error:0.248092\ttrain-error:0.065776\n",
      "[56]\teval-error:0.251908\ttrain-error:0.065776\n",
      "[57]\teval-error:0.244275\ttrain-error:0.065776\n",
      "[58]\teval-error:0.255725\ttrain-error:0.065776\n",
      "[59]\teval-error:0.248092\ttrain-error:0.065776\n",
      "[60]\teval-error:0.248092\ttrain-error:0.065776\n",
      "[61]\teval-error:0.248092\ttrain-error:0.065776\n",
      "[62]\teval-error:0.248092\ttrain-error:0.065776\n",
      "[63]\teval-error:0.251908\ttrain-error:0.065776\n",
      "[64]\teval-error:0.259542\ttrain-error:0.065776\n",
      "[65]\teval-error:0.248092\ttrain-error:0.065776\n",
      "[66]\teval-error:0.248092\ttrain-error:0.065776\n",
      "[67]\teval-error:0.255725\ttrain-error:0.065776\n",
      "[68]\teval-error:0.259542\ttrain-error:0.065776\n",
      "[69]\teval-error:0.255725\ttrain-error:0.065776\n",
      "[0]\teval-error:0.278626\ttrain-error:0.090844\n",
      "[1]\teval-error:0.244275\ttrain-error:0.077132\n",
      "[2]\teval-error:0.267176\ttrain-error:0.070347\n",
      "[3]\teval-error:0.251908\ttrain-error:0.067133\n",
      "[4]\teval-error:0.263359\ttrain-error:0.065919\n",
      "[5]\teval-error:0.251908\ttrain-error:0.065848\n",
      "[6]\teval-error:0.248092\ttrain-error:0.065776\n",
      "[7]\teval-error:0.244275\ttrain-error:0.065776\n",
      "[8]\teval-error:0.259542\ttrain-error:0.065776\n",
      "[9]\teval-error:0.255725\ttrain-error:0.065705\n",
      "[10]\teval-error:0.244275\ttrain-error:0.065705\n",
      "[11]\teval-error:0.255725\ttrain-error:0.065705\n",
      "[12]\teval-error:0.244275\ttrain-error:0.065705\n",
      "[13]\teval-error:0.236641\ttrain-error:0.065705\n",
      "[14]\teval-error:0.244275\ttrain-error:0.065705\n",
      "[15]\teval-error:0.244275\ttrain-error:0.065705\n",
      "[16]\teval-error:0.244275\ttrain-error:0.065705\n",
      "[17]\teval-error:0.248092\ttrain-error:0.065705\n",
      "[18]\teval-error:0.255725\ttrain-error:0.065705\n",
      "[19]\teval-error:0.248092\ttrain-error:0.065705\n",
      "[20]\teval-error:0.240458\ttrain-error:0.065705\n",
      "[21]\teval-error:0.236641\ttrain-error:0.065705\n",
      "[22]\teval-error:0.240458\ttrain-error:0.065705\n",
      "[23]\teval-error:0.240458\ttrain-error:0.065705\n",
      "[24]\teval-error:0.244275\ttrain-error:0.065705\n",
      "[25]\teval-error:0.232824\ttrain-error:0.065705\n",
      "[26]\teval-error:0.236641\ttrain-error:0.065705\n",
      "[27]\teval-error:0.232824\ttrain-error:0.065705\n",
      "[28]\teval-error:0.244275\ttrain-error:0.065705\n",
      "[29]\teval-error:0.236641\ttrain-error:0.065705\n",
      "[30]\teval-error:0.225191\ttrain-error:0.065705\n",
      "[31]\teval-error:0.236641\ttrain-error:0.065705\n",
      "[32]\teval-error:0.225191\ttrain-error:0.065705\n",
      "[33]\teval-error:0.232824\ttrain-error:0.065705\n",
      "[34]\teval-error:0.240458\ttrain-error:0.065705\n",
      "[35]\teval-error:0.240458\ttrain-error:0.065705\n",
      "[36]\teval-error:0.255725\ttrain-error:0.065705\n",
      "[37]\teval-error:0.244275\ttrain-error:0.065705\n",
      "[38]\teval-error:0.244275\ttrain-error:0.065705\n",
      "[39]\teval-error:0.251908\ttrain-error:0.065705\n",
      "[40]\teval-error:0.229008\ttrain-error:0.065705\n",
      "[41]\teval-error:0.240458\ttrain-error:0.065705\n",
      "[42]\teval-error:0.236641\ttrain-error:0.065705\n",
      "[43]\teval-error:0.232824\ttrain-error:0.065705\n",
      "[44]\teval-error:0.240458\ttrain-error:0.065705\n",
      "[45]\teval-error:0.236641\ttrain-error:0.065705\n",
      "[46]\teval-error:0.236641\ttrain-error:0.065705\n",
      "[47]\teval-error:0.240458\ttrain-error:0.065705\n",
      "[48]\teval-error:0.236641\ttrain-error:0.065705\n",
      "[49]\teval-error:0.244275\ttrain-error:0.065705\n",
      "[50]\teval-error:0.236641\ttrain-error:0.065705\n",
      "[51]\teval-error:0.240458\ttrain-error:0.065705\n",
      "[52]\teval-error:0.240458\ttrain-error:0.065705\n",
      "[53]\teval-error:0.244275\ttrain-error:0.065705\n",
      "[54]\teval-error:0.240458\ttrain-error:0.065705\n",
      "[55]\teval-error:0.248092\ttrain-error:0.065705\n",
      "[56]\teval-error:0.236641\ttrain-error:0.065705\n",
      "[57]\teval-error:0.240458\ttrain-error:0.065705\n",
      "[58]\teval-error:0.236641\ttrain-error:0.065705\n",
      "[59]\teval-error:0.236641\ttrain-error:0.065705\n",
      "[60]\teval-error:0.240458\ttrain-error:0.065705\n",
      "[61]\teval-error:0.229008\ttrain-error:0.065705\n",
      "[62]\teval-error:0.229008\ttrain-error:0.065705\n",
      "[63]\teval-error:0.229008\ttrain-error:0.065705\n",
      "[64]\teval-error:0.232824\ttrain-error:0.065705\n",
      "[65]\teval-error:0.221374\ttrain-error:0.065705\n",
      "[66]\teval-error:0.229008\ttrain-error:0.065705\n",
      "[67]\teval-error:0.232824\ttrain-error:0.065705\n",
      "[68]\teval-error:0.225191\ttrain-error:0.065705\n",
      "[69]\teval-error:0.225191\ttrain-error:0.065705\n",
      "[0]\teval-error:0.431818\ttrain-error:0.223563\n",
      "[1]\teval-error:0.409091\ttrain-error:0.200685\n",
      "[2]\teval-error:0.409091\ttrain-error:0.188975\n",
      "[3]\teval-error:0.397727\ttrain-error:0.184471\n",
      "[4]\teval-error:0.443182\ttrain-error:0.181409\n",
      "[5]\teval-error:0.431818\ttrain-error:0.180508\n",
      "[6]\teval-error:0.431818\ttrain-error:0.179247\n",
      "[7]\teval-error:0.454545\ttrain-error:0.178887\n",
      "[8]\teval-error:0.431818\ttrain-error:0.178166\n",
      "[9]\teval-error:0.420455\ttrain-error:0.178166\n",
      "[10]\teval-error:0.409091\ttrain-error:0.177986\n",
      "[11]\teval-error:0.409091\ttrain-error:0.177986\n",
      "[12]\teval-error:0.397727\ttrain-error:0.177986\n",
      "[13]\teval-error:0.420455\ttrain-error:0.177986\n",
      "[14]\teval-error:0.420455\ttrain-error:0.177986\n",
      "[15]\teval-error:0.431818\ttrain-error:0.177986\n",
      "[16]\teval-error:0.420455\ttrain-error:0.177986\n",
      "[17]\teval-error:0.409091\ttrain-error:0.177986\n",
      "[18]\teval-error:0.420455\ttrain-error:0.177986\n",
      "[19]\teval-error:0.409091\ttrain-error:0.177986\n",
      "[20]\teval-error:0.409091\ttrain-error:0.177986\n",
      "[21]\teval-error:0.409091\ttrain-error:0.177986\n",
      "[22]\teval-error:0.409091\ttrain-error:0.177986\n",
      "[23]\teval-error:0.409091\ttrain-error:0.177986\n",
      "[24]\teval-error:0.420455\ttrain-error:0.177986\n",
      "[25]\teval-error:0.431818\ttrain-error:0.177986\n",
      "[26]\teval-error:0.431818\ttrain-error:0.177986\n",
      "[27]\teval-error:0.431818\ttrain-error:0.177986\n",
      "[28]\teval-error:0.420455\ttrain-error:0.177986\n",
      "[29]\teval-error:0.431818\ttrain-error:0.177986\n",
      "[30]\teval-error:0.431818\ttrain-error:0.177986\n",
      "[31]\teval-error:0.409091\ttrain-error:0.177986\n",
      "[32]\teval-error:0.409091\ttrain-error:0.177986\n",
      "[33]\teval-error:0.420455\ttrain-error:0.177986\n",
      "[34]\teval-error:0.420455\ttrain-error:0.177986\n",
      "[35]\teval-error:0.420455\ttrain-error:0.177986\n",
      "[36]\teval-error:0.420455\ttrain-error:0.177986\n",
      "[37]\teval-error:0.420455\ttrain-error:0.177986\n",
      "[38]\teval-error:0.420455\ttrain-error:0.177986\n",
      "[39]\teval-error:0.431818\ttrain-error:0.177986\n",
      "[40]\teval-error:0.431818\ttrain-error:0.177986\n",
      "[41]\teval-error:0.431818\ttrain-error:0.177986\n",
      "[42]\teval-error:0.420455\ttrain-error:0.177986\n",
      "[43]\teval-error:0.420455\ttrain-error:0.177986\n",
      "[44]\teval-error:0.420455\ttrain-error:0.177986\n",
      "[45]\teval-error:0.420455\ttrain-error:0.177986\n",
      "[46]\teval-error:0.431818\ttrain-error:0.177986\n",
      "[47]\teval-error:0.431818\ttrain-error:0.177986\n",
      "[48]\teval-error:0.420455\ttrain-error:0.177986\n",
      "[49]\teval-error:0.431818\ttrain-error:0.177986\n",
      "[50]\teval-error:0.431818\ttrain-error:0.177986\n",
      "[51]\teval-error:0.431818\ttrain-error:0.177986\n",
      "[52]\teval-error:0.431818\ttrain-error:0.177986\n",
      "[53]\teval-error:0.431818\ttrain-error:0.177986\n",
      "[54]\teval-error:0.431818\ttrain-error:0.177986\n",
      "[55]\teval-error:0.431818\ttrain-error:0.177986\n",
      "[56]\teval-error:0.431818\ttrain-error:0.177986\n",
      "[57]\teval-error:0.431818\ttrain-error:0.177986\n",
      "[58]\teval-error:0.431818\ttrain-error:0.177986\n",
      "[59]\teval-error:0.431818\ttrain-error:0.177986\n",
      "[60]\teval-error:0.431818\ttrain-error:0.177986\n",
      "[61]\teval-error:0.431818\ttrain-error:0.177986\n",
      "[62]\teval-error:0.431818\ttrain-error:0.177986\n",
      "[63]\teval-error:0.420455\ttrain-error:0.177986\n",
      "[64]\teval-error:0.420455\ttrain-error:0.177986\n",
      "[65]\teval-error:0.420455\ttrain-error:0.177986\n",
      "[66]\teval-error:0.420455\ttrain-error:0.177986\n",
      "[67]\teval-error:0.420455\ttrain-error:0.177986\n",
      "[68]\teval-error:0.431818\ttrain-error:0.177986\n",
      "[69]\teval-error:0.420455\ttrain-error:0.177986\n",
      "[0]\teval-error:0.409091\ttrain-error:0.228247\n",
      "[1]\teval-error:0.375\ttrain-error:0.208071\n",
      "[2]\teval-error:0.352273\ttrain-error:0.197262\n",
      "[3]\teval-error:0.386364\ttrain-error:0.190596\n",
      "[4]\teval-error:0.409091\ttrain-error:0.185732\n",
      "[5]\teval-error:0.375\ttrain-error:0.184651\n",
      "[6]\teval-error:0.454545\ttrain-error:0.184651\n",
      "[7]\teval-error:0.431818\ttrain-error:0.183931\n",
      "[8]\teval-error:0.409091\ttrain-error:0.18339\n",
      "[9]\teval-error:0.386364\ttrain-error:0.18303\n",
      "[10]\teval-error:0.409091\ttrain-error:0.18303\n",
      "[11]\teval-error:0.420455\ttrain-error:0.18285\n",
      "[12]\teval-error:0.431818\ttrain-error:0.18285\n",
      "[13]\teval-error:0.465909\ttrain-error:0.18285\n",
      "[14]\teval-error:0.465909\ttrain-error:0.18285\n",
      "[15]\teval-error:0.454545\ttrain-error:0.18285\n",
      "[16]\teval-error:0.477273\ttrain-error:0.18285\n",
      "[17]\teval-error:0.477273\ttrain-error:0.18285\n",
      "[18]\teval-error:0.454545\ttrain-error:0.18285\n",
      "[19]\teval-error:0.454545\ttrain-error:0.18285\n",
      "[20]\teval-error:0.454545\ttrain-error:0.18285\n",
      "[21]\teval-error:0.454545\ttrain-error:0.18285\n",
      "[22]\teval-error:0.454545\ttrain-error:0.18285\n",
      "[23]\teval-error:0.454545\ttrain-error:0.18285\n",
      "[24]\teval-error:0.454545\ttrain-error:0.18285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25]\teval-error:0.443182\ttrain-error:0.18285\n",
      "[26]\teval-error:0.477273\ttrain-error:0.18285\n",
      "[27]\teval-error:0.477273\ttrain-error:0.18285\n",
      "[28]\teval-error:0.477273\ttrain-error:0.18285\n",
      "[29]\teval-error:0.477273\ttrain-error:0.18285\n",
      "[30]\teval-error:0.477273\ttrain-error:0.18285\n",
      "[31]\teval-error:0.477273\ttrain-error:0.18285\n",
      "[32]\teval-error:0.477273\ttrain-error:0.18285\n",
      "[33]\teval-error:0.477273\ttrain-error:0.18285\n",
      "[34]\teval-error:0.477273\ttrain-error:0.18285\n",
      "[35]\teval-error:0.477273\ttrain-error:0.18285\n",
      "[36]\teval-error:0.477273\ttrain-error:0.18285\n",
      "[37]\teval-error:0.477273\ttrain-error:0.18285\n",
      "[38]\teval-error:0.477273\ttrain-error:0.18285\n",
      "[39]\teval-error:0.477273\ttrain-error:0.18285\n",
      "[40]\teval-error:0.477273\ttrain-error:0.18285\n",
      "[41]\teval-error:0.454545\ttrain-error:0.18285\n",
      "[42]\teval-error:0.454545\ttrain-error:0.18285\n",
      "[43]\teval-error:0.477273\ttrain-error:0.18285\n",
      "[44]\teval-error:0.454545\ttrain-error:0.18285\n",
      "[45]\teval-error:0.454545\ttrain-error:0.18285\n",
      "[46]\teval-error:0.465909\ttrain-error:0.18285\n",
      "[47]\teval-error:0.465909\ttrain-error:0.18285\n",
      "[48]\teval-error:0.465909\ttrain-error:0.18285\n",
      "[49]\teval-error:0.465909\ttrain-error:0.18285\n",
      "[50]\teval-error:0.465909\ttrain-error:0.18285\n",
      "[51]\teval-error:0.454545\ttrain-error:0.18285\n",
      "[52]\teval-error:0.454545\ttrain-error:0.18285\n",
      "[53]\teval-error:0.465909\ttrain-error:0.18285\n",
      "[54]\teval-error:0.465909\ttrain-error:0.18285\n",
      "[55]\teval-error:0.465909\ttrain-error:0.18285\n",
      "[56]\teval-error:0.465909\ttrain-error:0.18285\n",
      "[57]\teval-error:0.465909\ttrain-error:0.18285\n",
      "[58]\teval-error:0.465909\ttrain-error:0.18285\n",
      "[59]\teval-error:0.465909\ttrain-error:0.18285\n",
      "[60]\teval-error:0.465909\ttrain-error:0.18285\n",
      "[61]\teval-error:0.488636\ttrain-error:0.18285\n",
      "[62]\teval-error:0.477273\ttrain-error:0.18285\n",
      "[63]\teval-error:0.477273\ttrain-error:0.18285\n",
      "[64]\teval-error:0.488636\ttrain-error:0.18285\n",
      "[65]\teval-error:0.488636\ttrain-error:0.18285\n",
      "[66]\teval-error:0.477273\ttrain-error:0.18285\n",
      "[67]\teval-error:0.465909\ttrain-error:0.18285\n",
      "[68]\teval-error:0.477273\ttrain-error:0.18285\n",
      "[69]\teval-error:0.488636\ttrain-error:0.18285\n",
      "[0]\teval-error:0.431818\ttrain-error:0.233832\n",
      "[1]\teval-error:0.386364\ttrain-error:0.216538\n",
      "[2]\teval-error:0.352273\ttrain-error:0.205729\n",
      "[3]\teval-error:0.375\ttrain-error:0.200144\n",
      "[4]\teval-error:0.375\ttrain-error:0.196361\n",
      "[5]\teval-error:0.352273\ttrain-error:0.193299\n",
      "[6]\teval-error:0.375\ttrain-error:0.192218\n",
      "[7]\teval-error:0.375\ttrain-error:0.191137\n",
      "[8]\teval-error:0.397727\ttrain-error:0.190957\n",
      "[9]\teval-error:0.420455\ttrain-error:0.190957\n",
      "[10]\teval-error:0.397727\ttrain-error:0.190236\n",
      "[11]\teval-error:0.409091\ttrain-error:0.189696\n",
      "[12]\teval-error:0.420455\ttrain-error:0.189876\n",
      "[13]\teval-error:0.420455\ttrain-error:0.189876\n",
      "[14]\teval-error:0.409091\ttrain-error:0.189696\n",
      "[15]\teval-error:0.409091\ttrain-error:0.189515\n",
      "[16]\teval-error:0.409091\ttrain-error:0.189515\n",
      "[17]\teval-error:0.397727\ttrain-error:0.189515\n",
      "[18]\teval-error:0.397727\ttrain-error:0.189515\n",
      "[19]\teval-error:0.397727\ttrain-error:0.189515\n",
      "[20]\teval-error:0.397727\ttrain-error:0.189515\n",
      "[21]\teval-error:0.397727\ttrain-error:0.189515\n",
      "[22]\teval-error:0.397727\ttrain-error:0.189515\n",
      "[23]\teval-error:0.397727\ttrain-error:0.189515\n",
      "[24]\teval-error:0.386364\ttrain-error:0.189515\n",
      "[25]\teval-error:0.397727\ttrain-error:0.189515\n",
      "[26]\teval-error:0.397727\ttrain-error:0.189515\n",
      "[27]\teval-error:0.397727\ttrain-error:0.189515\n",
      "[28]\teval-error:0.397727\ttrain-error:0.189515\n",
      "[29]\teval-error:0.397727\ttrain-error:0.189515\n",
      "[30]\teval-error:0.397727\ttrain-error:0.189515\n",
      "[31]\teval-error:0.409091\ttrain-error:0.189515\n",
      "[32]\teval-error:0.397727\ttrain-error:0.189515\n",
      "[33]\teval-error:0.409091\ttrain-error:0.189515\n",
      "[34]\teval-error:0.409091\ttrain-error:0.189515\n",
      "[35]\teval-error:0.409091\ttrain-error:0.189515\n",
      "[36]\teval-error:0.409091\ttrain-error:0.189515\n",
      "[37]\teval-error:0.409091\ttrain-error:0.189515\n",
      "[38]\teval-error:0.409091\ttrain-error:0.189515\n",
      "[39]\teval-error:0.409091\ttrain-error:0.189515\n",
      "[40]\teval-error:0.409091\ttrain-error:0.189515\n",
      "[41]\teval-error:0.409091\ttrain-error:0.189515\n",
      "[42]\teval-error:0.409091\ttrain-error:0.189515\n",
      "[43]\teval-error:0.409091\ttrain-error:0.189515\n",
      "[44]\teval-error:0.409091\ttrain-error:0.189515\n",
      "[45]\teval-error:0.409091\ttrain-error:0.189515\n",
      "[46]\teval-error:0.409091\ttrain-error:0.189515\n",
      "[47]\teval-error:0.409091\ttrain-error:0.189515\n",
      "[48]\teval-error:0.409091\ttrain-error:0.189515\n",
      "[49]\teval-error:0.409091\ttrain-error:0.189515\n",
      "[50]\teval-error:0.409091\ttrain-error:0.189515\n",
      "[51]\teval-error:0.409091\ttrain-error:0.189515\n",
      "[52]\teval-error:0.409091\ttrain-error:0.189515\n",
      "[53]\teval-error:0.409091\ttrain-error:0.189515\n",
      "[54]\teval-error:0.409091\ttrain-error:0.189515\n",
      "[55]\teval-error:0.409091\ttrain-error:0.189515\n",
      "[56]\teval-error:0.409091\ttrain-error:0.189515\n",
      "[57]\teval-error:0.409091\ttrain-error:0.189515\n",
      "[58]\teval-error:0.409091\ttrain-error:0.189515\n",
      "[59]\teval-error:0.420455\ttrain-error:0.189515\n",
      "[60]\teval-error:0.409091\ttrain-error:0.189515\n",
      "[61]\teval-error:0.409091\ttrain-error:0.189515\n",
      "[62]\teval-error:0.409091\ttrain-error:0.189515\n",
      "[63]\teval-error:0.409091\ttrain-error:0.189515\n",
      "[64]\teval-error:0.409091\ttrain-error:0.189515\n",
      "[65]\teval-error:0.409091\ttrain-error:0.189515\n",
      "[66]\teval-error:0.409091\ttrain-error:0.189515\n",
      "[67]\teval-error:0.409091\ttrain-error:0.189515\n",
      "[68]\teval-error:0.409091\ttrain-error:0.189515\n",
      "[69]\teval-error:0.409091\ttrain-error:0.189515\n",
      "[0]\teval-error:0.465909\ttrain-error:0.222302\n",
      "[1]\teval-error:0.443182\ttrain-error:0.203927\n",
      "[2]\teval-error:0.454545\ttrain-error:0.192758\n",
      "[3]\teval-error:0.420455\ttrain-error:0.184832\n",
      "[4]\teval-error:0.409091\ttrain-error:0.181589\n",
      "[5]\teval-error:0.409091\ttrain-error:0.179427\n",
      "[6]\teval-error:0.409091\ttrain-error:0.178346\n",
      "[7]\teval-error:0.431818\ttrain-error:0.178166\n",
      "[8]\teval-error:0.431818\ttrain-error:0.178166\n",
      "[9]\teval-error:0.409091\ttrain-error:0.177446\n",
      "[10]\teval-error:0.397727\ttrain-error:0.177265\n",
      "[11]\teval-error:0.409091\ttrain-error:0.177265\n",
      "[12]\teval-error:0.420455\ttrain-error:0.177265\n",
      "[13]\teval-error:0.409091\ttrain-error:0.177265\n",
      "[14]\teval-error:0.397727\ttrain-error:0.177265\n",
      "[15]\teval-error:0.397727\ttrain-error:0.177265\n",
      "[16]\teval-error:0.397727\ttrain-error:0.177265\n",
      "[17]\teval-error:0.420455\ttrain-error:0.177265\n",
      "[18]\teval-error:0.397727\ttrain-error:0.177265\n",
      "[19]\teval-error:0.420455\ttrain-error:0.177265\n",
      "[20]\teval-error:0.397727\ttrain-error:0.177265\n",
      "[21]\teval-error:0.397727\ttrain-error:0.177265\n",
      "[22]\teval-error:0.409091\ttrain-error:0.177265\n",
      "[23]\teval-error:0.397727\ttrain-error:0.177265\n",
      "[24]\teval-error:0.397727\ttrain-error:0.177265\n",
      "[25]\teval-error:0.409091\ttrain-error:0.177265\n",
      "[26]\teval-error:0.397727\ttrain-error:0.177265\n",
      "[27]\teval-error:0.397727\ttrain-error:0.177265\n",
      "[28]\teval-error:0.397727\ttrain-error:0.177265\n",
      "[29]\teval-error:0.397727\ttrain-error:0.177265\n",
      "[30]\teval-error:0.397727\ttrain-error:0.177265\n",
      "[31]\teval-error:0.409091\ttrain-error:0.177265\n",
      "[32]\teval-error:0.420455\ttrain-error:0.177265\n",
      "[33]\teval-error:0.420455\ttrain-error:0.177265\n",
      "[34]\teval-error:0.420455\ttrain-error:0.177265\n",
      "[35]\teval-error:0.420455\ttrain-error:0.177265\n",
      "[36]\teval-error:0.420455\ttrain-error:0.177265\n",
      "[37]\teval-error:0.409091\ttrain-error:0.177265\n",
      "[38]\teval-error:0.420455\ttrain-error:0.177265\n",
      "[39]\teval-error:0.420455\ttrain-error:0.177265\n",
      "[40]\teval-error:0.420455\ttrain-error:0.177265\n",
      "[41]\teval-error:0.420455\ttrain-error:0.177265\n",
      "[42]\teval-error:0.409091\ttrain-error:0.177265\n",
      "[43]\teval-error:0.420455\ttrain-error:0.177265\n",
      "[44]\teval-error:0.431818\ttrain-error:0.177265\n",
      "[45]\teval-error:0.431818\ttrain-error:0.177265\n",
      "[46]\teval-error:0.420455\ttrain-error:0.177265\n",
      "[47]\teval-error:0.420455\ttrain-error:0.177265\n",
      "[48]\teval-error:0.420455\ttrain-error:0.177265\n",
      "[49]\teval-error:0.409091\ttrain-error:0.177265\n",
      "[50]\teval-error:0.409091\ttrain-error:0.177265\n",
      "[51]\teval-error:0.409091\ttrain-error:0.177265\n",
      "[52]\teval-error:0.409091\ttrain-error:0.177265\n",
      "[53]\teval-error:0.409091\ttrain-error:0.177265\n",
      "[54]\teval-error:0.397727\ttrain-error:0.177265\n",
      "[55]\teval-error:0.397727\ttrain-error:0.177265\n",
      "[56]\teval-error:0.397727\ttrain-error:0.177265\n",
      "[57]\teval-error:0.397727\ttrain-error:0.177265\n",
      "[58]\teval-error:0.397727\ttrain-error:0.177265\n",
      "[59]\teval-error:0.409091\ttrain-error:0.177265\n",
      "[60]\teval-error:0.409091\ttrain-error:0.177265\n",
      "[61]\teval-error:0.409091\ttrain-error:0.177265\n",
      "[62]\teval-error:0.409091\ttrain-error:0.177265\n",
      "[63]\teval-error:0.409091\ttrain-error:0.177265\n",
      "[64]\teval-error:0.409091\ttrain-error:0.177265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65]\teval-error:0.409091\ttrain-error:0.177265\n",
      "[66]\teval-error:0.409091\ttrain-error:0.177265\n",
      "[67]\teval-error:0.409091\ttrain-error:0.177265\n",
      "[68]\teval-error:0.409091\ttrain-error:0.177265\n",
      "[69]\teval-error:0.409091\ttrain-error:0.177265\n",
      "[0]\teval-error:0.361702\ttrain-error:0.214691\n",
      "[1]\teval-error:0.297872\ttrain-error:0.193648\n",
      "[2]\teval-error:0.329787\ttrain-error:0.183062\n",
      "[3]\teval-error:0.340426\ttrain-error:0.178286\n",
      "[4]\teval-error:0.319149\ttrain-error:0.175574\n",
      "[5]\teval-error:0.297872\ttrain-error:0.1748\n",
      "[6]\teval-error:0.319149\ttrain-error:0.174154\n",
      "[7]\teval-error:0.297872\ttrain-error:0.174025\n",
      "[8]\teval-error:0.329787\ttrain-error:0.174025\n",
      "[9]\teval-error:0.329787\ttrain-error:0.174025\n",
      "[10]\teval-error:0.319149\ttrain-error:0.174025\n",
      "[11]\teval-error:0.329787\ttrain-error:0.174025\n",
      "[12]\teval-error:0.340426\ttrain-error:0.174025\n",
      "[13]\teval-error:0.329787\ttrain-error:0.174025\n",
      "[14]\teval-error:0.329787\ttrain-error:0.174025\n",
      "[15]\teval-error:0.329787\ttrain-error:0.174025\n",
      "[16]\teval-error:0.329787\ttrain-error:0.174025\n",
      "[17]\teval-error:0.319149\ttrain-error:0.173896\n",
      "[18]\teval-error:0.319149\ttrain-error:0.174025\n",
      "[19]\teval-error:0.319149\ttrain-error:0.173896\n",
      "[20]\teval-error:0.319149\ttrain-error:0.174025\n",
      "[21]\teval-error:0.319149\ttrain-error:0.174025\n",
      "[22]\teval-error:0.308511\ttrain-error:0.173896\n",
      "[23]\teval-error:0.319149\ttrain-error:0.173896\n",
      "[24]\teval-error:0.319149\ttrain-error:0.173896\n",
      "[25]\teval-error:0.319149\ttrain-error:0.173896\n",
      "[26]\teval-error:0.319149\ttrain-error:0.173896\n",
      "[27]\teval-error:0.319149\ttrain-error:0.173896\n",
      "[28]\teval-error:0.319149\ttrain-error:0.173896\n",
      "[29]\teval-error:0.308511\ttrain-error:0.173896\n",
      "[30]\teval-error:0.319149\ttrain-error:0.173896\n",
      "[31]\teval-error:0.319149\ttrain-error:0.173896\n",
      "[32]\teval-error:0.319149\ttrain-error:0.173896\n",
      "[33]\teval-error:0.329787\ttrain-error:0.173896\n",
      "[34]\teval-error:0.329787\ttrain-error:0.173896\n",
      "[35]\teval-error:0.329787\ttrain-error:0.173896\n",
      "[36]\teval-error:0.340426\ttrain-error:0.173896\n",
      "[37]\teval-error:0.340426\ttrain-error:0.173896\n",
      "[38]\teval-error:0.329787\ttrain-error:0.173896\n",
      "[39]\teval-error:0.329787\ttrain-error:0.173896\n",
      "[40]\teval-error:0.340426\ttrain-error:0.173896\n",
      "[41]\teval-error:0.329787\ttrain-error:0.173896\n",
      "[42]\teval-error:0.340426\ttrain-error:0.173896\n",
      "[43]\teval-error:0.340426\ttrain-error:0.173896\n",
      "[44]\teval-error:0.340426\ttrain-error:0.173896\n",
      "[45]\teval-error:0.340426\ttrain-error:0.173896\n",
      "[46]\teval-error:0.340426\ttrain-error:0.173896\n",
      "[47]\teval-error:0.340426\ttrain-error:0.173896\n",
      "[48]\teval-error:0.340426\ttrain-error:0.173896\n",
      "[49]\teval-error:0.340426\ttrain-error:0.173896\n",
      "[50]\teval-error:0.340426\ttrain-error:0.173896\n",
      "[51]\teval-error:0.340426\ttrain-error:0.173896\n",
      "[52]\teval-error:0.340426\ttrain-error:0.173896\n",
      "[53]\teval-error:0.340426\ttrain-error:0.173896\n",
      "[54]\teval-error:0.340426\ttrain-error:0.173896\n",
      "[55]\teval-error:0.340426\ttrain-error:0.173896\n",
      "[56]\teval-error:0.340426\ttrain-error:0.173896\n",
      "[57]\teval-error:0.340426\ttrain-error:0.173896\n",
      "[58]\teval-error:0.340426\ttrain-error:0.173896\n",
      "[59]\teval-error:0.340426\ttrain-error:0.173896\n",
      "[60]\teval-error:0.340426\ttrain-error:0.173896\n",
      "[61]\teval-error:0.340426\ttrain-error:0.173896\n",
      "[62]\teval-error:0.340426\ttrain-error:0.173896\n",
      "[63]\teval-error:0.329787\ttrain-error:0.173896\n",
      "[64]\teval-error:0.340426\ttrain-error:0.173896\n",
      "[65]\teval-error:0.340426\ttrain-error:0.173896\n",
      "[66]\teval-error:0.329787\ttrain-error:0.173896\n",
      "[67]\teval-error:0.329787\ttrain-error:0.173896\n",
      "[68]\teval-error:0.329787\ttrain-error:0.173896\n",
      "[69]\teval-error:0.319149\ttrain-error:0.173896\n",
      "[0]\teval-error:0.308511\ttrain-error:0.219855\n",
      "[1]\teval-error:0.308511\ttrain-error:0.204105\n",
      "[2]\teval-error:0.319149\ttrain-error:0.192616\n",
      "[3]\teval-error:0.287234\ttrain-error:0.186677\n",
      "[4]\teval-error:0.287234\ttrain-error:0.182804\n",
      "[5]\teval-error:0.276596\ttrain-error:0.181255\n",
      "[6]\teval-error:0.287234\ttrain-error:0.180093\n",
      "[7]\teval-error:0.287234\ttrain-error:0.179706\n",
      "[8]\teval-error:0.329787\ttrain-error:0.179447\n",
      "[9]\teval-error:0.329787\ttrain-error:0.179318\n",
      "[10]\teval-error:0.340426\ttrain-error:0.179318\n",
      "[11]\teval-error:0.329787\ttrain-error:0.179318\n",
      "[12]\teval-error:0.329787\ttrain-error:0.179318\n",
      "[13]\teval-error:0.361702\ttrain-error:0.179318\n",
      "[14]\teval-error:0.329787\ttrain-error:0.179318\n",
      "[15]\teval-error:0.340426\ttrain-error:0.179318\n",
      "[16]\teval-error:0.340426\ttrain-error:0.179318\n",
      "[17]\teval-error:0.351064\ttrain-error:0.179318\n",
      "[18]\teval-error:0.351064\ttrain-error:0.179318\n",
      "[19]\teval-error:0.340426\ttrain-error:0.179318\n",
      "[20]\teval-error:0.351064\ttrain-error:0.179318\n",
      "[21]\teval-error:0.351064\ttrain-error:0.179318\n",
      "[22]\teval-error:0.351064\ttrain-error:0.179318\n",
      "[23]\teval-error:0.351064\ttrain-error:0.179318\n",
      "[24]\teval-error:0.361702\ttrain-error:0.179318\n",
      "[25]\teval-error:0.351064\ttrain-error:0.179318\n",
      "[26]\teval-error:0.329787\ttrain-error:0.179318\n",
      "[27]\teval-error:0.340426\ttrain-error:0.179318\n",
      "[28]\teval-error:0.329787\ttrain-error:0.179318\n",
      "[29]\teval-error:0.329787\ttrain-error:0.179318\n",
      "[30]\teval-error:0.340426\ttrain-error:0.179318\n",
      "[31]\teval-error:0.319149\ttrain-error:0.179318\n",
      "[32]\teval-error:0.308511\ttrain-error:0.179318\n",
      "[33]\teval-error:0.340426\ttrain-error:0.179318\n",
      "[34]\teval-error:0.351064\ttrain-error:0.179318\n",
      "[35]\teval-error:0.340426\ttrain-error:0.179318\n",
      "[36]\teval-error:0.340426\ttrain-error:0.179318\n",
      "[37]\teval-error:0.351064\ttrain-error:0.179318\n",
      "[38]\teval-error:0.340426\ttrain-error:0.179318\n",
      "[39]\teval-error:0.329787\ttrain-error:0.179318\n",
      "[40]\teval-error:0.329787\ttrain-error:0.179318\n",
      "[41]\teval-error:0.351064\ttrain-error:0.179318\n",
      "[42]\teval-error:0.340426\ttrain-error:0.179318\n",
      "[43]\teval-error:0.329787\ttrain-error:0.179318\n",
      "[44]\teval-error:0.329787\ttrain-error:0.179318\n",
      "[45]\teval-error:0.340426\ttrain-error:0.179318\n",
      "[46]\teval-error:0.340426\ttrain-error:0.179318\n",
      "[47]\teval-error:0.340426\ttrain-error:0.179318\n",
      "[48]\teval-error:0.351064\ttrain-error:0.179318\n",
      "[49]\teval-error:0.340426\ttrain-error:0.179318\n",
      "[50]\teval-error:0.351064\ttrain-error:0.179318\n",
      "[51]\teval-error:0.361702\ttrain-error:0.179318\n",
      "[52]\teval-error:0.361702\ttrain-error:0.179318\n",
      "[53]\teval-error:0.361702\ttrain-error:0.179318\n",
      "[54]\teval-error:0.361702\ttrain-error:0.179318\n",
      "[55]\teval-error:0.351064\ttrain-error:0.179318\n",
      "[56]\teval-error:0.351064\ttrain-error:0.179318\n",
      "[57]\teval-error:0.351064\ttrain-error:0.179318\n",
      "[58]\teval-error:0.361702\ttrain-error:0.179318\n",
      "[59]\teval-error:0.361702\ttrain-error:0.179318\n",
      "[60]\teval-error:0.351064\ttrain-error:0.179318\n",
      "[61]\teval-error:0.361702\ttrain-error:0.179318\n",
      "[62]\teval-error:0.351064\ttrain-error:0.179318\n",
      "[63]\teval-error:0.361702\ttrain-error:0.179318\n",
      "[64]\teval-error:0.340426\ttrain-error:0.179318\n",
      "[65]\teval-error:0.340426\ttrain-error:0.179318\n",
      "[66]\teval-error:0.340426\ttrain-error:0.179318\n",
      "[67]\teval-error:0.340426\ttrain-error:0.179318\n",
      "[68]\teval-error:0.340426\ttrain-error:0.179318\n",
      "[69]\teval-error:0.340426\ttrain-error:0.179318\n",
      "[0]\teval-error:0.351064\ttrain-error:0.22631\n",
      "[1]\teval-error:0.340426\ttrain-error:0.209527\n",
      "[2]\teval-error:0.329787\ttrain-error:0.200878\n",
      "[3]\teval-error:0.340426\ttrain-error:0.194165\n",
      "[4]\teval-error:0.351064\ttrain-error:0.190034\n",
      "[5]\teval-error:0.329787\ttrain-error:0.188613\n",
      "[6]\teval-error:0.329787\ttrain-error:0.187452\n",
      "[7]\teval-error:0.329787\ttrain-error:0.18629\n",
      "[8]\teval-error:0.319149\ttrain-error:0.185773\n",
      "[9]\teval-error:0.319149\ttrain-error:0.186032\n",
      "[10]\teval-error:0.308511\ttrain-error:0.185386\n",
      "[11]\teval-error:0.297872\ttrain-error:0.185386\n",
      "[12]\teval-error:0.308511\ttrain-error:0.185257\n",
      "[13]\teval-error:0.340426\ttrain-error:0.185386\n",
      "[14]\teval-error:0.308511\ttrain-error:0.185257\n",
      "[15]\teval-error:0.308511\ttrain-error:0.185257\n",
      "[16]\teval-error:0.308511\ttrain-error:0.185257\n",
      "[17]\teval-error:0.329787\ttrain-error:0.185386\n",
      "[18]\teval-error:0.319149\ttrain-error:0.185257\n",
      "[19]\teval-error:0.329787\ttrain-error:0.185257\n",
      "[20]\teval-error:0.319149\ttrain-error:0.185257\n",
      "[21]\teval-error:0.319149\ttrain-error:0.185257\n",
      "[22]\teval-error:0.319149\ttrain-error:0.185257\n",
      "[23]\teval-error:0.319149\ttrain-error:0.185257\n",
      "[24]\teval-error:0.319149\ttrain-error:0.185257\n",
      "[25]\teval-error:0.319149\ttrain-error:0.185257\n",
      "[26]\teval-error:0.319149\ttrain-error:0.185257\n",
      "[27]\teval-error:0.319149\ttrain-error:0.185257\n",
      "[28]\teval-error:0.319149\ttrain-error:0.185257\n",
      "[29]\teval-error:0.319149\ttrain-error:0.185257\n",
      "[30]\teval-error:0.319149\ttrain-error:0.185257\n",
      "[31]\teval-error:0.319149\ttrain-error:0.185257\n",
      "[32]\teval-error:0.319149\ttrain-error:0.185257\n",
      "[33]\teval-error:0.319149\ttrain-error:0.185257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34]\teval-error:0.319149\ttrain-error:0.185257\n",
      "[35]\teval-error:0.319149\ttrain-error:0.185257\n",
      "[36]\teval-error:0.319149\ttrain-error:0.185257\n",
      "[37]\teval-error:0.319149\ttrain-error:0.185257\n",
      "[38]\teval-error:0.319149\ttrain-error:0.185257\n",
      "[39]\teval-error:0.329787\ttrain-error:0.185257\n",
      "[40]\teval-error:0.329787\ttrain-error:0.185257\n",
      "[41]\teval-error:0.329787\ttrain-error:0.185257\n",
      "[42]\teval-error:0.329787\ttrain-error:0.185257\n",
      "[43]\teval-error:0.329787\ttrain-error:0.185257\n",
      "[44]\teval-error:0.329787\ttrain-error:0.185257\n",
      "[45]\teval-error:0.329787\ttrain-error:0.185257\n",
      "[46]\teval-error:0.329787\ttrain-error:0.185257\n",
      "[47]\teval-error:0.329787\ttrain-error:0.185257\n",
      "[48]\teval-error:0.329787\ttrain-error:0.185257\n",
      "[49]\teval-error:0.329787\ttrain-error:0.185257\n",
      "[50]\teval-error:0.329787\ttrain-error:0.185257\n",
      "[51]\teval-error:0.329787\ttrain-error:0.185257\n",
      "[52]\teval-error:0.329787\ttrain-error:0.185257\n",
      "[53]\teval-error:0.329787\ttrain-error:0.185257\n",
      "[54]\teval-error:0.329787\ttrain-error:0.185257\n",
      "[55]\teval-error:0.329787\ttrain-error:0.185257\n",
      "[56]\teval-error:0.329787\ttrain-error:0.185257\n",
      "[57]\teval-error:0.329787\ttrain-error:0.185257\n",
      "[58]\teval-error:0.329787\ttrain-error:0.185257\n",
      "[59]\teval-error:0.329787\ttrain-error:0.185257\n",
      "[60]\teval-error:0.329787\ttrain-error:0.185257\n",
      "[61]\teval-error:0.329787\ttrain-error:0.185257\n",
      "[62]\teval-error:0.329787\ttrain-error:0.185257\n",
      "[63]\teval-error:0.329787\ttrain-error:0.185257\n",
      "[64]\teval-error:0.329787\ttrain-error:0.185257\n",
      "[65]\teval-error:0.329787\ttrain-error:0.185257\n",
      "[66]\teval-error:0.329787\ttrain-error:0.185257\n",
      "[67]\teval-error:0.329787\ttrain-error:0.185257\n",
      "[68]\teval-error:0.329787\ttrain-error:0.185257\n",
      "[69]\teval-error:0.329787\ttrain-error:0.185257\n",
      "[0]\teval-error:0.340426\ttrain-error:0.214821\n",
      "[1]\teval-error:0.361702\ttrain-error:0.196876\n",
      "[2]\teval-error:0.329787\ttrain-error:0.186032\n",
      "[3]\teval-error:0.340426\ttrain-error:0.179318\n",
      "[4]\teval-error:0.340426\ttrain-error:0.175316\n",
      "[5]\teval-error:0.340426\ttrain-error:0.174284\n",
      "[6]\teval-error:0.361702\ttrain-error:0.173638\n",
      "[7]\teval-error:0.329787\ttrain-error:0.173251\n",
      "[8]\teval-error:0.340426\ttrain-error:0.173251\n",
      "[9]\teval-error:0.329787\ttrain-error:0.173251\n",
      "[10]\teval-error:0.351064\ttrain-error:0.173251\n",
      "[11]\teval-error:0.340426\ttrain-error:0.173251\n",
      "[12]\teval-error:0.340426\ttrain-error:0.173251\n",
      "[13]\teval-error:0.340426\ttrain-error:0.173251\n",
      "[14]\teval-error:0.340426\ttrain-error:0.173251\n",
      "[15]\teval-error:0.351064\ttrain-error:0.173251\n",
      "[16]\teval-error:0.361702\ttrain-error:0.173251\n",
      "[17]\teval-error:0.351064\ttrain-error:0.173251\n",
      "[18]\teval-error:0.351064\ttrain-error:0.173251\n",
      "[19]\teval-error:0.351064\ttrain-error:0.173251\n",
      "[20]\teval-error:0.351064\ttrain-error:0.173251\n",
      "[21]\teval-error:0.340426\ttrain-error:0.173251\n",
      "[22]\teval-error:0.340426\ttrain-error:0.173251\n",
      "[23]\teval-error:0.340426\ttrain-error:0.173251\n",
      "[24]\teval-error:0.340426\ttrain-error:0.173251\n",
      "[25]\teval-error:0.340426\ttrain-error:0.173251\n",
      "[26]\teval-error:0.340426\ttrain-error:0.173251\n",
      "[27]\teval-error:0.340426\ttrain-error:0.173251\n",
      "[28]\teval-error:0.340426\ttrain-error:0.173251\n",
      "[29]\teval-error:0.351064\ttrain-error:0.173251\n",
      "[30]\teval-error:0.351064\ttrain-error:0.173251\n",
      "[31]\teval-error:0.361702\ttrain-error:0.173251\n",
      "[32]\teval-error:0.351064\ttrain-error:0.173251\n",
      "[33]\teval-error:0.351064\ttrain-error:0.173251\n",
      "[34]\teval-error:0.340426\ttrain-error:0.173251\n",
      "[35]\teval-error:0.340426\ttrain-error:0.173251\n",
      "[36]\teval-error:0.351064\ttrain-error:0.173251\n",
      "[37]\teval-error:0.351064\ttrain-error:0.173251\n",
      "[38]\teval-error:0.351064\ttrain-error:0.173251\n",
      "[39]\teval-error:0.351064\ttrain-error:0.173251\n",
      "[40]\teval-error:0.351064\ttrain-error:0.173251\n",
      "[41]\teval-error:0.351064\ttrain-error:0.173251\n",
      "[42]\teval-error:0.361702\ttrain-error:0.173251\n",
      "[43]\teval-error:0.361702\ttrain-error:0.173251\n",
      "[44]\teval-error:0.361702\ttrain-error:0.173251\n",
      "[45]\teval-error:0.361702\ttrain-error:0.173251\n",
      "[46]\teval-error:0.361702\ttrain-error:0.173251\n",
      "[47]\teval-error:0.361702\ttrain-error:0.173251\n",
      "[48]\teval-error:0.361702\ttrain-error:0.173251\n",
      "[49]\teval-error:0.361702\ttrain-error:0.173251\n",
      "[50]\teval-error:0.361702\ttrain-error:0.173251\n",
      "[51]\teval-error:0.361702\ttrain-error:0.173251\n",
      "[52]\teval-error:0.361702\ttrain-error:0.173251\n",
      "[53]\teval-error:0.361702\ttrain-error:0.173251\n",
      "[54]\teval-error:0.361702\ttrain-error:0.173251\n",
      "[55]\teval-error:0.361702\ttrain-error:0.173251\n",
      "[56]\teval-error:0.361702\ttrain-error:0.173251\n",
      "[57]\teval-error:0.361702\ttrain-error:0.173251\n",
      "[58]\teval-error:0.361702\ttrain-error:0.173251\n",
      "[59]\teval-error:0.361702\ttrain-error:0.173251\n",
      "[60]\teval-error:0.361702\ttrain-error:0.173251\n",
      "[61]\teval-error:0.361702\ttrain-error:0.173251\n",
      "[62]\teval-error:0.361702\ttrain-error:0.173251\n",
      "[63]\teval-error:0.361702\ttrain-error:0.173251\n",
      "[64]\teval-error:0.361702\ttrain-error:0.173251\n",
      "[65]\teval-error:0.361702\ttrain-error:0.173251\n",
      "[66]\teval-error:0.361702\ttrain-error:0.173251\n",
      "[67]\teval-error:0.361702\ttrain-error:0.173251\n",
      "[68]\teval-error:0.361702\ttrain-error:0.173251\n",
      "[69]\teval-error:0.361702\ttrain-error:0.173251\n",
      "[0]\teval-error:0.324427\ttrain-error:0.178046\n",
      "[1]\teval-error:0.278626\ttrain-error:0.164548\n",
      "[2]\teval-error:0.282443\ttrain-error:0.156978\n",
      "[3]\teval-error:0.282443\ttrain-error:0.153264\n",
      "[4]\teval-error:0.270992\ttrain-error:0.151907\n",
      "[5]\teval-error:0.270992\ttrain-error:0.15105\n",
      "[6]\teval-error:0.267176\ttrain-error:0.150621\n",
      "[7]\teval-error:0.263359\ttrain-error:0.149979\n",
      "[8]\teval-error:0.290076\ttrain-error:0.149907\n",
      "[9]\teval-error:0.274809\ttrain-error:0.149907\n",
      "[10]\teval-error:0.28626\ttrain-error:0.149907\n",
      "[11]\teval-error:0.29771\ttrain-error:0.149907\n",
      "[12]\teval-error:0.301527\ttrain-error:0.149836\n",
      "[13]\teval-error:0.290076\ttrain-error:0.149836\n",
      "[14]\teval-error:0.293893\ttrain-error:0.149836\n",
      "[15]\teval-error:0.290076\ttrain-error:0.149836\n",
      "[16]\teval-error:0.282443\ttrain-error:0.149836\n",
      "[17]\teval-error:0.301527\ttrain-error:0.149836\n",
      "[18]\teval-error:0.293893\ttrain-error:0.149836\n",
      "[19]\teval-error:0.290076\ttrain-error:0.149836\n",
      "[20]\teval-error:0.270992\ttrain-error:0.149836\n",
      "[21]\teval-error:0.263359\ttrain-error:0.149836\n",
      "[22]\teval-error:0.263359\ttrain-error:0.149836\n",
      "[23]\teval-error:0.267176\ttrain-error:0.149836\n",
      "[24]\teval-error:0.278626\ttrain-error:0.149836\n",
      "[25]\teval-error:0.282443\ttrain-error:0.149836\n",
      "[26]\teval-error:0.270992\ttrain-error:0.149836\n",
      "[27]\teval-error:0.267176\ttrain-error:0.149836\n",
      "[28]\teval-error:0.274809\ttrain-error:0.149836\n",
      "[29]\teval-error:0.267176\ttrain-error:0.149836\n",
      "[30]\teval-error:0.267176\ttrain-error:0.149836\n",
      "[31]\teval-error:0.267176\ttrain-error:0.149836\n",
      "[32]\teval-error:0.274809\ttrain-error:0.149836\n",
      "[33]\teval-error:0.274809\ttrain-error:0.149836\n",
      "[34]\teval-error:0.282443\ttrain-error:0.149836\n",
      "[35]\teval-error:0.278626\ttrain-error:0.149836\n",
      "[36]\teval-error:0.274809\ttrain-error:0.149836\n",
      "[37]\teval-error:0.274809\ttrain-error:0.149836\n",
      "[38]\teval-error:0.274809\ttrain-error:0.149836\n",
      "[39]\teval-error:0.282443\ttrain-error:0.149836\n",
      "[40]\teval-error:0.278626\ttrain-error:0.149836\n",
      "[41]\teval-error:0.267176\ttrain-error:0.149836\n",
      "[42]\teval-error:0.274809\ttrain-error:0.149836\n",
      "[43]\teval-error:0.274809\ttrain-error:0.149836\n",
      "[44]\teval-error:0.274809\ttrain-error:0.149836\n",
      "[45]\teval-error:0.282443\ttrain-error:0.149836\n",
      "[46]\teval-error:0.278626\ttrain-error:0.149836\n",
      "[47]\teval-error:0.270992\ttrain-error:0.149836\n",
      "[48]\teval-error:0.259542\ttrain-error:0.149836\n",
      "[49]\teval-error:0.263359\ttrain-error:0.149836\n",
      "[50]\teval-error:0.274809\ttrain-error:0.149836\n",
      "[51]\teval-error:0.274809\ttrain-error:0.149836\n",
      "[52]\teval-error:0.278626\ttrain-error:0.149836\n",
      "[53]\teval-error:0.270992\ttrain-error:0.149836\n",
      "[54]\teval-error:0.267176\ttrain-error:0.149836\n",
      "[55]\teval-error:0.274809\ttrain-error:0.149836\n",
      "[56]\teval-error:0.270992\ttrain-error:0.149836\n",
      "[57]\teval-error:0.290076\ttrain-error:0.149836\n",
      "[58]\teval-error:0.270992\ttrain-error:0.149836\n",
      "[59]\teval-error:0.274809\ttrain-error:0.149836\n",
      "[60]\teval-error:0.270992\ttrain-error:0.149836\n",
      "[61]\teval-error:0.28626\ttrain-error:0.149836\n",
      "[62]\teval-error:0.274809\ttrain-error:0.149836\n",
      "[63]\teval-error:0.270992\ttrain-error:0.149836\n",
      "[64]\teval-error:0.263359\ttrain-error:0.149836\n",
      "[65]\teval-error:0.255725\ttrain-error:0.149836\n",
      "[66]\teval-error:0.263359\ttrain-error:0.149836\n",
      "[67]\teval-error:0.255725\ttrain-error:0.149836\n",
      "[68]\teval-error:0.282443\ttrain-error:0.149836\n",
      "[69]\teval-error:0.278626\ttrain-error:0.149836\n",
      "[0]\teval-error:0.362595\ttrain-error:0.183402\n",
      "[1]\teval-error:0.335878\ttrain-error:0.171475\n",
      "[2]\teval-error:0.312977\ttrain-error:0.164191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\teval-error:0.312977\ttrain-error:0.159192\n",
      "[4]\teval-error:0.332061\ttrain-error:0.156978\n",
      "[5]\teval-error:0.347328\ttrain-error:0.155835\n",
      "[6]\teval-error:0.320611\ttrain-error:0.155264\n",
      "[7]\teval-error:0.305344\ttrain-error:0.154692\n",
      "[8]\teval-error:0.312977\ttrain-error:0.154621\n",
      "[9]\teval-error:0.305344\ttrain-error:0.154621\n",
      "[10]\teval-error:0.301527\ttrain-error:0.154549\n",
      "[11]\teval-error:0.30916\ttrain-error:0.154549\n",
      "[12]\teval-error:0.301527\ttrain-error:0.154549\n",
      "[13]\teval-error:0.305344\ttrain-error:0.154549\n",
      "[14]\teval-error:0.320611\ttrain-error:0.154549\n",
      "[15]\teval-error:0.30916\ttrain-error:0.154549\n",
      "[16]\teval-error:0.301527\ttrain-error:0.154549\n",
      "[17]\teval-error:0.316794\ttrain-error:0.154549\n",
      "[18]\teval-error:0.320611\ttrain-error:0.154549\n",
      "[19]\teval-error:0.305344\ttrain-error:0.154549\n",
      "[20]\teval-error:0.312977\ttrain-error:0.154549\n",
      "[21]\teval-error:0.305344\ttrain-error:0.154549\n",
      "[22]\teval-error:0.30916\ttrain-error:0.154549\n",
      "[23]\teval-error:0.301527\ttrain-error:0.154549\n",
      "[24]\teval-error:0.30916\ttrain-error:0.154549\n",
      "[25]\teval-error:0.316794\ttrain-error:0.154549\n",
      "[26]\teval-error:0.305344\ttrain-error:0.154549\n",
      "[27]\teval-error:0.29771\ttrain-error:0.154549\n",
      "[28]\teval-error:0.301527\ttrain-error:0.154549\n",
      "[29]\teval-error:0.305344\ttrain-error:0.154549\n",
      "[30]\teval-error:0.320611\ttrain-error:0.154549\n",
      "[31]\teval-error:0.305344\ttrain-error:0.154549\n",
      "[32]\teval-error:0.293893\ttrain-error:0.154549\n",
      "[33]\teval-error:0.29771\ttrain-error:0.154549\n",
      "[34]\teval-error:0.28626\ttrain-error:0.154549\n",
      "[35]\teval-error:0.28626\ttrain-error:0.154549\n",
      "[36]\teval-error:0.28626\ttrain-error:0.154549\n",
      "[37]\teval-error:0.290076\ttrain-error:0.154549\n",
      "[38]\teval-error:0.290076\ttrain-error:0.154549\n",
      "[39]\teval-error:0.293893\ttrain-error:0.154549\n",
      "[40]\teval-error:0.290076\ttrain-error:0.154549\n",
      "[41]\teval-error:0.290076\ttrain-error:0.154549\n",
      "[42]\teval-error:0.305344\ttrain-error:0.154549\n",
      "[43]\teval-error:0.290076\ttrain-error:0.154549\n",
      "[44]\teval-error:0.290076\ttrain-error:0.154549\n",
      "[45]\teval-error:0.293893\ttrain-error:0.154549\n",
      "[46]\teval-error:0.28626\ttrain-error:0.154549\n",
      "[47]\teval-error:0.293893\ttrain-error:0.154549\n",
      "[48]\teval-error:0.29771\ttrain-error:0.154549\n",
      "[49]\teval-error:0.301527\ttrain-error:0.154549\n",
      "[50]\teval-error:0.28626\ttrain-error:0.154549\n",
      "[51]\teval-error:0.305344\ttrain-error:0.154549\n",
      "[52]\teval-error:0.293893\ttrain-error:0.154549\n",
      "[53]\teval-error:0.293893\ttrain-error:0.154549\n",
      "[54]\teval-error:0.293893\ttrain-error:0.154549\n",
      "[55]\teval-error:0.290076\ttrain-error:0.154549\n",
      "[56]\teval-error:0.290076\ttrain-error:0.154549\n",
      "[57]\teval-error:0.28626\ttrain-error:0.154549\n",
      "[58]\teval-error:0.28626\ttrain-error:0.154549\n",
      "[59]\teval-error:0.312977\ttrain-error:0.154549\n",
      "[60]\teval-error:0.29771\ttrain-error:0.154549\n",
      "[61]\teval-error:0.29771\ttrain-error:0.154549\n",
      "[62]\teval-error:0.29771\ttrain-error:0.154549\n",
      "[63]\teval-error:0.29771\ttrain-error:0.154549\n",
      "[64]\teval-error:0.28626\ttrain-error:0.154549\n",
      "[65]\teval-error:0.290076\ttrain-error:0.154549\n",
      "[66]\teval-error:0.29771\ttrain-error:0.154549\n",
      "[67]\teval-error:0.293893\ttrain-error:0.154549\n",
      "[68]\teval-error:0.293893\ttrain-error:0.154549\n",
      "[69]\teval-error:0.290076\ttrain-error:0.154549\n",
      "[0]\teval-error:0.381679\ttrain-error:0.191901\n",
      "[1]\teval-error:0.370229\ttrain-error:0.180903\n",
      "[2]\teval-error:0.358779\ttrain-error:0.174261\n",
      "[3]\teval-error:0.358779\ttrain-error:0.16969\n",
      "[4]\teval-error:0.354962\ttrain-error:0.167405\n",
      "[5]\teval-error:0.343511\ttrain-error:0.166833\n",
      "[6]\teval-error:0.354962\ttrain-error:0.166476\n",
      "[7]\teval-error:0.354962\ttrain-error:0.165905\n",
      "[8]\teval-error:0.339695\ttrain-error:0.165833\n",
      "[9]\teval-error:0.358779\ttrain-error:0.165691\n",
      "[10]\teval-error:0.339695\ttrain-error:0.165476\n",
      "[11]\teval-error:0.343511\ttrain-error:0.165548\n",
      "[12]\teval-error:0.347328\ttrain-error:0.165476\n",
      "[13]\teval-error:0.335878\ttrain-error:0.165262\n",
      "[14]\teval-error:0.339695\ttrain-error:0.165191\n",
      "[15]\teval-error:0.343511\ttrain-error:0.165119\n",
      "[16]\teval-error:0.347328\ttrain-error:0.165191\n",
      "[17]\teval-error:0.335878\ttrain-error:0.165119\n",
      "[18]\teval-error:0.354962\ttrain-error:0.165119\n",
      "[19]\teval-error:0.343511\ttrain-error:0.165119\n",
      "[20]\teval-error:0.335878\ttrain-error:0.165119\n",
      "[21]\teval-error:0.343511\ttrain-error:0.165048\n",
      "[22]\teval-error:0.332061\ttrain-error:0.165048\n",
      "[23]\teval-error:0.343511\ttrain-error:0.165048\n",
      "[24]\teval-error:0.354962\ttrain-error:0.165048\n",
      "[25]\teval-error:0.339695\ttrain-error:0.165048\n",
      "[26]\teval-error:0.347328\ttrain-error:0.165048\n",
      "[27]\teval-error:0.354962\ttrain-error:0.165048\n",
      "[28]\teval-error:0.347328\ttrain-error:0.165048\n",
      "[29]\teval-error:0.351145\ttrain-error:0.165048\n",
      "[30]\teval-error:0.343511\ttrain-error:0.165048\n",
      "[31]\teval-error:0.354962\ttrain-error:0.165048\n",
      "[32]\teval-error:0.335878\ttrain-error:0.165048\n",
      "[33]\teval-error:0.351145\ttrain-error:0.165048\n",
      "[34]\teval-error:0.347328\ttrain-error:0.165048\n",
      "[35]\teval-error:0.347328\ttrain-error:0.165048\n",
      "[36]\teval-error:0.347328\ttrain-error:0.165048\n",
      "[37]\teval-error:0.351145\ttrain-error:0.165048\n",
      "[38]\teval-error:0.343511\ttrain-error:0.165048\n",
      "[39]\teval-error:0.332061\ttrain-error:0.165048\n",
      "[40]\teval-error:0.347328\ttrain-error:0.165048\n",
      "[41]\teval-error:0.332061\ttrain-error:0.165048\n",
      "[42]\teval-error:0.347328\ttrain-error:0.165048\n",
      "[43]\teval-error:0.339695\ttrain-error:0.165048\n",
      "[44]\teval-error:0.335878\ttrain-error:0.165048\n",
      "[45]\teval-error:0.351145\ttrain-error:0.165048\n",
      "[46]\teval-error:0.351145\ttrain-error:0.165048\n",
      "[47]\teval-error:0.351145\ttrain-error:0.165048\n",
      "[48]\teval-error:0.351145\ttrain-error:0.165048\n",
      "[49]\teval-error:0.339695\ttrain-error:0.165048\n",
      "[50]\teval-error:0.354962\ttrain-error:0.165048\n",
      "[51]\teval-error:0.335878\ttrain-error:0.165048\n",
      "[52]\teval-error:0.358779\ttrain-error:0.165048\n",
      "[53]\teval-error:0.339695\ttrain-error:0.165048\n",
      "[54]\teval-error:0.339695\ttrain-error:0.165048\n",
      "[55]\teval-error:0.347328\ttrain-error:0.165048\n",
      "[56]\teval-error:0.332061\ttrain-error:0.165048\n",
      "[57]\teval-error:0.343511\ttrain-error:0.165048\n",
      "[58]\teval-error:0.335878\ttrain-error:0.165048\n",
      "[59]\teval-error:0.335878\ttrain-error:0.165048\n",
      "[60]\teval-error:0.347328\ttrain-error:0.165048\n",
      "[61]\teval-error:0.351145\ttrain-error:0.165048\n",
      "[62]\teval-error:0.347328\ttrain-error:0.165048\n",
      "[63]\teval-error:0.347328\ttrain-error:0.165048\n",
      "[64]\teval-error:0.339695\ttrain-error:0.165048\n",
      "[65]\teval-error:0.339695\ttrain-error:0.165048\n",
      "[66]\teval-error:0.354962\ttrain-error:0.165048\n",
      "[67]\teval-error:0.354962\ttrain-error:0.165048\n",
      "[68]\teval-error:0.358779\ttrain-error:0.165048\n",
      "[69]\teval-error:0.366412\ttrain-error:0.165048\n",
      "[0]\teval-error:0.316794\ttrain-error:0.17676\n",
      "[1]\teval-error:0.324427\ttrain-error:0.164405\n",
      "[2]\teval-error:0.320611\ttrain-error:0.157335\n",
      "[3]\teval-error:0.312977\ttrain-error:0.152978\n",
      "[4]\teval-error:0.312977\ttrain-error:0.151407\n",
      "[5]\teval-error:0.301527\ttrain-error:0.149693\n",
      "[6]\teval-error:0.301527\ttrain-error:0.14905\n",
      "[7]\teval-error:0.312977\ttrain-error:0.148979\n",
      "[8]\teval-error:0.312977\ttrain-error:0.148836\n",
      "[9]\teval-error:0.29771\ttrain-error:0.148479\n",
      "[10]\teval-error:0.28626\ttrain-error:0.148407\n",
      "[11]\teval-error:0.293893\ttrain-error:0.148265\n",
      "[12]\teval-error:0.290076\ttrain-error:0.148122\n",
      "[13]\teval-error:0.28626\ttrain-error:0.148122\n",
      "[14]\teval-error:0.278626\ttrain-error:0.148122\n",
      "[15]\teval-error:0.270992\ttrain-error:0.148122\n",
      "[16]\teval-error:0.274809\ttrain-error:0.148122\n",
      "[17]\teval-error:0.274809\ttrain-error:0.148122\n",
      "[18]\teval-error:0.274809\ttrain-error:0.148122\n",
      "[19]\teval-error:0.274809\ttrain-error:0.148122\n",
      "[20]\teval-error:0.28626\ttrain-error:0.148122\n",
      "[21]\teval-error:0.290076\ttrain-error:0.148122\n",
      "[22]\teval-error:0.282443\ttrain-error:0.148122\n",
      "[23]\teval-error:0.282443\ttrain-error:0.148122\n",
      "[24]\teval-error:0.282443\ttrain-error:0.148122\n",
      "[25]\teval-error:0.290076\ttrain-error:0.148122\n",
      "[26]\teval-error:0.293893\ttrain-error:0.148122\n",
      "[27]\teval-error:0.278626\ttrain-error:0.148122\n",
      "[28]\teval-error:0.282443\ttrain-error:0.148122\n",
      "[29]\teval-error:0.290076\ttrain-error:0.148122\n",
      "[30]\teval-error:0.293893\ttrain-error:0.148122\n",
      "[31]\teval-error:0.28626\ttrain-error:0.148122\n",
      "[32]\teval-error:0.282443\ttrain-error:0.148122\n",
      "[33]\teval-error:0.282443\ttrain-error:0.148122\n",
      "[34]\teval-error:0.278626\ttrain-error:0.148122\n",
      "[35]\teval-error:0.278626\ttrain-error:0.148122\n",
      "[36]\teval-error:0.274809\ttrain-error:0.148122\n",
      "[37]\teval-error:0.278626\ttrain-error:0.148122\n",
      "[38]\teval-error:0.274809\ttrain-error:0.148122\n",
      "[39]\teval-error:0.274809\ttrain-error:0.148122\n",
      "[40]\teval-error:0.267176\ttrain-error:0.148122\n",
      "[41]\teval-error:0.278626\ttrain-error:0.148122\n",
      "[42]\teval-error:0.278626\ttrain-error:0.148122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43]\teval-error:0.28626\ttrain-error:0.148122\n",
      "[44]\teval-error:0.270992\ttrain-error:0.148122\n",
      "[45]\teval-error:0.274809\ttrain-error:0.148122\n",
      "[46]\teval-error:0.282443\ttrain-error:0.148122\n",
      "[47]\teval-error:0.274809\ttrain-error:0.148122\n",
      "[48]\teval-error:0.282443\ttrain-error:0.148122\n",
      "[49]\teval-error:0.267176\ttrain-error:0.148122\n",
      "[50]\teval-error:0.267176\ttrain-error:0.148122\n",
      "[51]\teval-error:0.278626\ttrain-error:0.148122\n",
      "[52]\teval-error:0.267176\ttrain-error:0.148122\n",
      "[53]\teval-error:0.274809\ttrain-error:0.148122\n",
      "[54]\teval-error:0.270992\ttrain-error:0.148122\n",
      "[55]\teval-error:0.274809\ttrain-error:0.148122\n",
      "[56]\teval-error:0.278626\ttrain-error:0.148122\n",
      "[57]\teval-error:0.278626\ttrain-error:0.148122\n",
      "[58]\teval-error:0.274809\ttrain-error:0.148122\n",
      "[59]\teval-error:0.270992\ttrain-error:0.148122\n",
      "[60]\teval-error:0.278626\ttrain-error:0.148122\n",
      "[61]\teval-error:0.274809\ttrain-error:0.148122\n",
      "[62]\teval-error:0.274809\ttrain-error:0.148122\n",
      "[63]\teval-error:0.278626\ttrain-error:0.148122\n",
      "[64]\teval-error:0.274809\ttrain-error:0.148122\n",
      "[65]\teval-error:0.28626\ttrain-error:0.148122\n",
      "[66]\teval-error:0.282443\ttrain-error:0.148122\n",
      "[67]\teval-error:0.270992\ttrain-error:0.148122\n",
      "[68]\teval-error:0.282443\ttrain-error:0.148122\n",
      "[69]\teval-error:0.278626\ttrain-error:0.148122\n",
      "[0]\teval-error:0.329545\ttrain-error:0.125203\n",
      "[1]\teval-error:0.25\ttrain-error:0.091335\n",
      "[2]\teval-error:0.227273\ttrain-error:0.0735\n",
      "[3]\teval-error:0.181818\ttrain-error:0.067736\n",
      "[4]\teval-error:0.193182\ttrain-error:0.064493\n",
      "[5]\teval-error:0.215909\ttrain-error:0.062331\n",
      "[6]\teval-error:0.227273\ttrain-error:0.06143\n",
      "[7]\teval-error:0.227273\ttrain-error:0.06107\n",
      "[8]\teval-error:0.227273\ttrain-error:0.06107\n",
      "[9]\teval-error:0.204545\ttrain-error:0.06107\n",
      "[10]\teval-error:0.204545\ttrain-error:0.06107\n",
      "[11]\teval-error:0.204545\ttrain-error:0.06107\n",
      "[12]\teval-error:0.215909\ttrain-error:0.06107\n",
      "[13]\teval-error:0.238636\ttrain-error:0.06107\n",
      "[14]\teval-error:0.204545\ttrain-error:0.06107\n",
      "[15]\teval-error:0.215909\ttrain-error:0.06107\n",
      "[16]\teval-error:0.227273\ttrain-error:0.06089\n",
      "[17]\teval-error:0.227273\ttrain-error:0.06089\n",
      "[18]\teval-error:0.227273\ttrain-error:0.06089\n",
      "[19]\teval-error:0.227273\ttrain-error:0.06089\n",
      "[20]\teval-error:0.215909\ttrain-error:0.06089\n",
      "[21]\teval-error:0.193182\ttrain-error:0.06089\n",
      "[22]\teval-error:0.193182\ttrain-error:0.06089\n",
      "[23]\teval-error:0.181818\ttrain-error:0.06089\n",
      "[24]\teval-error:0.181818\ttrain-error:0.06089\n",
      "[25]\teval-error:0.193182\ttrain-error:0.06089\n",
      "[26]\teval-error:0.181818\ttrain-error:0.06089\n",
      "[27]\teval-error:0.181818\ttrain-error:0.06089\n",
      "[28]\teval-error:0.170455\ttrain-error:0.06089\n",
      "[29]\teval-error:0.181818\ttrain-error:0.06089\n",
      "[30]\teval-error:0.170455\ttrain-error:0.06089\n",
      "[31]\teval-error:0.170455\ttrain-error:0.06089\n",
      "[32]\teval-error:0.170455\ttrain-error:0.06089\n",
      "[33]\teval-error:0.193182\ttrain-error:0.06089\n",
      "[34]\teval-error:0.181818\ttrain-error:0.06089\n",
      "[35]\teval-error:0.181818\ttrain-error:0.06089\n",
      "[36]\teval-error:0.181818\ttrain-error:0.06089\n",
      "[37]\teval-error:0.181818\ttrain-error:0.06089\n",
      "[38]\teval-error:0.193182\ttrain-error:0.06089\n",
      "[39]\teval-error:0.181818\ttrain-error:0.06089\n",
      "[40]\teval-error:0.193182\ttrain-error:0.06089\n",
      "[41]\teval-error:0.181818\ttrain-error:0.06089\n",
      "[42]\teval-error:0.181818\ttrain-error:0.06089\n",
      "[43]\teval-error:0.181818\ttrain-error:0.06089\n",
      "[44]\teval-error:0.181818\ttrain-error:0.06089\n",
      "[45]\teval-error:0.181818\ttrain-error:0.06089\n",
      "[46]\teval-error:0.181818\ttrain-error:0.06089\n",
      "[47]\teval-error:0.181818\ttrain-error:0.06089\n",
      "[48]\teval-error:0.181818\ttrain-error:0.06089\n",
      "[49]\teval-error:0.181818\ttrain-error:0.06089\n",
      "[50]\teval-error:0.181818\ttrain-error:0.06089\n",
      "[51]\teval-error:0.181818\ttrain-error:0.06089\n",
      "[52]\teval-error:0.181818\ttrain-error:0.06089\n",
      "[53]\teval-error:0.181818\ttrain-error:0.06089\n",
      "[54]\teval-error:0.181818\ttrain-error:0.06089\n",
      "[55]\teval-error:0.181818\ttrain-error:0.06089\n",
      "[56]\teval-error:0.181818\ttrain-error:0.06089\n",
      "[57]\teval-error:0.181818\ttrain-error:0.06089\n",
      "[58]\teval-error:0.181818\ttrain-error:0.06089\n",
      "[59]\teval-error:0.181818\ttrain-error:0.06089\n",
      "[60]\teval-error:0.181818\ttrain-error:0.06089\n",
      "[61]\teval-error:0.181818\ttrain-error:0.06089\n",
      "[62]\teval-error:0.181818\ttrain-error:0.06089\n",
      "[63]\teval-error:0.181818\ttrain-error:0.06089\n",
      "[64]\teval-error:0.181818\ttrain-error:0.06089\n",
      "[65]\teval-error:0.181818\ttrain-error:0.06089\n",
      "[66]\teval-error:0.181818\ttrain-error:0.06089\n",
      "[67]\teval-error:0.193182\ttrain-error:0.06089\n",
      "[68]\teval-error:0.193182\ttrain-error:0.06089\n",
      "[69]\teval-error:0.193182\ttrain-error:0.06089\n",
      "[0]\teval-error:0.284091\ttrain-error:0.118717\n",
      "[1]\teval-error:0.238636\ttrain-error:0.091335\n",
      "[2]\teval-error:0.238636\ttrain-error:0.07332\n",
      "[3]\teval-error:0.227273\ttrain-error:0.067015\n",
      "[4]\teval-error:0.25\ttrain-error:0.063232\n",
      "[5]\teval-error:0.25\ttrain-error:0.061611\n",
      "[6]\teval-error:0.238636\ttrain-error:0.06125\n",
      "[7]\teval-error:0.238636\ttrain-error:0.06107\n",
      "[8]\teval-error:0.238636\ttrain-error:0.06071\n",
      "[9]\teval-error:0.227273\ttrain-error:0.06071\n",
      "[10]\teval-error:0.238636\ttrain-error:0.06071\n",
      "[11]\teval-error:0.238636\ttrain-error:0.06053\n",
      "[12]\teval-error:0.261364\ttrain-error:0.06053\n",
      "[13]\teval-error:0.238636\ttrain-error:0.06053\n",
      "[14]\teval-error:0.261364\ttrain-error:0.06053\n",
      "[15]\teval-error:0.261364\ttrain-error:0.06053\n",
      "[16]\teval-error:0.25\ttrain-error:0.06053\n",
      "[17]\teval-error:0.25\ttrain-error:0.06053\n",
      "[18]\teval-error:0.261364\ttrain-error:0.06053\n",
      "[19]\teval-error:0.25\ttrain-error:0.06053\n",
      "[20]\teval-error:0.25\ttrain-error:0.06053\n",
      "[21]\teval-error:0.25\ttrain-error:0.06053\n",
      "[22]\teval-error:0.227273\ttrain-error:0.06053\n",
      "[23]\teval-error:0.238636\ttrain-error:0.06053\n",
      "[24]\teval-error:0.227273\ttrain-error:0.06053\n",
      "[25]\teval-error:0.227273\ttrain-error:0.06053\n",
      "[26]\teval-error:0.238636\ttrain-error:0.06053\n",
      "[27]\teval-error:0.25\ttrain-error:0.06053\n",
      "[28]\teval-error:0.25\ttrain-error:0.06053\n",
      "[29]\teval-error:0.227273\ttrain-error:0.06053\n",
      "[30]\teval-error:0.227273\ttrain-error:0.06053\n",
      "[31]\teval-error:0.238636\ttrain-error:0.06053\n",
      "[32]\teval-error:0.227273\ttrain-error:0.06053\n",
      "[33]\teval-error:0.238636\ttrain-error:0.06053\n",
      "[34]\teval-error:0.238636\ttrain-error:0.06053\n",
      "[35]\teval-error:0.227273\ttrain-error:0.06053\n",
      "[36]\teval-error:0.238636\ttrain-error:0.06053\n",
      "[37]\teval-error:0.227273\ttrain-error:0.06053\n",
      "[38]\teval-error:0.227273\ttrain-error:0.06053\n",
      "[39]\teval-error:0.227273\ttrain-error:0.06053\n",
      "[40]\teval-error:0.227273\ttrain-error:0.06053\n",
      "[41]\teval-error:0.215909\ttrain-error:0.06053\n",
      "[42]\teval-error:0.215909\ttrain-error:0.06053\n",
      "[43]\teval-error:0.215909\ttrain-error:0.06053\n",
      "[44]\teval-error:0.215909\ttrain-error:0.06053\n",
      "[45]\teval-error:0.215909\ttrain-error:0.06053\n",
      "[46]\teval-error:0.215909\ttrain-error:0.06053\n",
      "[47]\teval-error:0.215909\ttrain-error:0.06053\n",
      "[48]\teval-error:0.204545\ttrain-error:0.06053\n",
      "[49]\teval-error:0.204545\ttrain-error:0.06053\n",
      "[50]\teval-error:0.215909\ttrain-error:0.06053\n",
      "[51]\teval-error:0.215909\ttrain-error:0.06053\n",
      "[52]\teval-error:0.215909\ttrain-error:0.06053\n",
      "[53]\teval-error:0.215909\ttrain-error:0.06053\n",
      "[54]\teval-error:0.215909\ttrain-error:0.06053\n",
      "[55]\teval-error:0.215909\ttrain-error:0.06053\n",
      "[56]\teval-error:0.215909\ttrain-error:0.06053\n",
      "[57]\teval-error:0.215909\ttrain-error:0.06053\n",
      "[58]\teval-error:0.215909\ttrain-error:0.06053\n",
      "[59]\teval-error:0.215909\ttrain-error:0.06053\n",
      "[60]\teval-error:0.215909\ttrain-error:0.06053\n",
      "[61]\teval-error:0.215909\ttrain-error:0.06053\n",
      "[62]\teval-error:0.215909\ttrain-error:0.06053\n",
      "[63]\teval-error:0.215909\ttrain-error:0.06053\n",
      "[64]\teval-error:0.204545\ttrain-error:0.06053\n",
      "[65]\teval-error:0.204545\ttrain-error:0.06053\n",
      "[66]\teval-error:0.204545\ttrain-error:0.06053\n",
      "[67]\teval-error:0.204545\ttrain-error:0.06053\n",
      "[68]\teval-error:0.215909\ttrain-error:0.06053\n",
      "[69]\teval-error:0.204545\ttrain-error:0.06053\n",
      "[0]\teval-error:0.318182\ttrain-error:0.126103\n",
      "[1]\teval-error:0.295455\ttrain-error:0.094938\n",
      "[2]\teval-error:0.261364\ttrain-error:0.077824\n",
      "[3]\teval-error:0.261364\ttrain-error:0.070077\n",
      "[4]\teval-error:0.284091\ttrain-error:0.065394\n",
      "[5]\teval-error:0.284091\ttrain-error:0.064853\n",
      "[6]\teval-error:0.295455\ttrain-error:0.063772\n",
      "[7]\teval-error:0.284091\ttrain-error:0.063052\n",
      "[8]\teval-error:0.295455\ttrain-error:0.062872\n",
      "[9]\teval-error:0.306818\ttrain-error:0.062872\n",
      "[10]\teval-error:0.295455\ttrain-error:0.062872\n",
      "[11]\teval-error:0.284091\ttrain-error:0.062872\n",
      "[12]\teval-error:0.295455\ttrain-error:0.062872\n",
      "[13]\teval-error:0.295455\ttrain-error:0.062872\n",
      "[14]\teval-error:0.284091\ttrain-error:0.062691\n",
      "[15]\teval-error:0.272727\ttrain-error:0.062691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16]\teval-error:0.272727\ttrain-error:0.062872\n",
      "[17]\teval-error:0.284091\ttrain-error:0.062691\n",
      "[18]\teval-error:0.284091\ttrain-error:0.062691\n",
      "[19]\teval-error:0.272727\ttrain-error:0.062691\n",
      "[20]\teval-error:0.272727\ttrain-error:0.062691\n",
      "[21]\teval-error:0.261364\ttrain-error:0.062691\n",
      "[22]\teval-error:0.272727\ttrain-error:0.062691\n",
      "[23]\teval-error:0.272727\ttrain-error:0.062691\n",
      "[24]\teval-error:0.272727\ttrain-error:0.062691\n",
      "[25]\teval-error:0.284091\ttrain-error:0.062691\n",
      "[26]\teval-error:0.284091\ttrain-error:0.062691\n",
      "[27]\teval-error:0.284091\ttrain-error:0.062691\n",
      "[28]\teval-error:0.284091\ttrain-error:0.062691\n",
      "[29]\teval-error:0.272727\ttrain-error:0.062691\n",
      "[30]\teval-error:0.272727\ttrain-error:0.062691\n",
      "[31]\teval-error:0.272727\ttrain-error:0.062691\n",
      "[32]\teval-error:0.272727\ttrain-error:0.062691\n",
      "[33]\teval-error:0.272727\ttrain-error:0.062691\n",
      "[34]\teval-error:0.272727\ttrain-error:0.062691\n",
      "[35]\teval-error:0.272727\ttrain-error:0.062691\n",
      "[36]\teval-error:0.272727\ttrain-error:0.062691\n",
      "[37]\teval-error:0.272727\ttrain-error:0.062691\n",
      "[38]\teval-error:0.272727\ttrain-error:0.062691\n",
      "[39]\teval-error:0.272727\ttrain-error:0.062691\n",
      "[40]\teval-error:0.261364\ttrain-error:0.062691\n",
      "[41]\teval-error:0.272727\ttrain-error:0.062691\n",
      "[42]\teval-error:0.272727\ttrain-error:0.062691\n",
      "[43]\teval-error:0.284091\ttrain-error:0.062691\n",
      "[44]\teval-error:0.272727\ttrain-error:0.062691\n",
      "[45]\teval-error:0.272727\ttrain-error:0.062691\n",
      "[46]\teval-error:0.261364\ttrain-error:0.062691\n",
      "[47]\teval-error:0.272727\ttrain-error:0.062691\n",
      "[48]\teval-error:0.284091\ttrain-error:0.062691\n",
      "[49]\teval-error:0.284091\ttrain-error:0.062691\n",
      "[50]\teval-error:0.272727\ttrain-error:0.062691\n",
      "[51]\teval-error:0.272727\ttrain-error:0.062691\n",
      "[52]\teval-error:0.272727\ttrain-error:0.062691\n",
      "[53]\teval-error:0.261364\ttrain-error:0.062691\n",
      "[54]\teval-error:0.261364\ttrain-error:0.062691\n",
      "[55]\teval-error:0.272727\ttrain-error:0.062691\n",
      "[56]\teval-error:0.261364\ttrain-error:0.062691\n",
      "[57]\teval-error:0.25\ttrain-error:0.062691\n",
      "[58]\teval-error:0.25\ttrain-error:0.062691\n",
      "[59]\teval-error:0.261364\ttrain-error:0.062691\n",
      "[60]\teval-error:0.261364\ttrain-error:0.062691\n",
      "[61]\teval-error:0.25\ttrain-error:0.062691\n",
      "[62]\teval-error:0.25\ttrain-error:0.062691\n",
      "[63]\teval-error:0.261364\ttrain-error:0.062691\n",
      "[64]\teval-error:0.261364\ttrain-error:0.062691\n",
      "[65]\teval-error:0.261364\ttrain-error:0.062691\n",
      "[66]\teval-error:0.25\ttrain-error:0.062691\n",
      "[67]\teval-error:0.25\ttrain-error:0.062691\n",
      "[68]\teval-error:0.261364\ttrain-error:0.062691\n",
      "[69]\teval-error:0.261364\ttrain-error:0.062691\n",
      "[0]\teval-error:0.272727\ttrain-error:0.125383\n",
      "[1]\teval-error:0.25\ttrain-error:0.090434\n",
      "[2]\teval-error:0.238636\ttrain-error:0.07278\n",
      "[3]\teval-error:0.238636\ttrain-error:0.066475\n",
      "[4]\teval-error:0.204545\ttrain-error:0.063592\n",
      "[5]\teval-error:0.204545\ttrain-error:0.062151\n",
      "[6]\teval-error:0.215909\ttrain-error:0.061611\n",
      "[7]\teval-error:0.204545\ttrain-error:0.06143\n",
      "[8]\teval-error:0.181818\ttrain-error:0.06107\n",
      "[9]\teval-error:0.181818\ttrain-error:0.06107\n",
      "[10]\teval-error:0.170455\ttrain-error:0.06071\n",
      "[11]\teval-error:0.170455\ttrain-error:0.06071\n",
      "[12]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[13]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[14]\teval-error:0.170455\ttrain-error:0.06071\n",
      "[15]\teval-error:0.193182\ttrain-error:0.06071\n",
      "[16]\teval-error:0.204545\ttrain-error:0.06071\n",
      "[17]\teval-error:0.215909\ttrain-error:0.06071\n",
      "[18]\teval-error:0.215909\ttrain-error:0.06071\n",
      "[19]\teval-error:0.193182\ttrain-error:0.06071\n",
      "[20]\teval-error:0.193182\ttrain-error:0.06071\n",
      "[21]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[22]\teval-error:0.204545\ttrain-error:0.06071\n",
      "[23]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[24]\teval-error:0.193182\ttrain-error:0.06071\n",
      "[25]\teval-error:0.193182\ttrain-error:0.06071\n",
      "[26]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[27]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[28]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[29]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[30]\teval-error:0.193182\ttrain-error:0.06071\n",
      "[31]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[32]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[33]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[34]\teval-error:0.193182\ttrain-error:0.06071\n",
      "[35]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[36]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[37]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[38]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[39]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[40]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[41]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[42]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[43]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[44]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[45]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[46]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[47]\teval-error:0.193182\ttrain-error:0.06071\n",
      "[48]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[49]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[50]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[51]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[52]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[53]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[54]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[55]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[56]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[57]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[58]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[59]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[60]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[61]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[62]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[63]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[64]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[65]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[66]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[67]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[68]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[69]\teval-error:0.181818\ttrain-error:0.06071\n",
      "[0]\teval-error:0.308511\ttrain-error:0.115285\n",
      "[1]\teval-error:0.287234\ttrain-error:0.090627\n",
      "[2]\teval-error:0.265957\ttrain-error:0.076039\n",
      "[3]\teval-error:0.255319\ttrain-error:0.069584\n",
      "[4]\teval-error:0.255319\ttrain-error:0.066486\n",
      "[5]\teval-error:0.255319\ttrain-error:0.065453\n",
      "[6]\teval-error:0.244681\ttrain-error:0.065453\n",
      "[7]\teval-error:0.255319\ttrain-error:0.065195\n",
      "[8]\teval-error:0.255319\ttrain-error:0.065066\n",
      "[9]\teval-error:0.255319\ttrain-error:0.065066\n",
      "[10]\teval-error:0.244681\ttrain-error:0.065066\n",
      "[11]\teval-error:0.234043\ttrain-error:0.065066\n",
      "[12]\teval-error:0.244681\ttrain-error:0.065066\n",
      "[13]\teval-error:0.255319\ttrain-error:0.065066\n",
      "[14]\teval-error:0.265957\ttrain-error:0.065066\n",
      "[15]\teval-error:0.276596\ttrain-error:0.065066\n",
      "[16]\teval-error:0.265957\ttrain-error:0.065066\n",
      "[17]\teval-error:0.276596\ttrain-error:0.065066\n",
      "[18]\teval-error:0.255319\ttrain-error:0.065066\n",
      "[19]\teval-error:0.244681\ttrain-error:0.065066\n",
      "[20]\teval-error:0.276596\ttrain-error:0.065066\n",
      "[21]\teval-error:0.276596\ttrain-error:0.065066\n",
      "[22]\teval-error:0.265957\ttrain-error:0.065066\n",
      "[23]\teval-error:0.265957\ttrain-error:0.065066\n",
      "[24]\teval-error:0.265957\ttrain-error:0.065066\n",
      "[25]\teval-error:0.244681\ttrain-error:0.065066\n",
      "[26]\teval-error:0.276596\ttrain-error:0.065066\n",
      "[27]\teval-error:0.265957\ttrain-error:0.065066\n",
      "[28]\teval-error:0.265957\ttrain-error:0.065066\n",
      "[29]\teval-error:0.276596\ttrain-error:0.065066\n",
      "[30]\teval-error:0.244681\ttrain-error:0.065066\n",
      "[31]\teval-error:0.244681\ttrain-error:0.065066\n",
      "[32]\teval-error:0.244681\ttrain-error:0.065066\n",
      "[33]\teval-error:0.244681\ttrain-error:0.065066\n",
      "[34]\teval-error:0.244681\ttrain-error:0.065066\n",
      "[35]\teval-error:0.244681\ttrain-error:0.065066\n",
      "[36]\teval-error:0.244681\ttrain-error:0.065066\n",
      "[37]\teval-error:0.244681\ttrain-error:0.065066\n",
      "[38]\teval-error:0.244681\ttrain-error:0.065066\n",
      "[39]\teval-error:0.244681\ttrain-error:0.065066\n",
      "[40]\teval-error:0.244681\ttrain-error:0.065066\n",
      "[41]\teval-error:0.244681\ttrain-error:0.065066\n",
      "[42]\teval-error:0.244681\ttrain-error:0.065066\n",
      "[43]\teval-error:0.244681\ttrain-error:0.065066\n",
      "[44]\teval-error:0.244681\ttrain-error:0.065066\n",
      "[45]\teval-error:0.244681\ttrain-error:0.065066\n",
      "[46]\teval-error:0.244681\ttrain-error:0.065066\n",
      "[47]\teval-error:0.244681\ttrain-error:0.065066\n",
      "[48]\teval-error:0.244681\ttrain-error:0.065066\n",
      "[49]\teval-error:0.265957\ttrain-error:0.065066\n",
      "[50]\teval-error:0.265957\ttrain-error:0.065066\n",
      "[51]\teval-error:0.265957\ttrain-error:0.065066\n",
      "[52]\teval-error:0.265957\ttrain-error:0.065066\n",
      "[53]\teval-error:0.265957\ttrain-error:0.065066\n",
      "[54]\teval-error:0.287234\ttrain-error:0.065066\n",
      "[55]\teval-error:0.276596\ttrain-error:0.065066\n",
      "[56]\teval-error:0.276596\ttrain-error:0.065066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57]\teval-error:0.276596\ttrain-error:0.065066\n",
      "[58]\teval-error:0.265957\ttrain-error:0.065066\n",
      "[59]\teval-error:0.265957\ttrain-error:0.065066\n",
      "[60]\teval-error:0.265957\ttrain-error:0.065066\n",
      "[61]\teval-error:0.265957\ttrain-error:0.065066\n",
      "[62]\teval-error:0.265957\ttrain-error:0.065066\n",
      "[63]\teval-error:0.255319\ttrain-error:0.065066\n",
      "[64]\teval-error:0.255319\ttrain-error:0.065066\n",
      "[65]\teval-error:0.255319\ttrain-error:0.065066\n",
      "[66]\teval-error:0.255319\ttrain-error:0.065066\n",
      "[67]\teval-error:0.255319\ttrain-error:0.065066\n",
      "[68]\teval-error:0.255319\ttrain-error:0.065066\n",
      "[69]\teval-error:0.255319\ttrain-error:0.065066\n",
      "[0]\teval-error:0.319149\ttrain-error:0.115802\n",
      "[1]\teval-error:0.329787\ttrain-error:0.090369\n",
      "[2]\teval-error:0.37234\ttrain-error:0.076685\n",
      "[3]\teval-error:0.319149\ttrain-error:0.070101\n",
      "[4]\teval-error:0.361702\ttrain-error:0.067648\n",
      "[5]\teval-error:0.351064\ttrain-error:0.066228\n",
      "[6]\teval-error:0.329787\ttrain-error:0.066099\n",
      "[7]\teval-error:0.308511\ttrain-error:0.065582\n",
      "[8]\teval-error:0.319149\ttrain-error:0.065453\n",
      "[9]\teval-error:0.308511\ttrain-error:0.065453\n",
      "[10]\teval-error:0.308511\ttrain-error:0.065453\n",
      "[11]\teval-error:0.308511\ttrain-error:0.065453\n",
      "[12]\teval-error:0.287234\ttrain-error:0.065453\n",
      "[13]\teval-error:0.297872\ttrain-error:0.065453\n",
      "[14]\teval-error:0.297872\ttrain-error:0.065453\n",
      "[15]\teval-error:0.308511\ttrain-error:0.065453\n",
      "[16]\teval-error:0.308511\ttrain-error:0.065453\n",
      "[17]\teval-error:0.308511\ttrain-error:0.065453\n",
      "[18]\teval-error:0.297872\ttrain-error:0.065453\n",
      "[19]\teval-error:0.297872\ttrain-error:0.065453\n",
      "[20]\teval-error:0.297872\ttrain-error:0.065453\n",
      "[21]\teval-error:0.308511\ttrain-error:0.065453\n",
      "[22]\teval-error:0.319149\ttrain-error:0.065453\n",
      "[23]\teval-error:0.308511\ttrain-error:0.065453\n",
      "[24]\teval-error:0.308511\ttrain-error:0.065453\n",
      "[25]\teval-error:0.308511\ttrain-error:0.065453\n",
      "[26]\teval-error:0.308511\ttrain-error:0.065453\n",
      "[27]\teval-error:0.297872\ttrain-error:0.065453\n",
      "[28]\teval-error:0.297872\ttrain-error:0.065453\n",
      "[29]\teval-error:0.308511\ttrain-error:0.065453\n",
      "[30]\teval-error:0.308511\ttrain-error:0.065453\n",
      "[31]\teval-error:0.308511\ttrain-error:0.065453\n",
      "[32]\teval-error:0.308511\ttrain-error:0.065453\n",
      "[33]\teval-error:0.308511\ttrain-error:0.065453\n",
      "[34]\teval-error:0.308511\ttrain-error:0.065453\n",
      "[35]\teval-error:0.297872\ttrain-error:0.065453\n",
      "[36]\teval-error:0.297872\ttrain-error:0.065453\n",
      "[37]\teval-error:0.297872\ttrain-error:0.065453\n",
      "[38]\teval-error:0.308511\ttrain-error:0.065453\n",
      "[39]\teval-error:0.308511\ttrain-error:0.065453\n",
      "[40]\teval-error:0.308511\ttrain-error:0.065453\n",
      "[41]\teval-error:0.308511\ttrain-error:0.065453\n",
      "[42]\teval-error:0.308511\ttrain-error:0.065453\n",
      "[43]\teval-error:0.308511\ttrain-error:0.065453\n",
      "[44]\teval-error:0.308511\ttrain-error:0.065453\n",
      "[45]\teval-error:0.297872\ttrain-error:0.065453\n",
      "[46]\teval-error:0.297872\ttrain-error:0.065453\n",
      "[47]\teval-error:0.297872\ttrain-error:0.065453\n",
      "[48]\teval-error:0.308511\ttrain-error:0.065453\n",
      "[49]\teval-error:0.308511\ttrain-error:0.065453\n",
      "[50]\teval-error:0.308511\ttrain-error:0.065453\n",
      "[51]\teval-error:0.308511\ttrain-error:0.065453\n",
      "[52]\teval-error:0.308511\ttrain-error:0.065453\n",
      "[53]\teval-error:0.308511\ttrain-error:0.065453\n",
      "[54]\teval-error:0.319149\ttrain-error:0.065453\n",
      "[55]\teval-error:0.308511\ttrain-error:0.065453\n",
      "[56]\teval-error:0.319149\ttrain-error:0.065453\n",
      "[57]\teval-error:0.319149\ttrain-error:0.065453\n",
      "[58]\teval-error:0.319149\ttrain-error:0.065453\n",
      "[59]\teval-error:0.319149\ttrain-error:0.065453\n",
      "[60]\teval-error:0.319149\ttrain-error:0.065453\n",
      "[61]\teval-error:0.319149\ttrain-error:0.065453\n",
      "[62]\teval-error:0.319149\ttrain-error:0.065453\n",
      "[63]\teval-error:0.319149\ttrain-error:0.065453\n",
      "[64]\teval-error:0.319149\ttrain-error:0.065453\n",
      "[65]\teval-error:0.319149\ttrain-error:0.065453\n",
      "[66]\teval-error:0.319149\ttrain-error:0.065453\n",
      "[67]\teval-error:0.308511\ttrain-error:0.065453\n",
      "[68]\teval-error:0.308511\ttrain-error:0.065453\n",
      "[69]\teval-error:0.308511\ttrain-error:0.065453\n",
      "[0]\teval-error:0.382979\ttrain-error:0.118513\n",
      "[1]\teval-error:0.340426\ttrain-error:0.091531\n",
      "[2]\teval-error:0.351064\ttrain-error:0.077718\n",
      "[3]\teval-error:0.276596\ttrain-error:0.071133\n",
      "[4]\teval-error:0.276596\ttrain-error:0.068552\n",
      "[5]\teval-error:0.276596\ttrain-error:0.067648\n",
      "[6]\teval-error:0.287234\ttrain-error:0.066486\n",
      "[7]\teval-error:0.265957\ttrain-error:0.066357\n",
      "[8]\teval-error:0.265957\ttrain-error:0.066357\n",
      "[9]\teval-error:0.265957\ttrain-error:0.066228\n",
      "[10]\teval-error:0.276596\ttrain-error:0.066099\n",
      "[11]\teval-error:0.297872\ttrain-error:0.066099\n",
      "[12]\teval-error:0.276596\ttrain-error:0.066099\n",
      "[13]\teval-error:0.276596\ttrain-error:0.066099\n",
      "[14]\teval-error:0.276596\ttrain-error:0.066099\n",
      "[15]\teval-error:0.287234\ttrain-error:0.066099\n",
      "[16]\teval-error:0.276596\ttrain-error:0.066099\n",
      "[17]\teval-error:0.276596\ttrain-error:0.066099\n",
      "[18]\teval-error:0.265957\ttrain-error:0.066099\n",
      "[19]\teval-error:0.265957\ttrain-error:0.066099\n",
      "[20]\teval-error:0.276596\ttrain-error:0.066099\n",
      "[21]\teval-error:0.287234\ttrain-error:0.066099\n",
      "[22]\teval-error:0.265957\ttrain-error:0.066099\n",
      "[23]\teval-error:0.276596\ttrain-error:0.066099\n",
      "[24]\teval-error:0.265957\ttrain-error:0.066099\n",
      "[25]\teval-error:0.265957\ttrain-error:0.066099\n",
      "[26]\teval-error:0.265957\ttrain-error:0.066099\n",
      "[27]\teval-error:0.265957\ttrain-error:0.066099\n",
      "[28]\teval-error:0.265957\ttrain-error:0.066099\n",
      "[29]\teval-error:0.265957\ttrain-error:0.066099\n",
      "[30]\teval-error:0.265957\ttrain-error:0.066099\n",
      "[31]\teval-error:0.276596\ttrain-error:0.066099\n",
      "[32]\teval-error:0.265957\ttrain-error:0.066099\n",
      "[33]\teval-error:0.276596\ttrain-error:0.066099\n",
      "[34]\teval-error:0.255319\ttrain-error:0.066099\n",
      "[35]\teval-error:0.265957\ttrain-error:0.066099\n",
      "[36]\teval-error:0.255319\ttrain-error:0.066099\n",
      "[37]\teval-error:0.265957\ttrain-error:0.066099\n",
      "[38]\teval-error:0.265957\ttrain-error:0.066099\n",
      "[39]\teval-error:0.265957\ttrain-error:0.066099\n",
      "[40]\teval-error:0.265957\ttrain-error:0.066099\n",
      "[41]\teval-error:0.265957\ttrain-error:0.066099\n",
      "[42]\teval-error:0.276596\ttrain-error:0.066099\n",
      "[43]\teval-error:0.276596\ttrain-error:0.066099\n",
      "[44]\teval-error:0.276596\ttrain-error:0.066099\n",
      "[45]\teval-error:0.276596\ttrain-error:0.066099\n",
      "[46]\teval-error:0.265957\ttrain-error:0.066099\n",
      "[47]\teval-error:0.276596\ttrain-error:0.066099\n",
      "[48]\teval-error:0.276596\ttrain-error:0.066099\n",
      "[49]\teval-error:0.276596\ttrain-error:0.066099\n",
      "[50]\teval-error:0.276596\ttrain-error:0.066099\n",
      "[51]\teval-error:0.276596\ttrain-error:0.066099\n",
      "[52]\teval-error:0.265957\ttrain-error:0.066099\n",
      "[53]\teval-error:0.255319\ttrain-error:0.066099\n",
      "[54]\teval-error:0.276596\ttrain-error:0.066099\n",
      "[55]\teval-error:0.265957\ttrain-error:0.066099\n",
      "[56]\teval-error:0.265957\ttrain-error:0.066099\n",
      "[57]\teval-error:0.265957\ttrain-error:0.066099\n",
      "[58]\teval-error:0.276596\ttrain-error:0.066099\n",
      "[59]\teval-error:0.265957\ttrain-error:0.066099\n",
      "[60]\teval-error:0.265957\ttrain-error:0.066099\n",
      "[61]\teval-error:0.265957\ttrain-error:0.066099\n",
      "[62]\teval-error:0.265957\ttrain-error:0.066099\n",
      "[63]\teval-error:0.265957\ttrain-error:0.066099\n",
      "[64]\teval-error:0.265957\ttrain-error:0.066099\n",
      "[65]\teval-error:0.255319\ttrain-error:0.066099\n",
      "[66]\teval-error:0.265957\ttrain-error:0.066099\n",
      "[67]\teval-error:0.255319\ttrain-error:0.066099\n",
      "[68]\teval-error:0.265957\ttrain-error:0.066099\n",
      "[69]\teval-error:0.255319\ttrain-error:0.066099\n",
      "[0]\teval-error:0.382979\ttrain-error:0.114769\n",
      "[1]\teval-error:0.308511\ttrain-error:0.088691\n",
      "[2]\teval-error:0.319149\ttrain-error:0.075523\n",
      "[3]\teval-error:0.297872\ttrain-error:0.069455\n",
      "[4]\teval-error:0.329787\ttrain-error:0.067261\n",
      "[5]\teval-error:0.287234\ttrain-error:0.065453\n",
      "[6]\teval-error:0.265957\ttrain-error:0.064937\n",
      "[7]\teval-error:0.287234\ttrain-error:0.064937\n",
      "[8]\teval-error:0.255319\ttrain-error:0.064937\n",
      "[9]\teval-error:0.255319\ttrain-error:0.064937\n",
      "[10]\teval-error:0.255319\ttrain-error:0.064937\n",
      "[11]\teval-error:0.255319\ttrain-error:0.064937\n",
      "[12]\teval-error:0.276596\ttrain-error:0.064937\n",
      "[13]\teval-error:0.265957\ttrain-error:0.064937\n",
      "[14]\teval-error:0.244681\ttrain-error:0.064937\n",
      "[15]\teval-error:0.255319\ttrain-error:0.064937\n",
      "[16]\teval-error:0.276596\ttrain-error:0.064937\n",
      "[17]\teval-error:0.287234\ttrain-error:0.064937\n",
      "[18]\teval-error:0.287234\ttrain-error:0.064937\n",
      "[19]\teval-error:0.276596\ttrain-error:0.064937\n",
      "[20]\teval-error:0.276596\ttrain-error:0.064937\n",
      "[21]\teval-error:0.265957\ttrain-error:0.064937\n",
      "[22]\teval-error:0.265957\ttrain-error:0.064937\n",
      "[23]\teval-error:0.265957\ttrain-error:0.064937\n",
      "[24]\teval-error:0.287234\ttrain-error:0.064937\n",
      "[25]\teval-error:0.287234\ttrain-error:0.064937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26]\teval-error:0.287234\ttrain-error:0.064937\n",
      "[27]\teval-error:0.276596\ttrain-error:0.064937\n",
      "[28]\teval-error:0.276596\ttrain-error:0.064937\n",
      "[29]\teval-error:0.276596\ttrain-error:0.064937\n",
      "[30]\teval-error:0.276596\ttrain-error:0.064937\n",
      "[31]\teval-error:0.276596\ttrain-error:0.064937\n",
      "[32]\teval-error:0.265957\ttrain-error:0.064937\n",
      "[33]\teval-error:0.276596\ttrain-error:0.064937\n",
      "[34]\teval-error:0.276596\ttrain-error:0.064937\n",
      "[35]\teval-error:0.276596\ttrain-error:0.064937\n",
      "[36]\teval-error:0.276596\ttrain-error:0.064937\n",
      "[37]\teval-error:0.265957\ttrain-error:0.064937\n",
      "[38]\teval-error:0.265957\ttrain-error:0.064937\n",
      "[39]\teval-error:0.265957\ttrain-error:0.064937\n",
      "[40]\teval-error:0.265957\ttrain-error:0.064937\n",
      "[41]\teval-error:0.265957\ttrain-error:0.064937\n",
      "[42]\teval-error:0.265957\ttrain-error:0.064937\n",
      "[43]\teval-error:0.297872\ttrain-error:0.064937\n",
      "[44]\teval-error:0.297872\ttrain-error:0.064937\n",
      "[45]\teval-error:0.276596\ttrain-error:0.064937\n",
      "[46]\teval-error:0.265957\ttrain-error:0.064937\n",
      "[47]\teval-error:0.265957\ttrain-error:0.064937\n",
      "[48]\teval-error:0.255319\ttrain-error:0.064937\n",
      "[49]\teval-error:0.255319\ttrain-error:0.064937\n",
      "[50]\teval-error:0.276596\ttrain-error:0.064937\n",
      "[51]\teval-error:0.276596\ttrain-error:0.064937\n",
      "[52]\teval-error:0.276596\ttrain-error:0.064937\n",
      "[53]\teval-error:0.255319\ttrain-error:0.064937\n",
      "[54]\teval-error:0.276596\ttrain-error:0.064937\n",
      "[55]\teval-error:0.265957\ttrain-error:0.064937\n",
      "[56]\teval-error:0.276596\ttrain-error:0.064937\n",
      "[57]\teval-error:0.276596\ttrain-error:0.064937\n",
      "[58]\teval-error:0.265957\ttrain-error:0.064937\n",
      "[59]\teval-error:0.276596\ttrain-error:0.064937\n",
      "[60]\teval-error:0.276596\ttrain-error:0.064937\n",
      "[61]\teval-error:0.276596\ttrain-error:0.064937\n",
      "[62]\teval-error:0.255319\ttrain-error:0.064937\n",
      "[63]\teval-error:0.255319\ttrain-error:0.064937\n",
      "[64]\teval-error:0.255319\ttrain-error:0.064937\n",
      "[65]\teval-error:0.287234\ttrain-error:0.064937\n",
      "[66]\teval-error:0.287234\ttrain-error:0.064937\n",
      "[67]\teval-error:0.276596\ttrain-error:0.064937\n",
      "[68]\teval-error:0.265957\ttrain-error:0.064937\n",
      "[69]\teval-error:0.244681\ttrain-error:0.064937\n",
      "[0]\teval-error:0.28626\ttrain-error:0.0987\n",
      "[1]\teval-error:0.255725\ttrain-error:0.081631\n",
      "[2]\teval-error:0.263359\ttrain-error:0.073775\n",
      "[3]\teval-error:0.248092\ttrain-error:0.069561\n",
      "[4]\teval-error:0.255725\ttrain-error:0.06799\n",
      "[5]\teval-error:0.259542\ttrain-error:0.066919\n",
      "[6]\teval-error:0.251908\ttrain-error:0.066705\n",
      "[7]\teval-error:0.263359\ttrain-error:0.066491\n",
      "[8]\teval-error:0.274809\ttrain-error:0.066348\n",
      "[9]\teval-error:0.263359\ttrain-error:0.066348\n",
      "[10]\teval-error:0.267176\ttrain-error:0.066276\n",
      "[11]\teval-error:0.270992\ttrain-error:0.066276\n",
      "[12]\teval-error:0.263359\ttrain-error:0.066205\n",
      "[13]\teval-error:0.251908\ttrain-error:0.066205\n",
      "[14]\teval-error:0.255725\ttrain-error:0.066205\n",
      "[15]\teval-error:0.255725\ttrain-error:0.066205\n",
      "[16]\teval-error:0.263359\ttrain-error:0.066205\n",
      "[17]\teval-error:0.263359\ttrain-error:0.066205\n",
      "[18]\teval-error:0.259542\ttrain-error:0.066205\n",
      "[19]\teval-error:0.263359\ttrain-error:0.066205\n",
      "[20]\teval-error:0.259542\ttrain-error:0.066205\n",
      "[21]\teval-error:0.259542\ttrain-error:0.066205\n",
      "[22]\teval-error:0.259542\ttrain-error:0.066205\n",
      "[23]\teval-error:0.259542\ttrain-error:0.066205\n",
      "[24]\teval-error:0.259542\ttrain-error:0.066205\n",
      "[25]\teval-error:0.255725\ttrain-error:0.066205\n",
      "[26]\teval-error:0.255725\ttrain-error:0.066205\n",
      "[27]\teval-error:0.255725\ttrain-error:0.066205\n",
      "[28]\teval-error:0.255725\ttrain-error:0.066205\n",
      "[29]\teval-error:0.251908\ttrain-error:0.066205\n",
      "[30]\teval-error:0.251908\ttrain-error:0.066205\n",
      "[31]\teval-error:0.255725\ttrain-error:0.066205\n",
      "[32]\teval-error:0.251908\ttrain-error:0.066205\n",
      "[33]\teval-error:0.263359\ttrain-error:0.066205\n",
      "[34]\teval-error:0.267176\ttrain-error:0.066205\n",
      "[35]\teval-error:0.251908\ttrain-error:0.066205\n",
      "[36]\teval-error:0.255725\ttrain-error:0.066205\n",
      "[37]\teval-error:0.263359\ttrain-error:0.066205\n",
      "[38]\teval-error:0.251908\ttrain-error:0.066205\n",
      "[39]\teval-error:0.251908\ttrain-error:0.066205\n",
      "[40]\teval-error:0.255725\ttrain-error:0.066205\n",
      "[41]\teval-error:0.248092\ttrain-error:0.066205\n",
      "[42]\teval-error:0.244275\ttrain-error:0.066205\n",
      "[43]\teval-error:0.251908\ttrain-error:0.066205\n",
      "[44]\teval-error:0.251908\ttrain-error:0.066205\n",
      "[45]\teval-error:0.255725\ttrain-error:0.066205\n",
      "[46]\teval-error:0.251908\ttrain-error:0.066205\n",
      "[47]\teval-error:0.248092\ttrain-error:0.066205\n",
      "[48]\teval-error:0.251908\ttrain-error:0.066205\n",
      "[49]\teval-error:0.248092\ttrain-error:0.066205\n",
      "[50]\teval-error:0.248092\ttrain-error:0.066205\n",
      "[51]\teval-error:0.240458\ttrain-error:0.066205\n",
      "[52]\teval-error:0.248092\ttrain-error:0.066205\n",
      "[53]\teval-error:0.251908\ttrain-error:0.066205\n",
      "[54]\teval-error:0.248092\ttrain-error:0.066205\n",
      "[55]\teval-error:0.244275\ttrain-error:0.066205\n",
      "[56]\teval-error:0.236641\ttrain-error:0.066205\n",
      "[57]\teval-error:0.240458\ttrain-error:0.066205\n",
      "[58]\teval-error:0.248092\ttrain-error:0.066205\n",
      "[59]\teval-error:0.244275\ttrain-error:0.066205\n",
      "[60]\teval-error:0.244275\ttrain-error:0.066205\n",
      "[61]\teval-error:0.248092\ttrain-error:0.066205\n",
      "[62]\teval-error:0.244275\ttrain-error:0.066205\n",
      "[63]\teval-error:0.244275\ttrain-error:0.066205\n",
      "[64]\teval-error:0.244275\ttrain-error:0.066205\n",
      "[65]\teval-error:0.244275\ttrain-error:0.066205\n",
      "[66]\teval-error:0.248092\ttrain-error:0.066205\n",
      "[67]\teval-error:0.244275\ttrain-error:0.066205\n",
      "[68]\teval-error:0.236641\ttrain-error:0.066205\n",
      "[69]\teval-error:0.236641\ttrain-error:0.066205\n",
      "[0]\teval-error:0.267176\ttrain-error:0.101557\n",
      "[1]\teval-error:0.229008\ttrain-error:0.084131\n",
      "[2]\teval-error:0.255725\ttrain-error:0.074275\n",
      "[3]\teval-error:0.259542\ttrain-error:0.070204\n",
      "[4]\teval-error:0.255725\ttrain-error:0.068776\n",
      "[5]\teval-error:0.244275\ttrain-error:0.067847\n",
      "[6]\teval-error:0.255725\ttrain-error:0.06749\n",
      "[7]\teval-error:0.255725\ttrain-error:0.067205\n",
      "[8]\teval-error:0.251908\ttrain-error:0.067062\n",
      "[9]\teval-error:0.240458\ttrain-error:0.06699\n",
      "[10]\teval-error:0.244275\ttrain-error:0.066919\n",
      "[11]\teval-error:0.255725\ttrain-error:0.066848\n",
      "[12]\teval-error:0.244275\ttrain-error:0.066848\n",
      "[13]\teval-error:0.259542\ttrain-error:0.066776\n",
      "[14]\teval-error:0.259542\ttrain-error:0.066776\n",
      "[15]\teval-error:0.255725\ttrain-error:0.066776\n",
      "[16]\teval-error:0.255725\ttrain-error:0.066848\n",
      "[17]\teval-error:0.255725\ttrain-error:0.066776\n",
      "[18]\teval-error:0.259542\ttrain-error:0.066776\n",
      "[19]\teval-error:0.270992\ttrain-error:0.066776\n",
      "[20]\teval-error:0.255725\ttrain-error:0.066776\n",
      "[21]\teval-error:0.251908\ttrain-error:0.066776\n",
      "[22]\teval-error:0.259542\ttrain-error:0.066776\n",
      "[23]\teval-error:0.251908\ttrain-error:0.066776\n",
      "[24]\teval-error:0.255725\ttrain-error:0.066776\n",
      "[25]\teval-error:0.255725\ttrain-error:0.066776\n",
      "[26]\teval-error:0.251908\ttrain-error:0.066776\n",
      "[27]\teval-error:0.251908\ttrain-error:0.066776\n",
      "[28]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[29]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[30]\teval-error:0.251908\ttrain-error:0.066776\n",
      "[31]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[32]\teval-error:0.244275\ttrain-error:0.066776\n",
      "[33]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[34]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[35]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[36]\teval-error:0.240458\ttrain-error:0.066776\n",
      "[37]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[38]\teval-error:0.244275\ttrain-error:0.066776\n",
      "[39]\teval-error:0.244275\ttrain-error:0.066776\n",
      "[40]\teval-error:0.244275\ttrain-error:0.066776\n",
      "[41]\teval-error:0.236641\ttrain-error:0.066776\n",
      "[42]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[43]\teval-error:0.244275\ttrain-error:0.066776\n",
      "[44]\teval-error:0.251908\ttrain-error:0.066776\n",
      "[45]\teval-error:0.244275\ttrain-error:0.066776\n",
      "[46]\teval-error:0.244275\ttrain-error:0.066776\n",
      "[47]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[48]\teval-error:0.255725\ttrain-error:0.066776\n",
      "[49]\teval-error:0.251908\ttrain-error:0.066776\n",
      "[50]\teval-error:0.244275\ttrain-error:0.066776\n",
      "[51]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[52]\teval-error:0.259542\ttrain-error:0.066776\n",
      "[53]\teval-error:0.251908\ttrain-error:0.066776\n",
      "[54]\teval-error:0.240458\ttrain-error:0.066776\n",
      "[55]\teval-error:0.236641\ttrain-error:0.066776\n",
      "[56]\teval-error:0.240458\ttrain-error:0.066776\n",
      "[57]\teval-error:0.240458\ttrain-error:0.066776\n",
      "[58]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[59]\teval-error:0.255725\ttrain-error:0.066776\n",
      "[60]\teval-error:0.244275\ttrain-error:0.066776\n",
      "[61]\teval-error:0.232824\ttrain-error:0.066776\n",
      "[62]\teval-error:0.248092\ttrain-error:0.066776\n",
      "[63]\teval-error:0.236641\ttrain-error:0.066776\n",
      "[64]\teval-error:0.248092\ttrain-error:0.066776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65]\teval-error:0.240458\ttrain-error:0.066776\n",
      "[66]\teval-error:0.244275\ttrain-error:0.066776\n",
      "[67]\teval-error:0.240458\ttrain-error:0.066776\n",
      "[68]\teval-error:0.244275\ttrain-error:0.066776\n",
      "[69]\teval-error:0.244275\ttrain-error:0.066776\n",
      "[0]\teval-error:0.28626\ttrain-error:0.100914\n",
      "[1]\teval-error:0.274809\ttrain-error:0.086845\n",
      "[2]\teval-error:0.267176\ttrain-error:0.076632\n",
      "[3]\teval-error:0.270992\ttrain-error:0.071133\n",
      "[4]\teval-error:0.263359\ttrain-error:0.069133\n",
      "[5]\teval-error:0.251908\ttrain-error:0.068347\n",
      "[6]\teval-error:0.255725\ttrain-error:0.06799\n",
      "[7]\teval-error:0.251908\ttrain-error:0.067776\n",
      "[8]\teval-error:0.259542\ttrain-error:0.067705\n",
      "[9]\teval-error:0.259542\ttrain-error:0.067705\n",
      "[10]\teval-error:0.259542\ttrain-error:0.06749\n",
      "[11]\teval-error:0.263359\ttrain-error:0.067419\n",
      "[12]\teval-error:0.263359\ttrain-error:0.067419\n",
      "[13]\teval-error:0.259542\ttrain-error:0.067419\n",
      "[14]\teval-error:0.259542\ttrain-error:0.067419\n",
      "[15]\teval-error:0.278626\ttrain-error:0.067419\n",
      "[16]\teval-error:0.270992\ttrain-error:0.067419\n",
      "[17]\teval-error:0.270992\ttrain-error:0.067419\n",
      "[18]\teval-error:0.278626\ttrain-error:0.067419\n",
      "[19]\teval-error:0.270992\ttrain-error:0.067419\n",
      "[20]\teval-error:0.270992\ttrain-error:0.067419\n",
      "[21]\teval-error:0.259542\ttrain-error:0.067419\n",
      "[22]\teval-error:0.263359\ttrain-error:0.067419\n",
      "[23]\teval-error:0.259542\ttrain-error:0.067419\n",
      "[24]\teval-error:0.278626\ttrain-error:0.067419\n",
      "[25]\teval-error:0.278626\ttrain-error:0.067419\n",
      "[26]\teval-error:0.270992\ttrain-error:0.067419\n",
      "[27]\teval-error:0.267176\ttrain-error:0.067419\n",
      "[28]\teval-error:0.278626\ttrain-error:0.067419\n",
      "[29]\teval-error:0.270992\ttrain-error:0.067419\n",
      "[30]\teval-error:0.282443\ttrain-error:0.067419\n",
      "[31]\teval-error:0.282443\ttrain-error:0.067419\n",
      "[32]\teval-error:0.282443\ttrain-error:0.067419\n",
      "[33]\teval-error:0.267176\ttrain-error:0.067419\n",
      "[34]\teval-error:0.267176\ttrain-error:0.067419\n",
      "[35]\teval-error:0.270992\ttrain-error:0.067419\n",
      "[36]\teval-error:0.278626\ttrain-error:0.067419\n",
      "[37]\teval-error:0.278626\ttrain-error:0.067419\n",
      "[38]\teval-error:0.282443\ttrain-error:0.067419\n",
      "[39]\teval-error:0.274809\ttrain-error:0.067419\n",
      "[40]\teval-error:0.267176\ttrain-error:0.067419\n",
      "[41]\teval-error:0.270992\ttrain-error:0.067419\n",
      "[42]\teval-error:0.274809\ttrain-error:0.067419\n",
      "[43]\teval-error:0.267176\ttrain-error:0.067419\n",
      "[44]\teval-error:0.270992\ttrain-error:0.067419\n",
      "[45]\teval-error:0.274809\ttrain-error:0.067419\n",
      "[46]\teval-error:0.278626\ttrain-error:0.067419\n",
      "[47]\teval-error:0.267176\ttrain-error:0.067419\n",
      "[48]\teval-error:0.274809\ttrain-error:0.067419\n",
      "[49]\teval-error:0.274809\ttrain-error:0.067419\n",
      "[50]\teval-error:0.267176\ttrain-error:0.067419\n",
      "[51]\teval-error:0.274809\ttrain-error:0.067419\n",
      "[52]\teval-error:0.270992\ttrain-error:0.067419\n",
      "[53]\teval-error:0.267176\ttrain-error:0.067419\n",
      "[54]\teval-error:0.270992\ttrain-error:0.067419\n",
      "[55]\teval-error:0.282443\ttrain-error:0.067419\n",
      "[56]\teval-error:0.274809\ttrain-error:0.067419\n",
      "[57]\teval-error:0.274809\ttrain-error:0.067419\n",
      "[58]\teval-error:0.270992\ttrain-error:0.067419\n",
      "[59]\teval-error:0.270992\ttrain-error:0.067419\n",
      "[60]\teval-error:0.282443\ttrain-error:0.067419\n",
      "[61]\teval-error:0.267176\ttrain-error:0.067419\n",
      "[62]\teval-error:0.263359\ttrain-error:0.067419\n",
      "[63]\teval-error:0.263359\ttrain-error:0.067419\n",
      "[64]\teval-error:0.263359\ttrain-error:0.067419\n",
      "[65]\teval-error:0.255725\ttrain-error:0.067419\n",
      "[66]\teval-error:0.255725\ttrain-error:0.067419\n",
      "[67]\teval-error:0.270992\ttrain-error:0.067419\n",
      "[68]\teval-error:0.270992\ttrain-error:0.067419\n",
      "[69]\teval-error:0.267176\ttrain-error:0.067419\n",
      "[0]\teval-error:0.28626\ttrain-error:0.098129\n",
      "[1]\teval-error:0.244275\ttrain-error:0.081845\n",
      "[2]\teval-error:0.240458\ttrain-error:0.073061\n",
      "[3]\teval-error:0.255725\ttrain-error:0.069776\n",
      "[4]\teval-error:0.236641\ttrain-error:0.067776\n",
      "[5]\teval-error:0.240458\ttrain-error:0.067062\n",
      "[6]\teval-error:0.232824\ttrain-error:0.066633\n",
      "[7]\teval-error:0.255725\ttrain-error:0.066491\n",
      "[8]\teval-error:0.248092\ttrain-error:0.066205\n",
      "[9]\teval-error:0.248092\ttrain-error:0.066205\n",
      "[10]\teval-error:0.240458\ttrain-error:0.066205\n",
      "[11]\teval-error:0.236641\ttrain-error:0.066205\n",
      "[12]\teval-error:0.240458\ttrain-error:0.066205\n",
      "[13]\teval-error:0.236641\ttrain-error:0.066205\n",
      "[14]\teval-error:0.240458\ttrain-error:0.066205\n",
      "[15]\teval-error:0.240458\ttrain-error:0.066205\n",
      "[16]\teval-error:0.240458\ttrain-error:0.066205\n",
      "[17]\teval-error:0.244275\ttrain-error:0.066205\n",
      "[18]\teval-error:0.248092\ttrain-error:0.066205\n",
      "[19]\teval-error:0.244275\ttrain-error:0.066205\n",
      "[20]\teval-error:0.232824\ttrain-error:0.066205\n",
      "[21]\teval-error:0.240458\ttrain-error:0.066205\n",
      "[22]\teval-error:0.240458\ttrain-error:0.066205\n",
      "[23]\teval-error:0.244275\ttrain-error:0.066062\n",
      "[24]\teval-error:0.232824\ttrain-error:0.066062\n",
      "[25]\teval-error:0.240458\ttrain-error:0.066062\n",
      "[26]\teval-error:0.232824\ttrain-error:0.066062\n",
      "[27]\teval-error:0.240458\ttrain-error:0.066062\n",
      "[28]\teval-error:0.240458\ttrain-error:0.066062\n",
      "[29]\teval-error:0.240458\ttrain-error:0.066062\n",
      "[30]\teval-error:0.236641\ttrain-error:0.066062\n",
      "[31]\teval-error:0.236641\ttrain-error:0.066062\n",
      "[32]\teval-error:0.236641\ttrain-error:0.066062\n",
      "[33]\teval-error:0.240458\ttrain-error:0.066062\n",
      "[34]\teval-error:0.240458\ttrain-error:0.066062\n",
      "[35]\teval-error:0.240458\ttrain-error:0.066062\n",
      "[36]\teval-error:0.236641\ttrain-error:0.066062\n",
      "[37]\teval-error:0.240458\ttrain-error:0.066062\n",
      "[38]\teval-error:0.236641\ttrain-error:0.066062\n",
      "[39]\teval-error:0.240458\ttrain-error:0.066062\n",
      "[40]\teval-error:0.244275\ttrain-error:0.066062\n",
      "[41]\teval-error:0.240458\ttrain-error:0.066062\n",
      "[42]\teval-error:0.244275\ttrain-error:0.066062\n",
      "[43]\teval-error:0.244275\ttrain-error:0.066062\n",
      "[44]\teval-error:0.240458\ttrain-error:0.066062\n",
      "[45]\teval-error:0.240458\ttrain-error:0.066062\n",
      "[46]\teval-error:0.236641\ttrain-error:0.066062\n",
      "[47]\teval-error:0.236641\ttrain-error:0.066062\n",
      "[48]\teval-error:0.232824\ttrain-error:0.066062\n",
      "[49]\teval-error:0.232824\ttrain-error:0.066062\n",
      "[50]\teval-error:0.236641\ttrain-error:0.066062\n",
      "[51]\teval-error:0.240458\ttrain-error:0.066062\n",
      "[52]\teval-error:0.236641\ttrain-error:0.066062\n",
      "[53]\teval-error:0.236641\ttrain-error:0.066062\n",
      "[54]\teval-error:0.229008\ttrain-error:0.066062\n",
      "[55]\teval-error:0.236641\ttrain-error:0.066062\n",
      "[56]\teval-error:0.236641\ttrain-error:0.066062\n",
      "[57]\teval-error:0.236641\ttrain-error:0.066062\n",
      "[58]\teval-error:0.232824\ttrain-error:0.066062\n",
      "[59]\teval-error:0.236641\ttrain-error:0.066062\n",
      "[60]\teval-error:0.232824\ttrain-error:0.066062\n",
      "[61]\teval-error:0.240458\ttrain-error:0.066062\n",
      "[62]\teval-error:0.240458\ttrain-error:0.066062\n",
      "[63]\teval-error:0.244275\ttrain-error:0.066062\n",
      "[64]\teval-error:0.236641\ttrain-error:0.066062\n",
      "[65]\teval-error:0.244275\ttrain-error:0.066062\n",
      "[66]\teval-error:0.240458\ttrain-error:0.066062\n",
      "[67]\teval-error:0.240458\ttrain-error:0.066062\n",
      "[68]\teval-error:0.244275\ttrain-error:0.066062\n",
      "[69]\teval-error:0.240458\ttrain-error:0.066062\n",
      "[0]\teval-error:0.420455\ttrain-error:0.265718\n",
      "[1]\teval-error:0.409091\ttrain-error:0.237975\n",
      "[2]\teval-error:0.386364\ttrain-error:0.21996\n",
      "[3]\teval-error:0.375\ttrain-error:0.212935\n",
      "[4]\teval-error:0.386364\ttrain-error:0.206449\n",
      "[5]\teval-error:0.375\ttrain-error:0.203387\n",
      "[6]\teval-error:0.386364\ttrain-error:0.200144\n",
      "[7]\teval-error:0.386364\ttrain-error:0.199604\n",
      "[8]\teval-error:0.386364\ttrain-error:0.199424\n",
      "[9]\teval-error:0.375\ttrain-error:0.198162\n",
      "[10]\teval-error:0.375\ttrain-error:0.198162\n",
      "[11]\teval-error:0.363636\ttrain-error:0.196721\n",
      "[12]\teval-error:0.352273\ttrain-error:0.196361\n",
      "[13]\teval-error:0.375\ttrain-error:0.196361\n",
      "[14]\teval-error:0.375\ttrain-error:0.196361\n",
      "[15]\teval-error:0.363636\ttrain-error:0.196181\n",
      "[16]\teval-error:0.375\ttrain-error:0.196181\n",
      "[17]\teval-error:0.363636\ttrain-error:0.196001\n",
      "[18]\teval-error:0.375\ttrain-error:0.196001\n",
      "[19]\teval-error:0.363636\ttrain-error:0.196001\n",
      "[20]\teval-error:0.363636\ttrain-error:0.196001\n",
      "[21]\teval-error:0.352273\ttrain-error:0.196001\n",
      "[22]\teval-error:0.352273\ttrain-error:0.196001\n",
      "[23]\teval-error:0.363636\ttrain-error:0.196001\n",
      "[24]\teval-error:0.363636\ttrain-error:0.196001\n",
      "[25]\teval-error:0.363636\ttrain-error:0.195821\n",
      "[26]\teval-error:0.363636\ttrain-error:0.196001\n",
      "[27]\teval-error:0.375\ttrain-error:0.196001\n",
      "[28]\teval-error:0.386364\ttrain-error:0.195821\n",
      "[29]\teval-error:0.386364\ttrain-error:0.195821\n",
      "[30]\teval-error:0.375\ttrain-error:0.195821\n",
      "[31]\teval-error:0.375\ttrain-error:0.195821\n",
      "[32]\teval-error:0.363636\ttrain-error:0.195821\n",
      "[33]\teval-error:0.363636\ttrain-error:0.195821\n",
      "[34]\teval-error:0.375\ttrain-error:0.195821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35]\teval-error:0.375\ttrain-error:0.195821\n",
      "[36]\teval-error:0.375\ttrain-error:0.195821\n",
      "[37]\teval-error:0.363636\ttrain-error:0.195821\n",
      "[38]\teval-error:0.352273\ttrain-error:0.195821\n",
      "[39]\teval-error:0.352273\ttrain-error:0.195821\n",
      "[40]\teval-error:0.352273\ttrain-error:0.195821\n",
      "[41]\teval-error:0.386364\ttrain-error:0.195821\n",
      "[42]\teval-error:0.386364\ttrain-error:0.195821\n",
      "[43]\teval-error:0.375\ttrain-error:0.195821\n",
      "[44]\teval-error:0.375\ttrain-error:0.195821\n",
      "[45]\teval-error:0.375\ttrain-error:0.195821\n",
      "[46]\teval-error:0.386364\ttrain-error:0.195821\n",
      "[47]\teval-error:0.375\ttrain-error:0.195821\n",
      "[48]\teval-error:0.375\ttrain-error:0.195821\n",
      "[49]\teval-error:0.375\ttrain-error:0.195821\n",
      "[50]\teval-error:0.375\ttrain-error:0.195821\n",
      "[51]\teval-error:0.375\ttrain-error:0.195821\n",
      "[52]\teval-error:0.375\ttrain-error:0.195821\n",
      "[53]\teval-error:0.375\ttrain-error:0.195821\n",
      "[54]\teval-error:0.375\ttrain-error:0.195821\n",
      "[55]\teval-error:0.375\ttrain-error:0.195821\n",
      "[56]\teval-error:0.375\ttrain-error:0.195821\n",
      "[57]\teval-error:0.375\ttrain-error:0.195821\n",
      "[58]\teval-error:0.375\ttrain-error:0.195821\n",
      "[59]\teval-error:0.375\ttrain-error:0.195821\n",
      "[60]\teval-error:0.375\ttrain-error:0.195821\n",
      "[61]\teval-error:0.375\ttrain-error:0.195821\n",
      "[62]\teval-error:0.375\ttrain-error:0.195821\n",
      "[63]\teval-error:0.375\ttrain-error:0.195821\n",
      "[64]\teval-error:0.375\ttrain-error:0.195821\n",
      "[65]\teval-error:0.375\ttrain-error:0.195821\n",
      "[66]\teval-error:0.363636\ttrain-error:0.195821\n",
      "[67]\teval-error:0.363636\ttrain-error:0.195821\n",
      "[68]\teval-error:0.363636\ttrain-error:0.195821\n",
      "[69]\teval-error:0.363636\ttrain-error:0.195821\n",
      "[0]\teval-error:0.420455\ttrain-error:0.265718\n",
      "[1]\teval-error:0.409091\ttrain-error:0.237975\n",
      "[2]\teval-error:0.386364\ttrain-error:0.21996\n",
      "[3]\teval-error:0.375\ttrain-error:0.212935\n",
      "[4]\teval-error:0.386364\ttrain-error:0.206449\n",
      "[5]\teval-error:0.375\ttrain-error:0.203387\n",
      "[6]\teval-error:0.386364\ttrain-error:0.200144\n",
      "[7]\teval-error:0.386364\ttrain-error:0.199604\n",
      "[8]\teval-error:0.386364\ttrain-error:0.199424\n",
      "[9]\teval-error:0.375\ttrain-error:0.198162\n",
      "[10]\teval-error:0.375\ttrain-error:0.198162\n",
      "[11]\teval-error:0.363636\ttrain-error:0.196721\n",
      "[12]\teval-error:0.352273\ttrain-error:0.196361\n",
      "[13]\teval-error:0.375\ttrain-error:0.196361\n",
      "[14]\teval-error:0.375\ttrain-error:0.196361\n",
      "[15]\teval-error:0.363636\ttrain-error:0.196181\n",
      "[16]\teval-error:0.375\ttrain-error:0.196181\n",
      "[17]\teval-error:0.363636\ttrain-error:0.196001\n",
      "[18]\teval-error:0.375\ttrain-error:0.196001\n",
      "[19]\teval-error:0.363636\ttrain-error:0.196001\n",
      "[20]\teval-error:0.363636\ttrain-error:0.196001\n",
      "[21]\teval-error:0.352273\ttrain-error:0.196001\n",
      "[22]\teval-error:0.352273\ttrain-error:0.196001\n",
      "[23]\teval-error:0.363636\ttrain-error:0.196001\n",
      "[24]\teval-error:0.363636\ttrain-error:0.196001\n",
      "[25]\teval-error:0.363636\ttrain-error:0.195821\n",
      "[26]\teval-error:0.363636\ttrain-error:0.196001\n",
      "[27]\teval-error:0.375\ttrain-error:0.196001\n",
      "[28]\teval-error:0.386364\ttrain-error:0.195821\n",
      "[29]\teval-error:0.386364\ttrain-error:0.195821\n",
      "[30]\teval-error:0.375\ttrain-error:0.195821\n",
      "[31]\teval-error:0.375\ttrain-error:0.195821\n",
      "[32]\teval-error:0.363636\ttrain-error:0.195821\n",
      "[33]\teval-error:0.363636\ttrain-error:0.195821\n",
      "[34]\teval-error:0.375\ttrain-error:0.195821\n",
      "[35]\teval-error:0.375\ttrain-error:0.195821\n",
      "[36]\teval-error:0.375\ttrain-error:0.195821\n",
      "[37]\teval-error:0.363636\ttrain-error:0.195821\n",
      "[38]\teval-error:0.352273\ttrain-error:0.195821\n",
      "[39]\teval-error:0.352273\ttrain-error:0.195821\n",
      "[40]\teval-error:0.352273\ttrain-error:0.195821\n",
      "[41]\teval-error:0.386364\ttrain-error:0.195821\n",
      "[42]\teval-error:0.386364\ttrain-error:0.195821\n",
      "[43]\teval-error:0.375\ttrain-error:0.195821\n",
      "[44]\teval-error:0.375\ttrain-error:0.195821\n",
      "[45]\teval-error:0.375\ttrain-error:0.195821\n",
      "[46]\teval-error:0.386364\ttrain-error:0.195821\n",
      "[47]\teval-error:0.375\ttrain-error:0.195821\n",
      "[48]\teval-error:0.375\ttrain-error:0.195821\n",
      "[49]\teval-error:0.375\ttrain-error:0.195821\n",
      "[50]\teval-error:0.375\ttrain-error:0.195821\n",
      "[51]\teval-error:0.375\ttrain-error:0.195821\n",
      "[52]\teval-error:0.375\ttrain-error:0.195821\n",
      "[53]\teval-error:0.375\ttrain-error:0.195821\n",
      "[54]\teval-error:0.375\ttrain-error:0.195821\n",
      "[55]\teval-error:0.375\ttrain-error:0.195821\n",
      "[56]\teval-error:0.375\ttrain-error:0.195821\n",
      "[57]\teval-error:0.375\ttrain-error:0.195821\n",
      "[58]\teval-error:0.375\ttrain-error:0.195821\n",
      "[59]\teval-error:0.375\ttrain-error:0.195821\n",
      "[60]\teval-error:0.375\ttrain-error:0.195821\n",
      "[61]\teval-error:0.375\ttrain-error:0.195821\n",
      "[62]\teval-error:0.375\ttrain-error:0.195821\n",
      "[63]\teval-error:0.375\ttrain-error:0.195821\n",
      "[64]\teval-error:0.375\ttrain-error:0.195821\n",
      "[65]\teval-error:0.375\ttrain-error:0.195821\n",
      "[66]\teval-error:0.363636\ttrain-error:0.195821\n",
      "[67]\teval-error:0.363636\ttrain-error:0.195821\n",
      "[68]\teval-error:0.363636\ttrain-error:0.195821\n",
      "[69]\teval-error:0.363636\ttrain-error:0.195821\n",
      "[0]\teval-error:0.420455\ttrain-error:0.265718\n",
      "[1]\teval-error:0.409091\ttrain-error:0.237975\n",
      "[2]\teval-error:0.386364\ttrain-error:0.21996\n",
      "[3]\teval-error:0.375\ttrain-error:0.212935\n",
      "[4]\teval-error:0.386364\ttrain-error:0.206449\n",
      "[5]\teval-error:0.375\ttrain-error:0.203387\n",
      "[6]\teval-error:0.386364\ttrain-error:0.200144\n",
      "[7]\teval-error:0.386364\ttrain-error:0.199604\n",
      "[8]\teval-error:0.386364\ttrain-error:0.199424\n",
      "[9]\teval-error:0.375\ttrain-error:0.198162\n",
      "[10]\teval-error:0.375\ttrain-error:0.198162\n",
      "[11]\teval-error:0.363636\ttrain-error:0.196721\n",
      "[12]\teval-error:0.352273\ttrain-error:0.196361\n",
      "[13]\teval-error:0.375\ttrain-error:0.196361\n",
      "[14]\teval-error:0.375\ttrain-error:0.196361\n",
      "[15]\teval-error:0.363636\ttrain-error:0.196181\n",
      "[16]\teval-error:0.375\ttrain-error:0.196181\n",
      "[17]\teval-error:0.363636\ttrain-error:0.196001\n",
      "[18]\teval-error:0.375\ttrain-error:0.196001\n",
      "[19]\teval-error:0.363636\ttrain-error:0.196001\n",
      "[20]\teval-error:0.363636\ttrain-error:0.196001\n",
      "[21]\teval-error:0.352273\ttrain-error:0.196001\n",
      "[22]\teval-error:0.352273\ttrain-error:0.196001\n",
      "[23]\teval-error:0.363636\ttrain-error:0.196001\n",
      "[24]\teval-error:0.363636\ttrain-error:0.196001\n",
      "[25]\teval-error:0.363636\ttrain-error:0.195821\n",
      "[26]\teval-error:0.363636\ttrain-error:0.196001\n",
      "[27]\teval-error:0.375\ttrain-error:0.196001\n",
      "[28]\teval-error:0.386364\ttrain-error:0.195821\n",
      "[29]\teval-error:0.386364\ttrain-error:0.195821\n",
      "[30]\teval-error:0.375\ttrain-error:0.195821\n",
      "[31]\teval-error:0.375\ttrain-error:0.195821\n",
      "[32]\teval-error:0.363636\ttrain-error:0.195821\n",
      "[33]\teval-error:0.363636\ttrain-error:0.195821\n",
      "[34]\teval-error:0.375\ttrain-error:0.195821\n",
      "[35]\teval-error:0.375\ttrain-error:0.195821\n",
      "[36]\teval-error:0.375\ttrain-error:0.195821\n",
      "[37]\teval-error:0.363636\ttrain-error:0.195821\n",
      "[38]\teval-error:0.352273\ttrain-error:0.195821\n",
      "[39]\teval-error:0.352273\ttrain-error:0.195821\n",
      "[40]\teval-error:0.352273\ttrain-error:0.195821\n",
      "[41]\teval-error:0.386364\ttrain-error:0.195821\n",
      "[42]\teval-error:0.386364\ttrain-error:0.195821\n",
      "[43]\teval-error:0.375\ttrain-error:0.195821\n",
      "[44]\teval-error:0.375\ttrain-error:0.195821\n",
      "[45]\teval-error:0.375\ttrain-error:0.195821\n",
      "[46]\teval-error:0.386364\ttrain-error:0.195821\n",
      "[47]\teval-error:0.375\ttrain-error:0.195821\n",
      "[48]\teval-error:0.375\ttrain-error:0.195821\n",
      "[49]\teval-error:0.375\ttrain-error:0.195821\n",
      "[50]\teval-error:0.375\ttrain-error:0.195821\n",
      "[51]\teval-error:0.375\ttrain-error:0.195821\n",
      "[52]\teval-error:0.375\ttrain-error:0.195821\n",
      "[53]\teval-error:0.375\ttrain-error:0.195821\n",
      "[54]\teval-error:0.375\ttrain-error:0.195821\n",
      "[55]\teval-error:0.375\ttrain-error:0.195821\n",
      "[56]\teval-error:0.375\ttrain-error:0.195821\n",
      "[57]\teval-error:0.375\ttrain-error:0.195821\n",
      "[58]\teval-error:0.375\ttrain-error:0.195821\n",
      "[59]\teval-error:0.375\ttrain-error:0.195821\n",
      "[60]\teval-error:0.375\ttrain-error:0.195821\n",
      "[61]\teval-error:0.375\ttrain-error:0.195821\n",
      "[62]\teval-error:0.375\ttrain-error:0.195821\n",
      "[63]\teval-error:0.375\ttrain-error:0.195821\n",
      "[64]\teval-error:0.375\ttrain-error:0.195821\n",
      "[65]\teval-error:0.375\ttrain-error:0.195821\n",
      "[66]\teval-error:0.363636\ttrain-error:0.195821\n",
      "[67]\teval-error:0.363636\ttrain-error:0.195821\n",
      "[68]\teval-error:0.363636\ttrain-error:0.195821\n",
      "[69]\teval-error:0.363636\ttrain-error:0.195821\n",
      "[0]\teval-error:0.420455\ttrain-error:0.265718\n",
      "[1]\teval-error:0.409091\ttrain-error:0.237975\n",
      "[2]\teval-error:0.386364\ttrain-error:0.21996\n",
      "[3]\teval-error:0.375\ttrain-error:0.212935\n",
      "[4]\teval-error:0.386364\ttrain-error:0.206449\n",
      "[5]\teval-error:0.375\ttrain-error:0.203387\n",
      "[6]\teval-error:0.386364\ttrain-error:0.200144\n",
      "[7]\teval-error:0.386364\ttrain-error:0.199604\n",
      "[8]\teval-error:0.386364\ttrain-error:0.199424\n",
      "[9]\teval-error:0.375\ttrain-error:0.198162\n",
      "[10]\teval-error:0.375\ttrain-error:0.198162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11]\teval-error:0.363636\ttrain-error:0.196721\n",
      "[12]\teval-error:0.352273\ttrain-error:0.196361\n",
      "[13]\teval-error:0.375\ttrain-error:0.196361\n",
      "[14]\teval-error:0.375\ttrain-error:0.196361\n",
      "[15]\teval-error:0.363636\ttrain-error:0.196181\n",
      "[16]\teval-error:0.375\ttrain-error:0.196181\n",
      "[17]\teval-error:0.363636\ttrain-error:0.196001\n",
      "[18]\teval-error:0.375\ttrain-error:0.196001\n",
      "[19]\teval-error:0.363636\ttrain-error:0.196001\n",
      "[20]\teval-error:0.363636\ttrain-error:0.196001\n",
      "[21]\teval-error:0.352273\ttrain-error:0.196001\n",
      "[22]\teval-error:0.352273\ttrain-error:0.196001\n",
      "[23]\teval-error:0.363636\ttrain-error:0.196001\n",
      "[24]\teval-error:0.363636\ttrain-error:0.196001\n",
      "[25]\teval-error:0.363636\ttrain-error:0.195821\n",
      "[26]\teval-error:0.363636\ttrain-error:0.196001\n",
      "[27]\teval-error:0.375\ttrain-error:0.196001\n",
      "[28]\teval-error:0.386364\ttrain-error:0.195821\n",
      "[29]\teval-error:0.386364\ttrain-error:0.195821\n",
      "[30]\teval-error:0.375\ttrain-error:0.195821\n",
      "[31]\teval-error:0.375\ttrain-error:0.195821\n",
      "[32]\teval-error:0.363636\ttrain-error:0.195821\n",
      "[33]\teval-error:0.363636\ttrain-error:0.195821\n",
      "[34]\teval-error:0.375\ttrain-error:0.195821\n",
      "[35]\teval-error:0.375\ttrain-error:0.195821\n",
      "[36]\teval-error:0.375\ttrain-error:0.195821\n",
      "[37]\teval-error:0.363636\ttrain-error:0.195821\n",
      "[38]\teval-error:0.352273\ttrain-error:0.195821\n",
      "[39]\teval-error:0.352273\ttrain-error:0.195821\n",
      "[40]\teval-error:0.352273\ttrain-error:0.195821\n",
      "[41]\teval-error:0.386364\ttrain-error:0.195821\n",
      "[42]\teval-error:0.386364\ttrain-error:0.195821\n",
      "[43]\teval-error:0.375\ttrain-error:0.195821\n",
      "[44]\teval-error:0.375\ttrain-error:0.195821\n",
      "[45]\teval-error:0.375\ttrain-error:0.195821\n",
      "[46]\teval-error:0.386364\ttrain-error:0.195821\n",
      "[47]\teval-error:0.375\ttrain-error:0.195821\n",
      "[48]\teval-error:0.375\ttrain-error:0.195821\n",
      "[49]\teval-error:0.375\ttrain-error:0.195821\n",
      "[50]\teval-error:0.375\ttrain-error:0.195821\n",
      "[51]\teval-error:0.375\ttrain-error:0.195821\n",
      "[52]\teval-error:0.375\ttrain-error:0.195821\n",
      "[53]\teval-error:0.375\ttrain-error:0.195821\n",
      "[54]\teval-error:0.375\ttrain-error:0.195821\n",
      "[55]\teval-error:0.375\ttrain-error:0.195821\n",
      "[56]\teval-error:0.375\ttrain-error:0.195821\n",
      "[57]\teval-error:0.375\ttrain-error:0.195821\n",
      "[58]\teval-error:0.375\ttrain-error:0.195821\n",
      "[59]\teval-error:0.375\ttrain-error:0.195821\n",
      "[60]\teval-error:0.375\ttrain-error:0.195821\n",
      "[61]\teval-error:0.375\ttrain-error:0.195821\n",
      "[62]\teval-error:0.375\ttrain-error:0.195821\n",
      "[63]\teval-error:0.375\ttrain-error:0.195821\n",
      "[64]\teval-error:0.375\ttrain-error:0.195821\n",
      "[65]\teval-error:0.375\ttrain-error:0.195821\n",
      "[66]\teval-error:0.363636\ttrain-error:0.195821\n",
      "[67]\teval-error:0.363636\ttrain-error:0.195821\n",
      "[68]\teval-error:0.363636\ttrain-error:0.195821\n",
      "[69]\teval-error:0.363636\ttrain-error:0.195821\n",
      "[0]\teval-error:0.361702\ttrain-error:0.235347\n",
      "[1]\teval-error:0.361702\ttrain-error:0.214433\n",
      "[2]\teval-error:0.361702\ttrain-error:0.203589\n",
      "[3]\teval-error:0.361702\ttrain-error:0.198941\n",
      "[4]\teval-error:0.308511\ttrain-error:0.190937\n",
      "[5]\teval-error:0.319149\ttrain-error:0.188355\n",
      "[6]\teval-error:0.319149\ttrain-error:0.18771\n",
      "[7]\teval-error:0.351064\ttrain-error:0.186419\n",
      "[8]\teval-error:0.361702\ttrain-error:0.184999\n",
      "[9]\teval-error:0.329787\ttrain-error:0.181642\n",
      "[10]\teval-error:0.340426\ttrain-error:0.180738\n",
      "[11]\teval-error:0.340426\ttrain-error:0.180222\n",
      "[12]\teval-error:0.351064\ttrain-error:0.180093\n",
      "[13]\teval-error:0.329787\ttrain-error:0.178673\n",
      "[14]\teval-error:0.37234\ttrain-error:0.177382\n",
      "[15]\teval-error:0.361702\ttrain-error:0.176736\n",
      "[16]\teval-error:0.37234\ttrain-error:0.176091\n",
      "[17]\teval-error:0.351064\ttrain-error:0.176091\n",
      "[18]\teval-error:0.361702\ttrain-error:0.175962\n",
      "[19]\teval-error:0.361702\ttrain-error:0.175704\n",
      "[20]\teval-error:0.361702\ttrain-error:0.175574\n",
      "[21]\teval-error:0.361702\ttrain-error:0.175574\n",
      "[22]\teval-error:0.351064\ttrain-error:0.175574\n",
      "[23]\teval-error:0.351064\ttrain-error:0.175574\n",
      "[24]\teval-error:0.361702\ttrain-error:0.175574\n",
      "[25]\teval-error:0.361702\ttrain-error:0.175574\n",
      "[26]\teval-error:0.361702\ttrain-error:0.175574\n",
      "[27]\teval-error:0.361702\ttrain-error:0.175574\n",
      "[28]\teval-error:0.361702\ttrain-error:0.175574\n",
      "[29]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[30]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[31]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[32]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[33]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[34]\teval-error:0.37234\ttrain-error:0.175445\n",
      "[35]\teval-error:0.37234\ttrain-error:0.175445\n",
      "[36]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[37]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[38]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[39]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[40]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[41]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[42]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[43]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[44]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[45]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[46]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[47]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[48]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[49]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[50]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[51]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[52]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[53]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[54]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[55]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[56]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[57]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[58]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[59]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[60]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[61]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[62]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[63]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[64]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[65]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[66]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[67]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[68]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[69]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[0]\teval-error:0.361702\ttrain-error:0.235347\n",
      "[1]\teval-error:0.361702\ttrain-error:0.214433\n",
      "[2]\teval-error:0.361702\ttrain-error:0.203589\n",
      "[3]\teval-error:0.361702\ttrain-error:0.198941\n",
      "[4]\teval-error:0.308511\ttrain-error:0.190937\n",
      "[5]\teval-error:0.319149\ttrain-error:0.188355\n",
      "[6]\teval-error:0.319149\ttrain-error:0.18771\n",
      "[7]\teval-error:0.351064\ttrain-error:0.186419\n",
      "[8]\teval-error:0.361702\ttrain-error:0.184999\n",
      "[9]\teval-error:0.329787\ttrain-error:0.181642\n",
      "[10]\teval-error:0.340426\ttrain-error:0.180738\n",
      "[11]\teval-error:0.340426\ttrain-error:0.180222\n",
      "[12]\teval-error:0.351064\ttrain-error:0.180093\n",
      "[13]\teval-error:0.329787\ttrain-error:0.178673\n",
      "[14]\teval-error:0.37234\ttrain-error:0.177382\n",
      "[15]\teval-error:0.361702\ttrain-error:0.176736\n",
      "[16]\teval-error:0.37234\ttrain-error:0.176091\n",
      "[17]\teval-error:0.351064\ttrain-error:0.176091\n",
      "[18]\teval-error:0.361702\ttrain-error:0.175962\n",
      "[19]\teval-error:0.361702\ttrain-error:0.175704\n",
      "[20]\teval-error:0.361702\ttrain-error:0.175574\n",
      "[21]\teval-error:0.361702\ttrain-error:0.175574\n",
      "[22]\teval-error:0.351064\ttrain-error:0.175574\n",
      "[23]\teval-error:0.351064\ttrain-error:0.175574\n",
      "[24]\teval-error:0.361702\ttrain-error:0.175574\n",
      "[25]\teval-error:0.361702\ttrain-error:0.175574\n",
      "[26]\teval-error:0.361702\ttrain-error:0.175574\n",
      "[27]\teval-error:0.361702\ttrain-error:0.175574\n",
      "[28]\teval-error:0.361702\ttrain-error:0.175574\n",
      "[29]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[30]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[31]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[32]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[33]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[34]\teval-error:0.37234\ttrain-error:0.175445\n",
      "[35]\teval-error:0.37234\ttrain-error:0.175445\n",
      "[36]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[37]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[38]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[39]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[40]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[41]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[42]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[43]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[44]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[45]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[46]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[47]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[48]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[49]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[50]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[51]\teval-error:0.361702\ttrain-error:0.175445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[53]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[54]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[55]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[56]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[57]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[58]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[59]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[60]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[61]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[62]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[63]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[64]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[65]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[66]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[67]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[68]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[69]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[0]\teval-error:0.361702\ttrain-error:0.235347\n",
      "[1]\teval-error:0.361702\ttrain-error:0.214433\n",
      "[2]\teval-error:0.361702\ttrain-error:0.203589\n",
      "[3]\teval-error:0.361702\ttrain-error:0.198941\n",
      "[4]\teval-error:0.308511\ttrain-error:0.190937\n",
      "[5]\teval-error:0.319149\ttrain-error:0.188355\n",
      "[6]\teval-error:0.319149\ttrain-error:0.18771\n",
      "[7]\teval-error:0.351064\ttrain-error:0.186419\n",
      "[8]\teval-error:0.361702\ttrain-error:0.184999\n",
      "[9]\teval-error:0.329787\ttrain-error:0.181642\n",
      "[10]\teval-error:0.340426\ttrain-error:0.180738\n",
      "[11]\teval-error:0.340426\ttrain-error:0.180222\n",
      "[12]\teval-error:0.351064\ttrain-error:0.180093\n",
      "[13]\teval-error:0.329787\ttrain-error:0.178673\n",
      "[14]\teval-error:0.37234\ttrain-error:0.177382\n",
      "[15]\teval-error:0.361702\ttrain-error:0.176736\n",
      "[16]\teval-error:0.37234\ttrain-error:0.176091\n",
      "[17]\teval-error:0.351064\ttrain-error:0.176091\n",
      "[18]\teval-error:0.361702\ttrain-error:0.175962\n",
      "[19]\teval-error:0.361702\ttrain-error:0.175704\n",
      "[20]\teval-error:0.361702\ttrain-error:0.175574\n",
      "[21]\teval-error:0.361702\ttrain-error:0.175574\n",
      "[22]\teval-error:0.351064\ttrain-error:0.175574\n",
      "[23]\teval-error:0.351064\ttrain-error:0.175574\n",
      "[24]\teval-error:0.361702\ttrain-error:0.175574\n",
      "[25]\teval-error:0.361702\ttrain-error:0.175574\n",
      "[26]\teval-error:0.361702\ttrain-error:0.175574\n",
      "[27]\teval-error:0.361702\ttrain-error:0.175574\n",
      "[28]\teval-error:0.361702\ttrain-error:0.175574\n",
      "[29]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[30]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[31]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[32]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[33]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[34]\teval-error:0.37234\ttrain-error:0.175445\n",
      "[35]\teval-error:0.37234\ttrain-error:0.175445\n",
      "[36]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[37]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[38]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[39]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[40]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[41]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[42]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[43]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[44]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[45]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[46]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[47]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[48]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[49]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[50]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[51]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[52]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[53]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[54]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[55]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[56]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[57]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[58]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[59]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[60]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[61]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[62]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[63]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[64]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[65]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[66]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[67]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[68]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[69]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[0]\teval-error:0.361702\ttrain-error:0.235347\n",
      "[1]\teval-error:0.361702\ttrain-error:0.214433\n",
      "[2]\teval-error:0.361702\ttrain-error:0.203589\n",
      "[3]\teval-error:0.361702\ttrain-error:0.198941\n",
      "[4]\teval-error:0.308511\ttrain-error:0.190937\n",
      "[5]\teval-error:0.319149\ttrain-error:0.188355\n",
      "[6]\teval-error:0.319149\ttrain-error:0.18771\n",
      "[7]\teval-error:0.351064\ttrain-error:0.186419\n",
      "[8]\teval-error:0.361702\ttrain-error:0.184999\n",
      "[9]\teval-error:0.329787\ttrain-error:0.181642\n",
      "[10]\teval-error:0.340426\ttrain-error:0.180738\n",
      "[11]\teval-error:0.340426\ttrain-error:0.180222\n",
      "[12]\teval-error:0.351064\ttrain-error:0.180093\n",
      "[13]\teval-error:0.329787\ttrain-error:0.178673\n",
      "[14]\teval-error:0.37234\ttrain-error:0.177382\n",
      "[15]\teval-error:0.361702\ttrain-error:0.176736\n",
      "[16]\teval-error:0.37234\ttrain-error:0.176091\n",
      "[17]\teval-error:0.351064\ttrain-error:0.176091\n",
      "[18]\teval-error:0.361702\ttrain-error:0.175962\n",
      "[19]\teval-error:0.361702\ttrain-error:0.175704\n",
      "[20]\teval-error:0.361702\ttrain-error:0.175574\n",
      "[21]\teval-error:0.361702\ttrain-error:0.175574\n",
      "[22]\teval-error:0.351064\ttrain-error:0.175574\n",
      "[23]\teval-error:0.351064\ttrain-error:0.175574\n",
      "[24]\teval-error:0.361702\ttrain-error:0.175574\n",
      "[25]\teval-error:0.361702\ttrain-error:0.175574\n",
      "[26]\teval-error:0.361702\ttrain-error:0.175574\n",
      "[27]\teval-error:0.361702\ttrain-error:0.175574\n",
      "[28]\teval-error:0.361702\ttrain-error:0.175574\n",
      "[29]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[30]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[31]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[32]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[33]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[34]\teval-error:0.37234\ttrain-error:0.175445\n",
      "[35]\teval-error:0.37234\ttrain-error:0.175445\n",
      "[36]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[37]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[38]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[39]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[40]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[41]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[42]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[43]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[44]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[45]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[46]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[47]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[48]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[49]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[50]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[51]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[52]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[53]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[54]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[55]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[56]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[57]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[58]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[59]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[60]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[61]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[62]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[63]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[64]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[65]\teval-error:0.351064\ttrain-error:0.175445\n",
      "[66]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[67]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[68]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[69]\teval-error:0.361702\ttrain-error:0.175445\n",
      "[0]\teval-error:0.358779\ttrain-error:0.208899\n",
      "[1]\teval-error:0.377863\ttrain-error:0.194686\n",
      "[2]\teval-error:0.362595\ttrain-error:0.186902\n",
      "[3]\teval-error:0.370229\ttrain-error:0.181474\n",
      "[4]\teval-error:0.358779\ttrain-error:0.178832\n",
      "[5]\teval-error:0.362595\ttrain-error:0.175546\n",
      "[6]\teval-error:0.362595\ttrain-error:0.174404\n",
      "[7]\teval-error:0.354962\ttrain-error:0.17219\n",
      "[8]\teval-error:0.354962\ttrain-error:0.171904\n",
      "[9]\teval-error:0.354962\ttrain-error:0.171404\n",
      "[10]\teval-error:0.354962\ttrain-error:0.170761\n",
      "[11]\teval-error:0.370229\ttrain-error:0.169333\n",
      "[12]\teval-error:0.358779\ttrain-error:0.167976\n",
      "[13]\teval-error:0.362595\ttrain-error:0.167262\n",
      "[14]\teval-error:0.366412\ttrain-error:0.166905\n",
      "[15]\teval-error:0.366412\ttrain-error:0.166833\n",
      "[16]\teval-error:0.370229\ttrain-error:0.166548\n",
      "[17]\teval-error:0.377863\ttrain-error:0.166191\n",
      "[18]\teval-error:0.385496\ttrain-error:0.166191\n",
      "[19]\teval-error:0.366412\ttrain-error:0.166048\n",
      "[20]\teval-error:0.362595\ttrain-error:0.166048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21]\teval-error:0.354962\ttrain-error:0.166048\n",
      "[22]\teval-error:0.358779\ttrain-error:0.166048\n",
      "[23]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[24]\teval-error:0.370229\ttrain-error:0.166119\n",
      "[25]\teval-error:0.366412\ttrain-error:0.166048\n",
      "[26]\teval-error:0.374046\ttrain-error:0.166048\n",
      "[27]\teval-error:0.366412\ttrain-error:0.166048\n",
      "[28]\teval-error:0.370229\ttrain-error:0.166048\n",
      "[29]\teval-error:0.374046\ttrain-error:0.166048\n",
      "[30]\teval-error:0.374046\ttrain-error:0.166048\n",
      "[31]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[32]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[33]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[34]\teval-error:0.385496\ttrain-error:0.166048\n",
      "[35]\teval-error:0.385496\ttrain-error:0.166048\n",
      "[36]\teval-error:0.381679\ttrain-error:0.166048\n",
      "[37]\teval-error:0.385496\ttrain-error:0.166048\n",
      "[38]\teval-error:0.385496\ttrain-error:0.166048\n",
      "[39]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[40]\teval-error:0.385496\ttrain-error:0.166048\n",
      "[41]\teval-error:0.381679\ttrain-error:0.166048\n",
      "[42]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[43]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[44]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[45]\teval-error:0.370229\ttrain-error:0.166048\n",
      "[46]\teval-error:0.374046\ttrain-error:0.166048\n",
      "[47]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[48]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[49]\teval-error:0.374046\ttrain-error:0.166048\n",
      "[50]\teval-error:0.381679\ttrain-error:0.166048\n",
      "[51]\teval-error:0.39313\ttrain-error:0.166048\n",
      "[52]\teval-error:0.385496\ttrain-error:0.166048\n",
      "[53]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[54]\teval-error:0.381679\ttrain-error:0.166048\n",
      "[55]\teval-error:0.381679\ttrain-error:0.166048\n",
      "[56]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[57]\teval-error:0.362595\ttrain-error:0.166048\n",
      "[58]\teval-error:0.374046\ttrain-error:0.166048\n",
      "[59]\teval-error:0.374046\ttrain-error:0.166048\n",
      "[60]\teval-error:0.385496\ttrain-error:0.166048\n",
      "[61]\teval-error:0.385496\ttrain-error:0.166048\n",
      "[62]\teval-error:0.370229\ttrain-error:0.166048\n",
      "[63]\teval-error:0.370229\ttrain-error:0.166048\n",
      "[64]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[65]\teval-error:0.374046\ttrain-error:0.166048\n",
      "[66]\teval-error:0.370229\ttrain-error:0.166048\n",
      "[67]\teval-error:0.366412\ttrain-error:0.166048\n",
      "[68]\teval-error:0.366412\ttrain-error:0.166048\n",
      "[69]\teval-error:0.366412\ttrain-error:0.166048\n",
      "[0]\teval-error:0.358779\ttrain-error:0.208899\n",
      "[1]\teval-error:0.377863\ttrain-error:0.194686\n",
      "[2]\teval-error:0.362595\ttrain-error:0.186902\n",
      "[3]\teval-error:0.370229\ttrain-error:0.181474\n",
      "[4]\teval-error:0.358779\ttrain-error:0.178832\n",
      "[5]\teval-error:0.362595\ttrain-error:0.175546\n",
      "[6]\teval-error:0.362595\ttrain-error:0.174404\n",
      "[7]\teval-error:0.354962\ttrain-error:0.17219\n",
      "[8]\teval-error:0.354962\ttrain-error:0.171904\n",
      "[9]\teval-error:0.354962\ttrain-error:0.171404\n",
      "[10]\teval-error:0.354962\ttrain-error:0.170761\n",
      "[11]\teval-error:0.370229\ttrain-error:0.169333\n",
      "[12]\teval-error:0.358779\ttrain-error:0.167976\n",
      "[13]\teval-error:0.362595\ttrain-error:0.167262\n",
      "[14]\teval-error:0.366412\ttrain-error:0.166905\n",
      "[15]\teval-error:0.366412\ttrain-error:0.166833\n",
      "[16]\teval-error:0.370229\ttrain-error:0.166548\n",
      "[17]\teval-error:0.377863\ttrain-error:0.166191\n",
      "[18]\teval-error:0.385496\ttrain-error:0.166191\n",
      "[19]\teval-error:0.366412\ttrain-error:0.166048\n",
      "[20]\teval-error:0.362595\ttrain-error:0.166048\n",
      "[21]\teval-error:0.354962\ttrain-error:0.166048\n",
      "[22]\teval-error:0.358779\ttrain-error:0.166048\n",
      "[23]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[24]\teval-error:0.370229\ttrain-error:0.166119\n",
      "[25]\teval-error:0.366412\ttrain-error:0.166048\n",
      "[26]\teval-error:0.374046\ttrain-error:0.166048\n",
      "[27]\teval-error:0.366412\ttrain-error:0.166048\n",
      "[28]\teval-error:0.370229\ttrain-error:0.166048\n",
      "[29]\teval-error:0.374046\ttrain-error:0.166048\n",
      "[30]\teval-error:0.374046\ttrain-error:0.166048\n",
      "[31]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[32]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[33]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[34]\teval-error:0.385496\ttrain-error:0.166048\n",
      "[35]\teval-error:0.385496\ttrain-error:0.166048\n",
      "[36]\teval-error:0.381679\ttrain-error:0.166048\n",
      "[37]\teval-error:0.385496\ttrain-error:0.166048\n",
      "[38]\teval-error:0.385496\ttrain-error:0.166048\n",
      "[39]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[40]\teval-error:0.385496\ttrain-error:0.166048\n",
      "[41]\teval-error:0.381679\ttrain-error:0.166048\n",
      "[42]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[43]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[44]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[45]\teval-error:0.370229\ttrain-error:0.166048\n",
      "[46]\teval-error:0.374046\ttrain-error:0.166048\n",
      "[47]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[48]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[49]\teval-error:0.374046\ttrain-error:0.166048\n",
      "[50]\teval-error:0.381679\ttrain-error:0.166048\n",
      "[51]\teval-error:0.39313\ttrain-error:0.166048\n",
      "[52]\teval-error:0.385496\ttrain-error:0.166048\n",
      "[53]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[54]\teval-error:0.381679\ttrain-error:0.166048\n",
      "[55]\teval-error:0.381679\ttrain-error:0.166048\n",
      "[56]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[57]\teval-error:0.362595\ttrain-error:0.166048\n",
      "[58]\teval-error:0.374046\ttrain-error:0.166048\n",
      "[59]\teval-error:0.374046\ttrain-error:0.166048\n",
      "[60]\teval-error:0.385496\ttrain-error:0.166048\n",
      "[61]\teval-error:0.385496\ttrain-error:0.166048\n",
      "[62]\teval-error:0.370229\ttrain-error:0.166048\n",
      "[63]\teval-error:0.370229\ttrain-error:0.166048\n",
      "[64]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[65]\teval-error:0.374046\ttrain-error:0.166048\n",
      "[66]\teval-error:0.370229\ttrain-error:0.166048\n",
      "[67]\teval-error:0.366412\ttrain-error:0.166048\n",
      "[68]\teval-error:0.366412\ttrain-error:0.166048\n",
      "[69]\teval-error:0.366412\ttrain-error:0.166048\n",
      "[0]\teval-error:0.358779\ttrain-error:0.208899\n",
      "[1]\teval-error:0.377863\ttrain-error:0.194686\n",
      "[2]\teval-error:0.362595\ttrain-error:0.186902\n",
      "[3]\teval-error:0.370229\ttrain-error:0.181474\n",
      "[4]\teval-error:0.358779\ttrain-error:0.178832\n",
      "[5]\teval-error:0.362595\ttrain-error:0.175546\n",
      "[6]\teval-error:0.362595\ttrain-error:0.174404\n",
      "[7]\teval-error:0.354962\ttrain-error:0.17219\n",
      "[8]\teval-error:0.354962\ttrain-error:0.171904\n",
      "[9]\teval-error:0.354962\ttrain-error:0.171404\n",
      "[10]\teval-error:0.354962\ttrain-error:0.170761\n",
      "[11]\teval-error:0.370229\ttrain-error:0.169333\n",
      "[12]\teval-error:0.358779\ttrain-error:0.167976\n",
      "[13]\teval-error:0.362595\ttrain-error:0.167262\n",
      "[14]\teval-error:0.366412\ttrain-error:0.166905\n",
      "[15]\teval-error:0.366412\ttrain-error:0.166833\n",
      "[16]\teval-error:0.370229\ttrain-error:0.166548\n",
      "[17]\teval-error:0.377863\ttrain-error:0.166191\n",
      "[18]\teval-error:0.385496\ttrain-error:0.166191\n",
      "[19]\teval-error:0.366412\ttrain-error:0.166048\n",
      "[20]\teval-error:0.362595\ttrain-error:0.166048\n",
      "[21]\teval-error:0.354962\ttrain-error:0.166048\n",
      "[22]\teval-error:0.358779\ttrain-error:0.166048\n",
      "[23]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[24]\teval-error:0.370229\ttrain-error:0.166119\n",
      "[25]\teval-error:0.366412\ttrain-error:0.166048\n",
      "[26]\teval-error:0.374046\ttrain-error:0.166048\n",
      "[27]\teval-error:0.366412\ttrain-error:0.166048\n",
      "[28]\teval-error:0.370229\ttrain-error:0.166048\n",
      "[29]\teval-error:0.374046\ttrain-error:0.166048\n",
      "[30]\teval-error:0.374046\ttrain-error:0.166048\n",
      "[31]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[32]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[33]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[34]\teval-error:0.385496\ttrain-error:0.166048\n",
      "[35]\teval-error:0.385496\ttrain-error:0.166048\n",
      "[36]\teval-error:0.381679\ttrain-error:0.166048\n",
      "[37]\teval-error:0.385496\ttrain-error:0.166048\n",
      "[38]\teval-error:0.385496\ttrain-error:0.166048\n",
      "[39]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[40]\teval-error:0.385496\ttrain-error:0.166048\n",
      "[41]\teval-error:0.381679\ttrain-error:0.166048\n",
      "[42]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[43]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[44]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[45]\teval-error:0.370229\ttrain-error:0.166048\n",
      "[46]\teval-error:0.374046\ttrain-error:0.166048\n",
      "[47]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[48]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[49]\teval-error:0.374046\ttrain-error:0.166048\n",
      "[50]\teval-error:0.381679\ttrain-error:0.166048\n",
      "[51]\teval-error:0.39313\ttrain-error:0.166048\n",
      "[52]\teval-error:0.385496\ttrain-error:0.166048\n",
      "[53]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[54]\teval-error:0.381679\ttrain-error:0.166048\n",
      "[55]\teval-error:0.381679\ttrain-error:0.166048\n",
      "[56]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[57]\teval-error:0.362595\ttrain-error:0.166048\n",
      "[58]\teval-error:0.374046\ttrain-error:0.166048\n",
      "[59]\teval-error:0.374046\ttrain-error:0.166048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60]\teval-error:0.385496\ttrain-error:0.166048\n",
      "[61]\teval-error:0.385496\ttrain-error:0.166048\n",
      "[62]\teval-error:0.370229\ttrain-error:0.166048\n",
      "[63]\teval-error:0.370229\ttrain-error:0.166048\n",
      "[64]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[65]\teval-error:0.374046\ttrain-error:0.166048\n",
      "[66]\teval-error:0.370229\ttrain-error:0.166048\n",
      "[67]\teval-error:0.366412\ttrain-error:0.166048\n",
      "[68]\teval-error:0.366412\ttrain-error:0.166048\n",
      "[69]\teval-error:0.366412\ttrain-error:0.166048\n",
      "[0]\teval-error:0.358779\ttrain-error:0.208899\n",
      "[1]\teval-error:0.377863\ttrain-error:0.194686\n",
      "[2]\teval-error:0.362595\ttrain-error:0.186902\n",
      "[3]\teval-error:0.370229\ttrain-error:0.181474\n",
      "[4]\teval-error:0.358779\ttrain-error:0.178832\n",
      "[5]\teval-error:0.362595\ttrain-error:0.175546\n",
      "[6]\teval-error:0.362595\ttrain-error:0.174404\n",
      "[7]\teval-error:0.354962\ttrain-error:0.17219\n",
      "[8]\teval-error:0.354962\ttrain-error:0.171904\n",
      "[9]\teval-error:0.354962\ttrain-error:0.171404\n",
      "[10]\teval-error:0.354962\ttrain-error:0.170761\n",
      "[11]\teval-error:0.370229\ttrain-error:0.169333\n",
      "[12]\teval-error:0.358779\ttrain-error:0.167976\n",
      "[13]\teval-error:0.362595\ttrain-error:0.167262\n",
      "[14]\teval-error:0.366412\ttrain-error:0.166905\n",
      "[15]\teval-error:0.366412\ttrain-error:0.166833\n",
      "[16]\teval-error:0.370229\ttrain-error:0.166548\n",
      "[17]\teval-error:0.377863\ttrain-error:0.166191\n",
      "[18]\teval-error:0.385496\ttrain-error:0.166191\n",
      "[19]\teval-error:0.366412\ttrain-error:0.166048\n",
      "[20]\teval-error:0.362595\ttrain-error:0.166048\n",
      "[21]\teval-error:0.354962\ttrain-error:0.166048\n",
      "[22]\teval-error:0.358779\ttrain-error:0.166048\n",
      "[23]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[24]\teval-error:0.370229\ttrain-error:0.166119\n",
      "[25]\teval-error:0.366412\ttrain-error:0.166048\n",
      "[26]\teval-error:0.374046\ttrain-error:0.166048\n",
      "[27]\teval-error:0.366412\ttrain-error:0.166048\n",
      "[28]\teval-error:0.370229\ttrain-error:0.166048\n",
      "[29]\teval-error:0.374046\ttrain-error:0.166048\n",
      "[30]\teval-error:0.374046\ttrain-error:0.166048\n",
      "[31]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[32]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[33]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[34]\teval-error:0.385496\ttrain-error:0.166048\n",
      "[35]\teval-error:0.385496\ttrain-error:0.166048\n",
      "[36]\teval-error:0.381679\ttrain-error:0.166048\n",
      "[37]\teval-error:0.385496\ttrain-error:0.166048\n",
      "[38]\teval-error:0.385496\ttrain-error:0.166048\n",
      "[39]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[40]\teval-error:0.385496\ttrain-error:0.166048\n",
      "[41]\teval-error:0.381679\ttrain-error:0.166048\n",
      "[42]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[43]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[44]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[45]\teval-error:0.370229\ttrain-error:0.166048\n",
      "[46]\teval-error:0.374046\ttrain-error:0.166048\n",
      "[47]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[48]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[49]\teval-error:0.374046\ttrain-error:0.166048\n",
      "[50]\teval-error:0.381679\ttrain-error:0.166048\n",
      "[51]\teval-error:0.39313\ttrain-error:0.166048\n",
      "[52]\teval-error:0.385496\ttrain-error:0.166048\n",
      "[53]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[54]\teval-error:0.381679\ttrain-error:0.166048\n",
      "[55]\teval-error:0.381679\ttrain-error:0.166048\n",
      "[56]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[57]\teval-error:0.362595\ttrain-error:0.166048\n",
      "[58]\teval-error:0.374046\ttrain-error:0.166048\n",
      "[59]\teval-error:0.374046\ttrain-error:0.166048\n",
      "[60]\teval-error:0.385496\ttrain-error:0.166048\n",
      "[61]\teval-error:0.385496\ttrain-error:0.166048\n",
      "[62]\teval-error:0.370229\ttrain-error:0.166048\n",
      "[63]\teval-error:0.370229\ttrain-error:0.166048\n",
      "[64]\teval-error:0.377863\ttrain-error:0.166048\n",
      "[65]\teval-error:0.374046\ttrain-error:0.166048\n",
      "[66]\teval-error:0.370229\ttrain-error:0.166048\n",
      "[67]\teval-error:0.366412\ttrain-error:0.166048\n",
      "[68]\teval-error:0.366412\ttrain-error:0.166048\n",
      "[69]\teval-error:0.366412\ttrain-error:0.166048\n",
      "[0]\teval-error:0.386364\ttrain-error:0.20753\n",
      "[1]\teval-error:0.329545\ttrain-error:0.179067\n",
      "[2]\teval-error:0.329545\ttrain-error:0.160512\n",
      "[3]\teval-error:0.318182\ttrain-error:0.153666\n",
      "[4]\teval-error:0.352273\ttrain-error:0.148802\n",
      "[5]\teval-error:0.363636\ttrain-error:0.145019\n",
      "[6]\teval-error:0.340909\ttrain-error:0.144118\n",
      "[7]\teval-error:0.352273\ttrain-error:0.143578\n",
      "[8]\teval-error:0.340909\ttrain-error:0.143398\n",
      "[9]\teval-error:0.329545\ttrain-error:0.143037\n",
      "[10]\teval-error:0.329545\ttrain-error:0.143037\n",
      "[11]\teval-error:0.352273\ttrain-error:0.143037\n",
      "[12]\teval-error:0.363636\ttrain-error:0.143037\n",
      "[13]\teval-error:0.363636\ttrain-error:0.143037\n",
      "[14]\teval-error:0.363636\ttrain-error:0.143037\n",
      "[15]\teval-error:0.352273\ttrain-error:0.143037\n",
      "[16]\teval-error:0.363636\ttrain-error:0.143037\n",
      "[17]\teval-error:0.352273\ttrain-error:0.143037\n",
      "[18]\teval-error:0.340909\ttrain-error:0.143037\n",
      "[19]\teval-error:0.340909\ttrain-error:0.143037\n",
      "[20]\teval-error:0.363636\ttrain-error:0.143037\n",
      "[21]\teval-error:0.352273\ttrain-error:0.143037\n",
      "[22]\teval-error:0.352273\ttrain-error:0.143037\n",
      "[23]\teval-error:0.340909\ttrain-error:0.143037\n",
      "[24]\teval-error:0.329545\ttrain-error:0.143037\n",
      "[25]\teval-error:0.352273\ttrain-error:0.143037\n",
      "[26]\teval-error:0.363636\ttrain-error:0.143037\n",
      "[27]\teval-error:0.363636\ttrain-error:0.143037\n",
      "[28]\teval-error:0.363636\ttrain-error:0.143037\n",
      "[29]\teval-error:0.363636\ttrain-error:0.143037\n",
      "[30]\teval-error:0.352273\ttrain-error:0.143037\n",
      "[31]\teval-error:0.363636\ttrain-error:0.143037\n",
      "[32]\teval-error:0.375\ttrain-error:0.143037\n",
      "[33]\teval-error:0.363636\ttrain-error:0.143037\n",
      "[34]\teval-error:0.386364\ttrain-error:0.143037\n",
      "[35]\teval-error:0.375\ttrain-error:0.143037\n",
      "[36]\teval-error:0.363636\ttrain-error:0.143037\n",
      "[37]\teval-error:0.363636\ttrain-error:0.143037\n",
      "[38]\teval-error:0.386364\ttrain-error:0.143037\n",
      "[39]\teval-error:0.386364\ttrain-error:0.143037\n",
      "[40]\teval-error:0.386364\ttrain-error:0.143037\n",
      "[41]\teval-error:0.386364\ttrain-error:0.143037\n",
      "[42]\teval-error:0.386364\ttrain-error:0.143037\n",
      "[43]\teval-error:0.386364\ttrain-error:0.143037\n",
      "[44]\teval-error:0.386364\ttrain-error:0.143037\n",
      "[45]\teval-error:0.375\ttrain-error:0.143037\n",
      "[46]\teval-error:0.375\ttrain-error:0.143037\n",
      "[47]\teval-error:0.386364\ttrain-error:0.143037\n",
      "[48]\teval-error:0.386364\ttrain-error:0.143037\n",
      "[49]\teval-error:0.375\ttrain-error:0.143037\n",
      "[50]\teval-error:0.363636\ttrain-error:0.143037\n",
      "[51]\teval-error:0.375\ttrain-error:0.143037\n",
      "[52]\teval-error:0.363636\ttrain-error:0.143037\n",
      "[53]\teval-error:0.352273\ttrain-error:0.143037\n",
      "[54]\teval-error:0.352273\ttrain-error:0.143037\n",
      "[55]\teval-error:0.352273\ttrain-error:0.143037\n",
      "[56]\teval-error:0.340909\ttrain-error:0.143037\n",
      "[57]\teval-error:0.352273\ttrain-error:0.143037\n",
      "[58]\teval-error:0.352273\ttrain-error:0.143037\n",
      "[59]\teval-error:0.340909\ttrain-error:0.143037\n",
      "[60]\teval-error:0.340909\ttrain-error:0.143037\n",
      "[61]\teval-error:0.340909\ttrain-error:0.143037\n",
      "[62]\teval-error:0.340909\ttrain-error:0.143037\n",
      "[63]\teval-error:0.340909\ttrain-error:0.143037\n",
      "[64]\teval-error:0.352273\ttrain-error:0.143037\n",
      "[65]\teval-error:0.340909\ttrain-error:0.143037\n",
      "[66]\teval-error:0.352273\ttrain-error:0.143037\n",
      "[67]\teval-error:0.340909\ttrain-error:0.143037\n",
      "[68]\teval-error:0.352273\ttrain-error:0.143037\n",
      "[69]\teval-error:0.340909\ttrain-error:0.143037\n",
      "[0]\teval-error:0.329545\ttrain-error:0.213115\n",
      "[1]\teval-error:0.340909\ttrain-error:0.191317\n",
      "[2]\teval-error:0.295455\ttrain-error:0.176365\n",
      "[3]\teval-error:0.352273\ttrain-error:0.169519\n",
      "[4]\teval-error:0.352273\ttrain-error:0.166456\n",
      "[5]\teval-error:0.329545\ttrain-error:0.164115\n",
      "[6]\teval-error:0.352273\ttrain-error:0.163214\n",
      "[7]\teval-error:0.340909\ttrain-error:0.162133\n",
      "[8]\teval-error:0.363636\ttrain-error:0.161592\n",
      "[9]\teval-error:0.340909\ttrain-error:0.161052\n",
      "[10]\teval-error:0.352273\ttrain-error:0.161052\n",
      "[11]\teval-error:0.352273\ttrain-error:0.161052\n",
      "[12]\teval-error:0.340909\ttrain-error:0.161052\n",
      "[13]\teval-error:0.340909\ttrain-error:0.161052\n",
      "[14]\teval-error:0.352273\ttrain-error:0.161052\n",
      "[15]\teval-error:0.340909\ttrain-error:0.161052\n",
      "[16]\teval-error:0.352273\ttrain-error:0.161052\n",
      "[17]\teval-error:0.340909\ttrain-error:0.161052\n",
      "[18]\teval-error:0.352273\ttrain-error:0.161052\n",
      "[19]\teval-error:0.352273\ttrain-error:0.161052\n",
      "[20]\teval-error:0.352273\ttrain-error:0.161052\n",
      "[21]\teval-error:0.352273\ttrain-error:0.161052\n",
      "[22]\teval-error:0.352273\ttrain-error:0.161052\n",
      "[23]\teval-error:0.352273\ttrain-error:0.161052\n",
      "[24]\teval-error:0.352273\ttrain-error:0.161052\n",
      "[25]\teval-error:0.352273\ttrain-error:0.161052\n",
      "[26]\teval-error:0.340909\ttrain-error:0.161052\n",
      "[27]\teval-error:0.340909\ttrain-error:0.161052\n",
      "[28]\teval-error:0.340909\ttrain-error:0.161052\n",
      "[29]\teval-error:0.352273\ttrain-error:0.161052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30]\teval-error:0.352273\ttrain-error:0.161052\n",
      "[31]\teval-error:0.352273\ttrain-error:0.161052\n",
      "[32]\teval-error:0.352273\ttrain-error:0.161052\n",
      "[33]\teval-error:0.352273\ttrain-error:0.161052\n",
      "[34]\teval-error:0.352273\ttrain-error:0.161052\n",
      "[35]\teval-error:0.352273\ttrain-error:0.161052\n",
      "[36]\teval-error:0.352273\ttrain-error:0.161052\n",
      "[37]\teval-error:0.352273\ttrain-error:0.161052\n",
      "[38]\teval-error:0.363636\ttrain-error:0.161052\n",
      "[39]\teval-error:0.352273\ttrain-error:0.161052\n",
      "[40]\teval-error:0.352273\ttrain-error:0.161052\n",
      "[41]\teval-error:0.352273\ttrain-error:0.161052\n",
      "[42]\teval-error:0.352273\ttrain-error:0.161052\n",
      "[43]\teval-error:0.352273\ttrain-error:0.161052\n",
      "[44]\teval-error:0.363636\ttrain-error:0.161052\n",
      "[45]\teval-error:0.363636\ttrain-error:0.161052\n",
      "[46]\teval-error:0.363636\ttrain-error:0.161052\n",
      "[47]\teval-error:0.363636\ttrain-error:0.161052\n",
      "[48]\teval-error:0.363636\ttrain-error:0.161052\n",
      "[49]\teval-error:0.363636\ttrain-error:0.161052\n",
      "[50]\teval-error:0.363636\ttrain-error:0.161052\n",
      "[51]\teval-error:0.363636\ttrain-error:0.161052\n",
      "[52]\teval-error:0.363636\ttrain-error:0.161052\n",
      "[53]\teval-error:0.363636\ttrain-error:0.161052\n",
      "[54]\teval-error:0.363636\ttrain-error:0.161052\n",
      "[55]\teval-error:0.363636\ttrain-error:0.161052\n",
      "[56]\teval-error:0.363636\ttrain-error:0.161052\n",
      "[57]\teval-error:0.363636\ttrain-error:0.161052\n",
      "[58]\teval-error:0.363636\ttrain-error:0.161052\n",
      "[59]\teval-error:0.363636\ttrain-error:0.161052\n",
      "[60]\teval-error:0.363636\ttrain-error:0.161052\n",
      "[61]\teval-error:0.363636\ttrain-error:0.161052\n",
      "[62]\teval-error:0.363636\ttrain-error:0.161052\n",
      "[63]\teval-error:0.363636\ttrain-error:0.161052\n",
      "[64]\teval-error:0.363636\ttrain-error:0.161052\n",
      "[65]\teval-error:0.363636\ttrain-error:0.161052\n",
      "[66]\teval-error:0.363636\ttrain-error:0.161052\n",
      "[67]\teval-error:0.363636\ttrain-error:0.161052\n",
      "[68]\teval-error:0.363636\ttrain-error:0.161052\n",
      "[69]\teval-error:0.363636\ttrain-error:0.161052\n",
      "[0]\teval-error:0.488636\ttrain-error:0.229688\n",
      "[1]\teval-error:0.420455\ttrain-error:0.20735\n",
      "[2]\teval-error:0.409091\ttrain-error:0.19546\n",
      "[3]\teval-error:0.443182\ttrain-error:0.185912\n",
      "[4]\teval-error:0.454545\ttrain-error:0.179427\n",
      "[5]\teval-error:0.454545\ttrain-error:0.177265\n",
      "[6]\teval-error:0.443182\ttrain-error:0.175644\n",
      "[7]\teval-error:0.443182\ttrain-error:0.174923\n",
      "[8]\teval-error:0.454545\ttrain-error:0.174383\n",
      "[9]\teval-error:0.454545\ttrain-error:0.174383\n",
      "[10]\teval-error:0.443182\ttrain-error:0.174203\n",
      "[11]\teval-error:0.454545\ttrain-error:0.174023\n",
      "[12]\teval-error:0.454545\ttrain-error:0.173662\n",
      "[13]\teval-error:0.443182\ttrain-error:0.173843\n",
      "[14]\teval-error:0.443182\ttrain-error:0.173843\n",
      "[15]\teval-error:0.431818\ttrain-error:0.173662\n",
      "[16]\teval-error:0.386364\ttrain-error:0.173662\n",
      "[17]\teval-error:0.420455\ttrain-error:0.173662\n",
      "[18]\teval-error:0.420455\ttrain-error:0.173662\n",
      "[19]\teval-error:0.409091\ttrain-error:0.173662\n",
      "[20]\teval-error:0.397727\ttrain-error:0.173662\n",
      "[21]\teval-error:0.397727\ttrain-error:0.173662\n",
      "[22]\teval-error:0.397727\ttrain-error:0.173662\n",
      "[23]\teval-error:0.397727\ttrain-error:0.173662\n",
      "[24]\teval-error:0.386364\ttrain-error:0.173662\n",
      "[25]\teval-error:0.397727\ttrain-error:0.173662\n",
      "[26]\teval-error:0.397727\ttrain-error:0.173662\n",
      "[27]\teval-error:0.386364\ttrain-error:0.173662\n",
      "[28]\teval-error:0.386364\ttrain-error:0.173662\n",
      "[29]\teval-error:0.420455\ttrain-error:0.173662\n",
      "[30]\teval-error:0.375\ttrain-error:0.173662\n",
      "[31]\teval-error:0.397727\ttrain-error:0.173662\n",
      "[32]\teval-error:0.386364\ttrain-error:0.173662\n",
      "[33]\teval-error:0.375\ttrain-error:0.173662\n",
      "[34]\teval-error:0.386364\ttrain-error:0.173662\n",
      "[35]\teval-error:0.386364\ttrain-error:0.173662\n",
      "[36]\teval-error:0.386364\ttrain-error:0.173662\n",
      "[37]\teval-error:0.386364\ttrain-error:0.173662\n",
      "[38]\teval-error:0.386364\ttrain-error:0.173662\n",
      "[39]\teval-error:0.386364\ttrain-error:0.173662\n",
      "[40]\teval-error:0.409091\ttrain-error:0.173662\n",
      "[41]\teval-error:0.431818\ttrain-error:0.173662\n",
      "[42]\teval-error:0.397727\ttrain-error:0.173662\n",
      "[43]\teval-error:0.420455\ttrain-error:0.173662\n",
      "[44]\teval-error:0.386364\ttrain-error:0.173662\n",
      "[45]\teval-error:0.397727\ttrain-error:0.173662\n",
      "[46]\teval-error:0.397727\ttrain-error:0.173662\n",
      "[47]\teval-error:0.397727\ttrain-error:0.173662\n",
      "[48]\teval-error:0.397727\ttrain-error:0.173662\n",
      "[49]\teval-error:0.409091\ttrain-error:0.173662\n",
      "[50]\teval-error:0.431818\ttrain-error:0.173662\n",
      "[51]\teval-error:0.420455\ttrain-error:0.173662\n",
      "[52]\teval-error:0.431818\ttrain-error:0.173662\n",
      "[53]\teval-error:0.431818\ttrain-error:0.173662\n",
      "[54]\teval-error:0.420455\ttrain-error:0.173662\n",
      "[55]\teval-error:0.409091\ttrain-error:0.173662\n",
      "[56]\teval-error:0.431818\ttrain-error:0.173662\n",
      "[57]\teval-error:0.420455\ttrain-error:0.173662\n",
      "[58]\teval-error:0.420455\ttrain-error:0.173662\n",
      "[59]\teval-error:0.431818\ttrain-error:0.173662\n",
      "[60]\teval-error:0.420455\ttrain-error:0.173662\n",
      "[61]\teval-error:0.420455\ttrain-error:0.173662\n",
      "[62]\teval-error:0.465909\ttrain-error:0.173662\n",
      "[63]\teval-error:0.420455\ttrain-error:0.173662\n",
      "[64]\teval-error:0.465909\ttrain-error:0.173662\n",
      "[65]\teval-error:0.443182\ttrain-error:0.173662\n",
      "[66]\teval-error:0.409091\ttrain-error:0.173662\n",
      "[67]\teval-error:0.443182\ttrain-error:0.173662\n",
      "[68]\teval-error:0.409091\ttrain-error:0.173662\n",
      "[69]\teval-error:0.420455\ttrain-error:0.173662\n",
      "[0]\teval-error:0.409091\ttrain-error:0.196541\n",
      "[1]\teval-error:0.397727\ttrain-error:0.174743\n",
      "[2]\teval-error:0.329545\ttrain-error:0.160331\n",
      "[3]\teval-error:0.284091\ttrain-error:0.151504\n",
      "[4]\teval-error:0.295455\ttrain-error:0.14682\n",
      "[5]\teval-error:0.295455\ttrain-error:0.144659\n",
      "[6]\teval-error:0.318182\ttrain-error:0.143398\n",
      "[7]\teval-error:0.318182\ttrain-error:0.143217\n",
      "[8]\teval-error:0.295455\ttrain-error:0.143217\n",
      "[9]\teval-error:0.306818\ttrain-error:0.143217\n",
      "[10]\teval-error:0.306818\ttrain-error:0.142857\n",
      "[11]\teval-error:0.295455\ttrain-error:0.142677\n",
      "[12]\teval-error:0.284091\ttrain-error:0.142677\n",
      "[13]\teval-error:0.295455\ttrain-error:0.142677\n",
      "[14]\teval-error:0.284091\ttrain-error:0.142677\n",
      "[15]\teval-error:0.284091\ttrain-error:0.142677\n",
      "[16]\teval-error:0.295455\ttrain-error:0.142677\n",
      "[17]\teval-error:0.272727\ttrain-error:0.142677\n",
      "[18]\teval-error:0.284091\ttrain-error:0.142677\n",
      "[19]\teval-error:0.295455\ttrain-error:0.142677\n",
      "[20]\teval-error:0.295455\ttrain-error:0.142677\n",
      "[21]\teval-error:0.295455\ttrain-error:0.142677\n",
      "[22]\teval-error:0.284091\ttrain-error:0.142677\n",
      "[23]\teval-error:0.284091\ttrain-error:0.142677\n",
      "[24]\teval-error:0.284091\ttrain-error:0.142677\n",
      "[25]\teval-error:0.284091\ttrain-error:0.142677\n",
      "[26]\teval-error:0.284091\ttrain-error:0.142677\n",
      "[27]\teval-error:0.284091\ttrain-error:0.142677\n",
      "[28]\teval-error:0.284091\ttrain-error:0.142677\n",
      "[29]\teval-error:0.284091\ttrain-error:0.142677\n",
      "[30]\teval-error:0.284091\ttrain-error:0.142677\n",
      "[31]\teval-error:0.284091\ttrain-error:0.142677\n",
      "[32]\teval-error:0.284091\ttrain-error:0.142677\n",
      "[33]\teval-error:0.284091\ttrain-error:0.142677\n",
      "[34]\teval-error:0.284091\ttrain-error:0.142677\n",
      "[35]\teval-error:0.272727\ttrain-error:0.142677\n",
      "[36]\teval-error:0.272727\ttrain-error:0.142677\n",
      "[37]\teval-error:0.284091\ttrain-error:0.142677\n",
      "[38]\teval-error:0.272727\ttrain-error:0.142677\n",
      "[39]\teval-error:0.272727\ttrain-error:0.142677\n",
      "[40]\teval-error:0.272727\ttrain-error:0.142677\n",
      "[41]\teval-error:0.272727\ttrain-error:0.142677\n",
      "[42]\teval-error:0.272727\ttrain-error:0.142677\n",
      "[43]\teval-error:0.272727\ttrain-error:0.142677\n",
      "[44]\teval-error:0.272727\ttrain-error:0.142677\n",
      "[45]\teval-error:0.272727\ttrain-error:0.142677\n",
      "[46]\teval-error:0.272727\ttrain-error:0.142677\n",
      "[47]\teval-error:0.272727\ttrain-error:0.142677\n",
      "[48]\teval-error:0.272727\ttrain-error:0.142677\n",
      "[49]\teval-error:0.261364\ttrain-error:0.142677\n",
      "[50]\teval-error:0.261364\ttrain-error:0.142677\n",
      "[51]\teval-error:0.261364\ttrain-error:0.142677\n",
      "[52]\teval-error:0.261364\ttrain-error:0.142677\n",
      "[53]\teval-error:0.261364\ttrain-error:0.142677\n",
      "[54]\teval-error:0.261364\ttrain-error:0.142677\n",
      "[55]\teval-error:0.261364\ttrain-error:0.142677\n",
      "[56]\teval-error:0.261364\ttrain-error:0.142677\n",
      "[57]\teval-error:0.261364\ttrain-error:0.142677\n",
      "[58]\teval-error:0.261364\ttrain-error:0.142677\n",
      "[59]\teval-error:0.261364\ttrain-error:0.142677\n",
      "[60]\teval-error:0.261364\ttrain-error:0.142677\n",
      "[61]\teval-error:0.261364\ttrain-error:0.142677\n",
      "[62]\teval-error:0.272727\ttrain-error:0.142677\n",
      "[63]\teval-error:0.272727\ttrain-error:0.142677\n",
      "[64]\teval-error:0.272727\ttrain-error:0.142677\n",
      "[65]\teval-error:0.272727\ttrain-error:0.142677\n",
      "[66]\teval-error:0.272727\ttrain-error:0.142677\n",
      "[67]\teval-error:0.272727\ttrain-error:0.142677\n",
      "[68]\teval-error:0.272727\ttrain-error:0.142677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69]\teval-error:0.261364\ttrain-error:0.142677\n",
      "[0]\teval-error:0.404255\ttrain-error:0.164988\n",
      "[1]\teval-error:0.361702\ttrain-error:0.139556\n",
      "[2]\teval-error:0.351064\ttrain-error:0.123419\n",
      "[3]\teval-error:0.382979\ttrain-error:0.11606\n",
      "[4]\teval-error:0.308511\ttrain-error:0.112832\n",
      "[5]\teval-error:0.297872\ttrain-error:0.110509\n",
      "[6]\teval-error:0.287234\ttrain-error:0.110121\n",
      "[7]\teval-error:0.297872\ttrain-error:0.109347\n",
      "[8]\teval-error:0.308511\ttrain-error:0.109089\n",
      "[9]\teval-error:0.287234\ttrain-error:0.108959\n",
      "[10]\teval-error:0.297872\ttrain-error:0.108959\n",
      "[11]\teval-error:0.287234\ttrain-error:0.108959\n",
      "[12]\teval-error:0.287234\ttrain-error:0.108959\n",
      "[13]\teval-error:0.308511\ttrain-error:0.108959\n",
      "[14]\teval-error:0.297872\ttrain-error:0.108959\n",
      "[15]\teval-error:0.297872\ttrain-error:0.108959\n",
      "[16]\teval-error:0.297872\ttrain-error:0.108959\n",
      "[17]\teval-error:0.276596\ttrain-error:0.108959\n",
      "[18]\teval-error:0.276596\ttrain-error:0.108959\n",
      "[19]\teval-error:0.276596\ttrain-error:0.108959\n",
      "[20]\teval-error:0.276596\ttrain-error:0.108959\n",
      "[21]\teval-error:0.276596\ttrain-error:0.108959\n",
      "[22]\teval-error:0.276596\ttrain-error:0.108959\n",
      "[23]\teval-error:0.276596\ttrain-error:0.108959\n",
      "[24]\teval-error:0.276596\ttrain-error:0.108959\n",
      "[25]\teval-error:0.276596\ttrain-error:0.108959\n",
      "[26]\teval-error:0.265957\ttrain-error:0.108959\n",
      "[27]\teval-error:0.276596\ttrain-error:0.108959\n",
      "[28]\teval-error:0.276596\ttrain-error:0.108959\n",
      "[29]\teval-error:0.276596\ttrain-error:0.108959\n",
      "[30]\teval-error:0.287234\ttrain-error:0.108959\n",
      "[31]\teval-error:0.276596\ttrain-error:0.108959\n",
      "[32]\teval-error:0.287234\ttrain-error:0.108959\n",
      "[33]\teval-error:0.276596\ttrain-error:0.108959\n",
      "[34]\teval-error:0.297872\ttrain-error:0.108959\n",
      "[35]\teval-error:0.287234\ttrain-error:0.108959\n",
      "[36]\teval-error:0.297872\ttrain-error:0.108959\n",
      "[37]\teval-error:0.287234\ttrain-error:0.108959\n",
      "[38]\teval-error:0.287234\ttrain-error:0.108959\n",
      "[39]\teval-error:0.287234\ttrain-error:0.108959\n",
      "[40]\teval-error:0.276596\ttrain-error:0.108959\n",
      "[41]\teval-error:0.265957\ttrain-error:0.108959\n",
      "[42]\teval-error:0.276596\ttrain-error:0.108959\n",
      "[43]\teval-error:0.287234\ttrain-error:0.108959\n",
      "[44]\teval-error:0.287234\ttrain-error:0.108959\n",
      "[45]\teval-error:0.276596\ttrain-error:0.108959\n",
      "[46]\teval-error:0.276596\ttrain-error:0.108959\n",
      "[47]\teval-error:0.276596\ttrain-error:0.108959\n",
      "[48]\teval-error:0.265957\ttrain-error:0.108959\n",
      "[49]\teval-error:0.297872\ttrain-error:0.108959\n",
      "[50]\teval-error:0.276596\ttrain-error:0.108959\n",
      "[51]\teval-error:0.287234\ttrain-error:0.108959\n",
      "[52]\teval-error:0.287234\ttrain-error:0.108959\n",
      "[53]\teval-error:0.297872\ttrain-error:0.108959\n",
      "[54]\teval-error:0.276596\ttrain-error:0.108959\n",
      "[55]\teval-error:0.276596\ttrain-error:0.108959\n",
      "[56]\teval-error:0.276596\ttrain-error:0.108959\n",
      "[57]\teval-error:0.276596\ttrain-error:0.108959\n",
      "[58]\teval-error:0.276596\ttrain-error:0.108959\n",
      "[59]\teval-error:0.276596\ttrain-error:0.108959\n",
      "[60]\teval-error:0.276596\ttrain-error:0.108959\n",
      "[61]\teval-error:0.276596\ttrain-error:0.108959\n",
      "[62]\teval-error:0.276596\ttrain-error:0.108959\n",
      "[63]\teval-error:0.276596\ttrain-error:0.108959\n",
      "[64]\teval-error:0.265957\ttrain-error:0.108959\n",
      "[65]\teval-error:0.276596\ttrain-error:0.108959\n",
      "[66]\teval-error:0.276596\ttrain-error:0.108959\n",
      "[67]\teval-error:0.276596\ttrain-error:0.108959\n",
      "[68]\teval-error:0.287234\ttrain-error:0.108959\n",
      "[69]\teval-error:0.276596\ttrain-error:0.108959\n",
      "[0]\teval-error:0.308511\ttrain-error:0.174284\n",
      "[1]\teval-error:0.308511\ttrain-error:0.150788\n",
      "[2]\teval-error:0.319149\ttrain-error:0.139298\n",
      "[3]\teval-error:0.297872\ttrain-error:0.131681\n",
      "[4]\teval-error:0.319149\ttrain-error:0.128453\n",
      "[5]\teval-error:0.265957\ttrain-error:0.126904\n",
      "[6]\teval-error:0.329787\ttrain-error:0.125226\n",
      "[7]\teval-error:0.329787\ttrain-error:0.124968\n",
      "[8]\teval-error:0.308511\ttrain-error:0.12458\n",
      "[9]\teval-error:0.340426\ttrain-error:0.124322\n",
      "[10]\teval-error:0.319149\ttrain-error:0.124322\n",
      "[11]\teval-error:0.276596\ttrain-error:0.124322\n",
      "[12]\teval-error:0.287234\ttrain-error:0.124322\n",
      "[13]\teval-error:0.287234\ttrain-error:0.124322\n",
      "[14]\teval-error:0.287234\ttrain-error:0.124322\n",
      "[15]\teval-error:0.297872\ttrain-error:0.124322\n",
      "[16]\teval-error:0.308511\ttrain-error:0.124322\n",
      "[17]\teval-error:0.308511\ttrain-error:0.124322\n",
      "[18]\teval-error:0.297872\ttrain-error:0.124322\n",
      "[19]\teval-error:0.297872\ttrain-error:0.124322\n",
      "[20]\teval-error:0.308511\ttrain-error:0.124322\n",
      "[21]\teval-error:0.297872\ttrain-error:0.124322\n",
      "[22]\teval-error:0.287234\ttrain-error:0.124322\n",
      "[23]\teval-error:0.276596\ttrain-error:0.124322\n",
      "[24]\teval-error:0.276596\ttrain-error:0.124322\n",
      "[25]\teval-error:0.287234\ttrain-error:0.124322\n",
      "[26]\teval-error:0.287234\ttrain-error:0.124322\n",
      "[27]\teval-error:0.276596\ttrain-error:0.124322\n",
      "[28]\teval-error:0.265957\ttrain-error:0.124322\n",
      "[29]\teval-error:0.276596\ttrain-error:0.124322\n",
      "[30]\teval-error:0.287234\ttrain-error:0.124322\n",
      "[31]\teval-error:0.276596\ttrain-error:0.124322\n",
      "[32]\teval-error:0.297872\ttrain-error:0.124322\n",
      "[33]\teval-error:0.287234\ttrain-error:0.124322\n",
      "[34]\teval-error:0.287234\ttrain-error:0.124322\n",
      "[35]\teval-error:0.297872\ttrain-error:0.124322\n",
      "[36]\teval-error:0.287234\ttrain-error:0.124322\n",
      "[37]\teval-error:0.287234\ttrain-error:0.124322\n",
      "[38]\teval-error:0.276596\ttrain-error:0.124322\n",
      "[39]\teval-error:0.276596\ttrain-error:0.124322\n",
      "[40]\teval-error:0.287234\ttrain-error:0.124322\n",
      "[41]\teval-error:0.276596\ttrain-error:0.124322\n",
      "[42]\teval-error:0.276596\ttrain-error:0.124322\n",
      "[43]\teval-error:0.276596\ttrain-error:0.124322\n",
      "[44]\teval-error:0.276596\ttrain-error:0.124322\n",
      "[45]\teval-error:0.265957\ttrain-error:0.124322\n",
      "[46]\teval-error:0.276596\ttrain-error:0.124322\n",
      "[47]\teval-error:0.276596\ttrain-error:0.124322\n",
      "[48]\teval-error:0.276596\ttrain-error:0.124322\n",
      "[49]\teval-error:0.276596\ttrain-error:0.124322\n",
      "[50]\teval-error:0.287234\ttrain-error:0.124322\n",
      "[51]\teval-error:0.265957\ttrain-error:0.124322\n",
      "[52]\teval-error:0.276596\ttrain-error:0.124322\n",
      "[53]\teval-error:0.276596\ttrain-error:0.124322\n",
      "[54]\teval-error:0.287234\ttrain-error:0.124322\n",
      "[55]\teval-error:0.276596\ttrain-error:0.124322\n",
      "[56]\teval-error:0.276596\ttrain-error:0.124322\n",
      "[57]\teval-error:0.287234\ttrain-error:0.124322\n",
      "[58]\teval-error:0.287234\ttrain-error:0.124322\n",
      "[59]\teval-error:0.287234\ttrain-error:0.124322\n",
      "[60]\teval-error:0.276596\ttrain-error:0.124322\n",
      "[61]\teval-error:0.276596\ttrain-error:0.124322\n",
      "[62]\teval-error:0.287234\ttrain-error:0.124322\n",
      "[63]\teval-error:0.276596\ttrain-error:0.124322\n",
      "[64]\teval-error:0.276596\ttrain-error:0.124322\n",
      "[65]\teval-error:0.276596\ttrain-error:0.124322\n",
      "[66]\teval-error:0.276596\ttrain-error:0.124322\n",
      "[67]\teval-error:0.287234\ttrain-error:0.124322\n",
      "[68]\teval-error:0.276596\ttrain-error:0.124322\n",
      "[69]\teval-error:0.287234\ttrain-error:0.124322\n",
      "[0]\teval-error:0.37234\ttrain-error:0.181771\n",
      "[1]\teval-error:0.37234\ttrain-error:0.16331\n",
      "[2]\teval-error:0.393617\ttrain-error:0.149626\n",
      "[3]\teval-error:0.404255\ttrain-error:0.144203\n",
      "[4]\teval-error:0.414894\ttrain-error:0.141621\n",
      "[5]\teval-error:0.404255\ttrain-error:0.139685\n",
      "[6]\teval-error:0.382979\ttrain-error:0.138652\n",
      "[7]\teval-error:0.37234\ttrain-error:0.137749\n",
      "[8]\teval-error:0.37234\ttrain-error:0.137232\n",
      "[9]\teval-error:0.382979\ttrain-error:0.136974\n",
      "[10]\teval-error:0.382979\ttrain-error:0.136974\n",
      "[11]\teval-error:0.37234\ttrain-error:0.136716\n",
      "[12]\teval-error:0.393617\ttrain-error:0.136716\n",
      "[13]\teval-error:0.393617\ttrain-error:0.136716\n",
      "[14]\teval-error:0.393617\ttrain-error:0.136716\n",
      "[15]\teval-error:0.37234\ttrain-error:0.136716\n",
      "[16]\teval-error:0.37234\ttrain-error:0.136716\n",
      "[17]\teval-error:0.37234\ttrain-error:0.136716\n",
      "[18]\teval-error:0.37234\ttrain-error:0.136716\n",
      "[19]\teval-error:0.382979\ttrain-error:0.136716\n",
      "[20]\teval-error:0.37234\ttrain-error:0.136716\n",
      "[21]\teval-error:0.382979\ttrain-error:0.136716\n",
      "[22]\teval-error:0.37234\ttrain-error:0.136716\n",
      "[23]\teval-error:0.382979\ttrain-error:0.136716\n",
      "[24]\teval-error:0.37234\ttrain-error:0.136716\n",
      "[25]\teval-error:0.382979\ttrain-error:0.136716\n",
      "[26]\teval-error:0.382979\ttrain-error:0.136716\n",
      "[27]\teval-error:0.382979\ttrain-error:0.136716\n",
      "[28]\teval-error:0.382979\ttrain-error:0.136716\n",
      "[29]\teval-error:0.382979\ttrain-error:0.136716\n",
      "[30]\teval-error:0.382979\ttrain-error:0.136716\n",
      "[31]\teval-error:0.382979\ttrain-error:0.136716\n",
      "[32]\teval-error:0.382979\ttrain-error:0.136716\n",
      "[33]\teval-error:0.37234\ttrain-error:0.136716\n",
      "[34]\teval-error:0.37234\ttrain-error:0.136716\n",
      "[35]\teval-error:0.37234\ttrain-error:0.136716\n",
      "[36]\teval-error:0.382979\ttrain-error:0.136716\n",
      "[37]\teval-error:0.37234\ttrain-error:0.136716\n",
      "[38]\teval-error:0.382979\ttrain-error:0.136716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39]\teval-error:0.37234\ttrain-error:0.136716\n",
      "[40]\teval-error:0.37234\ttrain-error:0.136716\n",
      "[41]\teval-error:0.382979\ttrain-error:0.136716\n",
      "[42]\teval-error:0.382979\ttrain-error:0.136716\n",
      "[43]\teval-error:0.382979\ttrain-error:0.136716\n",
      "[44]\teval-error:0.37234\ttrain-error:0.136716\n",
      "[45]\teval-error:0.37234\ttrain-error:0.136716\n",
      "[46]\teval-error:0.382979\ttrain-error:0.136716\n",
      "[47]\teval-error:0.382979\ttrain-error:0.136716\n",
      "[48]\teval-error:0.382979\ttrain-error:0.136716\n",
      "[49]\teval-error:0.382979\ttrain-error:0.136716\n",
      "[50]\teval-error:0.382979\ttrain-error:0.136716\n",
      "[51]\teval-error:0.37234\ttrain-error:0.136716\n",
      "[52]\teval-error:0.37234\ttrain-error:0.136716\n",
      "[53]\teval-error:0.382979\ttrain-error:0.136716\n",
      "[54]\teval-error:0.382979\ttrain-error:0.136716\n",
      "[55]\teval-error:0.382979\ttrain-error:0.136716\n",
      "[56]\teval-error:0.382979\ttrain-error:0.136716\n",
      "[57]\teval-error:0.382979\ttrain-error:0.136716\n",
      "[58]\teval-error:0.37234\ttrain-error:0.136716\n",
      "[59]\teval-error:0.382979\ttrain-error:0.136716\n",
      "[60]\teval-error:0.382979\ttrain-error:0.136716\n",
      "[61]\teval-error:0.382979\ttrain-error:0.136716\n",
      "[62]\teval-error:0.37234\ttrain-error:0.136716\n",
      "[63]\teval-error:0.382979\ttrain-error:0.136716\n",
      "[64]\teval-error:0.382979\ttrain-error:0.136716\n",
      "[65]\teval-error:0.382979\ttrain-error:0.136716\n",
      "[66]\teval-error:0.382979\ttrain-error:0.136716\n",
      "[67]\teval-error:0.382979\ttrain-error:0.136716\n",
      "[68]\teval-error:0.382979\ttrain-error:0.136716\n",
      "[69]\teval-error:0.382979\ttrain-error:0.136716\n",
      "[0]\teval-error:0.382979\ttrain-error:0.165505\n",
      "[1]\teval-error:0.308511\ttrain-error:0.141234\n",
      "[2]\teval-error:0.329787\ttrain-error:0.127292\n",
      "[3]\teval-error:0.297872\ttrain-error:0.117867\n",
      "[4]\teval-error:0.329787\ttrain-error:0.11322\n",
      "[5]\teval-error:0.340426\ttrain-error:0.110638\n",
      "[6]\teval-error:0.308511\ttrain-error:0.109863\n",
      "[7]\teval-error:0.319149\ttrain-error:0.109476\n",
      "[8]\teval-error:0.319149\ttrain-error:0.109476\n",
      "[9]\teval-error:0.319149\ttrain-error:0.108959\n",
      "[10]\teval-error:0.319149\ttrain-error:0.108959\n",
      "[11]\teval-error:0.308511\ttrain-error:0.108959\n",
      "[12]\teval-error:0.329787\ttrain-error:0.108959\n",
      "[13]\teval-error:0.329787\ttrain-error:0.108959\n",
      "[14]\teval-error:0.308511\ttrain-error:0.108959\n",
      "[15]\teval-error:0.319149\ttrain-error:0.108959\n",
      "[16]\teval-error:0.319149\ttrain-error:0.108959\n",
      "[17]\teval-error:0.319149\ttrain-error:0.108959\n",
      "[18]\teval-error:0.329787\ttrain-error:0.108959\n",
      "[19]\teval-error:0.329787\ttrain-error:0.108959\n",
      "[20]\teval-error:0.329787\ttrain-error:0.108959\n",
      "[21]\teval-error:0.329787\ttrain-error:0.108959\n",
      "[22]\teval-error:0.329787\ttrain-error:0.108959\n",
      "[23]\teval-error:0.329787\ttrain-error:0.108959\n",
      "[24]\teval-error:0.329787\ttrain-error:0.108959\n",
      "[25]\teval-error:0.329787\ttrain-error:0.108959\n",
      "[26]\teval-error:0.340426\ttrain-error:0.108959\n",
      "[27]\teval-error:0.329787\ttrain-error:0.108959\n",
      "[28]\teval-error:0.329787\ttrain-error:0.108959\n",
      "[29]\teval-error:0.329787\ttrain-error:0.108959\n",
      "[30]\teval-error:0.329787\ttrain-error:0.108959\n",
      "[31]\teval-error:0.329787\ttrain-error:0.108959\n",
      "[32]\teval-error:0.340426\ttrain-error:0.108959\n",
      "[33]\teval-error:0.340426\ttrain-error:0.108959\n",
      "[34]\teval-error:0.340426\ttrain-error:0.108959\n",
      "[35]\teval-error:0.340426\ttrain-error:0.108959\n",
      "[36]\teval-error:0.340426\ttrain-error:0.108959\n",
      "[37]\teval-error:0.340426\ttrain-error:0.108959\n",
      "[38]\teval-error:0.340426\ttrain-error:0.108959\n",
      "[39]\teval-error:0.340426\ttrain-error:0.108959\n",
      "[40]\teval-error:0.340426\ttrain-error:0.108959\n",
      "[41]\teval-error:0.340426\ttrain-error:0.108959\n",
      "[42]\teval-error:0.340426\ttrain-error:0.108959\n",
      "[43]\teval-error:0.340426\ttrain-error:0.108959\n",
      "[44]\teval-error:0.340426\ttrain-error:0.108959\n",
      "[45]\teval-error:0.340426\ttrain-error:0.108959\n",
      "[46]\teval-error:0.340426\ttrain-error:0.108959\n",
      "[47]\teval-error:0.340426\ttrain-error:0.108959\n",
      "[48]\teval-error:0.340426\ttrain-error:0.108959\n",
      "[49]\teval-error:0.340426\ttrain-error:0.108959\n",
      "[50]\teval-error:0.340426\ttrain-error:0.108959\n",
      "[51]\teval-error:0.340426\ttrain-error:0.108959\n",
      "[52]\teval-error:0.340426\ttrain-error:0.108959\n",
      "[53]\teval-error:0.340426\ttrain-error:0.108959\n",
      "[54]\teval-error:0.340426\ttrain-error:0.108959\n",
      "[55]\teval-error:0.340426\ttrain-error:0.108959\n",
      "[56]\teval-error:0.340426\ttrain-error:0.108959\n",
      "[57]\teval-error:0.340426\ttrain-error:0.108959\n",
      "[58]\teval-error:0.340426\ttrain-error:0.108959\n",
      "[59]\teval-error:0.340426\ttrain-error:0.108959\n",
      "[60]\teval-error:0.340426\ttrain-error:0.108959\n",
      "[61]\teval-error:0.340426\ttrain-error:0.108959\n",
      "[62]\teval-error:0.340426\ttrain-error:0.108959\n",
      "[63]\teval-error:0.340426\ttrain-error:0.108959\n",
      "[64]\teval-error:0.340426\ttrain-error:0.108959\n",
      "[65]\teval-error:0.340426\ttrain-error:0.108959\n",
      "[66]\teval-error:0.340426\ttrain-error:0.108959\n",
      "[67]\teval-error:0.340426\ttrain-error:0.108959\n",
      "[68]\teval-error:0.340426\ttrain-error:0.108959\n",
      "[69]\teval-error:0.340426\ttrain-error:0.108959\n",
      "[0]\teval-error:0.351145\ttrain-error:0.131053\n",
      "[1]\teval-error:0.328244\ttrain-error:0.11277\n",
      "[2]\teval-error:0.339695\ttrain-error:0.100843\n",
      "[3]\teval-error:0.332061\ttrain-error:0.094915\n",
      "[4]\teval-error:0.339695\ttrain-error:0.091201\n",
      "[5]\teval-error:0.332061\ttrain-error:0.089487\n",
      "[6]\teval-error:0.324427\ttrain-error:0.08813\n",
      "[7]\teval-error:0.339695\ttrain-error:0.087345\n",
      "[8]\teval-error:0.339695\ttrain-error:0.086845\n",
      "[9]\teval-error:0.347328\ttrain-error:0.086702\n",
      "[10]\teval-error:0.343511\ttrain-error:0.086559\n",
      "[11]\teval-error:0.324427\ttrain-error:0.086559\n",
      "[12]\teval-error:0.332061\ttrain-error:0.086559\n",
      "[13]\teval-error:0.320611\ttrain-error:0.086559\n",
      "[14]\teval-error:0.324427\ttrain-error:0.086559\n",
      "[15]\teval-error:0.328244\ttrain-error:0.086559\n",
      "[16]\teval-error:0.328244\ttrain-error:0.086559\n",
      "[17]\teval-error:0.332061\ttrain-error:0.086559\n",
      "[18]\teval-error:0.328244\ttrain-error:0.086559\n",
      "[19]\teval-error:0.316794\ttrain-error:0.086559\n",
      "[20]\teval-error:0.312977\ttrain-error:0.086559\n",
      "[21]\teval-error:0.30916\ttrain-error:0.086559\n",
      "[22]\teval-error:0.30916\ttrain-error:0.086559\n",
      "[23]\teval-error:0.30916\ttrain-error:0.086559\n",
      "[24]\teval-error:0.312977\ttrain-error:0.086559\n",
      "[25]\teval-error:0.312977\ttrain-error:0.086559\n",
      "[26]\teval-error:0.312977\ttrain-error:0.086559\n",
      "[27]\teval-error:0.305344\ttrain-error:0.086559\n",
      "[28]\teval-error:0.305344\ttrain-error:0.086559\n",
      "[29]\teval-error:0.316794\ttrain-error:0.086559\n",
      "[30]\teval-error:0.320611\ttrain-error:0.086559\n",
      "[31]\teval-error:0.316794\ttrain-error:0.086559\n",
      "[32]\teval-error:0.312977\ttrain-error:0.086559\n",
      "[33]\teval-error:0.320611\ttrain-error:0.086559\n",
      "[34]\teval-error:0.320611\ttrain-error:0.086559\n",
      "[35]\teval-error:0.316794\ttrain-error:0.086559\n",
      "[36]\teval-error:0.312977\ttrain-error:0.086559\n",
      "[37]\teval-error:0.305344\ttrain-error:0.086559\n",
      "[38]\teval-error:0.305344\ttrain-error:0.086559\n",
      "[39]\teval-error:0.324427\ttrain-error:0.086559\n",
      "[40]\teval-error:0.320611\ttrain-error:0.086559\n",
      "[41]\teval-error:0.312977\ttrain-error:0.086559\n",
      "[42]\teval-error:0.312977\ttrain-error:0.086559\n",
      "[43]\teval-error:0.312977\ttrain-error:0.086559\n",
      "[44]\teval-error:0.30916\ttrain-error:0.086559\n",
      "[45]\teval-error:0.305344\ttrain-error:0.086559\n",
      "[46]\teval-error:0.316794\ttrain-error:0.086559\n",
      "[47]\teval-error:0.30916\ttrain-error:0.086559\n",
      "[48]\teval-error:0.312977\ttrain-error:0.086559\n",
      "[49]\teval-error:0.30916\ttrain-error:0.086559\n",
      "[50]\teval-error:0.305344\ttrain-error:0.086559\n",
      "[51]\teval-error:0.305344\ttrain-error:0.086559\n",
      "[52]\teval-error:0.30916\ttrain-error:0.086559\n",
      "[53]\teval-error:0.30916\ttrain-error:0.086559\n",
      "[54]\teval-error:0.312977\ttrain-error:0.086559\n",
      "[55]\teval-error:0.30916\ttrain-error:0.086559\n",
      "[56]\teval-error:0.30916\ttrain-error:0.086559\n",
      "[57]\teval-error:0.30916\ttrain-error:0.086559\n",
      "[58]\teval-error:0.312977\ttrain-error:0.086559\n",
      "[59]\teval-error:0.30916\ttrain-error:0.086559\n",
      "[60]\teval-error:0.316794\ttrain-error:0.086559\n",
      "[61]\teval-error:0.305344\ttrain-error:0.086559\n",
      "[62]\teval-error:0.305344\ttrain-error:0.086559\n",
      "[63]\teval-error:0.312977\ttrain-error:0.086559\n",
      "[64]\teval-error:0.320611\ttrain-error:0.086559\n",
      "[65]\teval-error:0.316794\ttrain-error:0.086559\n",
      "[66]\teval-error:0.30916\ttrain-error:0.086559\n",
      "[67]\teval-error:0.30916\ttrain-error:0.086559\n",
      "[68]\teval-error:0.316794\ttrain-error:0.086559\n",
      "[69]\teval-error:0.312977\ttrain-error:0.086559\n",
      "[0]\teval-error:0.366412\ttrain-error:0.135409\n",
      "[1]\teval-error:0.339695\ttrain-error:0.123482\n",
      "[2]\teval-error:0.339695\ttrain-error:0.115126\n",
      "[3]\teval-error:0.347328\ttrain-error:0.111484\n",
      "[4]\teval-error:0.347328\ttrain-error:0.10827\n",
      "[5]\teval-error:0.339695\ttrain-error:0.106485\n",
      "[6]\teval-error:0.335878\ttrain-error:0.104628\n",
      "[7]\teval-error:0.343511\ttrain-error:0.104271\n",
      "[8]\teval-error:0.339695\ttrain-error:0.103914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9]\teval-error:0.351145\ttrain-error:0.103699\n",
      "[10]\teval-error:0.343511\ttrain-error:0.103628\n",
      "[11]\teval-error:0.351145\ttrain-error:0.103628\n",
      "[12]\teval-error:0.362595\ttrain-error:0.103557\n",
      "[13]\teval-error:0.354962\ttrain-error:0.103485\n",
      "[14]\teval-error:0.362595\ttrain-error:0.103485\n",
      "[15]\teval-error:0.358779\ttrain-error:0.103485\n",
      "[16]\teval-error:0.354962\ttrain-error:0.103485\n",
      "[17]\teval-error:0.351145\ttrain-error:0.103485\n",
      "[18]\teval-error:0.358779\ttrain-error:0.103485\n",
      "[19]\teval-error:0.358779\ttrain-error:0.103485\n",
      "[20]\teval-error:0.358779\ttrain-error:0.103485\n",
      "[21]\teval-error:0.351145\ttrain-error:0.103485\n",
      "[22]\teval-error:0.366412\ttrain-error:0.103485\n",
      "[23]\teval-error:0.366412\ttrain-error:0.103485\n",
      "[24]\teval-error:0.362595\ttrain-error:0.103485\n",
      "[25]\teval-error:0.366412\ttrain-error:0.103485\n",
      "[26]\teval-error:0.370229\ttrain-error:0.103485\n",
      "[27]\teval-error:0.354962\ttrain-error:0.103485\n",
      "[28]\teval-error:0.358779\ttrain-error:0.103485\n",
      "[29]\teval-error:0.374046\ttrain-error:0.103485\n",
      "[30]\teval-error:0.366412\ttrain-error:0.103485\n",
      "[31]\teval-error:0.374046\ttrain-error:0.103485\n",
      "[32]\teval-error:0.370229\ttrain-error:0.103485\n",
      "[33]\teval-error:0.366412\ttrain-error:0.103485\n",
      "[34]\teval-error:0.358779\ttrain-error:0.103485\n",
      "[35]\teval-error:0.362595\ttrain-error:0.103485\n",
      "[36]\teval-error:0.366412\ttrain-error:0.103485\n",
      "[37]\teval-error:0.370229\ttrain-error:0.103485\n",
      "[38]\teval-error:0.374046\ttrain-error:0.103485\n",
      "[39]\teval-error:0.358779\ttrain-error:0.103485\n",
      "[40]\teval-error:0.362595\ttrain-error:0.103485\n",
      "[41]\teval-error:0.370229\ttrain-error:0.103485\n",
      "[42]\teval-error:0.366412\ttrain-error:0.103485\n",
      "[43]\teval-error:0.370229\ttrain-error:0.103485\n",
      "[44]\teval-error:0.370229\ttrain-error:0.103485\n",
      "[45]\teval-error:0.370229\ttrain-error:0.103485\n",
      "[46]\teval-error:0.381679\ttrain-error:0.103485\n",
      "[47]\teval-error:0.377863\ttrain-error:0.103485\n",
      "[48]\teval-error:0.374046\ttrain-error:0.103485\n",
      "[49]\teval-error:0.370229\ttrain-error:0.103485\n",
      "[50]\teval-error:0.377863\ttrain-error:0.103485\n",
      "[51]\teval-error:0.377863\ttrain-error:0.103485\n",
      "[52]\teval-error:0.370229\ttrain-error:0.103485\n",
      "[53]\teval-error:0.374046\ttrain-error:0.103485\n",
      "[54]\teval-error:0.385496\ttrain-error:0.103485\n",
      "[55]\teval-error:0.374046\ttrain-error:0.103485\n",
      "[56]\teval-error:0.381679\ttrain-error:0.103485\n",
      "[57]\teval-error:0.389313\ttrain-error:0.103485\n",
      "[58]\teval-error:0.381679\ttrain-error:0.103485\n",
      "[59]\teval-error:0.381679\ttrain-error:0.103485\n",
      "[60]\teval-error:0.385496\ttrain-error:0.103485\n",
      "[61]\teval-error:0.385496\ttrain-error:0.103485\n",
      "[62]\teval-error:0.374046\ttrain-error:0.103485\n",
      "[63]\teval-error:0.377863\ttrain-error:0.103485\n",
      "[64]\teval-error:0.377863\ttrain-error:0.103485\n",
      "[65]\teval-error:0.381679\ttrain-error:0.103485\n",
      "[66]\teval-error:0.385496\ttrain-error:0.103485\n",
      "[67]\teval-error:0.381679\ttrain-error:0.103485\n",
      "[68]\teval-error:0.377863\ttrain-error:0.103485\n",
      "[69]\teval-error:0.389313\ttrain-error:0.103485\n",
      "[0]\teval-error:0.461832\ttrain-error:0.151264\n",
      "[1]\teval-error:0.446565\ttrain-error:0.138766\n",
      "[2]\teval-error:0.442748\ttrain-error:0.130267\n",
      "[3]\teval-error:0.438931\ttrain-error:0.125768\n",
      "[4]\teval-error:0.438931\ttrain-error:0.12284\n",
      "[5]\teval-error:0.431298\ttrain-error:0.121697\n",
      "[6]\teval-error:0.427481\ttrain-error:0.120554\n",
      "[7]\teval-error:0.427481\ttrain-error:0.119554\n",
      "[8]\teval-error:0.423664\ttrain-error:0.118983\n",
      "[9]\teval-error:0.427481\ttrain-error:0.11884\n",
      "[10]\teval-error:0.431298\ttrain-error:0.118554\n",
      "[11]\teval-error:0.431298\ttrain-error:0.118412\n",
      "[12]\teval-error:0.423664\ttrain-error:0.11834\n",
      "[13]\teval-error:0.423664\ttrain-error:0.118126\n",
      "[14]\teval-error:0.431298\ttrain-error:0.118126\n",
      "[15]\teval-error:0.435115\ttrain-error:0.118055\n",
      "[16]\teval-error:0.435115\ttrain-error:0.118055\n",
      "[17]\teval-error:0.431298\ttrain-error:0.118055\n",
      "[18]\teval-error:0.431298\ttrain-error:0.118055\n",
      "[19]\teval-error:0.435115\ttrain-error:0.118055\n",
      "[20]\teval-error:0.431298\ttrain-error:0.118055\n",
      "[21]\teval-error:0.431298\ttrain-error:0.118055\n",
      "[22]\teval-error:0.431298\ttrain-error:0.118055\n",
      "[23]\teval-error:0.427481\ttrain-error:0.118055\n",
      "[24]\teval-error:0.435115\ttrain-error:0.118055\n",
      "[25]\teval-error:0.435115\ttrain-error:0.118055\n",
      "[26]\teval-error:0.438931\ttrain-error:0.118055\n",
      "[27]\teval-error:0.431298\ttrain-error:0.118055\n",
      "[28]\teval-error:0.431298\ttrain-error:0.118055\n",
      "[29]\teval-error:0.435115\ttrain-error:0.117983\n",
      "[30]\teval-error:0.435115\ttrain-error:0.117983\n",
      "[31]\teval-error:0.435115\ttrain-error:0.117983\n",
      "[32]\teval-error:0.435115\ttrain-error:0.117983\n",
      "[33]\teval-error:0.435115\ttrain-error:0.117983\n",
      "[34]\teval-error:0.431298\ttrain-error:0.117983\n",
      "[35]\teval-error:0.442748\ttrain-error:0.117983\n",
      "[36]\teval-error:0.442748\ttrain-error:0.117983\n",
      "[37]\teval-error:0.435115\ttrain-error:0.117983\n",
      "[38]\teval-error:0.427481\ttrain-error:0.117983\n",
      "[39]\teval-error:0.435115\ttrain-error:0.117983\n",
      "[40]\teval-error:0.438931\ttrain-error:0.117983\n",
      "[41]\teval-error:0.435115\ttrain-error:0.117983\n",
      "[42]\teval-error:0.446565\ttrain-error:0.117983\n",
      "[43]\teval-error:0.438931\ttrain-error:0.117983\n",
      "[44]\teval-error:0.435115\ttrain-error:0.117983\n",
      "[45]\teval-error:0.438931\ttrain-error:0.117983\n",
      "[46]\teval-error:0.438931\ttrain-error:0.117983\n",
      "[47]\teval-error:0.431298\ttrain-error:0.117983\n",
      "[48]\teval-error:0.427481\ttrain-error:0.117983\n",
      "[49]\teval-error:0.438931\ttrain-error:0.117983\n",
      "[50]\teval-error:0.442748\ttrain-error:0.117983\n",
      "[51]\teval-error:0.446565\ttrain-error:0.117983\n",
      "[52]\teval-error:0.442748\ttrain-error:0.117983\n",
      "[53]\teval-error:0.438931\ttrain-error:0.117983\n",
      "[54]\teval-error:0.438931\ttrain-error:0.117983\n",
      "[55]\teval-error:0.438931\ttrain-error:0.117983\n",
      "[56]\teval-error:0.435115\ttrain-error:0.117983\n",
      "[57]\teval-error:0.438931\ttrain-error:0.117983\n",
      "[58]\teval-error:0.438931\ttrain-error:0.117983\n",
      "[59]\teval-error:0.438931\ttrain-error:0.117983\n",
      "[60]\teval-error:0.435115\ttrain-error:0.117983\n",
      "[61]\teval-error:0.438931\ttrain-error:0.117983\n",
      "[62]\teval-error:0.435115\ttrain-error:0.117983\n",
      "[63]\teval-error:0.442748\ttrain-error:0.117983\n",
      "[64]\teval-error:0.435115\ttrain-error:0.117983\n",
      "[65]\teval-error:0.438931\ttrain-error:0.117983\n",
      "[66]\teval-error:0.435115\ttrain-error:0.117983\n",
      "[67]\teval-error:0.435115\ttrain-error:0.117983\n",
      "[68]\teval-error:0.438931\ttrain-error:0.117983\n",
      "[69]\teval-error:0.438931\ttrain-error:0.117983\n",
      "[0]\teval-error:0.358779\ttrain-error:0.13041\n",
      "[1]\teval-error:0.29771\ttrain-error:0.112984\n",
      "[2]\teval-error:0.29771\ttrain-error:0.099343\n",
      "[3]\teval-error:0.29771\ttrain-error:0.094201\n",
      "[4]\teval-error:0.29771\ttrain-error:0.090201\n",
      "[5]\teval-error:0.305344\ttrain-error:0.087987\n",
      "[6]\teval-error:0.305344\ttrain-error:0.08713\n",
      "[7]\teval-error:0.316794\ttrain-error:0.086988\n",
      "[8]\teval-error:0.301527\ttrain-error:0.086702\n",
      "[9]\teval-error:0.301527\ttrain-error:0.086559\n",
      "[10]\teval-error:0.305344\ttrain-error:0.086559\n",
      "[11]\teval-error:0.305344\ttrain-error:0.086488\n",
      "[12]\teval-error:0.305344\ttrain-error:0.086488\n",
      "[13]\teval-error:0.305344\ttrain-error:0.086488\n",
      "[14]\teval-error:0.29771\ttrain-error:0.086488\n",
      "[15]\teval-error:0.29771\ttrain-error:0.086488\n",
      "[16]\teval-error:0.293893\ttrain-error:0.086488\n",
      "[17]\teval-error:0.290076\ttrain-error:0.086488\n",
      "[18]\teval-error:0.29771\ttrain-error:0.086488\n",
      "[19]\teval-error:0.293893\ttrain-error:0.086488\n",
      "[20]\teval-error:0.305344\ttrain-error:0.086488\n",
      "[21]\teval-error:0.30916\ttrain-error:0.086416\n",
      "[22]\teval-error:0.301527\ttrain-error:0.086416\n",
      "[23]\teval-error:0.305344\ttrain-error:0.086416\n",
      "[24]\teval-error:0.30916\ttrain-error:0.086416\n",
      "[25]\teval-error:0.290076\ttrain-error:0.086416\n",
      "[26]\teval-error:0.301527\ttrain-error:0.086416\n",
      "[27]\teval-error:0.293893\ttrain-error:0.086416\n",
      "[28]\teval-error:0.282443\ttrain-error:0.086416\n",
      "[29]\teval-error:0.278626\ttrain-error:0.086416\n",
      "[30]\teval-error:0.282443\ttrain-error:0.086416\n",
      "[31]\teval-error:0.282443\ttrain-error:0.086416\n",
      "[32]\teval-error:0.282443\ttrain-error:0.086416\n",
      "[33]\teval-error:0.278626\ttrain-error:0.086416\n",
      "[34]\teval-error:0.290076\ttrain-error:0.086416\n",
      "[35]\teval-error:0.290076\ttrain-error:0.086416\n",
      "[36]\teval-error:0.28626\ttrain-error:0.086416\n",
      "[37]\teval-error:0.278626\ttrain-error:0.086416\n",
      "[38]\teval-error:0.282443\ttrain-error:0.086416\n",
      "[39]\teval-error:0.278626\ttrain-error:0.086416\n",
      "[40]\teval-error:0.28626\ttrain-error:0.086416\n",
      "[41]\teval-error:0.282443\ttrain-error:0.086416\n",
      "[42]\teval-error:0.278626\ttrain-error:0.086416\n",
      "[43]\teval-error:0.282443\ttrain-error:0.086416\n",
      "[44]\teval-error:0.290076\ttrain-error:0.086416\n",
      "[45]\teval-error:0.290076\ttrain-error:0.086416\n",
      "[46]\teval-error:0.290076\ttrain-error:0.086416\n",
      "[47]\teval-error:0.29771\ttrain-error:0.086416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48]\teval-error:0.290076\ttrain-error:0.086416\n",
      "[49]\teval-error:0.28626\ttrain-error:0.086416\n",
      "[50]\teval-error:0.290076\ttrain-error:0.086416\n",
      "[51]\teval-error:0.28626\ttrain-error:0.086416\n",
      "[52]\teval-error:0.28626\ttrain-error:0.086416\n",
      "[53]\teval-error:0.282443\ttrain-error:0.086416\n",
      "[54]\teval-error:0.293893\ttrain-error:0.086416\n",
      "[55]\teval-error:0.290076\ttrain-error:0.086416\n",
      "[56]\teval-error:0.290076\ttrain-error:0.086416\n",
      "[57]\teval-error:0.28626\ttrain-error:0.086416\n",
      "[58]\teval-error:0.29771\ttrain-error:0.086416\n",
      "[59]\teval-error:0.290076\ttrain-error:0.086416\n",
      "[60]\teval-error:0.293893\ttrain-error:0.086416\n",
      "[61]\teval-error:0.293893\ttrain-error:0.086416\n",
      "[62]\teval-error:0.293893\ttrain-error:0.086416\n",
      "[63]\teval-error:0.290076\ttrain-error:0.086416\n",
      "[64]\teval-error:0.28626\ttrain-error:0.086416\n",
      "[65]\teval-error:0.290076\ttrain-error:0.086416\n",
      "[66]\teval-error:0.28626\ttrain-error:0.086416\n",
      "[67]\teval-error:0.29771\ttrain-error:0.086416\n",
      "[68]\teval-error:0.29771\ttrain-error:0.086416\n",
      "[69]\teval-error:0.29771\ttrain-error:0.086416\n",
      "[0]\teval-error:0.261364\ttrain-error:0.105386\n",
      "[1]\teval-error:0.204545\ttrain-error:0.064673\n",
      "[2]\teval-error:0.159091\ttrain-error:0.049901\n",
      "[3]\teval-error:0.181818\ttrain-error:0.039092\n",
      "[4]\teval-error:0.170455\ttrain-error:0.033507\n",
      "[5]\teval-error:0.170455\ttrain-error:0.032607\n",
      "[6]\teval-error:0.181818\ttrain-error:0.031166\n",
      "[7]\teval-error:0.193182\ttrain-error:0.029905\n",
      "[8]\teval-error:0.181818\ttrain-error:0.029184\n",
      "[9]\teval-error:0.181818\ttrain-error:0.028824\n",
      "[10]\teval-error:0.170455\ttrain-error:0.028643\n",
      "[11]\teval-error:0.181818\ttrain-error:0.028283\n",
      "[12]\teval-error:0.181818\ttrain-error:0.028283\n",
      "[13]\teval-error:0.181818\ttrain-error:0.028283\n",
      "[14]\teval-error:0.159091\ttrain-error:0.028283\n",
      "[15]\teval-error:0.181818\ttrain-error:0.028283\n",
      "[16]\teval-error:0.181818\ttrain-error:0.028283\n",
      "[17]\teval-error:0.181818\ttrain-error:0.028283\n",
      "[18]\teval-error:0.170455\ttrain-error:0.028283\n",
      "[19]\teval-error:0.170455\ttrain-error:0.028283\n",
      "[20]\teval-error:0.181818\ttrain-error:0.028283\n",
      "[21]\teval-error:0.170455\ttrain-error:0.028283\n",
      "[22]\teval-error:0.159091\ttrain-error:0.028283\n",
      "[23]\teval-error:0.159091\ttrain-error:0.028283\n",
      "[24]\teval-error:0.170455\ttrain-error:0.028283\n",
      "[25]\teval-error:0.170455\ttrain-error:0.028283\n",
      "[26]\teval-error:0.181818\ttrain-error:0.028283\n",
      "[27]\teval-error:0.181818\ttrain-error:0.028283\n",
      "[28]\teval-error:0.181818\ttrain-error:0.028283\n",
      "[29]\teval-error:0.181818\ttrain-error:0.028283\n",
      "[30]\teval-error:0.181818\ttrain-error:0.028283\n",
      "[31]\teval-error:0.181818\ttrain-error:0.028283\n",
      "[32]\teval-error:0.204545\ttrain-error:0.028283\n",
      "[33]\teval-error:0.193182\ttrain-error:0.028283\n",
      "[34]\teval-error:0.181818\ttrain-error:0.028283\n",
      "[35]\teval-error:0.204545\ttrain-error:0.028283\n",
      "[36]\teval-error:0.204545\ttrain-error:0.028283\n",
      "[37]\teval-error:0.204545\ttrain-error:0.028283\n",
      "[38]\teval-error:0.204545\ttrain-error:0.028283\n",
      "[39]\teval-error:0.204545\ttrain-error:0.028283\n",
      "[40]\teval-error:0.204545\ttrain-error:0.028283\n",
      "[41]\teval-error:0.193182\ttrain-error:0.028283\n",
      "[42]\teval-error:0.193182\ttrain-error:0.028283\n",
      "[43]\teval-error:0.193182\ttrain-error:0.028283\n",
      "[44]\teval-error:0.193182\ttrain-error:0.028283\n",
      "[45]\teval-error:0.193182\ttrain-error:0.028283\n",
      "[46]\teval-error:0.193182\ttrain-error:0.028283\n",
      "[47]\teval-error:0.193182\ttrain-error:0.028283\n",
      "[48]\teval-error:0.193182\ttrain-error:0.028283\n",
      "[49]\teval-error:0.193182\ttrain-error:0.028283\n",
      "[50]\teval-error:0.193182\ttrain-error:0.028283\n",
      "[51]\teval-error:0.204545\ttrain-error:0.028283\n",
      "[52]\teval-error:0.204545\ttrain-error:0.028283\n",
      "[53]\teval-error:0.204545\ttrain-error:0.028283\n",
      "[54]\teval-error:0.215909\ttrain-error:0.028283\n",
      "[55]\teval-error:0.215909\ttrain-error:0.028283\n",
      "[56]\teval-error:0.193182\ttrain-error:0.028283\n",
      "[57]\teval-error:0.193182\ttrain-error:0.028283\n",
      "[58]\teval-error:0.193182\ttrain-error:0.028283\n",
      "[59]\teval-error:0.215909\ttrain-error:0.028283\n",
      "[60]\teval-error:0.215909\ttrain-error:0.028283\n",
      "[61]\teval-error:0.227273\ttrain-error:0.028283\n",
      "[62]\teval-error:0.215909\ttrain-error:0.028283\n",
      "[63]\teval-error:0.215909\ttrain-error:0.028283\n",
      "[64]\teval-error:0.215909\ttrain-error:0.028283\n",
      "[65]\teval-error:0.215909\ttrain-error:0.028283\n",
      "[66]\teval-error:0.215909\ttrain-error:0.028283\n",
      "[67]\teval-error:0.215909\ttrain-error:0.028283\n",
      "[68]\teval-error:0.215909\ttrain-error:0.028283\n",
      "[69]\teval-error:0.215909\ttrain-error:0.028283\n",
      "[0]\teval-error:0.352273\ttrain-error:0.101243\n",
      "[1]\teval-error:0.306818\ttrain-error:0.068997\n",
      "[2]\teval-error:0.261364\ttrain-error:0.047379\n",
      "[3]\teval-error:0.306818\ttrain-error:0.038732\n",
      "[4]\teval-error:0.295455\ttrain-error:0.033868\n",
      "[5]\teval-error:0.272727\ttrain-error:0.031166\n",
      "[6]\teval-error:0.261364\ttrain-error:0.029544\n",
      "[7]\teval-error:0.295455\ttrain-error:0.029184\n",
      "[8]\teval-error:0.295455\ttrain-error:0.029184\n",
      "[9]\teval-error:0.284091\ttrain-error:0.029004\n",
      "[10]\teval-error:0.272727\ttrain-error:0.028824\n",
      "[11]\teval-error:0.261364\ttrain-error:0.028824\n",
      "[12]\teval-error:0.261364\ttrain-error:0.028824\n",
      "[13]\teval-error:0.284091\ttrain-error:0.028643\n",
      "[14]\teval-error:0.284091\ttrain-error:0.028643\n",
      "[15]\teval-error:0.25\ttrain-error:0.028643\n",
      "[16]\teval-error:0.272727\ttrain-error:0.028643\n",
      "[17]\teval-error:0.261364\ttrain-error:0.028643\n",
      "[18]\teval-error:0.272727\ttrain-error:0.028643\n",
      "[19]\teval-error:0.272727\ttrain-error:0.028643\n",
      "[20]\teval-error:0.295455\ttrain-error:0.028643\n",
      "[21]\teval-error:0.284091\ttrain-error:0.028643\n",
      "[22]\teval-error:0.284091\ttrain-error:0.028643\n",
      "[23]\teval-error:0.284091\ttrain-error:0.028643\n",
      "[24]\teval-error:0.284091\ttrain-error:0.028643\n",
      "[25]\teval-error:0.284091\ttrain-error:0.028643\n",
      "[26]\teval-error:0.272727\ttrain-error:0.028643\n",
      "[27]\teval-error:0.284091\ttrain-error:0.028643\n",
      "[28]\teval-error:0.284091\ttrain-error:0.028643\n",
      "[29]\teval-error:0.284091\ttrain-error:0.028643\n",
      "[30]\teval-error:0.284091\ttrain-error:0.028643\n",
      "[31]\teval-error:0.284091\ttrain-error:0.028643\n",
      "[32]\teval-error:0.284091\ttrain-error:0.028643\n",
      "[33]\teval-error:0.284091\ttrain-error:0.028643\n",
      "[34]\teval-error:0.284091\ttrain-error:0.028643\n",
      "[35]\teval-error:0.284091\ttrain-error:0.028643\n",
      "[36]\teval-error:0.272727\ttrain-error:0.028643\n",
      "[37]\teval-error:0.272727\ttrain-error:0.028643\n",
      "[38]\teval-error:0.272727\ttrain-error:0.028643\n",
      "[39]\teval-error:0.272727\ttrain-error:0.028643\n",
      "[40]\teval-error:0.261364\ttrain-error:0.028643\n",
      "[41]\teval-error:0.261364\ttrain-error:0.028643\n",
      "[42]\teval-error:0.261364\ttrain-error:0.028643\n",
      "[43]\teval-error:0.261364\ttrain-error:0.028643\n",
      "[44]\teval-error:0.261364\ttrain-error:0.028643\n",
      "[45]\teval-error:0.261364\ttrain-error:0.028643\n",
      "[46]\teval-error:0.261364\ttrain-error:0.028643\n",
      "[47]\teval-error:0.261364\ttrain-error:0.028643\n",
      "[48]\teval-error:0.261364\ttrain-error:0.028643\n",
      "[49]\teval-error:0.261364\ttrain-error:0.028643\n",
      "[50]\teval-error:0.261364\ttrain-error:0.028643\n",
      "[51]\teval-error:0.261364\ttrain-error:0.028643\n",
      "[52]\teval-error:0.261364\ttrain-error:0.028643\n",
      "[53]\teval-error:0.261364\ttrain-error:0.028643\n",
      "[54]\teval-error:0.261364\ttrain-error:0.028643\n",
      "[55]\teval-error:0.261364\ttrain-error:0.028643\n",
      "[56]\teval-error:0.261364\ttrain-error:0.028643\n",
      "[57]\teval-error:0.261364\ttrain-error:0.028643\n",
      "[58]\teval-error:0.261364\ttrain-error:0.028643\n",
      "[59]\teval-error:0.261364\ttrain-error:0.028643\n",
      "[60]\teval-error:0.261364\ttrain-error:0.028643\n",
      "[61]\teval-error:0.261364\ttrain-error:0.028643\n",
      "[62]\teval-error:0.261364\ttrain-error:0.028643\n",
      "[63]\teval-error:0.261364\ttrain-error:0.028643\n",
      "[64]\teval-error:0.261364\ttrain-error:0.028643\n",
      "[65]\teval-error:0.261364\ttrain-error:0.028643\n",
      "[66]\teval-error:0.261364\ttrain-error:0.028643\n",
      "[67]\teval-error:0.261364\ttrain-error:0.028643\n",
      "[68]\teval-error:0.25\ttrain-error:0.028643\n",
      "[69]\teval-error:0.25\ttrain-error:0.028643\n",
      "[0]\teval-error:0.397727\ttrain-error:0.10917\n",
      "[1]\teval-error:0.352273\ttrain-error:0.069177\n",
      "[2]\teval-error:0.295455\ttrain-error:0.049721\n",
      "[3]\teval-error:0.306818\ttrain-error:0.039993\n",
      "[4]\teval-error:0.261364\ttrain-error:0.035129\n",
      "[5]\teval-error:0.318182\ttrain-error:0.032246\n",
      "[6]\teval-error:0.306818\ttrain-error:0.030265\n",
      "[7]\teval-error:0.306818\ttrain-error:0.030265\n",
      "[8]\teval-error:0.306818\ttrain-error:0.029544\n",
      "[9]\teval-error:0.295455\ttrain-error:0.029544\n",
      "[10]\teval-error:0.295455\ttrain-error:0.029184\n",
      "[11]\teval-error:0.295455\ttrain-error:0.029004\n",
      "[12]\teval-error:0.295455\ttrain-error:0.029004\n",
      "[13]\teval-error:0.306818\ttrain-error:0.029004\n",
      "[14]\teval-error:0.284091\ttrain-error:0.029004\n",
      "[15]\teval-error:0.272727\ttrain-error:0.029004\n",
      "[16]\teval-error:0.272727\ttrain-error:0.029004\n",
      "[17]\teval-error:0.261364\ttrain-error:0.029004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18]\teval-error:0.238636\ttrain-error:0.029004\n",
      "[19]\teval-error:0.238636\ttrain-error:0.029004\n",
      "[20]\teval-error:0.238636\ttrain-error:0.029004\n",
      "[21]\teval-error:0.25\ttrain-error:0.029004\n",
      "[22]\teval-error:0.238636\ttrain-error:0.029004\n",
      "[23]\teval-error:0.227273\ttrain-error:0.029004\n",
      "[24]\teval-error:0.238636\ttrain-error:0.029004\n",
      "[25]\teval-error:0.227273\ttrain-error:0.029004\n",
      "[26]\teval-error:0.238636\ttrain-error:0.029004\n",
      "[27]\teval-error:0.227273\ttrain-error:0.029004\n",
      "[28]\teval-error:0.238636\ttrain-error:0.029004\n",
      "[29]\teval-error:0.227273\ttrain-error:0.029004\n",
      "[30]\teval-error:0.261364\ttrain-error:0.029004\n",
      "[31]\teval-error:0.261364\ttrain-error:0.029004\n",
      "[32]\teval-error:0.25\ttrain-error:0.029004\n",
      "[33]\teval-error:0.238636\ttrain-error:0.029004\n",
      "[34]\teval-error:0.238636\ttrain-error:0.029004\n",
      "[35]\teval-error:0.25\ttrain-error:0.029004\n",
      "[36]\teval-error:0.25\ttrain-error:0.029004\n",
      "[37]\teval-error:0.25\ttrain-error:0.029004\n",
      "[38]\teval-error:0.25\ttrain-error:0.029004\n",
      "[39]\teval-error:0.25\ttrain-error:0.029004\n",
      "[40]\teval-error:0.25\ttrain-error:0.029004\n",
      "[41]\teval-error:0.25\ttrain-error:0.029004\n",
      "[42]\teval-error:0.25\ttrain-error:0.029004\n",
      "[43]\teval-error:0.25\ttrain-error:0.029004\n",
      "[44]\teval-error:0.261364\ttrain-error:0.029004\n",
      "[45]\teval-error:0.25\ttrain-error:0.029004\n",
      "[46]\teval-error:0.238636\ttrain-error:0.029004\n",
      "[47]\teval-error:0.25\ttrain-error:0.029004\n",
      "[48]\teval-error:0.25\ttrain-error:0.029004\n",
      "[49]\teval-error:0.25\ttrain-error:0.029004\n",
      "[50]\teval-error:0.25\ttrain-error:0.029004\n",
      "[51]\teval-error:0.25\ttrain-error:0.029004\n",
      "[52]\teval-error:0.261364\ttrain-error:0.029004\n",
      "[53]\teval-error:0.238636\ttrain-error:0.029004\n",
      "[54]\teval-error:0.261364\ttrain-error:0.029004\n",
      "[55]\teval-error:0.25\ttrain-error:0.029004\n",
      "[56]\teval-error:0.25\ttrain-error:0.029004\n",
      "[57]\teval-error:0.25\ttrain-error:0.029004\n",
      "[58]\teval-error:0.25\ttrain-error:0.029004\n",
      "[59]\teval-error:0.25\ttrain-error:0.029004\n",
      "[60]\teval-error:0.25\ttrain-error:0.029004\n",
      "[61]\teval-error:0.25\ttrain-error:0.029004\n",
      "[62]\teval-error:0.25\ttrain-error:0.029004\n",
      "[63]\teval-error:0.25\ttrain-error:0.029004\n",
      "[64]\teval-error:0.25\ttrain-error:0.029004\n",
      "[65]\teval-error:0.25\ttrain-error:0.029004\n",
      "[66]\teval-error:0.25\ttrain-error:0.029004\n",
      "[67]\teval-error:0.25\ttrain-error:0.029004\n",
      "[68]\teval-error:0.25\ttrain-error:0.029004\n",
      "[69]\teval-error:0.238636\ttrain-error:0.029004\n",
      "[0]\teval-error:0.352273\ttrain-error:0.108089\n",
      "[1]\teval-error:0.340909\ttrain-error:0.064673\n",
      "[2]\teval-error:0.340909\ttrain-error:0.047919\n",
      "[3]\teval-error:0.306818\ttrain-error:0.03675\n",
      "[4]\teval-error:0.261364\ttrain-error:0.032607\n",
      "[5]\teval-error:0.25\ttrain-error:0.031526\n",
      "[6]\teval-error:0.227273\ttrain-error:0.029724\n",
      "[7]\teval-error:0.25\ttrain-error:0.028643\n",
      "[8]\teval-error:0.25\ttrain-error:0.028463\n",
      "[9]\teval-error:0.261364\ttrain-error:0.028643\n",
      "[10]\teval-error:0.261364\ttrain-error:0.028643\n",
      "[11]\teval-error:0.25\ttrain-error:0.028463\n",
      "[12]\teval-error:0.227273\ttrain-error:0.028643\n",
      "[13]\teval-error:0.261364\ttrain-error:0.028283\n",
      "[14]\teval-error:0.261364\ttrain-error:0.028283\n",
      "[15]\teval-error:0.261364\ttrain-error:0.028283\n",
      "[16]\teval-error:0.238636\ttrain-error:0.028283\n",
      "[17]\teval-error:0.238636\ttrain-error:0.028283\n",
      "[18]\teval-error:0.25\ttrain-error:0.028283\n",
      "[19]\teval-error:0.238636\ttrain-error:0.028283\n",
      "[20]\teval-error:0.238636\ttrain-error:0.028283\n",
      "[21]\teval-error:0.25\ttrain-error:0.028283\n",
      "[22]\teval-error:0.25\ttrain-error:0.028283\n",
      "[23]\teval-error:0.238636\ttrain-error:0.028283\n",
      "[24]\teval-error:0.238636\ttrain-error:0.028283\n",
      "[25]\teval-error:0.215909\ttrain-error:0.028283\n",
      "[26]\teval-error:0.215909\ttrain-error:0.028283\n",
      "[27]\teval-error:0.227273\ttrain-error:0.028283\n",
      "[28]\teval-error:0.227273\ttrain-error:0.028283\n",
      "[29]\teval-error:0.227273\ttrain-error:0.028283\n",
      "[30]\teval-error:0.215909\ttrain-error:0.028283\n",
      "[31]\teval-error:0.227273\ttrain-error:0.028283\n",
      "[32]\teval-error:0.215909\ttrain-error:0.028283\n",
      "[33]\teval-error:0.215909\ttrain-error:0.028283\n",
      "[34]\teval-error:0.204545\ttrain-error:0.028283\n",
      "[35]\teval-error:0.215909\ttrain-error:0.028283\n",
      "[36]\teval-error:0.204545\ttrain-error:0.028283\n",
      "[37]\teval-error:0.193182\ttrain-error:0.028283\n",
      "[38]\teval-error:0.193182\ttrain-error:0.028283\n",
      "[39]\teval-error:0.204545\ttrain-error:0.028283\n",
      "[40]\teval-error:0.193182\ttrain-error:0.028283\n",
      "[41]\teval-error:0.193182\ttrain-error:0.028283\n",
      "[42]\teval-error:0.193182\ttrain-error:0.028283\n",
      "[43]\teval-error:0.193182\ttrain-error:0.028283\n",
      "[44]\teval-error:0.193182\ttrain-error:0.028283\n",
      "[45]\teval-error:0.193182\ttrain-error:0.028283\n",
      "[46]\teval-error:0.193182\ttrain-error:0.028283\n",
      "[47]\teval-error:0.193182\ttrain-error:0.028283\n",
      "[48]\teval-error:0.193182\ttrain-error:0.028283\n",
      "[49]\teval-error:0.181818\ttrain-error:0.028283\n",
      "[50]\teval-error:0.181818\ttrain-error:0.028283\n",
      "[51]\teval-error:0.181818\ttrain-error:0.028283\n",
      "[52]\teval-error:0.181818\ttrain-error:0.028283\n",
      "[53]\teval-error:0.181818\ttrain-error:0.028283\n",
      "[54]\teval-error:0.181818\ttrain-error:0.028283\n",
      "[55]\teval-error:0.181818\ttrain-error:0.028283\n",
      "[56]\teval-error:0.181818\ttrain-error:0.028283\n",
      "[57]\teval-error:0.181818\ttrain-error:0.028283\n",
      "[58]\teval-error:0.181818\ttrain-error:0.028283\n",
      "[59]\teval-error:0.181818\ttrain-error:0.028283\n",
      "[60]\teval-error:0.181818\ttrain-error:0.028283\n",
      "[61]\teval-error:0.181818\ttrain-error:0.028283\n",
      "[62]\teval-error:0.181818\ttrain-error:0.028283\n",
      "[63]\teval-error:0.170455\ttrain-error:0.028283\n",
      "[64]\teval-error:0.181818\ttrain-error:0.028283\n",
      "[65]\teval-error:0.170455\ttrain-error:0.028283\n",
      "[66]\teval-error:0.170455\ttrain-error:0.028283\n",
      "[67]\teval-error:0.170455\ttrain-error:0.028283\n",
      "[68]\teval-error:0.193182\ttrain-error:0.028283\n",
      "[69]\teval-error:0.170455\ttrain-error:0.028283\n",
      "[0]\teval-error:0.329787\ttrain-error:0.103666\n",
      "[1]\teval-error:0.297872\ttrain-error:0.070359\n",
      "[2]\teval-error:0.287234\ttrain-error:0.054609\n",
      "[3]\teval-error:0.255319\ttrain-error:0.046476\n",
      "[4]\teval-error:0.276596\ttrain-error:0.043894\n",
      "[5]\teval-error:0.255319\ttrain-error:0.041957\n",
      "[6]\teval-error:0.276596\ttrain-error:0.040924\n",
      "[7]\teval-error:0.297872\ttrain-error:0.039892\n",
      "[8]\teval-error:0.319149\ttrain-error:0.039762\n",
      "[9]\teval-error:0.297872\ttrain-error:0.039504\n",
      "[10]\teval-error:0.287234\ttrain-error:0.038988\n",
      "[11]\teval-error:0.287234\ttrain-error:0.038859\n",
      "[12]\teval-error:0.276596\ttrain-error:0.03873\n",
      "[13]\teval-error:0.265957\ttrain-error:0.03873\n",
      "[14]\teval-error:0.276596\ttrain-error:0.03873\n",
      "[15]\teval-error:0.265957\ttrain-error:0.03873\n",
      "[16]\teval-error:0.265957\ttrain-error:0.03873\n",
      "[17]\teval-error:0.255319\ttrain-error:0.03873\n",
      "[18]\teval-error:0.255319\ttrain-error:0.03873\n",
      "[19]\teval-error:0.244681\ttrain-error:0.03873\n",
      "[20]\teval-error:0.255319\ttrain-error:0.03873\n",
      "[21]\teval-error:0.276596\ttrain-error:0.03873\n",
      "[22]\teval-error:0.276596\ttrain-error:0.03873\n",
      "[23]\teval-error:0.287234\ttrain-error:0.03873\n",
      "[24]\teval-error:0.276596\ttrain-error:0.03873\n",
      "[25]\teval-error:0.265957\ttrain-error:0.03873\n",
      "[26]\teval-error:0.287234\ttrain-error:0.03873\n",
      "[27]\teval-error:0.287234\ttrain-error:0.03873\n",
      "[28]\teval-error:0.287234\ttrain-error:0.03873\n",
      "[29]\teval-error:0.287234\ttrain-error:0.03873\n",
      "[30]\teval-error:0.287234\ttrain-error:0.03873\n",
      "[31]\teval-error:0.287234\ttrain-error:0.03873\n",
      "[32]\teval-error:0.287234\ttrain-error:0.03873\n",
      "[33]\teval-error:0.287234\ttrain-error:0.03873\n",
      "[34]\teval-error:0.276596\ttrain-error:0.03873\n",
      "[35]\teval-error:0.276596\ttrain-error:0.03873\n",
      "[36]\teval-error:0.265957\ttrain-error:0.03873\n",
      "[37]\teval-error:0.265957\ttrain-error:0.03873\n",
      "[38]\teval-error:0.276596\ttrain-error:0.03873\n",
      "[39]\teval-error:0.276596\ttrain-error:0.03873\n",
      "[40]\teval-error:0.276596\ttrain-error:0.03873\n",
      "[41]\teval-error:0.287234\ttrain-error:0.03873\n",
      "[42]\teval-error:0.287234\ttrain-error:0.03873\n",
      "[43]\teval-error:0.276596\ttrain-error:0.03873\n",
      "[44]\teval-error:0.276596\ttrain-error:0.03873\n",
      "[45]\teval-error:0.276596\ttrain-error:0.03873\n",
      "[46]\teval-error:0.276596\ttrain-error:0.03873\n",
      "[47]\teval-error:0.276596\ttrain-error:0.03873\n",
      "[48]\teval-error:0.276596\ttrain-error:0.03873\n",
      "[49]\teval-error:0.276596\ttrain-error:0.03873\n",
      "[50]\teval-error:0.276596\ttrain-error:0.03873\n",
      "[51]\teval-error:0.276596\ttrain-error:0.03873\n",
      "[52]\teval-error:0.276596\ttrain-error:0.03873\n",
      "[53]\teval-error:0.276596\ttrain-error:0.03873\n",
      "[54]\teval-error:0.276596\ttrain-error:0.03873\n",
      "[55]\teval-error:0.276596\ttrain-error:0.03873\n",
      "[56]\teval-error:0.276596\ttrain-error:0.03873\n",
      "[57]\teval-error:0.276596\ttrain-error:0.03873\n",
      "[58]\teval-error:0.276596\ttrain-error:0.03873\n",
      "[59]\teval-error:0.276596\ttrain-error:0.03873\n",
      "[60]\teval-error:0.276596\ttrain-error:0.03873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61]\teval-error:0.276596\ttrain-error:0.03873\n",
      "[62]\teval-error:0.276596\ttrain-error:0.03873\n",
      "[63]\teval-error:0.276596\ttrain-error:0.03873\n",
      "[64]\teval-error:0.276596\ttrain-error:0.03873\n",
      "[65]\teval-error:0.276596\ttrain-error:0.03873\n",
      "[66]\teval-error:0.276596\ttrain-error:0.03873\n",
      "[67]\teval-error:0.276596\ttrain-error:0.03873\n",
      "[68]\teval-error:0.276596\ttrain-error:0.03873\n",
      "[69]\teval-error:0.276596\ttrain-error:0.03873\n",
      "[0]\teval-error:0.287234\ttrain-error:0.100568\n",
      "[1]\teval-error:0.265957\ttrain-error:0.073845\n",
      "[2]\teval-error:0.244681\ttrain-error:0.056804\n",
      "[3]\teval-error:0.244681\ttrain-error:0.049574\n",
      "[4]\teval-error:0.234043\ttrain-error:0.045443\n",
      "[5]\teval-error:0.212766\ttrain-error:0.043635\n",
      "[6]\teval-error:0.223404\ttrain-error:0.042861\n",
      "[7]\teval-error:0.223404\ttrain-error:0.041441\n",
      "[8]\teval-error:0.234043\ttrain-error:0.040924\n",
      "[9]\teval-error:0.223404\ttrain-error:0.04015\n",
      "[10]\teval-error:0.234043\ttrain-error:0.040021\n",
      "[11]\teval-error:0.212766\ttrain-error:0.040021\n",
      "[12]\teval-error:0.223404\ttrain-error:0.039892\n",
      "[13]\teval-error:0.212766\ttrain-error:0.039892\n",
      "[14]\teval-error:0.212766\ttrain-error:0.039892\n",
      "[15]\teval-error:0.202128\ttrain-error:0.039892\n",
      "[16]\teval-error:0.212766\ttrain-error:0.039892\n",
      "[17]\teval-error:0.212766\ttrain-error:0.039892\n",
      "[18]\teval-error:0.223404\ttrain-error:0.039892\n",
      "[19]\teval-error:0.223404\ttrain-error:0.039892\n",
      "[20]\teval-error:0.223404\ttrain-error:0.039892\n",
      "[21]\teval-error:0.223404\ttrain-error:0.039892\n",
      "[22]\teval-error:0.223404\ttrain-error:0.039892\n",
      "[23]\teval-error:0.212766\ttrain-error:0.039892\n",
      "[24]\teval-error:0.212766\ttrain-error:0.039892\n",
      "[25]\teval-error:0.212766\ttrain-error:0.039892\n",
      "[26]\teval-error:0.212766\ttrain-error:0.039892\n",
      "[27]\teval-error:0.223404\ttrain-error:0.039892\n",
      "[28]\teval-error:0.212766\ttrain-error:0.039892\n",
      "[29]\teval-error:0.234043\ttrain-error:0.039892\n",
      "[30]\teval-error:0.223404\ttrain-error:0.039892\n",
      "[31]\teval-error:0.223404\ttrain-error:0.039892\n",
      "[32]\teval-error:0.234043\ttrain-error:0.039892\n",
      "[33]\teval-error:0.234043\ttrain-error:0.039892\n",
      "[34]\teval-error:0.234043\ttrain-error:0.039892\n",
      "[35]\teval-error:0.234043\ttrain-error:0.039892\n",
      "[36]\teval-error:0.234043\ttrain-error:0.039892\n",
      "[37]\teval-error:0.234043\ttrain-error:0.039892\n",
      "[38]\teval-error:0.234043\ttrain-error:0.039892\n",
      "[39]\teval-error:0.244681\ttrain-error:0.039892\n",
      "[40]\teval-error:0.234043\ttrain-error:0.039892\n",
      "[41]\teval-error:0.244681\ttrain-error:0.039892\n",
      "[42]\teval-error:0.234043\ttrain-error:0.039892\n",
      "[43]\teval-error:0.234043\ttrain-error:0.039892\n",
      "[44]\teval-error:0.234043\ttrain-error:0.039892\n",
      "[45]\teval-error:0.234043\ttrain-error:0.039892\n",
      "[46]\teval-error:0.234043\ttrain-error:0.039892\n",
      "[47]\teval-error:0.223404\ttrain-error:0.039892\n",
      "[48]\teval-error:0.223404\ttrain-error:0.039892\n",
      "[49]\teval-error:0.223404\ttrain-error:0.039892\n",
      "[50]\teval-error:0.234043\ttrain-error:0.039892\n",
      "[51]\teval-error:0.234043\ttrain-error:0.039892\n",
      "[52]\teval-error:0.223404\ttrain-error:0.039892\n",
      "[53]\teval-error:0.223404\ttrain-error:0.039892\n",
      "[54]\teval-error:0.223404\ttrain-error:0.039892\n",
      "[55]\teval-error:0.234043\ttrain-error:0.039892\n",
      "[56]\teval-error:0.234043\ttrain-error:0.039892\n",
      "[57]\teval-error:0.234043\ttrain-error:0.039892\n",
      "[58]\teval-error:0.234043\ttrain-error:0.039892\n",
      "[59]\teval-error:0.234043\ttrain-error:0.039892\n",
      "[60]\teval-error:0.234043\ttrain-error:0.039892\n",
      "[61]\teval-error:0.234043\ttrain-error:0.039892\n",
      "[62]\teval-error:0.234043\ttrain-error:0.039892\n",
      "[63]\teval-error:0.223404\ttrain-error:0.039892\n",
      "[64]\teval-error:0.223404\ttrain-error:0.039892\n",
      "[65]\teval-error:0.234043\ttrain-error:0.039892\n",
      "[66]\teval-error:0.234043\ttrain-error:0.039892\n",
      "[67]\teval-error:0.223404\ttrain-error:0.039892\n",
      "[68]\teval-error:0.212766\ttrain-error:0.039892\n",
      "[69]\teval-error:0.223404\ttrain-error:0.039892\n",
      "[0]\teval-error:0.340426\ttrain-error:0.10457\n",
      "[1]\teval-error:0.265957\ttrain-error:0.075652\n",
      "[2]\teval-error:0.276596\ttrain-error:0.055642\n",
      "[3]\teval-error:0.234043\ttrain-error:0.049574\n",
      "[4]\teval-error:0.276596\ttrain-error:0.045572\n",
      "[5]\teval-error:0.265957\ttrain-error:0.043635\n",
      "[6]\teval-error:0.265957\ttrain-error:0.04157\n",
      "[7]\teval-error:0.265957\ttrain-error:0.040924\n",
      "[8]\teval-error:0.265957\ttrain-error:0.040279\n",
      "[9]\teval-error:0.234043\ttrain-error:0.040021\n",
      "[10]\teval-error:0.265957\ttrain-error:0.039892\n",
      "[11]\teval-error:0.255319\ttrain-error:0.040021\n",
      "[12]\teval-error:0.265957\ttrain-error:0.039892\n",
      "[13]\teval-error:0.276596\ttrain-error:0.039892\n",
      "[14]\teval-error:0.276596\ttrain-error:0.039892\n",
      "[15]\teval-error:0.297872\ttrain-error:0.039892\n",
      "[16]\teval-error:0.287234\ttrain-error:0.039892\n",
      "[17]\teval-error:0.276596\ttrain-error:0.039892\n",
      "[18]\teval-error:0.297872\ttrain-error:0.039892\n",
      "[19]\teval-error:0.297872\ttrain-error:0.039892\n",
      "[20]\teval-error:0.287234\ttrain-error:0.039892\n",
      "[21]\teval-error:0.297872\ttrain-error:0.039892\n",
      "[22]\teval-error:0.297872\ttrain-error:0.039892\n",
      "[23]\teval-error:0.287234\ttrain-error:0.039892\n",
      "[24]\teval-error:0.265957\ttrain-error:0.039892\n",
      "[25]\teval-error:0.287234\ttrain-error:0.039892\n",
      "[26]\teval-error:0.287234\ttrain-error:0.039892\n",
      "[27]\teval-error:0.276596\ttrain-error:0.039892\n",
      "[28]\teval-error:0.276596\ttrain-error:0.039892\n",
      "[29]\teval-error:0.276596\ttrain-error:0.039892\n",
      "[30]\teval-error:0.276596\ttrain-error:0.039892\n",
      "[31]\teval-error:0.276596\ttrain-error:0.039892\n",
      "[32]\teval-error:0.276596\ttrain-error:0.039892\n",
      "[33]\teval-error:0.276596\ttrain-error:0.039892\n",
      "[34]\teval-error:0.276596\ttrain-error:0.039892\n",
      "[35]\teval-error:0.276596\ttrain-error:0.039892\n",
      "[36]\teval-error:0.276596\ttrain-error:0.039892\n",
      "[37]\teval-error:0.276596\ttrain-error:0.039892\n",
      "[38]\teval-error:0.276596\ttrain-error:0.039892\n",
      "[39]\teval-error:0.276596\ttrain-error:0.039892\n",
      "[40]\teval-error:0.276596\ttrain-error:0.039892\n",
      "[41]\teval-error:0.276596\ttrain-error:0.039892\n",
      "[42]\teval-error:0.276596\ttrain-error:0.039892\n",
      "[43]\teval-error:0.265957\ttrain-error:0.039892\n",
      "[44]\teval-error:0.276596\ttrain-error:0.039892\n",
      "[45]\teval-error:0.276596\ttrain-error:0.039892\n",
      "[46]\teval-error:0.287234\ttrain-error:0.039892\n",
      "[47]\teval-error:0.287234\ttrain-error:0.039892\n",
      "[48]\teval-error:0.265957\ttrain-error:0.039892\n",
      "[49]\teval-error:0.265957\ttrain-error:0.039892\n",
      "[50]\teval-error:0.276596\ttrain-error:0.039892\n",
      "[51]\teval-error:0.276596\ttrain-error:0.039892\n",
      "[52]\teval-error:0.276596\ttrain-error:0.039892\n",
      "[53]\teval-error:0.255319\ttrain-error:0.039892\n",
      "[54]\teval-error:0.255319\ttrain-error:0.039892\n",
      "[55]\teval-error:0.255319\ttrain-error:0.039892\n",
      "[56]\teval-error:0.255319\ttrain-error:0.039892\n",
      "[57]\teval-error:0.255319\ttrain-error:0.039892\n",
      "[58]\teval-error:0.255319\ttrain-error:0.039892\n",
      "[59]\teval-error:0.255319\ttrain-error:0.039892\n",
      "[60]\teval-error:0.265957\ttrain-error:0.039892\n",
      "[61]\teval-error:0.265957\ttrain-error:0.039892\n",
      "[62]\teval-error:0.265957\ttrain-error:0.039892\n",
      "[63]\teval-error:0.255319\ttrain-error:0.039892\n",
      "[64]\teval-error:0.265957\ttrain-error:0.039892\n",
      "[65]\teval-error:0.265957\ttrain-error:0.039892\n",
      "[66]\teval-error:0.265957\ttrain-error:0.039892\n",
      "[67]\teval-error:0.265957\ttrain-error:0.039892\n",
      "[68]\teval-error:0.265957\ttrain-error:0.039892\n",
      "[69]\teval-error:0.255319\ttrain-error:0.039892\n",
      "[0]\teval-error:0.319149\ttrain-error:0.101988\n",
      "[1]\teval-error:0.276596\ttrain-error:0.070359\n",
      "[2]\teval-error:0.308511\ttrain-error:0.054867\n",
      "[3]\teval-error:0.340426\ttrain-error:0.046734\n",
      "[4]\teval-error:0.287234\ttrain-error:0.044152\n",
      "[5]\teval-error:0.308511\ttrain-error:0.041957\n",
      "[6]\teval-error:0.297872\ttrain-error:0.040795\n",
      "[7]\teval-error:0.319149\ttrain-error:0.04015\n",
      "[8]\teval-error:0.319149\ttrain-error:0.039246\n",
      "[9]\teval-error:0.308511\ttrain-error:0.039117\n",
      "[10]\teval-error:0.319149\ttrain-error:0.038988\n",
      "[11]\teval-error:0.319149\ttrain-error:0.038859\n",
      "[12]\teval-error:0.308511\ttrain-error:0.038988\n",
      "[13]\teval-error:0.308511\ttrain-error:0.038859\n",
      "[14]\teval-error:0.340426\ttrain-error:0.03873\n",
      "[15]\teval-error:0.297872\ttrain-error:0.03873\n",
      "[16]\teval-error:0.319149\ttrain-error:0.03873\n",
      "[17]\teval-error:0.297872\ttrain-error:0.03873\n",
      "[18]\teval-error:0.308511\ttrain-error:0.03873\n",
      "[19]\teval-error:0.297872\ttrain-error:0.03873\n",
      "[20]\teval-error:0.319149\ttrain-error:0.03873\n",
      "[21]\teval-error:0.340426\ttrain-error:0.03873\n",
      "[22]\teval-error:0.329787\ttrain-error:0.03873\n",
      "[23]\teval-error:0.319149\ttrain-error:0.03873\n",
      "[24]\teval-error:0.319149\ttrain-error:0.03873\n",
      "[25]\teval-error:0.308511\ttrain-error:0.03873\n",
      "[26]\teval-error:0.308511\ttrain-error:0.03873\n",
      "[27]\teval-error:0.297872\ttrain-error:0.03873\n",
      "[28]\teval-error:0.287234\ttrain-error:0.03873\n",
      "[29]\teval-error:0.297872\ttrain-error:0.03873\n",
      "[30]\teval-error:0.297872\ttrain-error:0.03873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31]\teval-error:0.308511\ttrain-error:0.03873\n",
      "[32]\teval-error:0.308511\ttrain-error:0.03873\n",
      "[33]\teval-error:0.297872\ttrain-error:0.03873\n",
      "[34]\teval-error:0.297872\ttrain-error:0.03873\n",
      "[35]\teval-error:0.308511\ttrain-error:0.03873\n",
      "[36]\teval-error:0.308511\ttrain-error:0.03873\n",
      "[37]\teval-error:0.319149\ttrain-error:0.03873\n",
      "[38]\teval-error:0.319149\ttrain-error:0.03873\n",
      "[39]\teval-error:0.308511\ttrain-error:0.03873\n",
      "[40]\teval-error:0.319149\ttrain-error:0.03873\n",
      "[41]\teval-error:0.308511\ttrain-error:0.03873\n",
      "[42]\teval-error:0.308511\ttrain-error:0.03873\n",
      "[43]\teval-error:0.308511\ttrain-error:0.03873\n",
      "[44]\teval-error:0.319149\ttrain-error:0.03873\n",
      "[45]\teval-error:0.319149\ttrain-error:0.03873\n",
      "[46]\teval-error:0.319149\ttrain-error:0.03873\n",
      "[47]\teval-error:0.319149\ttrain-error:0.03873\n",
      "[48]\teval-error:0.319149\ttrain-error:0.03873\n",
      "[49]\teval-error:0.319149\ttrain-error:0.03873\n",
      "[50]\teval-error:0.319149\ttrain-error:0.03873\n",
      "[51]\teval-error:0.319149\ttrain-error:0.03873\n",
      "[52]\teval-error:0.319149\ttrain-error:0.03873\n",
      "[53]\teval-error:0.319149\ttrain-error:0.03873\n",
      "[54]\teval-error:0.319149\ttrain-error:0.03873\n",
      "[55]\teval-error:0.319149\ttrain-error:0.03873\n",
      "[56]\teval-error:0.319149\ttrain-error:0.03873\n",
      "[57]\teval-error:0.319149\ttrain-error:0.03873\n",
      "[58]\teval-error:0.319149\ttrain-error:0.03873\n",
      "[59]\teval-error:0.319149\ttrain-error:0.03873\n",
      "[60]\teval-error:0.319149\ttrain-error:0.03873\n",
      "[61]\teval-error:0.319149\ttrain-error:0.03873\n",
      "[62]\teval-error:0.319149\ttrain-error:0.03873\n",
      "[63]\teval-error:0.319149\ttrain-error:0.03873\n",
      "[64]\teval-error:0.319149\ttrain-error:0.03873\n",
      "[65]\teval-error:0.319149\ttrain-error:0.03873\n",
      "[66]\teval-error:0.319149\ttrain-error:0.03873\n",
      "[67]\teval-error:0.297872\ttrain-error:0.03873\n",
      "[68]\teval-error:0.297872\ttrain-error:0.03873\n",
      "[69]\teval-error:0.297872\ttrain-error:0.03873\n",
      "[0]\teval-error:0.229008\ttrain-error:0.089844\n",
      "[1]\teval-error:0.209924\ttrain-error:0.068276\n",
      "[2]\teval-error:0.240458\ttrain-error:0.059134\n",
      "[3]\teval-error:0.225191\ttrain-error:0.052992\n",
      "[4]\teval-error:0.221374\ttrain-error:0.050207\n",
      "[5]\teval-error:0.225191\ttrain-error:0.048279\n",
      "[6]\teval-error:0.221374\ttrain-error:0.046993\n",
      "[7]\teval-error:0.217557\ttrain-error:0.046351\n",
      "[8]\teval-error:0.232824\ttrain-error:0.045993\n",
      "[9]\teval-error:0.229008\ttrain-error:0.045779\n",
      "[10]\teval-error:0.225191\ttrain-error:0.045779\n",
      "[11]\teval-error:0.21374\ttrain-error:0.045779\n",
      "[12]\teval-error:0.217557\ttrain-error:0.045708\n",
      "[13]\teval-error:0.225191\ttrain-error:0.045636\n",
      "[14]\teval-error:0.225191\ttrain-error:0.045636\n",
      "[15]\teval-error:0.221374\ttrain-error:0.045636\n",
      "[16]\teval-error:0.225191\ttrain-error:0.045636\n",
      "[17]\teval-error:0.229008\ttrain-error:0.045636\n",
      "[18]\teval-error:0.232824\ttrain-error:0.045565\n",
      "[19]\teval-error:0.240458\ttrain-error:0.045565\n",
      "[20]\teval-error:0.236641\ttrain-error:0.045565\n",
      "[21]\teval-error:0.229008\ttrain-error:0.045565\n",
      "[22]\teval-error:0.221374\ttrain-error:0.045565\n",
      "[23]\teval-error:0.225191\ttrain-error:0.045565\n",
      "[24]\teval-error:0.217557\ttrain-error:0.045494\n",
      "[25]\teval-error:0.217557\ttrain-error:0.045565\n",
      "[26]\teval-error:0.21374\ttrain-error:0.045494\n",
      "[27]\teval-error:0.225191\ttrain-error:0.045494\n",
      "[28]\teval-error:0.225191\ttrain-error:0.045494\n",
      "[29]\teval-error:0.229008\ttrain-error:0.045494\n",
      "[30]\teval-error:0.225191\ttrain-error:0.045494\n",
      "[31]\teval-error:0.225191\ttrain-error:0.045494\n",
      "[32]\teval-error:0.221374\ttrain-error:0.045494\n",
      "[33]\teval-error:0.225191\ttrain-error:0.045494\n",
      "[34]\teval-error:0.232824\ttrain-error:0.045494\n",
      "[35]\teval-error:0.236641\ttrain-error:0.045494\n",
      "[36]\teval-error:0.229008\ttrain-error:0.045494\n",
      "[37]\teval-error:0.236641\ttrain-error:0.045494\n",
      "[38]\teval-error:0.221374\ttrain-error:0.045494\n",
      "[39]\teval-error:0.225191\ttrain-error:0.045494\n",
      "[40]\teval-error:0.221374\ttrain-error:0.045494\n",
      "[41]\teval-error:0.229008\ttrain-error:0.045494\n",
      "[42]\teval-error:0.229008\ttrain-error:0.045494\n",
      "[43]\teval-error:0.225191\ttrain-error:0.045494\n",
      "[44]\teval-error:0.240458\ttrain-error:0.045494\n",
      "[45]\teval-error:0.229008\ttrain-error:0.045494\n",
      "[46]\teval-error:0.229008\ttrain-error:0.045494\n",
      "[47]\teval-error:0.225191\ttrain-error:0.045494\n",
      "[48]\teval-error:0.229008\ttrain-error:0.045494\n",
      "[49]\teval-error:0.225191\ttrain-error:0.045494\n",
      "[50]\teval-error:0.229008\ttrain-error:0.045494\n",
      "[51]\teval-error:0.229008\ttrain-error:0.045494\n",
      "[52]\teval-error:0.221374\ttrain-error:0.045494\n",
      "[53]\teval-error:0.225191\ttrain-error:0.045494\n",
      "[54]\teval-error:0.232824\ttrain-error:0.045494\n",
      "[55]\teval-error:0.229008\ttrain-error:0.045494\n",
      "[56]\teval-error:0.232824\ttrain-error:0.045494\n",
      "[57]\teval-error:0.232824\ttrain-error:0.045494\n",
      "[58]\teval-error:0.225191\ttrain-error:0.045494\n",
      "[59]\teval-error:0.232824\ttrain-error:0.045494\n",
      "[60]\teval-error:0.232824\ttrain-error:0.045494\n",
      "[61]\teval-error:0.225191\ttrain-error:0.045494\n",
      "[62]\teval-error:0.232824\ttrain-error:0.045494\n",
      "[63]\teval-error:0.221374\ttrain-error:0.045494\n",
      "[64]\teval-error:0.225191\ttrain-error:0.045494\n",
      "[65]\teval-error:0.221374\ttrain-error:0.045494\n",
      "[66]\teval-error:0.225191\ttrain-error:0.045494\n",
      "[67]\teval-error:0.229008\ttrain-error:0.045494\n",
      "[68]\teval-error:0.225191\ttrain-error:0.045494\n",
      "[69]\teval-error:0.221374\ttrain-error:0.045494\n",
      "[0]\teval-error:0.293893\ttrain-error:0.090487\n",
      "[1]\teval-error:0.251908\ttrain-error:0.069276\n",
      "[2]\teval-error:0.251908\ttrain-error:0.058349\n",
      "[3]\teval-error:0.236641\ttrain-error:0.052921\n",
      "[4]\teval-error:0.240458\ttrain-error:0.049136\n",
      "[5]\teval-error:0.267176\ttrain-error:0.048422\n",
      "[6]\teval-error:0.251908\ttrain-error:0.047707\n",
      "[7]\teval-error:0.267176\ttrain-error:0.047136\n",
      "[8]\teval-error:0.267176\ttrain-error:0.046708\n",
      "[9]\teval-error:0.251908\ttrain-error:0.046351\n",
      "[10]\teval-error:0.244275\ttrain-error:0.046279\n",
      "[11]\teval-error:0.244275\ttrain-error:0.046136\n",
      "[12]\teval-error:0.244275\ttrain-error:0.046065\n",
      "[13]\teval-error:0.232824\ttrain-error:0.046065\n",
      "[14]\teval-error:0.236641\ttrain-error:0.046065\n",
      "[15]\teval-error:0.232824\ttrain-error:0.046065\n",
      "[16]\teval-error:0.244275\ttrain-error:0.046065\n",
      "[17]\teval-error:0.240458\ttrain-error:0.046065\n",
      "[18]\teval-error:0.240458\ttrain-error:0.046065\n",
      "[19]\teval-error:0.232824\ttrain-error:0.045993\n",
      "[20]\teval-error:0.244275\ttrain-error:0.045993\n",
      "[21]\teval-error:0.236641\ttrain-error:0.045993\n",
      "[22]\teval-error:0.240458\ttrain-error:0.045993\n",
      "[23]\teval-error:0.240458\ttrain-error:0.045993\n",
      "[24]\teval-error:0.236641\ttrain-error:0.045993\n",
      "[25]\teval-error:0.240458\ttrain-error:0.045993\n",
      "[26]\teval-error:0.232824\ttrain-error:0.045993\n",
      "[27]\teval-error:0.244275\ttrain-error:0.045993\n",
      "[28]\teval-error:0.240458\ttrain-error:0.045993\n",
      "[29]\teval-error:0.251908\ttrain-error:0.045993\n",
      "[30]\teval-error:0.251908\ttrain-error:0.045993\n",
      "[31]\teval-error:0.248092\ttrain-error:0.045993\n",
      "[32]\teval-error:0.255725\ttrain-error:0.045993\n",
      "[33]\teval-error:0.244275\ttrain-error:0.045993\n",
      "[34]\teval-error:0.255725\ttrain-error:0.045993\n",
      "[35]\teval-error:0.240458\ttrain-error:0.045993\n",
      "[36]\teval-error:0.244275\ttrain-error:0.045993\n",
      "[37]\teval-error:0.240458\ttrain-error:0.045993\n",
      "[38]\teval-error:0.244275\ttrain-error:0.045993\n",
      "[39]\teval-error:0.240458\ttrain-error:0.045993\n",
      "[40]\teval-error:0.236641\ttrain-error:0.045993\n",
      "[41]\teval-error:0.240458\ttrain-error:0.045993\n",
      "[42]\teval-error:0.244275\ttrain-error:0.045993\n",
      "[43]\teval-error:0.244275\ttrain-error:0.045993\n",
      "[44]\teval-error:0.236641\ttrain-error:0.045993\n",
      "[45]\teval-error:0.244275\ttrain-error:0.045993\n",
      "[46]\teval-error:0.240458\ttrain-error:0.045993\n",
      "[47]\teval-error:0.244275\ttrain-error:0.045993\n",
      "[48]\teval-error:0.251908\ttrain-error:0.045993\n",
      "[49]\teval-error:0.248092\ttrain-error:0.045993\n",
      "[50]\teval-error:0.244275\ttrain-error:0.045993\n",
      "[51]\teval-error:0.244275\ttrain-error:0.045993\n",
      "[52]\teval-error:0.240458\ttrain-error:0.045993\n",
      "[53]\teval-error:0.240458\ttrain-error:0.045993\n",
      "[54]\teval-error:0.240458\ttrain-error:0.045993\n",
      "[55]\teval-error:0.240458\ttrain-error:0.045993\n",
      "[56]\teval-error:0.248092\ttrain-error:0.045993\n",
      "[57]\teval-error:0.248092\ttrain-error:0.045993\n",
      "[58]\teval-error:0.248092\ttrain-error:0.045993\n",
      "[59]\teval-error:0.244275\ttrain-error:0.045993\n",
      "[60]\teval-error:0.244275\ttrain-error:0.045993\n",
      "[61]\teval-error:0.240458\ttrain-error:0.045993\n",
      "[62]\teval-error:0.240458\ttrain-error:0.045993\n",
      "[63]\teval-error:0.248092\ttrain-error:0.045993\n",
      "[64]\teval-error:0.232824\ttrain-error:0.045993\n",
      "[65]\teval-error:0.244275\ttrain-error:0.045993\n",
      "[66]\teval-error:0.255725\ttrain-error:0.045993\n",
      "[67]\teval-error:0.244275\ttrain-error:0.045993\n",
      "[68]\teval-error:0.248092\ttrain-error:0.045993\n",
      "[69]\teval-error:0.244275\ttrain-error:0.045993\n",
      "[0]\teval-error:0.316794\ttrain-error:0.090273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\teval-error:0.28626\ttrain-error:0.071347\n",
      "[2]\teval-error:0.263359\ttrain-error:0.05992\n",
      "[3]\teval-error:0.278626\ttrain-error:0.054349\n",
      "[4]\teval-error:0.263359\ttrain-error:0.049921\n",
      "[5]\teval-error:0.255725\ttrain-error:0.048922\n",
      "[6]\teval-error:0.263359\ttrain-error:0.047493\n",
      "[7]\teval-error:0.259542\ttrain-error:0.04685\n",
      "[8]\teval-error:0.263359\ttrain-error:0.046565\n",
      "[9]\teval-error:0.270992\ttrain-error:0.046351\n",
      "[10]\teval-error:0.270992\ttrain-error:0.046279\n",
      "[11]\teval-error:0.251908\ttrain-error:0.046279\n",
      "[12]\teval-error:0.255725\ttrain-error:0.046279\n",
      "[13]\teval-error:0.255725\ttrain-error:0.046208\n",
      "[14]\teval-error:0.263359\ttrain-error:0.046136\n",
      "[15]\teval-error:0.259542\ttrain-error:0.046136\n",
      "[16]\teval-error:0.263359\ttrain-error:0.046208\n",
      "[17]\teval-error:0.255725\ttrain-error:0.046208\n",
      "[18]\teval-error:0.267176\ttrain-error:0.046136\n",
      "[19]\teval-error:0.278626\ttrain-error:0.046136\n",
      "[20]\teval-error:0.263359\ttrain-error:0.046136\n",
      "[21]\teval-error:0.274809\ttrain-error:0.046136\n",
      "[22]\teval-error:0.263359\ttrain-error:0.046136\n",
      "[23]\teval-error:0.263359\ttrain-error:0.046136\n",
      "[24]\teval-error:0.263359\ttrain-error:0.046136\n",
      "[25]\teval-error:0.251908\ttrain-error:0.046136\n",
      "[26]\teval-error:0.267176\ttrain-error:0.046136\n",
      "[27]\teval-error:0.267176\ttrain-error:0.046136\n",
      "[28]\teval-error:0.267176\ttrain-error:0.046136\n",
      "[29]\teval-error:0.255725\ttrain-error:0.046136\n",
      "[30]\teval-error:0.255725\ttrain-error:0.046136\n",
      "[31]\teval-error:0.263359\ttrain-error:0.046136\n",
      "[32]\teval-error:0.274809\ttrain-error:0.046136\n",
      "[33]\teval-error:0.270992\ttrain-error:0.046136\n",
      "[34]\teval-error:0.267176\ttrain-error:0.046136\n",
      "[35]\teval-error:0.274809\ttrain-error:0.046136\n",
      "[36]\teval-error:0.274809\ttrain-error:0.046136\n",
      "[37]\teval-error:0.263359\ttrain-error:0.046136\n",
      "[38]\teval-error:0.270992\ttrain-error:0.046136\n",
      "[39]\teval-error:0.267176\ttrain-error:0.046136\n",
      "[40]\teval-error:0.267176\ttrain-error:0.046136\n",
      "[41]\teval-error:0.274809\ttrain-error:0.046136\n",
      "[42]\teval-error:0.278626\ttrain-error:0.046136\n",
      "[43]\teval-error:0.270992\ttrain-error:0.046136\n",
      "[44]\teval-error:0.270992\ttrain-error:0.046136\n",
      "[45]\teval-error:0.270992\ttrain-error:0.046136\n",
      "[46]\teval-error:0.263359\ttrain-error:0.046136\n",
      "[47]\teval-error:0.267176\ttrain-error:0.046136\n",
      "[48]\teval-error:0.259542\ttrain-error:0.046136\n",
      "[49]\teval-error:0.263359\ttrain-error:0.046136\n",
      "[50]\teval-error:0.263359\ttrain-error:0.046136\n",
      "[51]\teval-error:0.263359\ttrain-error:0.046136\n",
      "[52]\teval-error:0.278626\ttrain-error:0.046136\n",
      "[53]\teval-error:0.270992\ttrain-error:0.046136\n",
      "[54]\teval-error:0.274809\ttrain-error:0.046136\n",
      "[55]\teval-error:0.267176\ttrain-error:0.046136\n",
      "[56]\teval-error:0.278626\ttrain-error:0.046136\n",
      "[57]\teval-error:0.270992\ttrain-error:0.046136\n",
      "[58]\teval-error:0.259542\ttrain-error:0.046136\n",
      "[59]\teval-error:0.263359\ttrain-error:0.046136\n",
      "[60]\teval-error:0.263359\ttrain-error:0.046136\n",
      "[61]\teval-error:0.274809\ttrain-error:0.046136\n",
      "[62]\teval-error:0.259542\ttrain-error:0.046136\n",
      "[63]\teval-error:0.274809\ttrain-error:0.046136\n",
      "[64]\teval-error:0.267176\ttrain-error:0.046136\n",
      "[65]\teval-error:0.274809\ttrain-error:0.046136\n",
      "[66]\teval-error:0.270992\ttrain-error:0.046136\n",
      "[67]\teval-error:0.267176\ttrain-error:0.046136\n",
      "[68]\teval-error:0.278626\ttrain-error:0.046136\n",
      "[69]\teval-error:0.267176\ttrain-error:0.046136\n",
      "[0]\teval-error:0.324427\ttrain-error:0.090273\n",
      "[1]\teval-error:0.251908\ttrain-error:0.069561\n",
      "[2]\teval-error:0.251908\ttrain-error:0.058206\n",
      "[3]\teval-error:0.274809\ttrain-error:0.053707\n",
      "[4]\teval-error:0.274809\ttrain-error:0.050707\n",
      "[5]\teval-error:0.267176\ttrain-error:0.048564\n",
      "[6]\teval-error:0.255725\ttrain-error:0.047422\n",
      "[7]\teval-error:0.259542\ttrain-error:0.046708\n",
      "[8]\teval-error:0.267176\ttrain-error:0.046279\n",
      "[9]\teval-error:0.244275\ttrain-error:0.046065\n",
      "[10]\teval-error:0.248092\ttrain-error:0.045993\n",
      "[11]\teval-error:0.259542\ttrain-error:0.045922\n",
      "[12]\teval-error:0.270992\ttrain-error:0.045779\n",
      "[13]\teval-error:0.251908\ttrain-error:0.045708\n",
      "[14]\teval-error:0.244275\ttrain-error:0.045636\n",
      "[15]\teval-error:0.251908\ttrain-error:0.045565\n",
      "[16]\teval-error:0.236641\ttrain-error:0.045565\n",
      "[17]\teval-error:0.240458\ttrain-error:0.045494\n",
      "[18]\teval-error:0.251908\ttrain-error:0.045565\n",
      "[19]\teval-error:0.244275\ttrain-error:0.045494\n",
      "[20]\teval-error:0.248092\ttrain-error:0.045565\n",
      "[21]\teval-error:0.251908\ttrain-error:0.045565\n",
      "[22]\teval-error:0.251908\ttrain-error:0.045494\n",
      "[23]\teval-error:0.236641\ttrain-error:0.045494\n",
      "[24]\teval-error:0.236641\ttrain-error:0.045494\n",
      "[25]\teval-error:0.232824\ttrain-error:0.045494\n",
      "[26]\teval-error:0.244275\ttrain-error:0.045494\n",
      "[27]\teval-error:0.248092\ttrain-error:0.045494\n",
      "[28]\teval-error:0.240458\ttrain-error:0.045494\n",
      "[29]\teval-error:0.248092\ttrain-error:0.045494\n",
      "[30]\teval-error:0.244275\ttrain-error:0.045494\n",
      "[31]\teval-error:0.240458\ttrain-error:0.045494\n",
      "[32]\teval-error:0.240458\ttrain-error:0.045494\n",
      "[33]\teval-error:0.236641\ttrain-error:0.045494\n",
      "[34]\teval-error:0.236641\ttrain-error:0.045494\n",
      "[35]\teval-error:0.248092\ttrain-error:0.045494\n",
      "[36]\teval-error:0.232824\ttrain-error:0.045494\n",
      "[37]\teval-error:0.232824\ttrain-error:0.045494\n",
      "[38]\teval-error:0.240458\ttrain-error:0.045494\n",
      "[39]\teval-error:0.236641\ttrain-error:0.045494\n",
      "[40]\teval-error:0.244275\ttrain-error:0.045494\n",
      "[41]\teval-error:0.240458\ttrain-error:0.045494\n",
      "[42]\teval-error:0.232824\ttrain-error:0.045494\n",
      "[43]\teval-error:0.240458\ttrain-error:0.045494\n",
      "[44]\teval-error:0.240458\ttrain-error:0.045494\n",
      "[45]\teval-error:0.225191\ttrain-error:0.045494\n",
      "[46]\teval-error:0.229008\ttrain-error:0.045494\n",
      "[47]\teval-error:0.232824\ttrain-error:0.045494\n",
      "[48]\teval-error:0.229008\ttrain-error:0.045494\n",
      "[49]\teval-error:0.225191\ttrain-error:0.045494\n",
      "[50]\teval-error:0.225191\ttrain-error:0.045494\n",
      "[51]\teval-error:0.236641\ttrain-error:0.045494\n",
      "[52]\teval-error:0.240458\ttrain-error:0.045494\n",
      "[53]\teval-error:0.236641\ttrain-error:0.045494\n",
      "[54]\teval-error:0.236641\ttrain-error:0.045494\n",
      "[55]\teval-error:0.240458\ttrain-error:0.045494\n",
      "[56]\teval-error:0.221374\ttrain-error:0.045494\n",
      "[57]\teval-error:0.221374\ttrain-error:0.045494\n",
      "[58]\teval-error:0.225191\ttrain-error:0.045494\n",
      "[59]\teval-error:0.232824\ttrain-error:0.045494\n",
      "[60]\teval-error:0.232824\ttrain-error:0.045494\n",
      "[61]\teval-error:0.232824\ttrain-error:0.045494\n",
      "[62]\teval-error:0.240458\ttrain-error:0.045494\n",
      "[63]\teval-error:0.236641\ttrain-error:0.045494\n",
      "[64]\teval-error:0.236641\ttrain-error:0.045494\n",
      "[65]\teval-error:0.240458\ttrain-error:0.045494\n",
      "[66]\teval-error:0.236641\ttrain-error:0.045494\n",
      "[67]\teval-error:0.240458\ttrain-error:0.045494\n",
      "[68]\teval-error:0.236641\ttrain-error:0.045494\n",
      "[69]\teval-error:0.244275\ttrain-error:0.045494\n",
      "[0]\teval-error:0.318182\ttrain-error:0.299406\n",
      "[1]\teval-error:0.295455\ttrain-error:0.297964\n",
      "[2]\teval-error:0.295455\ttrain-error:0.297244\n",
      "[3]\teval-error:0.318182\ttrain-error:0.295803\n",
      "[4]\teval-error:0.306818\ttrain-error:0.294361\n",
      "[5]\teval-error:0.295455\ttrain-error:0.293641\n",
      "[6]\teval-error:0.295455\ttrain-error:0.29328\n",
      "[7]\teval-error:0.318182\ttrain-error:0.2931\n",
      "[8]\teval-error:0.318182\ttrain-error:0.2931\n",
      "[9]\teval-error:0.295455\ttrain-error:0.29292\n",
      "[10]\teval-error:0.295455\ttrain-error:0.2931\n",
      "[11]\teval-error:0.295455\ttrain-error:0.29292\n",
      "[12]\teval-error:0.318182\ttrain-error:0.29274\n",
      "[13]\teval-error:0.284091\ttrain-error:0.2931\n",
      "[14]\teval-error:0.284091\ttrain-error:0.2931\n",
      "[15]\teval-error:0.284091\ttrain-error:0.29274\n",
      "[16]\teval-error:0.284091\ttrain-error:0.29274\n",
      "[17]\teval-error:0.295455\ttrain-error:0.29274\n",
      "[18]\teval-error:0.284091\ttrain-error:0.29274\n",
      "[19]\teval-error:0.295455\ttrain-error:0.29274\n",
      "[20]\teval-error:0.318182\ttrain-error:0.29274\n",
      "[21]\teval-error:0.295455\ttrain-error:0.29256\n",
      "[22]\teval-error:0.284091\ttrain-error:0.29256\n",
      "[23]\teval-error:0.284091\ttrain-error:0.29256\n",
      "[24]\teval-error:0.295455\ttrain-error:0.29238\n",
      "[25]\teval-error:0.295455\ttrain-error:0.29238\n",
      "[26]\teval-error:0.295455\ttrain-error:0.29238\n",
      "[27]\teval-error:0.284091\ttrain-error:0.29238\n",
      "[28]\teval-error:0.306818\ttrain-error:0.29238\n",
      "[29]\teval-error:0.295455\ttrain-error:0.29238\n",
      "[30]\teval-error:0.284091\ttrain-error:0.29238\n",
      "[31]\teval-error:0.295455\ttrain-error:0.29238\n",
      "[32]\teval-error:0.284091\ttrain-error:0.29238\n",
      "[33]\teval-error:0.306818\ttrain-error:0.29238\n",
      "[34]\teval-error:0.284091\ttrain-error:0.29238\n",
      "[35]\teval-error:0.306818\ttrain-error:0.29238\n",
      "[36]\teval-error:0.295455\ttrain-error:0.29238\n",
      "[37]\teval-error:0.284091\ttrain-error:0.29238\n",
      "[38]\teval-error:0.284091\ttrain-error:0.29238\n",
      "[39]\teval-error:0.284091\ttrain-error:0.29238\n",
      "[40]\teval-error:0.306818\ttrain-error:0.29238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41]\teval-error:0.318182\ttrain-error:0.29238\n",
      "[42]\teval-error:0.318182\ttrain-error:0.29238\n",
      "[43]\teval-error:0.295455\ttrain-error:0.29238\n",
      "[44]\teval-error:0.295455\ttrain-error:0.29238\n",
      "[45]\teval-error:0.318182\ttrain-error:0.29238\n",
      "[46]\teval-error:0.295455\ttrain-error:0.29238\n",
      "[47]\teval-error:0.295455\ttrain-error:0.29238\n",
      "[48]\teval-error:0.295455\ttrain-error:0.29238\n",
      "[49]\teval-error:0.295455\ttrain-error:0.29238\n",
      "[50]\teval-error:0.295455\ttrain-error:0.29238\n",
      "[51]\teval-error:0.295455\ttrain-error:0.29238\n",
      "[52]\teval-error:0.295455\ttrain-error:0.29238\n",
      "[53]\teval-error:0.295455\ttrain-error:0.29238\n",
      "[54]\teval-error:0.295455\ttrain-error:0.29238\n",
      "[55]\teval-error:0.306818\ttrain-error:0.29238\n",
      "[56]\teval-error:0.318182\ttrain-error:0.29238\n",
      "[57]\teval-error:0.284091\ttrain-error:0.29238\n",
      "[58]\teval-error:0.295455\ttrain-error:0.29238\n",
      "[59]\teval-error:0.284091\ttrain-error:0.29238\n",
      "[60]\teval-error:0.295455\ttrain-error:0.29238\n",
      "[61]\teval-error:0.284091\ttrain-error:0.29238\n",
      "[62]\teval-error:0.284091\ttrain-error:0.29238\n",
      "[63]\teval-error:0.295455\ttrain-error:0.29238\n",
      "[64]\teval-error:0.295455\ttrain-error:0.29238\n",
      "[65]\teval-error:0.284091\ttrain-error:0.29238\n",
      "[66]\teval-error:0.284091\ttrain-error:0.29238\n",
      "[67]\teval-error:0.295455\ttrain-error:0.29238\n",
      "[68]\teval-error:0.295455\ttrain-error:0.29238\n",
      "[69]\teval-error:0.295455\ttrain-error:0.29238\n",
      "[0]\teval-error:0.738636\ttrain-error:0.346064\n",
      "[1]\teval-error:0.738636\ttrain-error:0.344983\n",
      "[2]\teval-error:0.738636\ttrain-error:0.344803\n",
      "[3]\teval-error:0.738636\ttrain-error:0.344803\n",
      "[4]\teval-error:0.738636\ttrain-error:0.345163\n",
      "[5]\teval-error:0.738636\ttrain-error:0.344623\n",
      "[6]\teval-error:0.738636\ttrain-error:0.344623\n",
      "[7]\teval-error:0.738636\ttrain-error:0.344623\n",
      "[8]\teval-error:0.738636\ttrain-error:0.344262\n",
      "[9]\teval-error:0.738636\ttrain-error:0.344262\n",
      "[10]\teval-error:0.738636\ttrain-error:0.344442\n",
      "[11]\teval-error:0.738636\ttrain-error:0.344082\n",
      "[12]\teval-error:0.738636\ttrain-error:0.344082\n",
      "[13]\teval-error:0.738636\ttrain-error:0.344082\n",
      "[14]\teval-error:0.738636\ttrain-error:0.343902\n",
      "[15]\teval-error:0.738636\ttrain-error:0.343902\n",
      "[16]\teval-error:0.738636\ttrain-error:0.343902\n",
      "[17]\teval-error:0.738636\ttrain-error:0.344082\n",
      "[18]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[19]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[20]\teval-error:0.738636\ttrain-error:0.343902\n",
      "[21]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[22]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[23]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[24]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[25]\teval-error:0.738636\ttrain-error:0.343902\n",
      "[26]\teval-error:0.738636\ttrain-error:0.343902\n",
      "[27]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[28]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[29]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[30]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[31]\teval-error:0.738636\ttrain-error:0.343902\n",
      "[32]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[33]\teval-error:0.738636\ttrain-error:0.343902\n",
      "[34]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[35]\teval-error:0.738636\ttrain-error:0.343902\n",
      "[36]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[37]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[38]\teval-error:0.738636\ttrain-error:0.343902\n",
      "[39]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[40]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[41]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[42]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[43]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[44]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[45]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[46]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[47]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[48]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[49]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[50]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[51]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[52]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[53]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[54]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[55]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[56]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[57]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[58]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[59]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[60]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[61]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[62]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[63]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[64]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[65]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[66]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[67]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[68]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[69]\teval-error:0.738636\ttrain-error:0.343722\n",
      "[0]\teval-error:0.272727\ttrain-error:0.319222\n",
      "[1]\teval-error:0.272727\ttrain-error:0.318141\n",
      "[2]\teval-error:0.272727\ttrain-error:0.317961\n",
      "[3]\teval-error:0.272727\ttrain-error:0.317961\n",
      "[4]\teval-error:0.272727\ttrain-error:0.3176\n",
      "[5]\teval-error:0.272727\ttrain-error:0.317961\n",
      "[6]\teval-error:0.272727\ttrain-error:0.317781\n",
      "[7]\teval-error:0.272727\ttrain-error:0.3176\n",
      "[8]\teval-error:0.272727\ttrain-error:0.31742\n",
      "[9]\teval-error:0.272727\ttrain-error:0.3176\n",
      "[10]\teval-error:0.272727\ttrain-error:0.31742\n",
      "[11]\teval-error:0.272727\ttrain-error:0.31742\n",
      "[12]\teval-error:0.272727\ttrain-error:0.31724\n",
      "[13]\teval-error:0.272727\ttrain-error:0.31724\n",
      "[14]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[15]\teval-error:0.272727\ttrain-error:0.31724\n",
      "[16]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[17]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[18]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[19]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[20]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[21]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[22]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[23]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[24]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[25]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[26]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[27]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[28]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[29]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[30]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[31]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[32]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[33]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[34]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[35]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[36]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[37]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[38]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[39]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[40]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[41]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[42]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[43]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[44]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[45]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[46]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[47]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[48]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[49]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[50]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[51]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[52]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[53]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[54]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[55]\teval-error:0.272727\ttrain-error:0.31706\n",
      "[56]\teval-error:0.272727\ttrain-error:0.31688\n",
      "[57]\teval-error:0.272727\ttrain-error:0.31688\n",
      "[58]\teval-error:0.272727\ttrain-error:0.31688\n",
      "[59]\teval-error:0.272727\ttrain-error:0.31688\n",
      "[60]\teval-error:0.272727\ttrain-error:0.31688\n",
      "[61]\teval-error:0.272727\ttrain-error:0.31688\n",
      "[62]\teval-error:0.272727\ttrain-error:0.31688\n",
      "[63]\teval-error:0.272727\ttrain-error:0.31688\n",
      "[64]\teval-error:0.272727\ttrain-error:0.31688\n",
      "[65]\teval-error:0.272727\ttrain-error:0.31688\n",
      "[66]\teval-error:0.272727\ttrain-error:0.31688\n",
      "[67]\teval-error:0.272727\ttrain-error:0.31688\n",
      "[68]\teval-error:0.272727\ttrain-error:0.31688\n",
      "[69]\teval-error:0.272727\ttrain-error:0.31688\n",
      "[0]\teval-error:0.318182\ttrain-error:0.296343\n",
      "[1]\teval-error:0.318182\ttrain-error:0.293461\n",
      "[2]\teval-error:0.329545\ttrain-error:0.289497\n",
      "[3]\teval-error:0.329545\ttrain-error:0.286795\n",
      "[4]\teval-error:0.318182\ttrain-error:0.284453\n",
      "[5]\teval-error:0.318182\ttrain-error:0.282472\n",
      "[6]\teval-error:0.306818\ttrain-error:0.281931\n",
      "[7]\teval-error:0.306818\ttrain-error:0.281211\n",
      "[8]\teval-error:0.318182\ttrain-error:0.28067\n",
      "[9]\teval-error:0.295455\ttrain-error:0.28031\n",
      "[10]\teval-error:0.306818\ttrain-error:0.279769\n",
      "[11]\teval-error:0.318182\ttrain-error:0.279769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12]\teval-error:0.284091\ttrain-error:0.279409\n",
      "[13]\teval-error:0.295455\ttrain-error:0.279049\n",
      "[14]\teval-error:0.284091\ttrain-error:0.278689\n",
      "[15]\teval-error:0.284091\ttrain-error:0.278869\n",
      "[16]\teval-error:0.306818\ttrain-error:0.278508\n",
      "[17]\teval-error:0.295455\ttrain-error:0.278689\n",
      "[18]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[19]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[20]\teval-error:0.318182\ttrain-error:0.278508\n",
      "[21]\teval-error:0.306818\ttrain-error:0.278508\n",
      "[22]\teval-error:0.306818\ttrain-error:0.278508\n",
      "[23]\teval-error:0.318182\ttrain-error:0.278508\n",
      "[24]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[25]\teval-error:0.306818\ttrain-error:0.278508\n",
      "[26]\teval-error:0.306818\ttrain-error:0.278508\n",
      "[27]\teval-error:0.306818\ttrain-error:0.278508\n",
      "[28]\teval-error:0.306818\ttrain-error:0.278689\n",
      "[29]\teval-error:0.318182\ttrain-error:0.278508\n",
      "[30]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[31]\teval-error:0.306818\ttrain-error:0.278508\n",
      "[32]\teval-error:0.306818\ttrain-error:0.278508\n",
      "[33]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[34]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[35]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[36]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[37]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[38]\teval-error:0.306818\ttrain-error:0.278508\n",
      "[39]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[40]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[41]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[42]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[43]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[44]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[45]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[46]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[47]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[48]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[49]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[50]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[51]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[52]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[53]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[54]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[55]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[56]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[57]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[58]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[59]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[60]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[61]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[62]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[63]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[64]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[65]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[66]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[67]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[68]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[69]\teval-error:0.295455\ttrain-error:0.278508\n",
      "[0]\teval-error:0.340426\ttrain-error:0.285696\n",
      "[1]\teval-error:0.287234\ttrain-error:0.283759\n",
      "[2]\teval-error:0.351064\ttrain-error:0.283501\n",
      "[3]\teval-error:0.297872\ttrain-error:0.281952\n",
      "[4]\teval-error:0.287234\ttrain-error:0.281565\n",
      "[5]\teval-error:0.319149\ttrain-error:0.281436\n",
      "[6]\teval-error:0.319149\ttrain-error:0.28079\n",
      "[7]\teval-error:0.319149\ttrain-error:0.280274\n",
      "[8]\teval-error:0.297872\ttrain-error:0.280403\n",
      "[9]\teval-error:0.329787\ttrain-error:0.280145\n",
      "[10]\teval-error:0.319149\ttrain-error:0.280274\n",
      "[11]\teval-error:0.276596\ttrain-error:0.280274\n",
      "[12]\teval-error:0.308511\ttrain-error:0.280145\n",
      "[13]\teval-error:0.297872\ttrain-error:0.280274\n",
      "[14]\teval-error:0.308511\ttrain-error:0.280015\n",
      "[15]\teval-error:0.287234\ttrain-error:0.279886\n",
      "[16]\teval-error:0.319149\ttrain-error:0.279886\n",
      "[17]\teval-error:0.287234\ttrain-error:0.279886\n",
      "[18]\teval-error:0.319149\ttrain-error:0.279886\n",
      "[19]\teval-error:0.319149\ttrain-error:0.279886\n",
      "[20]\teval-error:0.308511\ttrain-error:0.279886\n",
      "[21]\teval-error:0.308511\ttrain-error:0.279886\n",
      "[22]\teval-error:0.287234\ttrain-error:0.279886\n",
      "[23]\teval-error:0.287234\ttrain-error:0.279886\n",
      "[24]\teval-error:0.329787\ttrain-error:0.279886\n",
      "[25]\teval-error:0.287234\ttrain-error:0.279886\n",
      "[26]\teval-error:0.297872\ttrain-error:0.279886\n",
      "[27]\teval-error:0.297872\ttrain-error:0.279886\n",
      "[28]\teval-error:0.297872\ttrain-error:0.279886\n",
      "[29]\teval-error:0.297872\ttrain-error:0.279886\n",
      "[30]\teval-error:0.319149\ttrain-error:0.279886\n",
      "[31]\teval-error:0.265957\ttrain-error:0.279886\n",
      "[32]\teval-error:0.308511\ttrain-error:0.279886\n",
      "[33]\teval-error:0.297872\ttrain-error:0.279886\n",
      "[34]\teval-error:0.319149\ttrain-error:0.279886\n",
      "[35]\teval-error:0.308511\ttrain-error:0.279886\n",
      "[36]\teval-error:0.297872\ttrain-error:0.279886\n",
      "[37]\teval-error:0.308511\ttrain-error:0.279886\n",
      "[38]\teval-error:0.319149\ttrain-error:0.279886\n",
      "[39]\teval-error:0.276596\ttrain-error:0.279886\n",
      "[40]\teval-error:0.308511\ttrain-error:0.279886\n",
      "[41]\teval-error:0.308511\ttrain-error:0.279886\n",
      "[42]\teval-error:0.276596\ttrain-error:0.279886\n",
      "[43]\teval-error:0.287234\ttrain-error:0.279886\n",
      "[44]\teval-error:0.308511\ttrain-error:0.279886\n",
      "[45]\teval-error:0.276596\ttrain-error:0.279886\n",
      "[46]\teval-error:0.308511\ttrain-error:0.279886\n",
      "[47]\teval-error:0.297872\ttrain-error:0.279886\n",
      "[48]\teval-error:0.319149\ttrain-error:0.279886\n",
      "[49]\teval-error:0.297872\ttrain-error:0.279886\n",
      "[50]\teval-error:0.276596\ttrain-error:0.279886\n",
      "[51]\teval-error:0.297872\ttrain-error:0.279886\n",
      "[52]\teval-error:0.297872\ttrain-error:0.279886\n",
      "[53]\teval-error:0.287234\ttrain-error:0.279886\n",
      "[54]\teval-error:0.297872\ttrain-error:0.279886\n",
      "[55]\teval-error:0.287234\ttrain-error:0.279886\n",
      "[56]\teval-error:0.287234\ttrain-error:0.279886\n",
      "[57]\teval-error:0.276596\ttrain-error:0.279886\n",
      "[58]\teval-error:0.308511\ttrain-error:0.279886\n",
      "[59]\teval-error:0.287234\ttrain-error:0.279886\n",
      "[60]\teval-error:0.308511\ttrain-error:0.279886\n",
      "[61]\teval-error:0.265957\ttrain-error:0.279886\n",
      "[62]\teval-error:0.308511\ttrain-error:0.279886\n",
      "[63]\teval-error:0.287234\ttrain-error:0.279886\n",
      "[64]\teval-error:0.308511\ttrain-error:0.279886\n",
      "[65]\teval-error:0.297872\ttrain-error:0.279886\n",
      "[66]\teval-error:0.276596\ttrain-error:0.279886\n",
      "[67]\teval-error:0.308511\ttrain-error:0.279886\n",
      "[68]\teval-error:0.287234\ttrain-error:0.279886\n",
      "[69]\teval-error:0.276596\ttrain-error:0.279886\n",
      "[0]\teval-error:0.659574\ttrain-error:0.323909\n",
      "[1]\teval-error:0.659574\ttrain-error:0.323651\n",
      "[2]\teval-error:0.659574\ttrain-error:0.323264\n",
      "[3]\teval-error:0.659574\ttrain-error:0.322747\n",
      "[4]\teval-error:0.659574\ttrain-error:0.323135\n",
      "[5]\teval-error:0.659574\ttrain-error:0.322747\n",
      "[6]\teval-error:0.659574\ttrain-error:0.322747\n",
      "[7]\teval-error:0.659574\ttrain-error:0.322747\n",
      "[8]\teval-error:0.659574\ttrain-error:0.322618\n",
      "[9]\teval-error:0.659574\ttrain-error:0.322618\n",
      "[10]\teval-error:0.659574\ttrain-error:0.322618\n",
      "[11]\teval-error:0.659574\ttrain-error:0.322618\n",
      "[12]\teval-error:0.659574\ttrain-error:0.322489\n",
      "[13]\teval-error:0.659574\ttrain-error:0.322618\n",
      "[14]\teval-error:0.659574\ttrain-error:0.322618\n",
      "[15]\teval-error:0.659574\ttrain-error:0.322618\n",
      "[16]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[17]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[18]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[19]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[20]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[21]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[22]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[23]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[24]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[25]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[26]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[27]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[28]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[29]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[30]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[31]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[32]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[33]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[34]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[35]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[36]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[37]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[38]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[39]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[40]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[41]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[42]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[43]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[44]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[45]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[46]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[47]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[48]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[49]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[50]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[51]\teval-error:0.659574\ttrain-error:0.32236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[53]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[54]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[55]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[56]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[57]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[58]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[59]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[60]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[61]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[62]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[63]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[64]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[65]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[66]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[67]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[68]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[69]\teval-error:0.659574\ttrain-error:0.32236\n",
      "[0]\teval-error:0.56383\ttrain-error:0.31668\n",
      "[1]\teval-error:0.56383\ttrain-error:0.316292\n",
      "[2]\teval-error:0.56383\ttrain-error:0.315905\n",
      "[3]\teval-error:0.56383\ttrain-error:0.315647\n",
      "[4]\teval-error:0.56383\ttrain-error:0.315647\n",
      "[5]\teval-error:0.56383\ttrain-error:0.315776\n",
      "[6]\teval-error:0.56383\ttrain-error:0.315389\n",
      "[7]\teval-error:0.56383\ttrain-error:0.315259\n",
      "[8]\teval-error:0.56383\ttrain-error:0.315259\n",
      "[9]\teval-error:0.56383\ttrain-error:0.315259\n",
      "[10]\teval-error:0.56383\ttrain-error:0.315259\n",
      "[11]\teval-error:0.56383\ttrain-error:0.31513\n",
      "[12]\teval-error:0.56383\ttrain-error:0.31513\n",
      "[13]\teval-error:0.56383\ttrain-error:0.315259\n",
      "[14]\teval-error:0.56383\ttrain-error:0.31513\n",
      "[15]\teval-error:0.56383\ttrain-error:0.31513\n",
      "[16]\teval-error:0.56383\ttrain-error:0.31513\n",
      "[17]\teval-error:0.56383\ttrain-error:0.31513\n",
      "[18]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[19]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[20]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[21]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[22]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[23]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[24]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[25]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[26]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[27]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[28]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[29]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[30]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[31]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[32]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[33]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[34]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[35]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[36]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[37]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[38]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[39]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[40]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[41]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[42]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[43]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[44]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[45]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[46]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[47]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[48]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[49]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[50]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[51]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[52]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[53]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[54]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[55]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[56]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[57]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[58]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[59]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[60]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[61]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[62]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[63]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[64]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[65]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[66]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[67]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[68]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[69]\teval-error:0.56383\ttrain-error:0.315001\n",
      "[0]\teval-error:0.297872\ttrain-error:0.279112\n",
      "[1]\teval-error:0.276596\ttrain-error:0.276659\n",
      "[2]\teval-error:0.276596\ttrain-error:0.274464\n",
      "[3]\teval-error:0.297872\ttrain-error:0.270849\n",
      "[4]\teval-error:0.287234\ttrain-error:0.270075\n",
      "[5]\teval-error:0.297872\ttrain-error:0.268655\n",
      "[6]\teval-error:0.287234\ttrain-error:0.267235\n",
      "[7]\teval-error:0.287234\ttrain-error:0.266847\n",
      "[8]\teval-error:0.287234\ttrain-error:0.267235\n",
      "[9]\teval-error:0.287234\ttrain-error:0.266202\n",
      "[10]\teval-error:0.319149\ttrain-error:0.266073\n",
      "[11]\teval-error:0.308511\ttrain-error:0.265556\n",
      "[12]\teval-error:0.297872\ttrain-error:0.265686\n",
      "[13]\teval-error:0.319149\ttrain-error:0.265556\n",
      "[14]\teval-error:0.319149\ttrain-error:0.265686\n",
      "[15]\teval-error:0.308511\ttrain-error:0.265298\n",
      "[16]\teval-error:0.308511\ttrain-error:0.265298\n",
      "[17]\teval-error:0.308511\ttrain-error:0.265298\n",
      "[18]\teval-error:0.308511\ttrain-error:0.265298\n",
      "[19]\teval-error:0.308511\ttrain-error:0.265298\n",
      "[20]\teval-error:0.319149\ttrain-error:0.265298\n",
      "[21]\teval-error:0.308511\ttrain-error:0.265298\n",
      "[22]\teval-error:0.308511\ttrain-error:0.265298\n",
      "[23]\teval-error:0.308511\ttrain-error:0.265298\n",
      "[24]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[25]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[26]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[27]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[28]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[29]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[30]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[31]\teval-error:0.308511\ttrain-error:0.265298\n",
      "[32]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[33]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[34]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[35]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[36]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[37]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[38]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[39]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[40]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[41]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[42]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[43]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[44]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[45]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[46]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[47]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[48]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[49]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[50]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[51]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[52]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[53]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[54]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[55]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[56]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[57]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[58]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[59]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[60]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[61]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[62]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[63]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[64]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[65]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[66]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[67]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[68]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[69]\teval-error:0.297872\ttrain-error:0.265298\n",
      "[0]\teval-error:0.343511\ttrain-error:0.260534\n",
      "[1]\teval-error:0.335878\ttrain-error:0.25982\n",
      "[2]\teval-error:0.332061\ttrain-error:0.259249\n",
      "[3]\teval-error:0.328244\ttrain-error:0.258892\n",
      "[4]\teval-error:0.328244\ttrain-error:0.25882\n",
      "[5]\teval-error:0.332061\ttrain-error:0.25832\n",
      "[6]\teval-error:0.328244\ttrain-error:0.258106\n",
      "[7]\teval-error:0.335878\ttrain-error:0.25782\n",
      "[8]\teval-error:0.335878\ttrain-error:0.257749\n",
      "[9]\teval-error:0.343511\ttrain-error:0.257606\n",
      "[10]\teval-error:0.335878\ttrain-error:0.257535\n",
      "[11]\teval-error:0.335878\ttrain-error:0.25732\n",
      "[12]\teval-error:0.335878\ttrain-error:0.25732\n",
      "[13]\teval-error:0.332061\ttrain-error:0.257249\n",
      "[14]\teval-error:0.335878\ttrain-error:0.257249\n",
      "[15]\teval-error:0.332061\ttrain-error:0.257178\n",
      "[16]\teval-error:0.335878\ttrain-error:0.257178\n",
      "[17]\teval-error:0.335878\ttrain-error:0.257178\n",
      "[18]\teval-error:0.335878\ttrain-error:0.257106\n",
      "[19]\teval-error:0.339695\ttrain-error:0.257106\n",
      "[20]\teval-error:0.335878\ttrain-error:0.257106\n",
      "[21]\teval-error:0.339695\ttrain-error:0.257106\n",
      "[22]\teval-error:0.335878\ttrain-error:0.257106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23]\teval-error:0.328244\ttrain-error:0.257106\n",
      "[24]\teval-error:0.332061\ttrain-error:0.257106\n",
      "[25]\teval-error:0.335878\ttrain-error:0.257106\n",
      "[26]\teval-error:0.332061\ttrain-error:0.257106\n",
      "[27]\teval-error:0.328244\ttrain-error:0.257106\n",
      "[28]\teval-error:0.328244\ttrain-error:0.257106\n",
      "[29]\teval-error:0.339695\ttrain-error:0.257106\n",
      "[30]\teval-error:0.335878\ttrain-error:0.257106\n",
      "[31]\teval-error:0.332061\ttrain-error:0.257106\n",
      "[32]\teval-error:0.328244\ttrain-error:0.257106\n",
      "[33]\teval-error:0.332061\ttrain-error:0.257106\n",
      "[34]\teval-error:0.335878\ttrain-error:0.257106\n",
      "[35]\teval-error:0.328244\ttrain-error:0.257106\n",
      "[36]\teval-error:0.324427\ttrain-error:0.257106\n",
      "[37]\teval-error:0.324427\ttrain-error:0.257106\n",
      "[38]\teval-error:0.328244\ttrain-error:0.257106\n",
      "[39]\teval-error:0.332061\ttrain-error:0.257106\n",
      "[40]\teval-error:0.332061\ttrain-error:0.257106\n",
      "[41]\teval-error:0.335878\ttrain-error:0.257106\n",
      "[42]\teval-error:0.332061\ttrain-error:0.257106\n",
      "[43]\teval-error:0.332061\ttrain-error:0.257106\n",
      "[44]\teval-error:0.324427\ttrain-error:0.257106\n",
      "[45]\teval-error:0.335878\ttrain-error:0.257106\n",
      "[46]\teval-error:0.328244\ttrain-error:0.257106\n",
      "[47]\teval-error:0.328244\ttrain-error:0.257106\n",
      "[48]\teval-error:0.332061\ttrain-error:0.257106\n",
      "[49]\teval-error:0.328244\ttrain-error:0.257106\n",
      "[50]\teval-error:0.328244\ttrain-error:0.257106\n",
      "[51]\teval-error:0.335878\ttrain-error:0.257106\n",
      "[52]\teval-error:0.335878\ttrain-error:0.257106\n",
      "[53]\teval-error:0.335878\ttrain-error:0.257106\n",
      "[54]\teval-error:0.328244\ttrain-error:0.257106\n",
      "[55]\teval-error:0.332061\ttrain-error:0.257106\n",
      "[56]\teval-error:0.328244\ttrain-error:0.257106\n",
      "[57]\teval-error:0.328244\ttrain-error:0.257106\n",
      "[58]\teval-error:0.324427\ttrain-error:0.257106\n",
      "[59]\teval-error:0.324427\ttrain-error:0.257106\n",
      "[60]\teval-error:0.320611\ttrain-error:0.257106\n",
      "[61]\teval-error:0.320611\ttrain-error:0.257106\n",
      "[62]\teval-error:0.328244\ttrain-error:0.257106\n",
      "[63]\teval-error:0.324427\ttrain-error:0.257106\n",
      "[64]\teval-error:0.332061\ttrain-error:0.257106\n",
      "[65]\teval-error:0.328244\ttrain-error:0.257106\n",
      "[66]\teval-error:0.328244\ttrain-error:0.257106\n",
      "[67]\teval-error:0.332061\ttrain-error:0.257106\n",
      "[68]\teval-error:0.328244\ttrain-error:0.257106\n",
      "[69]\teval-error:0.332061\ttrain-error:0.257106\n",
      "[0]\teval-error:0.625954\ttrain-error:0.320668\n",
      "[1]\teval-error:0.625954\ttrain-error:0.320383\n",
      "[2]\teval-error:0.629771\ttrain-error:0.31974\n",
      "[3]\teval-error:0.629771\ttrain-error:0.319883\n",
      "[4]\teval-error:0.629771\ttrain-error:0.319811\n",
      "[5]\teval-error:0.629771\ttrain-error:0.31974\n",
      "[6]\teval-error:0.629771\ttrain-error:0.31974\n",
      "[7]\teval-error:0.629771\ttrain-error:0.319669\n",
      "[8]\teval-error:0.629771\ttrain-error:0.31974\n",
      "[9]\teval-error:0.629771\ttrain-error:0.31974\n",
      "[10]\teval-error:0.629771\ttrain-error:0.319669\n",
      "[11]\teval-error:0.629771\ttrain-error:0.319669\n",
      "[12]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[13]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[14]\teval-error:0.629771\ttrain-error:0.319669\n",
      "[15]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[16]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[17]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[18]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[19]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[20]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[21]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[22]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[23]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[24]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[25]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[26]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[27]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[28]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[29]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[30]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[31]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[32]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[33]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[34]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[35]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[36]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[37]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[38]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[39]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[40]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[41]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[42]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[43]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[44]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[45]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[46]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[47]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[48]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[49]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[50]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[51]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[52]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[53]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[54]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[55]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[56]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[57]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[58]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[59]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[60]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[61]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[62]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[63]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[64]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[65]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[66]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[67]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[68]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[69]\teval-error:0.629771\ttrain-error:0.319597\n",
      "[0]\teval-error:0.568702\ttrain-error:0.30917\n",
      "[1]\teval-error:0.568702\ttrain-error:0.309027\n",
      "[2]\teval-error:0.568702\ttrain-error:0.308884\n",
      "[3]\teval-error:0.568702\ttrain-error:0.308456\n",
      "[4]\teval-error:0.568702\ttrain-error:0.308527\n",
      "[5]\teval-error:0.568702\ttrain-error:0.308456\n",
      "[6]\teval-error:0.568702\ttrain-error:0.308456\n",
      "[7]\teval-error:0.568702\ttrain-error:0.308527\n",
      "[8]\teval-error:0.568702\ttrain-error:0.308527\n",
      "[9]\teval-error:0.568702\ttrain-error:0.308456\n",
      "[10]\teval-error:0.568702\ttrain-error:0.308456\n",
      "[11]\teval-error:0.568702\ttrain-error:0.308456\n",
      "[12]\teval-error:0.568702\ttrain-error:0.308456\n",
      "[13]\teval-error:0.568702\ttrain-error:0.308456\n",
      "[14]\teval-error:0.568702\ttrain-error:0.308456\n",
      "[15]\teval-error:0.568702\ttrain-error:0.308385\n",
      "[16]\teval-error:0.568702\ttrain-error:0.308385\n",
      "[17]\teval-error:0.568702\ttrain-error:0.308385\n",
      "[18]\teval-error:0.568702\ttrain-error:0.308385\n",
      "[19]\teval-error:0.568702\ttrain-error:0.308385\n",
      "[20]\teval-error:0.568702\ttrain-error:0.308385\n",
      "[21]\teval-error:0.568702\ttrain-error:0.308385\n",
      "[22]\teval-error:0.568702\ttrain-error:0.308385\n",
      "[23]\teval-error:0.568702\ttrain-error:0.308385\n",
      "[24]\teval-error:0.568702\ttrain-error:0.308385\n",
      "[25]\teval-error:0.568702\ttrain-error:0.308385\n",
      "[26]\teval-error:0.568702\ttrain-error:0.308385\n",
      "[27]\teval-error:0.568702\ttrain-error:0.308385\n",
      "[28]\teval-error:0.568702\ttrain-error:0.308385\n",
      "[29]\teval-error:0.568702\ttrain-error:0.308385\n",
      "[30]\teval-error:0.568702\ttrain-error:0.308385\n",
      "[31]\teval-error:0.568702\ttrain-error:0.308385\n",
      "[32]\teval-error:0.568702\ttrain-error:0.308385\n",
      "[33]\teval-error:0.568702\ttrain-error:0.308385\n",
      "[34]\teval-error:0.568702\ttrain-error:0.308385\n",
      "[35]\teval-error:0.568702\ttrain-error:0.308385\n",
      "[36]\teval-error:0.568702\ttrain-error:0.308385\n",
      "[37]\teval-error:0.568702\ttrain-error:0.308385\n",
      "[38]\teval-error:0.568702\ttrain-error:0.308385\n",
      "[39]\teval-error:0.568702\ttrain-error:0.308385\n",
      "[40]\teval-error:0.568702\ttrain-error:0.308385\n",
      "[41]\teval-error:0.568702\ttrain-error:0.308385\n",
      "[42]\teval-error:0.568702\ttrain-error:0.308385\n",
      "[43]\teval-error:0.568702\ttrain-error:0.308385\n",
      "[44]\teval-error:0.568702\ttrain-error:0.308385\n",
      "[45]\teval-error:0.568702\ttrain-error:0.308385\n",
      "[46]\teval-error:0.568702\ttrain-error:0.308385\n",
      "[47]\teval-error:0.568702\ttrain-error:0.308385\n",
      "[48]\teval-error:0.568702\ttrain-error:0.308385\n",
      "[49]\teval-error:0.568702\ttrain-error:0.308385\n",
      "[50]\teval-error:0.568702\ttrain-error:0.308313\n",
      "[51]\teval-error:0.568702\ttrain-error:0.308313\n",
      "[52]\teval-error:0.568702\ttrain-error:0.308313\n",
      "[53]\teval-error:0.568702\ttrain-error:0.308385\n",
      "[54]\teval-error:0.568702\ttrain-error:0.308313\n",
      "[55]\teval-error:0.568702\ttrain-error:0.308313\n",
      "[56]\teval-error:0.568702\ttrain-error:0.308313\n",
      "[57]\teval-error:0.568702\ttrain-error:0.308313\n",
      "[58]\teval-error:0.568702\ttrain-error:0.308313\n",
      "[59]\teval-error:0.568702\ttrain-error:0.308313\n",
      "[60]\teval-error:0.568702\ttrain-error:0.308313\n",
      "[61]\teval-error:0.568702\ttrain-error:0.308313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62]\teval-error:0.568702\ttrain-error:0.308313\n",
      "[63]\teval-error:0.568702\ttrain-error:0.308313\n",
      "[64]\teval-error:0.568702\ttrain-error:0.308313\n",
      "[65]\teval-error:0.568702\ttrain-error:0.308313\n",
      "[66]\teval-error:0.568702\ttrain-error:0.308313\n",
      "[67]\teval-error:0.568702\ttrain-error:0.308313\n",
      "[68]\teval-error:0.568702\ttrain-error:0.308313\n",
      "[69]\teval-error:0.568702\ttrain-error:0.308313\n",
      "[0]\teval-error:0.362595\ttrain-error:0.256178\n",
      "[1]\teval-error:0.354962\ttrain-error:0.253178\n",
      "[2]\teval-error:0.358779\ttrain-error:0.250893\n",
      "[3]\teval-error:0.358779\ttrain-error:0.249322\n",
      "[4]\teval-error:0.362595\ttrain-error:0.247893\n",
      "[5]\teval-error:0.366412\ttrain-error:0.246251\n",
      "[6]\teval-error:0.362595\ttrain-error:0.245751\n",
      "[7]\teval-error:0.362595\ttrain-error:0.244679\n",
      "[8]\teval-error:0.351145\ttrain-error:0.244108\n",
      "[9]\teval-error:0.343511\ttrain-error:0.243751\n",
      "[10]\teval-error:0.339695\ttrain-error:0.243037\n",
      "[11]\teval-error:0.351145\ttrain-error:0.243037\n",
      "[12]\teval-error:0.351145\ttrain-error:0.242537\n",
      "[13]\teval-error:0.343511\ttrain-error:0.242537\n",
      "[14]\teval-error:0.343511\ttrain-error:0.242323\n",
      "[15]\teval-error:0.347328\ttrain-error:0.24218\n",
      "[16]\teval-error:0.347328\ttrain-error:0.242251\n",
      "[17]\teval-error:0.351145\ttrain-error:0.24218\n",
      "[18]\teval-error:0.351145\ttrain-error:0.24218\n",
      "[19]\teval-error:0.347328\ttrain-error:0.24218\n",
      "[20]\teval-error:0.347328\ttrain-error:0.24218\n",
      "[21]\teval-error:0.347328\ttrain-error:0.24218\n",
      "[22]\teval-error:0.339695\ttrain-error:0.24218\n",
      "[23]\teval-error:0.339695\ttrain-error:0.24218\n",
      "[24]\teval-error:0.347328\ttrain-error:0.24218\n",
      "[25]\teval-error:0.339695\ttrain-error:0.24218\n",
      "[26]\teval-error:0.347328\ttrain-error:0.24218\n",
      "[27]\teval-error:0.351145\ttrain-error:0.24218\n",
      "[28]\teval-error:0.343511\ttrain-error:0.24218\n",
      "[29]\teval-error:0.343511\ttrain-error:0.24218\n",
      "[30]\teval-error:0.343511\ttrain-error:0.24218\n",
      "[31]\teval-error:0.343511\ttrain-error:0.24218\n",
      "[32]\teval-error:0.343511\ttrain-error:0.24218\n",
      "[33]\teval-error:0.339695\ttrain-error:0.24218\n",
      "[34]\teval-error:0.343511\ttrain-error:0.24218\n",
      "[35]\teval-error:0.351145\ttrain-error:0.24218\n",
      "[36]\teval-error:0.347328\ttrain-error:0.24218\n",
      "[37]\teval-error:0.343511\ttrain-error:0.24218\n",
      "[38]\teval-error:0.343511\ttrain-error:0.24218\n",
      "[39]\teval-error:0.335878\ttrain-error:0.24218\n",
      "[40]\teval-error:0.343511\ttrain-error:0.24218\n",
      "[41]\teval-error:0.339695\ttrain-error:0.24218\n",
      "[42]\teval-error:0.347328\ttrain-error:0.24218\n",
      "[43]\teval-error:0.343511\ttrain-error:0.24218\n",
      "[44]\teval-error:0.347328\ttrain-error:0.24218\n",
      "[45]\teval-error:0.343511\ttrain-error:0.24218\n",
      "[46]\teval-error:0.343511\ttrain-error:0.24218\n",
      "[47]\teval-error:0.351145\ttrain-error:0.24218\n",
      "[48]\teval-error:0.347328\ttrain-error:0.24218\n",
      "[49]\teval-error:0.339695\ttrain-error:0.24218\n",
      "[50]\teval-error:0.339695\ttrain-error:0.24218\n",
      "[51]\teval-error:0.339695\ttrain-error:0.24218\n",
      "[52]\teval-error:0.343511\ttrain-error:0.24218\n",
      "[53]\teval-error:0.343511\ttrain-error:0.24218\n",
      "[54]\teval-error:0.343511\ttrain-error:0.24218\n",
      "[55]\teval-error:0.339695\ttrain-error:0.24218\n",
      "[56]\teval-error:0.339695\ttrain-error:0.24218\n",
      "[57]\teval-error:0.343511\ttrain-error:0.24218\n",
      "[58]\teval-error:0.339695\ttrain-error:0.24218\n",
      "[59]\teval-error:0.343511\ttrain-error:0.24218\n",
      "[60]\teval-error:0.343511\ttrain-error:0.24218\n",
      "[61]\teval-error:0.339695\ttrain-error:0.24218\n",
      "[62]\teval-error:0.343511\ttrain-error:0.24218\n",
      "[63]\teval-error:0.339695\ttrain-error:0.24218\n",
      "[64]\teval-error:0.343511\ttrain-error:0.24218\n",
      "[65]\teval-error:0.343511\ttrain-error:0.24218\n",
      "[66]\teval-error:0.339695\ttrain-error:0.24218\n",
      "[67]\teval-error:0.343511\ttrain-error:0.24218\n",
      "[68]\teval-error:0.332061\ttrain-error:0.24218\n",
      "[69]\teval-error:0.328244\ttrain-error:0.24218\n",
      "[0]\teval-error:0.363636\ttrain-error:0.090434\n",
      "[1]\teval-error:0.295455\ttrain-error:0.054405\n",
      "[2]\teval-error:0.261364\ttrain-error:0.037471\n",
      "[3]\teval-error:0.272727\ttrain-error:0.030985\n",
      "[4]\teval-error:0.272727\ttrain-error:0.027022\n",
      "[5]\teval-error:0.238636\ttrain-error:0.025761\n",
      "[6]\teval-error:0.261364\ttrain-error:0.02468\n",
      "[7]\teval-error:0.261364\ttrain-error:0.023779\n",
      "[8]\teval-error:0.25\ttrain-error:0.023239\n",
      "[9]\teval-error:0.227273\ttrain-error:0.023239\n",
      "[10]\teval-error:0.204545\ttrain-error:0.022699\n",
      "[11]\teval-error:0.204545\ttrain-error:0.022699\n",
      "[12]\teval-error:0.204545\ttrain-error:0.022699\n",
      "[13]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[14]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[15]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[16]\teval-error:0.227273\ttrain-error:0.022699\n",
      "[17]\teval-error:0.227273\ttrain-error:0.022699\n",
      "[18]\teval-error:0.227273\ttrain-error:0.022699\n",
      "[19]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[20]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[21]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[22]\teval-error:0.204545\ttrain-error:0.022699\n",
      "[23]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[24]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[25]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[26]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[27]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[28]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[29]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[30]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[31]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[32]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[33]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[34]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[35]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[36]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[37]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[38]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[39]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[40]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[41]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[42]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[43]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[44]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[45]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[46]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[47]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[48]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[49]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[50]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[51]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[52]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[53]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[54]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[55]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[56]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[57]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[58]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[59]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[60]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[61]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[62]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[63]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[64]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[65]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[66]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[67]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[68]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[69]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[0]\teval-error:0.375\ttrain-error:0.092416\n",
      "[1]\teval-error:0.295455\ttrain-error:0.052243\n",
      "[2]\teval-error:0.284091\ttrain-error:0.03675\n",
      "[3]\teval-error:0.227273\ttrain-error:0.030085\n",
      "[4]\teval-error:0.215909\ttrain-error:0.027202\n",
      "[5]\teval-error:0.238636\ttrain-error:0.026662\n",
      "[6]\teval-error:0.238636\ttrain-error:0.02468\n",
      "[7]\teval-error:0.215909\ttrain-error:0.02414\n",
      "[8]\teval-error:0.193182\ttrain-error:0.023059\n",
      "[9]\teval-error:0.25\ttrain-error:0.022879\n",
      "[10]\teval-error:0.238636\ttrain-error:0.022699\n",
      "[11]\teval-error:0.215909\ttrain-error:0.022879\n",
      "[12]\teval-error:0.204545\ttrain-error:0.022699\n",
      "[13]\teval-error:0.204545\ttrain-error:0.022699\n",
      "[14]\teval-error:0.193182\ttrain-error:0.022699\n",
      "[15]\teval-error:0.193182\ttrain-error:0.022699\n",
      "[16]\teval-error:0.193182\ttrain-error:0.022699\n",
      "[17]\teval-error:0.193182\ttrain-error:0.022699\n",
      "[18]\teval-error:0.193182\ttrain-error:0.022699\n",
      "[19]\teval-error:0.193182\ttrain-error:0.022699\n",
      "[20]\teval-error:0.193182\ttrain-error:0.022699\n",
      "[21]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[22]\teval-error:0.204545\ttrain-error:0.022699\n",
      "[23]\teval-error:0.193182\ttrain-error:0.022699\n",
      "[24]\teval-error:0.193182\ttrain-error:0.022699\n",
      "[25]\teval-error:0.193182\ttrain-error:0.022699\n",
      "[26]\teval-error:0.204545\ttrain-error:0.022699\n",
      "[27]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[28]\teval-error:0.204545\ttrain-error:0.022699\n",
      "[29]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[30]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[31]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[32]\teval-error:0.215909\ttrain-error:0.022699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[34]\teval-error:0.204545\ttrain-error:0.022699\n",
      "[35]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[36]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[37]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[38]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[39]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[40]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[41]\teval-error:0.204545\ttrain-error:0.022699\n",
      "[42]\teval-error:0.204545\ttrain-error:0.022699\n",
      "[43]\teval-error:0.204545\ttrain-error:0.022699\n",
      "[44]\teval-error:0.204545\ttrain-error:0.022699\n",
      "[45]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[46]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[47]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[48]\teval-error:0.204545\ttrain-error:0.022699\n",
      "[49]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[50]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[51]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[52]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[53]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[54]\teval-error:0.227273\ttrain-error:0.022699\n",
      "[55]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[56]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[57]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[58]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[59]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[60]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[61]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[62]\teval-error:0.204545\ttrain-error:0.022699\n",
      "[63]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[64]\teval-error:0.204545\ttrain-error:0.022699\n",
      "[65]\teval-error:0.204545\ttrain-error:0.022699\n",
      "[66]\teval-error:0.204545\ttrain-error:0.022699\n",
      "[67]\teval-error:0.204545\ttrain-error:0.022699\n",
      "[68]\teval-error:0.215909\ttrain-error:0.022699\n",
      "[69]\teval-error:0.204545\ttrain-error:0.022699\n",
      "[0]\teval-error:0.386364\ttrain-error:0.089173\n",
      "[1]\teval-error:0.352273\ttrain-error:0.053864\n",
      "[2]\teval-error:0.284091\ttrain-error:0.039632\n",
      "[3]\teval-error:0.284091\ttrain-error:0.032066\n",
      "[4]\teval-error:0.295455\ttrain-error:0.027923\n",
      "[5]\teval-error:0.295455\ttrain-error:0.025401\n",
      "[6]\teval-error:0.284091\ttrain-error:0.02432\n",
      "[7]\teval-error:0.284091\ttrain-error:0.02396\n",
      "[8]\teval-error:0.261364\ttrain-error:0.023599\n",
      "[9]\teval-error:0.261364\ttrain-error:0.023419\n",
      "[10]\teval-error:0.272727\ttrain-error:0.022879\n",
      "[11]\teval-error:0.272727\ttrain-error:0.023059\n",
      "[12]\teval-error:0.306818\ttrain-error:0.022879\n",
      "[13]\teval-error:0.295455\ttrain-error:0.022879\n",
      "[14]\teval-error:0.306818\ttrain-error:0.022699\n",
      "[15]\teval-error:0.295455\ttrain-error:0.022699\n",
      "[16]\teval-error:0.329545\ttrain-error:0.022699\n",
      "[17]\teval-error:0.329545\ttrain-error:0.022699\n",
      "[18]\teval-error:0.340909\ttrain-error:0.022699\n",
      "[19]\teval-error:0.318182\ttrain-error:0.022699\n",
      "[20]\teval-error:0.329545\ttrain-error:0.022699\n",
      "[21]\teval-error:0.329545\ttrain-error:0.022699\n",
      "[22]\teval-error:0.318182\ttrain-error:0.022699\n",
      "[23]\teval-error:0.284091\ttrain-error:0.022699\n",
      "[24]\teval-error:0.284091\ttrain-error:0.022699\n",
      "[25]\teval-error:0.295455\ttrain-error:0.022699\n",
      "[26]\teval-error:0.261364\ttrain-error:0.022699\n",
      "[27]\teval-error:0.261364\ttrain-error:0.022699\n",
      "[28]\teval-error:0.261364\ttrain-error:0.022699\n",
      "[29]\teval-error:0.261364\ttrain-error:0.022699\n",
      "[30]\teval-error:0.261364\ttrain-error:0.022699\n",
      "[31]\teval-error:0.272727\ttrain-error:0.022699\n",
      "[32]\teval-error:0.272727\ttrain-error:0.022699\n",
      "[33]\teval-error:0.284091\ttrain-error:0.022699\n",
      "[34]\teval-error:0.284091\ttrain-error:0.022699\n",
      "[35]\teval-error:0.284091\ttrain-error:0.022699\n",
      "[36]\teval-error:0.261364\ttrain-error:0.022699\n",
      "[37]\teval-error:0.272727\ttrain-error:0.022699\n",
      "[38]\teval-error:0.272727\ttrain-error:0.022699\n",
      "[39]\teval-error:0.272727\ttrain-error:0.022699\n",
      "[40]\teval-error:0.272727\ttrain-error:0.022699\n",
      "[41]\teval-error:0.284091\ttrain-error:0.022699\n",
      "[42]\teval-error:0.295455\ttrain-error:0.022699\n",
      "[43]\teval-error:0.284091\ttrain-error:0.022699\n",
      "[44]\teval-error:0.295455\ttrain-error:0.022699\n",
      "[45]\teval-error:0.295455\ttrain-error:0.022699\n",
      "[46]\teval-error:0.284091\ttrain-error:0.022699\n",
      "[47]\teval-error:0.272727\ttrain-error:0.022699\n",
      "[48]\teval-error:0.272727\ttrain-error:0.022699\n",
      "[49]\teval-error:0.272727\ttrain-error:0.022699\n",
      "[50]\teval-error:0.272727\ttrain-error:0.022699\n",
      "[51]\teval-error:0.261364\ttrain-error:0.022699\n",
      "[52]\teval-error:0.261364\ttrain-error:0.022699\n",
      "[53]\teval-error:0.272727\ttrain-error:0.022699\n",
      "[54]\teval-error:0.272727\ttrain-error:0.022699\n",
      "[55]\teval-error:0.272727\ttrain-error:0.022699\n",
      "[56]\teval-error:0.261364\ttrain-error:0.022699\n",
      "[57]\teval-error:0.25\ttrain-error:0.022699\n",
      "[58]\teval-error:0.25\ttrain-error:0.022699\n",
      "[59]\teval-error:0.261364\ttrain-error:0.022699\n",
      "[60]\teval-error:0.261364\ttrain-error:0.022699\n",
      "[61]\teval-error:0.261364\ttrain-error:0.022699\n",
      "[62]\teval-error:0.261364\ttrain-error:0.022699\n",
      "[63]\teval-error:0.261364\ttrain-error:0.022699\n",
      "[64]\teval-error:0.272727\ttrain-error:0.022699\n",
      "[65]\teval-error:0.272727\ttrain-error:0.022699\n",
      "[66]\teval-error:0.261364\ttrain-error:0.022699\n",
      "[67]\teval-error:0.272727\ttrain-error:0.022699\n",
      "[68]\teval-error:0.261364\ttrain-error:0.022699\n",
      "[69]\teval-error:0.261364\ttrain-error:0.022699\n",
      "[0]\teval-error:0.397727\ttrain-error:0.087912\n",
      "[1]\teval-error:0.284091\ttrain-error:0.052783\n",
      "[2]\teval-error:0.306818\ttrain-error:0.035669\n",
      "[3]\teval-error:0.318182\ttrain-error:0.030085\n",
      "[4]\teval-error:0.329545\ttrain-error:0.027923\n",
      "[5]\teval-error:0.329545\ttrain-error:0.025221\n",
      "[6]\teval-error:0.329545\ttrain-error:0.02468\n",
      "[7]\teval-error:0.329545\ttrain-error:0.023779\n",
      "[8]\teval-error:0.329545\ttrain-error:0.02396\n",
      "[9]\teval-error:0.340909\ttrain-error:0.023059\n",
      "[10]\teval-error:0.295455\ttrain-error:0.022879\n",
      "[11]\teval-error:0.306818\ttrain-error:0.022879\n",
      "[12]\teval-error:0.318182\ttrain-error:0.022699\n",
      "[13]\teval-error:0.318182\ttrain-error:0.022699\n",
      "[14]\teval-error:0.306818\ttrain-error:0.022699\n",
      "[15]\teval-error:0.318182\ttrain-error:0.022699\n",
      "[16]\teval-error:0.318182\ttrain-error:0.022699\n",
      "[17]\teval-error:0.306818\ttrain-error:0.022699\n",
      "[18]\teval-error:0.306818\ttrain-error:0.022699\n",
      "[19]\teval-error:0.318182\ttrain-error:0.022699\n",
      "[20]\teval-error:0.295455\ttrain-error:0.022699\n",
      "[21]\teval-error:0.284091\ttrain-error:0.022699\n",
      "[22]\teval-error:0.284091\ttrain-error:0.022699\n",
      "[23]\teval-error:0.295455\ttrain-error:0.022699\n",
      "[24]\teval-error:0.284091\ttrain-error:0.022699\n",
      "[25]\teval-error:0.284091\ttrain-error:0.022699\n",
      "[26]\teval-error:0.284091\ttrain-error:0.022699\n",
      "[27]\teval-error:0.284091\ttrain-error:0.022699\n",
      "[28]\teval-error:0.284091\ttrain-error:0.022699\n",
      "[29]\teval-error:0.284091\ttrain-error:0.022699\n",
      "[30]\teval-error:0.284091\ttrain-error:0.022699\n",
      "[31]\teval-error:0.284091\ttrain-error:0.022699\n",
      "[32]\teval-error:0.284091\ttrain-error:0.022699\n",
      "[33]\teval-error:0.284091\ttrain-error:0.022699\n",
      "[34]\teval-error:0.284091\ttrain-error:0.022699\n",
      "[35]\teval-error:0.284091\ttrain-error:0.022699\n",
      "[36]\teval-error:0.284091\ttrain-error:0.022699\n",
      "[37]\teval-error:0.261364\ttrain-error:0.022699\n",
      "[38]\teval-error:0.261364\ttrain-error:0.022699\n",
      "[39]\teval-error:0.261364\ttrain-error:0.022699\n",
      "[40]\teval-error:0.272727\ttrain-error:0.022699\n",
      "[41]\teval-error:0.261364\ttrain-error:0.022699\n",
      "[42]\teval-error:0.261364\ttrain-error:0.022699\n",
      "[43]\teval-error:0.272727\ttrain-error:0.022699\n",
      "[44]\teval-error:0.261364\ttrain-error:0.022699\n",
      "[45]\teval-error:0.261364\ttrain-error:0.022699\n",
      "[46]\teval-error:0.261364\ttrain-error:0.022699\n",
      "[47]\teval-error:0.261364\ttrain-error:0.022699\n",
      "[48]\teval-error:0.261364\ttrain-error:0.022699\n",
      "[49]\teval-error:0.25\ttrain-error:0.022699\n",
      "[50]\teval-error:0.25\ttrain-error:0.022699\n",
      "[51]\teval-error:0.25\ttrain-error:0.022699\n",
      "[52]\teval-error:0.25\ttrain-error:0.022699\n",
      "[53]\teval-error:0.25\ttrain-error:0.022699\n",
      "[54]\teval-error:0.25\ttrain-error:0.022699\n",
      "[55]\teval-error:0.25\ttrain-error:0.022699\n",
      "[56]\teval-error:0.25\ttrain-error:0.022699\n",
      "[57]\teval-error:0.25\ttrain-error:0.022699\n",
      "[58]\teval-error:0.25\ttrain-error:0.022699\n",
      "[59]\teval-error:0.25\ttrain-error:0.022699\n",
      "[60]\teval-error:0.25\ttrain-error:0.022699\n",
      "[61]\teval-error:0.25\ttrain-error:0.022699\n",
      "[62]\teval-error:0.25\ttrain-error:0.022699\n",
      "[63]\teval-error:0.25\ttrain-error:0.022699\n",
      "[64]\teval-error:0.25\ttrain-error:0.022699\n",
      "[65]\teval-error:0.238636\ttrain-error:0.022699\n",
      "[66]\teval-error:0.238636\ttrain-error:0.022699\n",
      "[67]\teval-error:0.238636\ttrain-error:0.022699\n",
      "[68]\teval-error:0.238636\ttrain-error:0.022699\n",
      "[69]\teval-error:0.238636\ttrain-error:0.022699\n",
      "[0]\teval-error:0.351064\ttrain-error:0.086754\n",
      "[1]\teval-error:0.297872\ttrain-error:0.059256\n",
      "[2]\teval-error:0.319149\ttrain-error:0.04867\n",
      "[3]\teval-error:0.308511\ttrain-error:0.042344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4]\teval-error:0.287234\ttrain-error:0.039117\n",
      "[5]\teval-error:0.287234\ttrain-error:0.037697\n",
      "[6]\teval-error:0.287234\ttrain-error:0.036406\n",
      "[7]\teval-error:0.265957\ttrain-error:0.035502\n",
      "[8]\teval-error:0.265957\ttrain-error:0.035115\n",
      "[9]\teval-error:0.276596\ttrain-error:0.034728\n",
      "[10]\teval-error:0.276596\ttrain-error:0.034599\n",
      "[11]\teval-error:0.265957\ttrain-error:0.034599\n",
      "[12]\teval-error:0.276596\ttrain-error:0.034599\n",
      "[13]\teval-error:0.276596\ttrain-error:0.034599\n",
      "[14]\teval-error:0.265957\ttrain-error:0.034469\n",
      "[15]\teval-error:0.276596\ttrain-error:0.034469\n",
      "[16]\teval-error:0.265957\ttrain-error:0.034469\n",
      "[17]\teval-error:0.265957\ttrain-error:0.034469\n",
      "[18]\teval-error:0.265957\ttrain-error:0.034469\n",
      "[19]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[20]\teval-error:0.297872\ttrain-error:0.034469\n",
      "[21]\teval-error:0.297872\ttrain-error:0.034469\n",
      "[22]\teval-error:0.297872\ttrain-error:0.034469\n",
      "[23]\teval-error:0.297872\ttrain-error:0.034469\n",
      "[24]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[25]\teval-error:0.308511\ttrain-error:0.034469\n",
      "[26]\teval-error:0.276596\ttrain-error:0.034469\n",
      "[27]\teval-error:0.297872\ttrain-error:0.034469\n",
      "[28]\teval-error:0.297872\ttrain-error:0.034469\n",
      "[29]\teval-error:0.276596\ttrain-error:0.034469\n",
      "[30]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[31]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[32]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[33]\teval-error:0.297872\ttrain-error:0.034469\n",
      "[34]\teval-error:0.308511\ttrain-error:0.034469\n",
      "[35]\teval-error:0.297872\ttrain-error:0.034469\n",
      "[36]\teval-error:0.308511\ttrain-error:0.034469\n",
      "[37]\teval-error:0.297872\ttrain-error:0.034469\n",
      "[38]\teval-error:0.297872\ttrain-error:0.034469\n",
      "[39]\teval-error:0.297872\ttrain-error:0.034469\n",
      "[40]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[41]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[42]\teval-error:0.297872\ttrain-error:0.034469\n",
      "[43]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[44]\teval-error:0.276596\ttrain-error:0.034469\n",
      "[45]\teval-error:0.265957\ttrain-error:0.034469\n",
      "[46]\teval-error:0.265957\ttrain-error:0.034469\n",
      "[47]\teval-error:0.276596\ttrain-error:0.034469\n",
      "[48]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[49]\teval-error:0.276596\ttrain-error:0.034469\n",
      "[50]\teval-error:0.276596\ttrain-error:0.034469\n",
      "[51]\teval-error:0.265957\ttrain-error:0.034469\n",
      "[52]\teval-error:0.276596\ttrain-error:0.034469\n",
      "[53]\teval-error:0.276596\ttrain-error:0.034469\n",
      "[54]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[55]\teval-error:0.276596\ttrain-error:0.034469\n",
      "[56]\teval-error:0.265957\ttrain-error:0.034469\n",
      "[57]\teval-error:0.265957\ttrain-error:0.034469\n",
      "[58]\teval-error:0.265957\ttrain-error:0.034469\n",
      "[59]\teval-error:0.276596\ttrain-error:0.034469\n",
      "[60]\teval-error:0.276596\ttrain-error:0.034469\n",
      "[61]\teval-error:0.276596\ttrain-error:0.034469\n",
      "[62]\teval-error:0.276596\ttrain-error:0.034469\n",
      "[63]\teval-error:0.276596\ttrain-error:0.034469\n",
      "[64]\teval-error:0.276596\ttrain-error:0.034469\n",
      "[65]\teval-error:0.276596\ttrain-error:0.034469\n",
      "[66]\teval-error:0.276596\ttrain-error:0.034469\n",
      "[67]\teval-error:0.276596\ttrain-error:0.034469\n",
      "[68]\teval-error:0.276596\ttrain-error:0.034469\n",
      "[69]\teval-error:0.276596\ttrain-error:0.034469\n",
      "[0]\teval-error:0.351064\ttrain-error:0.090111\n",
      "[1]\teval-error:0.351064\ttrain-error:0.062613\n",
      "[2]\teval-error:0.308511\ttrain-error:0.048154\n",
      "[3]\teval-error:0.265957\ttrain-error:0.04299\n",
      "[4]\teval-error:0.244681\ttrain-error:0.038988\n",
      "[5]\teval-error:0.276596\ttrain-error:0.036664\n",
      "[6]\teval-error:0.276596\ttrain-error:0.035631\n",
      "[7]\teval-error:0.265957\ttrain-error:0.035115\n",
      "[8]\teval-error:0.255319\ttrain-error:0.035244\n",
      "[9]\teval-error:0.234043\ttrain-error:0.034728\n",
      "[10]\teval-error:0.244681\ttrain-error:0.034599\n",
      "[11]\teval-error:0.234043\ttrain-error:0.034469\n",
      "[12]\teval-error:0.234043\ttrain-error:0.034469\n",
      "[13]\teval-error:0.234043\ttrain-error:0.034469\n",
      "[14]\teval-error:0.234043\ttrain-error:0.034469\n",
      "[15]\teval-error:0.234043\ttrain-error:0.034469\n",
      "[16]\teval-error:0.234043\ttrain-error:0.034469\n",
      "[17]\teval-error:0.223404\ttrain-error:0.034469\n",
      "[18]\teval-error:0.234043\ttrain-error:0.034469\n",
      "[19]\teval-error:0.244681\ttrain-error:0.034469\n",
      "[20]\teval-error:0.244681\ttrain-error:0.034469\n",
      "[21]\teval-error:0.255319\ttrain-error:0.034469\n",
      "[22]\teval-error:0.244681\ttrain-error:0.034469\n",
      "[23]\teval-error:0.244681\ttrain-error:0.034469\n",
      "[24]\teval-error:0.244681\ttrain-error:0.034469\n",
      "[25]\teval-error:0.255319\ttrain-error:0.034469\n",
      "[26]\teval-error:0.244681\ttrain-error:0.034469\n",
      "[27]\teval-error:0.244681\ttrain-error:0.034469\n",
      "[28]\teval-error:0.255319\ttrain-error:0.034469\n",
      "[29]\teval-error:0.255319\ttrain-error:0.034469\n",
      "[30]\teval-error:0.255319\ttrain-error:0.034469\n",
      "[31]\teval-error:0.255319\ttrain-error:0.034469\n",
      "[32]\teval-error:0.255319\ttrain-error:0.034469\n",
      "[33]\teval-error:0.255319\ttrain-error:0.034469\n",
      "[34]\teval-error:0.244681\ttrain-error:0.034469\n",
      "[35]\teval-error:0.255319\ttrain-error:0.034469\n",
      "[36]\teval-error:0.255319\ttrain-error:0.034469\n",
      "[37]\teval-error:0.255319\ttrain-error:0.034469\n",
      "[38]\teval-error:0.223404\ttrain-error:0.034469\n",
      "[39]\teval-error:0.223404\ttrain-error:0.034469\n",
      "[40]\teval-error:0.223404\ttrain-error:0.034469\n",
      "[41]\teval-error:0.223404\ttrain-error:0.034469\n",
      "[42]\teval-error:0.234043\ttrain-error:0.034469\n",
      "[43]\teval-error:0.234043\ttrain-error:0.034469\n",
      "[44]\teval-error:0.234043\ttrain-error:0.034469\n",
      "[45]\teval-error:0.223404\ttrain-error:0.034469\n",
      "[46]\teval-error:0.234043\ttrain-error:0.034469\n",
      "[47]\teval-error:0.234043\ttrain-error:0.034469\n",
      "[48]\teval-error:0.234043\ttrain-error:0.034469\n",
      "[49]\teval-error:0.234043\ttrain-error:0.034469\n",
      "[50]\teval-error:0.234043\ttrain-error:0.034469\n",
      "[51]\teval-error:0.234043\ttrain-error:0.034469\n",
      "[52]\teval-error:0.223404\ttrain-error:0.034469\n",
      "[53]\teval-error:0.234043\ttrain-error:0.034469\n",
      "[54]\teval-error:0.234043\ttrain-error:0.034469\n",
      "[55]\teval-error:0.212766\ttrain-error:0.034469\n",
      "[56]\teval-error:0.212766\ttrain-error:0.034469\n",
      "[57]\teval-error:0.234043\ttrain-error:0.034469\n",
      "[58]\teval-error:0.234043\ttrain-error:0.034469\n",
      "[59]\teval-error:0.234043\ttrain-error:0.034469\n",
      "[60]\teval-error:0.234043\ttrain-error:0.034469\n",
      "[61]\teval-error:0.244681\ttrain-error:0.034469\n",
      "[62]\teval-error:0.234043\ttrain-error:0.034469\n",
      "[63]\teval-error:0.223404\ttrain-error:0.034469\n",
      "[64]\teval-error:0.223404\ttrain-error:0.034469\n",
      "[65]\teval-error:0.234043\ttrain-error:0.034469\n",
      "[66]\teval-error:0.234043\ttrain-error:0.034469\n",
      "[67]\teval-error:0.234043\ttrain-error:0.034469\n",
      "[68]\teval-error:0.223404\ttrain-error:0.034469\n",
      "[69]\teval-error:0.244681\ttrain-error:0.034469\n",
      "[0]\teval-error:0.340426\ttrain-error:0.089982\n",
      "[1]\teval-error:0.319149\ttrain-error:0.061322\n",
      "[2]\teval-error:0.329787\ttrain-error:0.048025\n",
      "[3]\teval-error:0.287234\ttrain-error:0.041957\n",
      "[4]\teval-error:0.276596\ttrain-error:0.03873\n",
      "[5]\teval-error:0.265957\ttrain-error:0.036406\n",
      "[6]\teval-error:0.276596\ttrain-error:0.035631\n",
      "[7]\teval-error:0.276596\ttrain-error:0.035115\n",
      "[8]\teval-error:0.287234\ttrain-error:0.035244\n",
      "[9]\teval-error:0.265957\ttrain-error:0.034599\n",
      "[10]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[11]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[12]\teval-error:0.308511\ttrain-error:0.034469\n",
      "[13]\teval-error:0.297872\ttrain-error:0.034469\n",
      "[14]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[15]\teval-error:0.265957\ttrain-error:0.034469\n",
      "[16]\teval-error:0.265957\ttrain-error:0.034469\n",
      "[17]\teval-error:0.265957\ttrain-error:0.034469\n",
      "[18]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[19]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[20]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[21]\teval-error:0.265957\ttrain-error:0.034469\n",
      "[22]\teval-error:0.265957\ttrain-error:0.034469\n",
      "[23]\teval-error:0.255319\ttrain-error:0.034469\n",
      "[24]\teval-error:0.255319\ttrain-error:0.034469\n",
      "[25]\teval-error:0.255319\ttrain-error:0.034469\n",
      "[26]\teval-error:0.265957\ttrain-error:0.034469\n",
      "[27]\teval-error:0.265957\ttrain-error:0.034469\n",
      "[28]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[29]\teval-error:0.265957\ttrain-error:0.034469\n",
      "[30]\teval-error:0.276596\ttrain-error:0.034469\n",
      "[31]\teval-error:0.265957\ttrain-error:0.034469\n",
      "[32]\teval-error:0.265957\ttrain-error:0.034469\n",
      "[33]\teval-error:0.276596\ttrain-error:0.034469\n",
      "[34]\teval-error:0.297872\ttrain-error:0.034469\n",
      "[35]\teval-error:0.276596\ttrain-error:0.034469\n",
      "[36]\teval-error:0.265957\ttrain-error:0.034469\n",
      "[37]\teval-error:0.255319\ttrain-error:0.034469\n",
      "[38]\teval-error:0.255319\ttrain-error:0.034469\n",
      "[39]\teval-error:0.255319\ttrain-error:0.034469\n",
      "[40]\teval-error:0.276596\ttrain-error:0.034469\n",
      "[41]\teval-error:0.265957\ttrain-error:0.034469\n",
      "[42]\teval-error:0.276596\ttrain-error:0.034469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43]\teval-error:0.255319\ttrain-error:0.034469\n",
      "[44]\teval-error:0.265957\ttrain-error:0.034469\n",
      "[45]\teval-error:0.265957\ttrain-error:0.034469\n",
      "[46]\teval-error:0.265957\ttrain-error:0.034469\n",
      "[47]\teval-error:0.265957\ttrain-error:0.034469\n",
      "[48]\teval-error:0.265957\ttrain-error:0.034469\n",
      "[49]\teval-error:0.265957\ttrain-error:0.034469\n",
      "[50]\teval-error:0.265957\ttrain-error:0.034469\n",
      "[51]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[52]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[53]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[54]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[55]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[56]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[57]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[58]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[59]\teval-error:0.297872\ttrain-error:0.034469\n",
      "[60]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[61]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[62]\teval-error:0.297872\ttrain-error:0.034469\n",
      "[63]\teval-error:0.297872\ttrain-error:0.034469\n",
      "[64]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[65]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[66]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[67]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[68]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[69]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[0]\teval-error:0.457447\ttrain-error:0.087142\n",
      "[1]\teval-error:0.404255\ttrain-error:0.060547\n",
      "[2]\teval-error:0.414894\ttrain-error:0.048283\n",
      "[3]\teval-error:0.393617\ttrain-error:0.041183\n",
      "[4]\teval-error:0.37234\ttrain-error:0.037955\n",
      "[5]\teval-error:0.351064\ttrain-error:0.037051\n",
      "[6]\teval-error:0.361702\ttrain-error:0.035244\n",
      "[7]\teval-error:0.351064\ttrain-error:0.034986\n",
      "[8]\teval-error:0.351064\ttrain-error:0.034599\n",
      "[9]\teval-error:0.351064\ttrain-error:0.034728\n",
      "[10]\teval-error:0.340426\ttrain-error:0.034599\n",
      "[11]\teval-error:0.340426\ttrain-error:0.034469\n",
      "[12]\teval-error:0.340426\ttrain-error:0.034599\n",
      "[13]\teval-error:0.319149\ttrain-error:0.034469\n",
      "[14]\teval-error:0.319149\ttrain-error:0.034469\n",
      "[15]\teval-error:0.308511\ttrain-error:0.034599\n",
      "[16]\teval-error:0.297872\ttrain-error:0.034469\n",
      "[17]\teval-error:0.297872\ttrain-error:0.034469\n",
      "[18]\teval-error:0.308511\ttrain-error:0.034469\n",
      "[19]\teval-error:0.319149\ttrain-error:0.034469\n",
      "[20]\teval-error:0.297872\ttrain-error:0.034469\n",
      "[21]\teval-error:0.297872\ttrain-error:0.034469\n",
      "[22]\teval-error:0.308511\ttrain-error:0.034469\n",
      "[23]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[24]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[25]\teval-error:0.297872\ttrain-error:0.034469\n",
      "[26]\teval-error:0.297872\ttrain-error:0.034469\n",
      "[27]\teval-error:0.319149\ttrain-error:0.034469\n",
      "[28]\teval-error:0.308511\ttrain-error:0.034469\n",
      "[29]\teval-error:0.319149\ttrain-error:0.034469\n",
      "[30]\teval-error:0.319149\ttrain-error:0.034469\n",
      "[31]\teval-error:0.319149\ttrain-error:0.034469\n",
      "[32]\teval-error:0.308511\ttrain-error:0.034469\n",
      "[33]\teval-error:0.297872\ttrain-error:0.034469\n",
      "[34]\teval-error:0.297872\ttrain-error:0.034469\n",
      "[35]\teval-error:0.308511\ttrain-error:0.034469\n",
      "[36]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[37]\teval-error:0.297872\ttrain-error:0.034469\n",
      "[38]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[39]\teval-error:0.297872\ttrain-error:0.034469\n",
      "[40]\teval-error:0.308511\ttrain-error:0.034469\n",
      "[41]\teval-error:0.297872\ttrain-error:0.034469\n",
      "[42]\teval-error:0.319149\ttrain-error:0.034469\n",
      "[43]\teval-error:0.308511\ttrain-error:0.034469\n",
      "[44]\teval-error:0.319149\ttrain-error:0.034469\n",
      "[45]\teval-error:0.308511\ttrain-error:0.034469\n",
      "[46]\teval-error:0.308511\ttrain-error:0.034469\n",
      "[47]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[48]\teval-error:0.297872\ttrain-error:0.034469\n",
      "[49]\teval-error:0.297872\ttrain-error:0.034469\n",
      "[50]\teval-error:0.276596\ttrain-error:0.034469\n",
      "[51]\teval-error:0.276596\ttrain-error:0.034469\n",
      "[52]\teval-error:0.276596\ttrain-error:0.034469\n",
      "[53]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[54]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[55]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[56]\teval-error:0.276596\ttrain-error:0.034469\n",
      "[57]\teval-error:0.276596\ttrain-error:0.034469\n",
      "[58]\teval-error:0.276596\ttrain-error:0.034469\n",
      "[59]\teval-error:0.297872\ttrain-error:0.034469\n",
      "[60]\teval-error:0.297872\ttrain-error:0.034469\n",
      "[61]\teval-error:0.297872\ttrain-error:0.034469\n",
      "[62]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[63]\teval-error:0.297872\ttrain-error:0.034469\n",
      "[64]\teval-error:0.276596\ttrain-error:0.034469\n",
      "[65]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[66]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[67]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[68]\teval-error:0.287234\ttrain-error:0.034469\n",
      "[69]\teval-error:0.297872\ttrain-error:0.034469\n",
      "[0]\teval-error:0.267176\ttrain-error:0.080703\n",
      "[1]\teval-error:0.244275\ttrain-error:0.064419\n",
      "[2]\teval-error:0.221374\ttrain-error:0.054849\n",
      "[3]\teval-error:0.229008\ttrain-error:0.049779\n",
      "[4]\teval-error:0.221374\ttrain-error:0.047279\n",
      "[5]\teval-error:0.217557\ttrain-error:0.046351\n",
      "[6]\teval-error:0.221374\ttrain-error:0.045422\n",
      "[7]\teval-error:0.232824\ttrain-error:0.044994\n",
      "[8]\teval-error:0.225191\ttrain-error:0.044494\n",
      "[9]\teval-error:0.229008\ttrain-error:0.044208\n",
      "[10]\teval-error:0.240458\ttrain-error:0.044208\n",
      "[11]\teval-error:0.232824\ttrain-error:0.044208\n",
      "[12]\teval-error:0.236641\ttrain-error:0.044065\n",
      "[13]\teval-error:0.232824\ttrain-error:0.044065\n",
      "[14]\teval-error:0.240458\ttrain-error:0.044065\n",
      "[15]\teval-error:0.236641\ttrain-error:0.044065\n",
      "[16]\teval-error:0.229008\ttrain-error:0.044065\n",
      "[17]\teval-error:0.236641\ttrain-error:0.044065\n",
      "[18]\teval-error:0.236641\ttrain-error:0.044065\n",
      "[19]\teval-error:0.232824\ttrain-error:0.044065\n",
      "[20]\teval-error:0.236641\ttrain-error:0.044065\n",
      "[21]\teval-error:0.232824\ttrain-error:0.044065\n",
      "[22]\teval-error:0.236641\ttrain-error:0.044065\n",
      "[23]\teval-error:0.221374\ttrain-error:0.044065\n",
      "[24]\teval-error:0.236641\ttrain-error:0.044065\n",
      "[25]\teval-error:0.217557\ttrain-error:0.044065\n",
      "[26]\teval-error:0.225191\ttrain-error:0.044065\n",
      "[27]\teval-error:0.221374\ttrain-error:0.044065\n",
      "[28]\teval-error:0.225191\ttrain-error:0.044065\n",
      "[29]\teval-error:0.229008\ttrain-error:0.044065\n",
      "[30]\teval-error:0.21374\ttrain-error:0.044065\n",
      "[31]\teval-error:0.21374\ttrain-error:0.044065\n",
      "[32]\teval-error:0.21374\ttrain-error:0.044065\n",
      "[33]\teval-error:0.21374\ttrain-error:0.044065\n",
      "[34]\teval-error:0.209924\ttrain-error:0.044065\n",
      "[35]\teval-error:0.209924\ttrain-error:0.044065\n",
      "[36]\teval-error:0.217557\ttrain-error:0.044065\n",
      "[37]\teval-error:0.209924\ttrain-error:0.044065\n",
      "[38]\teval-error:0.20229\ttrain-error:0.044065\n",
      "[39]\teval-error:0.209924\ttrain-error:0.044065\n",
      "[40]\teval-error:0.209924\ttrain-error:0.044065\n",
      "[41]\teval-error:0.217557\ttrain-error:0.044065\n",
      "[42]\teval-error:0.21374\ttrain-error:0.044065\n",
      "[43]\teval-error:0.21374\ttrain-error:0.044065\n",
      "[44]\teval-error:0.206107\ttrain-error:0.044065\n",
      "[45]\teval-error:0.209924\ttrain-error:0.044065\n",
      "[46]\teval-error:0.217557\ttrain-error:0.044065\n",
      "[47]\teval-error:0.209924\ttrain-error:0.044065\n",
      "[48]\teval-error:0.21374\ttrain-error:0.044065\n",
      "[49]\teval-error:0.217557\ttrain-error:0.044065\n",
      "[50]\teval-error:0.221374\ttrain-error:0.044065\n",
      "[51]\teval-error:0.209924\ttrain-error:0.044065\n",
      "[52]\teval-error:0.209924\ttrain-error:0.044065\n",
      "[53]\teval-error:0.21374\ttrain-error:0.044065\n",
      "[54]\teval-error:0.209924\ttrain-error:0.044065\n",
      "[55]\teval-error:0.217557\ttrain-error:0.044065\n",
      "[56]\teval-error:0.209924\ttrain-error:0.044065\n",
      "[57]\teval-error:0.217557\ttrain-error:0.044065\n",
      "[58]\teval-error:0.217557\ttrain-error:0.044065\n",
      "[59]\teval-error:0.21374\ttrain-error:0.044065\n",
      "[60]\teval-error:0.206107\ttrain-error:0.044065\n",
      "[61]\teval-error:0.209924\ttrain-error:0.044065\n",
      "[62]\teval-error:0.221374\ttrain-error:0.044065\n",
      "[63]\teval-error:0.221374\ttrain-error:0.044065\n",
      "[64]\teval-error:0.221374\ttrain-error:0.044065\n",
      "[65]\teval-error:0.21374\ttrain-error:0.044065\n",
      "[66]\teval-error:0.217557\ttrain-error:0.044065\n",
      "[67]\teval-error:0.21374\ttrain-error:0.044065\n",
      "[68]\teval-error:0.21374\ttrain-error:0.044065\n",
      "[69]\teval-error:0.225191\ttrain-error:0.044065\n",
      "[0]\teval-error:0.244275\ttrain-error:0.080703\n",
      "[1]\teval-error:0.263359\ttrain-error:0.06142\n",
      "[2]\teval-error:0.244275\ttrain-error:0.053564\n",
      "[3]\teval-error:0.232824\ttrain-error:0.049779\n",
      "[4]\teval-error:0.232824\ttrain-error:0.047636\n",
      "[5]\teval-error:0.248092\ttrain-error:0.046422\n",
      "[6]\teval-error:0.255725\ttrain-error:0.045422\n",
      "[7]\teval-error:0.240458\ttrain-error:0.045065\n",
      "[8]\teval-error:0.236641\ttrain-error:0.044636\n",
      "[9]\teval-error:0.236641\ttrain-error:0.044422\n",
      "[10]\teval-error:0.248092\ttrain-error:0.044279\n",
      "[11]\teval-error:0.244275\ttrain-error:0.044279\n",
      "[12]\teval-error:0.248092\ttrain-error:0.044208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13]\teval-error:0.251908\ttrain-error:0.044137\n",
      "[14]\teval-error:0.248092\ttrain-error:0.044137\n",
      "[15]\teval-error:0.244275\ttrain-error:0.044137\n",
      "[16]\teval-error:0.240458\ttrain-error:0.044137\n",
      "[17]\teval-error:0.229008\ttrain-error:0.044137\n",
      "[18]\teval-error:0.240458\ttrain-error:0.044137\n",
      "[19]\teval-error:0.236641\ttrain-error:0.044137\n",
      "[20]\teval-error:0.248092\ttrain-error:0.044137\n",
      "[21]\teval-error:0.248092\ttrain-error:0.044137\n",
      "[22]\teval-error:0.240458\ttrain-error:0.044137\n",
      "[23]\teval-error:0.236641\ttrain-error:0.044137\n",
      "[24]\teval-error:0.248092\ttrain-error:0.044137\n",
      "[25]\teval-error:0.251908\ttrain-error:0.044137\n",
      "[26]\teval-error:0.255725\ttrain-error:0.044137\n",
      "[27]\teval-error:0.244275\ttrain-error:0.044137\n",
      "[28]\teval-error:0.244275\ttrain-error:0.044137\n",
      "[29]\teval-error:0.248092\ttrain-error:0.044137\n",
      "[30]\teval-error:0.244275\ttrain-error:0.044137\n",
      "[31]\teval-error:0.240458\ttrain-error:0.044137\n",
      "[32]\teval-error:0.248092\ttrain-error:0.044137\n",
      "[33]\teval-error:0.232824\ttrain-error:0.044137\n",
      "[34]\teval-error:0.244275\ttrain-error:0.044137\n",
      "[35]\teval-error:0.236641\ttrain-error:0.044137\n",
      "[36]\teval-error:0.255725\ttrain-error:0.044137\n",
      "[37]\teval-error:0.240458\ttrain-error:0.044137\n",
      "[38]\teval-error:0.229008\ttrain-error:0.044137\n",
      "[39]\teval-error:0.248092\ttrain-error:0.044137\n",
      "[40]\teval-error:0.240458\ttrain-error:0.044137\n",
      "[41]\teval-error:0.236641\ttrain-error:0.044137\n",
      "[42]\teval-error:0.240458\ttrain-error:0.044137\n",
      "[43]\teval-error:0.255725\ttrain-error:0.044137\n",
      "[44]\teval-error:0.248092\ttrain-error:0.044137\n",
      "[45]\teval-error:0.248092\ttrain-error:0.044137\n",
      "[46]\teval-error:0.240458\ttrain-error:0.044137\n",
      "[47]\teval-error:0.232824\ttrain-error:0.044137\n",
      "[48]\teval-error:0.251908\ttrain-error:0.044137\n",
      "[49]\teval-error:0.244275\ttrain-error:0.044137\n",
      "[50]\teval-error:0.240458\ttrain-error:0.044137\n",
      "[51]\teval-error:0.251908\ttrain-error:0.044137\n",
      "[52]\teval-error:0.244275\ttrain-error:0.044137\n",
      "[53]\teval-error:0.244275\ttrain-error:0.044137\n",
      "[54]\teval-error:0.240458\ttrain-error:0.044137\n",
      "[55]\teval-error:0.232824\ttrain-error:0.044137\n",
      "[56]\teval-error:0.240458\ttrain-error:0.044137\n",
      "[57]\teval-error:0.244275\ttrain-error:0.044137\n",
      "[58]\teval-error:0.240458\ttrain-error:0.044137\n",
      "[59]\teval-error:0.232824\ttrain-error:0.044137\n",
      "[60]\teval-error:0.232824\ttrain-error:0.044137\n",
      "[61]\teval-error:0.236641\ttrain-error:0.044137\n",
      "[62]\teval-error:0.236641\ttrain-error:0.044137\n",
      "[63]\teval-error:0.236641\ttrain-error:0.044137\n",
      "[64]\teval-error:0.236641\ttrain-error:0.044137\n",
      "[65]\teval-error:0.229008\ttrain-error:0.044137\n",
      "[66]\teval-error:0.244275\ttrain-error:0.044137\n",
      "[67]\teval-error:0.225191\ttrain-error:0.044137\n",
      "[68]\teval-error:0.236641\ttrain-error:0.044137\n",
      "[69]\teval-error:0.229008\ttrain-error:0.044137\n",
      "[0]\teval-error:0.274809\ttrain-error:0.081917\n",
      "[1]\teval-error:0.248092\ttrain-error:0.063634\n",
      "[2]\teval-error:0.240458\ttrain-error:0.056135\n",
      "[3]\teval-error:0.244275\ttrain-error:0.050921\n",
      "[4]\teval-error:0.225191\ttrain-error:0.048279\n",
      "[5]\teval-error:0.217557\ttrain-error:0.04685\n",
      "[6]\teval-error:0.217557\ttrain-error:0.045922\n",
      "[7]\teval-error:0.21374\ttrain-error:0.045065\n",
      "[8]\teval-error:0.221374\ttrain-error:0.044851\n",
      "[9]\teval-error:0.217557\ttrain-error:0.044422\n",
      "[10]\teval-error:0.221374\ttrain-error:0.044351\n",
      "[11]\teval-error:0.236641\ttrain-error:0.044279\n",
      "[12]\teval-error:0.229008\ttrain-error:0.044208\n",
      "[13]\teval-error:0.229008\ttrain-error:0.044208\n",
      "[14]\teval-error:0.229008\ttrain-error:0.044208\n",
      "[15]\teval-error:0.229008\ttrain-error:0.044208\n",
      "[16]\teval-error:0.232824\ttrain-error:0.044208\n",
      "[17]\teval-error:0.229008\ttrain-error:0.044208\n",
      "[18]\teval-error:0.229008\ttrain-error:0.044208\n",
      "[19]\teval-error:0.232824\ttrain-error:0.044208\n",
      "[20]\teval-error:0.225191\ttrain-error:0.044208\n",
      "[21]\teval-error:0.229008\ttrain-error:0.044208\n",
      "[22]\teval-error:0.225191\ttrain-error:0.044208\n",
      "[23]\teval-error:0.221374\ttrain-error:0.044208\n",
      "[24]\teval-error:0.221374\ttrain-error:0.044208\n",
      "[25]\teval-error:0.225191\ttrain-error:0.044208\n",
      "[26]\teval-error:0.225191\ttrain-error:0.044208\n",
      "[27]\teval-error:0.221374\ttrain-error:0.044208\n",
      "[28]\teval-error:0.229008\ttrain-error:0.044208\n",
      "[29]\teval-error:0.232824\ttrain-error:0.044208\n",
      "[30]\teval-error:0.232824\ttrain-error:0.044208\n",
      "[31]\teval-error:0.240458\ttrain-error:0.044208\n",
      "[32]\teval-error:0.232824\ttrain-error:0.044208\n",
      "[33]\teval-error:0.229008\ttrain-error:0.044208\n",
      "[34]\teval-error:0.229008\ttrain-error:0.044208\n",
      "[35]\teval-error:0.244275\ttrain-error:0.044208\n",
      "[36]\teval-error:0.229008\ttrain-error:0.044208\n",
      "[37]\teval-error:0.229008\ttrain-error:0.044208\n",
      "[38]\teval-error:0.225191\ttrain-error:0.044208\n",
      "[39]\teval-error:0.229008\ttrain-error:0.044208\n",
      "[40]\teval-error:0.225191\ttrain-error:0.044208\n",
      "[41]\teval-error:0.232824\ttrain-error:0.044208\n",
      "[42]\teval-error:0.229008\ttrain-error:0.044208\n",
      "[43]\teval-error:0.229008\ttrain-error:0.044208\n",
      "[44]\teval-error:0.232824\ttrain-error:0.044208\n",
      "[45]\teval-error:0.229008\ttrain-error:0.044208\n",
      "[46]\teval-error:0.232824\ttrain-error:0.044208\n",
      "[47]\teval-error:0.221374\ttrain-error:0.044208\n",
      "[48]\teval-error:0.232824\ttrain-error:0.044208\n",
      "[49]\teval-error:0.232824\ttrain-error:0.044208\n",
      "[50]\teval-error:0.232824\ttrain-error:0.044208\n",
      "[51]\teval-error:0.225191\ttrain-error:0.044208\n",
      "[52]\teval-error:0.221374\ttrain-error:0.044208\n",
      "[53]\teval-error:0.217557\ttrain-error:0.044208\n",
      "[54]\teval-error:0.217557\ttrain-error:0.044208\n",
      "[55]\teval-error:0.221374\ttrain-error:0.044208\n",
      "[56]\teval-error:0.217557\ttrain-error:0.044208\n",
      "[57]\teval-error:0.217557\ttrain-error:0.044208\n",
      "[58]\teval-error:0.225191\ttrain-error:0.044208\n",
      "[59]\teval-error:0.225191\ttrain-error:0.044208\n",
      "[60]\teval-error:0.217557\ttrain-error:0.044208\n",
      "[61]\teval-error:0.225191\ttrain-error:0.044208\n",
      "[62]\teval-error:0.221374\ttrain-error:0.044208\n",
      "[63]\teval-error:0.221374\ttrain-error:0.044208\n",
      "[64]\teval-error:0.217557\ttrain-error:0.044208\n",
      "[65]\teval-error:0.221374\ttrain-error:0.044208\n",
      "[66]\teval-error:0.225191\ttrain-error:0.044208\n",
      "[67]\teval-error:0.229008\ttrain-error:0.044208\n",
      "[68]\teval-error:0.225191\ttrain-error:0.044208\n",
      "[69]\teval-error:0.225191\ttrain-error:0.044208\n",
      "[0]\teval-error:0.251908\ttrain-error:0.081488\n",
      "[1]\teval-error:0.244275\ttrain-error:0.064205\n",
      "[2]\teval-error:0.267176\ttrain-error:0.055421\n",
      "[3]\teval-error:0.282443\ttrain-error:0.050421\n",
      "[4]\teval-error:0.263359\ttrain-error:0.048493\n",
      "[5]\teval-error:0.28626\ttrain-error:0.046922\n",
      "[6]\teval-error:0.274809\ttrain-error:0.046279\n",
      "[7]\teval-error:0.278626\ttrain-error:0.045351\n",
      "[8]\teval-error:0.282443\ttrain-error:0.044851\n",
      "[9]\teval-error:0.278626\ttrain-error:0.044351\n",
      "[10]\teval-error:0.282443\ttrain-error:0.044208\n",
      "[11]\teval-error:0.267176\ttrain-error:0.044137\n",
      "[12]\teval-error:0.270992\ttrain-error:0.044065\n",
      "[13]\teval-error:0.267176\ttrain-error:0.044065\n",
      "[14]\teval-error:0.259542\ttrain-error:0.044065\n",
      "[15]\teval-error:0.259542\ttrain-error:0.044065\n",
      "[16]\teval-error:0.259542\ttrain-error:0.044065\n",
      "[17]\teval-error:0.255725\ttrain-error:0.044065\n",
      "[18]\teval-error:0.255725\ttrain-error:0.044065\n",
      "[19]\teval-error:0.259542\ttrain-error:0.044065\n",
      "[20]\teval-error:0.270992\ttrain-error:0.044065\n",
      "[21]\teval-error:0.267176\ttrain-error:0.044065\n",
      "[22]\teval-error:0.263359\ttrain-error:0.044065\n",
      "[23]\teval-error:0.263359\ttrain-error:0.044065\n",
      "[24]\teval-error:0.263359\ttrain-error:0.044065\n",
      "[25]\teval-error:0.255725\ttrain-error:0.044065\n",
      "[26]\teval-error:0.259542\ttrain-error:0.044065\n",
      "[27]\teval-error:0.259542\ttrain-error:0.044065\n",
      "[28]\teval-error:0.255725\ttrain-error:0.044065\n",
      "[29]\teval-error:0.255725\ttrain-error:0.044065\n",
      "[30]\teval-error:0.251908\ttrain-error:0.044065\n",
      "[31]\teval-error:0.263359\ttrain-error:0.044065\n",
      "[32]\teval-error:0.259542\ttrain-error:0.044065\n",
      "[33]\teval-error:0.267176\ttrain-error:0.044065\n",
      "[34]\teval-error:0.263359\ttrain-error:0.044065\n",
      "[35]\teval-error:0.259542\ttrain-error:0.044065\n",
      "[36]\teval-error:0.255725\ttrain-error:0.044065\n",
      "[37]\teval-error:0.263359\ttrain-error:0.044065\n",
      "[38]\teval-error:0.267176\ttrain-error:0.044065\n",
      "[39]\teval-error:0.259542\ttrain-error:0.044065\n",
      "[40]\teval-error:0.259542\ttrain-error:0.044065\n",
      "[41]\teval-error:0.255725\ttrain-error:0.044065\n",
      "[42]\teval-error:0.251908\ttrain-error:0.044065\n",
      "[43]\teval-error:0.255725\ttrain-error:0.044065\n",
      "[44]\teval-error:0.255725\ttrain-error:0.044065\n",
      "[45]\teval-error:0.259542\ttrain-error:0.044065\n",
      "[46]\teval-error:0.248092\ttrain-error:0.044065\n",
      "[47]\teval-error:0.255725\ttrain-error:0.044065\n",
      "[48]\teval-error:0.259542\ttrain-error:0.044065\n",
      "[49]\teval-error:0.259542\ttrain-error:0.044065\n",
      "[50]\teval-error:0.255725\ttrain-error:0.044065\n",
      "[51]\teval-error:0.255725\ttrain-error:0.044065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52]\teval-error:0.255725\ttrain-error:0.044065\n",
      "[53]\teval-error:0.259542\ttrain-error:0.044065\n",
      "[54]\teval-error:0.259542\ttrain-error:0.044065\n",
      "[55]\teval-error:0.251908\ttrain-error:0.044065\n",
      "[56]\teval-error:0.251908\ttrain-error:0.044065\n",
      "[57]\teval-error:0.259542\ttrain-error:0.044065\n",
      "[58]\teval-error:0.259542\ttrain-error:0.044065\n",
      "[59]\teval-error:0.259542\ttrain-error:0.044065\n",
      "[60]\teval-error:0.255725\ttrain-error:0.044065\n",
      "[61]\teval-error:0.255725\ttrain-error:0.044065\n",
      "[62]\teval-error:0.259542\ttrain-error:0.044065\n",
      "[63]\teval-error:0.255725\ttrain-error:0.044065\n",
      "[64]\teval-error:0.251908\ttrain-error:0.044065\n",
      "[65]\teval-error:0.248092\ttrain-error:0.044065\n",
      "[66]\teval-error:0.251908\ttrain-error:0.044065\n",
      "[67]\teval-error:0.255725\ttrain-error:0.044065\n",
      "[68]\teval-error:0.255725\ttrain-error:0.044065\n",
      "[69]\teval-error:0.251908\ttrain-error:0.044065\n",
      "[0]\teval-error:0.295455\ttrain-error:0.10953\n",
      "[1]\teval-error:0.261364\ttrain-error:0.082508\n",
      "[2]\teval-error:0.272727\ttrain-error:0.069177\n",
      "[3]\teval-error:0.261364\ttrain-error:0.061971\n",
      "[4]\teval-error:0.272727\ttrain-error:0.06089\n",
      "[5]\teval-error:0.295455\ttrain-error:0.060169\n",
      "[6]\teval-error:0.284091\ttrain-error:0.058728\n",
      "[7]\teval-error:0.272727\ttrain-error:0.058548\n",
      "[8]\teval-error:0.272727\ttrain-error:0.058368\n",
      "[9]\teval-error:0.238636\ttrain-error:0.058188\n",
      "[10]\teval-error:0.238636\ttrain-error:0.058188\n",
      "[11]\teval-error:0.227273\ttrain-error:0.058188\n",
      "[12]\teval-error:0.238636\ttrain-error:0.058188\n",
      "[13]\teval-error:0.238636\ttrain-error:0.058188\n",
      "[14]\teval-error:0.227273\ttrain-error:0.058008\n",
      "[15]\teval-error:0.25\ttrain-error:0.058008\n",
      "[16]\teval-error:0.25\ttrain-error:0.058008\n",
      "[17]\teval-error:0.25\ttrain-error:0.058008\n",
      "[18]\teval-error:0.25\ttrain-error:0.058008\n",
      "[19]\teval-error:0.25\ttrain-error:0.058008\n",
      "[20]\teval-error:0.25\ttrain-error:0.058008\n",
      "[21]\teval-error:0.227273\ttrain-error:0.058008\n",
      "[22]\teval-error:0.25\ttrain-error:0.058008\n",
      "[23]\teval-error:0.25\ttrain-error:0.058008\n",
      "[24]\teval-error:0.25\ttrain-error:0.058008\n",
      "[25]\teval-error:0.261364\ttrain-error:0.058008\n",
      "[26]\teval-error:0.25\ttrain-error:0.058008\n",
      "[27]\teval-error:0.261364\ttrain-error:0.058008\n",
      "[28]\teval-error:0.261364\ttrain-error:0.058008\n",
      "[29]\teval-error:0.25\ttrain-error:0.058008\n",
      "[30]\teval-error:0.261364\ttrain-error:0.058008\n",
      "[31]\teval-error:0.25\ttrain-error:0.058008\n",
      "[32]\teval-error:0.25\ttrain-error:0.058008\n",
      "[33]\teval-error:0.25\ttrain-error:0.058008\n",
      "[34]\teval-error:0.25\ttrain-error:0.058008\n",
      "[35]\teval-error:0.25\ttrain-error:0.058008\n",
      "[36]\teval-error:0.25\ttrain-error:0.058008\n",
      "[37]\teval-error:0.25\ttrain-error:0.058008\n",
      "[38]\teval-error:0.25\ttrain-error:0.058008\n",
      "[39]\teval-error:0.25\ttrain-error:0.058008\n",
      "[40]\teval-error:0.25\ttrain-error:0.058008\n",
      "[41]\teval-error:0.227273\ttrain-error:0.058008\n",
      "[42]\teval-error:0.227273\ttrain-error:0.058008\n",
      "[43]\teval-error:0.227273\ttrain-error:0.058008\n",
      "[44]\teval-error:0.238636\ttrain-error:0.058008\n",
      "[45]\teval-error:0.227273\ttrain-error:0.058008\n",
      "[46]\teval-error:0.238636\ttrain-error:0.058008\n",
      "[47]\teval-error:0.227273\ttrain-error:0.058008\n",
      "[48]\teval-error:0.227273\ttrain-error:0.058008\n",
      "[49]\teval-error:0.227273\ttrain-error:0.058008\n",
      "[50]\teval-error:0.227273\ttrain-error:0.058008\n",
      "[51]\teval-error:0.227273\ttrain-error:0.058008\n",
      "[52]\teval-error:0.227273\ttrain-error:0.058008\n",
      "[53]\teval-error:0.227273\ttrain-error:0.058008\n",
      "[54]\teval-error:0.227273\ttrain-error:0.058008\n",
      "[55]\teval-error:0.227273\ttrain-error:0.058008\n",
      "[56]\teval-error:0.227273\ttrain-error:0.058008\n",
      "[57]\teval-error:0.227273\ttrain-error:0.058008\n",
      "[58]\teval-error:0.227273\ttrain-error:0.058008\n",
      "[59]\teval-error:0.227273\ttrain-error:0.058008\n",
      "[60]\teval-error:0.227273\ttrain-error:0.058008\n",
      "[61]\teval-error:0.227273\ttrain-error:0.058008\n",
      "[62]\teval-error:0.227273\ttrain-error:0.058008\n",
      "[63]\teval-error:0.227273\ttrain-error:0.058008\n",
      "[64]\teval-error:0.227273\ttrain-error:0.058008\n",
      "[65]\teval-error:0.227273\ttrain-error:0.058008\n",
      "[66]\teval-error:0.227273\ttrain-error:0.058008\n",
      "[67]\teval-error:0.227273\ttrain-error:0.058008\n",
      "[68]\teval-error:0.227273\ttrain-error:0.058008\n",
      "[69]\teval-error:0.227273\ttrain-error:0.058008\n",
      "[0]\teval-error:0.295455\ttrain-error:0.116015\n",
      "[1]\teval-error:0.25\ttrain-error:0.083589\n",
      "[2]\teval-error:0.25\ttrain-error:0.069357\n",
      "[3]\teval-error:0.25\ttrain-error:0.064313\n",
      "[4]\teval-error:0.238636\ttrain-error:0.06089\n",
      "[5]\teval-error:0.238636\ttrain-error:0.059269\n",
      "[6]\teval-error:0.193182\ttrain-error:0.058188\n",
      "[7]\teval-error:0.25\ttrain-error:0.058008\n",
      "[8]\teval-error:0.261364\ttrain-error:0.057827\n",
      "[9]\teval-error:0.261364\ttrain-error:0.057647\n",
      "[10]\teval-error:0.25\ttrain-error:0.057647\n",
      "[11]\teval-error:0.25\ttrain-error:0.057647\n",
      "[12]\teval-error:0.25\ttrain-error:0.057647\n",
      "[13]\teval-error:0.25\ttrain-error:0.057647\n",
      "[14]\teval-error:0.238636\ttrain-error:0.057647\n",
      "[15]\teval-error:0.238636\ttrain-error:0.057647\n",
      "[16]\teval-error:0.238636\ttrain-error:0.057647\n",
      "[17]\teval-error:0.238636\ttrain-error:0.057647\n",
      "[18]\teval-error:0.238636\ttrain-error:0.057647\n",
      "[19]\teval-error:0.227273\ttrain-error:0.057647\n",
      "[20]\teval-error:0.238636\ttrain-error:0.057647\n",
      "[21]\teval-error:0.238636\ttrain-error:0.057647\n",
      "[22]\teval-error:0.227273\ttrain-error:0.057647\n",
      "[23]\teval-error:0.261364\ttrain-error:0.057647\n",
      "[24]\teval-error:0.261364\ttrain-error:0.057647\n",
      "[25]\teval-error:0.238636\ttrain-error:0.057647\n",
      "[26]\teval-error:0.25\ttrain-error:0.057647\n",
      "[27]\teval-error:0.238636\ttrain-error:0.057647\n",
      "[28]\teval-error:0.238636\ttrain-error:0.057647\n",
      "[29]\teval-error:0.238636\ttrain-error:0.057647\n",
      "[30]\teval-error:0.25\ttrain-error:0.057647\n",
      "[31]\teval-error:0.25\ttrain-error:0.057647\n",
      "[32]\teval-error:0.25\ttrain-error:0.057647\n",
      "[33]\teval-error:0.25\ttrain-error:0.057647\n",
      "[34]\teval-error:0.25\ttrain-error:0.057647\n",
      "[35]\teval-error:0.25\ttrain-error:0.057647\n",
      "[36]\teval-error:0.25\ttrain-error:0.057647\n",
      "[37]\teval-error:0.25\ttrain-error:0.057647\n",
      "[38]\teval-error:0.238636\ttrain-error:0.057647\n",
      "[39]\teval-error:0.238636\ttrain-error:0.057647\n",
      "[40]\teval-error:0.238636\ttrain-error:0.057647\n",
      "[41]\teval-error:0.227273\ttrain-error:0.057647\n",
      "[42]\teval-error:0.227273\ttrain-error:0.057647\n",
      "[43]\teval-error:0.227273\ttrain-error:0.057647\n",
      "[44]\teval-error:0.227273\ttrain-error:0.057647\n",
      "[45]\teval-error:0.227273\ttrain-error:0.057647\n",
      "[46]\teval-error:0.227273\ttrain-error:0.057647\n",
      "[47]\teval-error:0.227273\ttrain-error:0.057647\n",
      "[48]\teval-error:0.227273\ttrain-error:0.057647\n",
      "[49]\teval-error:0.227273\ttrain-error:0.057647\n",
      "[50]\teval-error:0.227273\ttrain-error:0.057647\n",
      "[51]\teval-error:0.227273\ttrain-error:0.057647\n",
      "[52]\teval-error:0.227273\ttrain-error:0.057647\n",
      "[53]\teval-error:0.215909\ttrain-error:0.057647\n",
      "[54]\teval-error:0.215909\ttrain-error:0.057647\n",
      "[55]\teval-error:0.227273\ttrain-error:0.057647\n",
      "[56]\teval-error:0.215909\ttrain-error:0.057647\n",
      "[57]\teval-error:0.215909\ttrain-error:0.057647\n",
      "[58]\teval-error:0.227273\ttrain-error:0.057647\n",
      "[59]\teval-error:0.204545\ttrain-error:0.057647\n",
      "[60]\teval-error:0.204545\ttrain-error:0.057647\n",
      "[61]\teval-error:0.204545\ttrain-error:0.057647\n",
      "[62]\teval-error:0.215909\ttrain-error:0.057647\n",
      "[63]\teval-error:0.215909\ttrain-error:0.057647\n",
      "[64]\teval-error:0.215909\ttrain-error:0.057647\n",
      "[65]\teval-error:0.215909\ttrain-error:0.057647\n",
      "[66]\teval-error:0.204545\ttrain-error:0.057647\n",
      "[67]\teval-error:0.204545\ttrain-error:0.057647\n",
      "[68]\teval-error:0.215909\ttrain-error:0.057647\n",
      "[69]\teval-error:0.215909\ttrain-error:0.057647\n",
      "[0]\teval-error:0.340909\ttrain-error:0.120159\n",
      "[1]\teval-error:0.284091\ttrain-error:0.089533\n",
      "[2]\teval-error:0.306818\ttrain-error:0.07332\n",
      "[3]\teval-error:0.329545\ttrain-error:0.065574\n",
      "[4]\teval-error:0.352273\ttrain-error:0.061791\n",
      "[5]\teval-error:0.352273\ttrain-error:0.06053\n",
      "[6]\teval-error:0.340909\ttrain-error:0.060349\n",
      "[7]\teval-error:0.329545\ttrain-error:0.060169\n",
      "[8]\teval-error:0.340909\ttrain-error:0.059809\n",
      "[9]\teval-error:0.306818\ttrain-error:0.059809\n",
      "[10]\teval-error:0.318182\ttrain-error:0.059809\n",
      "[11]\teval-error:0.306818\ttrain-error:0.059809\n",
      "[12]\teval-error:0.306818\ttrain-error:0.059809\n",
      "[13]\teval-error:0.306818\ttrain-error:0.059809\n",
      "[14]\teval-error:0.306818\ttrain-error:0.059809\n",
      "[15]\teval-error:0.318182\ttrain-error:0.059809\n",
      "[16]\teval-error:0.318182\ttrain-error:0.059809\n",
      "[17]\teval-error:0.318182\ttrain-error:0.059809\n",
      "[18]\teval-error:0.318182\ttrain-error:0.059809\n",
      "[19]\teval-error:0.318182\ttrain-error:0.059809\n",
      "[20]\teval-error:0.318182\ttrain-error:0.059629\n",
      "[21]\teval-error:0.306818\ttrain-error:0.059809\n",
      "[22]\teval-error:0.295455\ttrain-error:0.059809\n",
      "[23]\teval-error:0.295455\ttrain-error:0.059809\n",
      "[24]\teval-error:0.306818\ttrain-error:0.059629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25]\teval-error:0.318182\ttrain-error:0.059629\n",
      "[26]\teval-error:0.306818\ttrain-error:0.059629\n",
      "[27]\teval-error:0.295455\ttrain-error:0.059629\n",
      "[28]\teval-error:0.295455\ttrain-error:0.059629\n",
      "[29]\teval-error:0.295455\ttrain-error:0.059629\n",
      "[30]\teval-error:0.295455\ttrain-error:0.059629\n",
      "[31]\teval-error:0.306818\ttrain-error:0.059629\n",
      "[32]\teval-error:0.318182\ttrain-error:0.059629\n",
      "[33]\teval-error:0.329545\ttrain-error:0.059629\n",
      "[34]\teval-error:0.295455\ttrain-error:0.059629\n",
      "[35]\teval-error:0.318182\ttrain-error:0.059629\n",
      "[36]\teval-error:0.295455\ttrain-error:0.059629\n",
      "[37]\teval-error:0.306818\ttrain-error:0.059629\n",
      "[38]\teval-error:0.306818\ttrain-error:0.059629\n",
      "[39]\teval-error:0.318182\ttrain-error:0.059629\n",
      "[40]\teval-error:0.318182\ttrain-error:0.059629\n",
      "[41]\teval-error:0.318182\ttrain-error:0.059629\n",
      "[42]\teval-error:0.295455\ttrain-error:0.059629\n",
      "[43]\teval-error:0.306818\ttrain-error:0.059629\n",
      "[44]\teval-error:0.306818\ttrain-error:0.059629\n",
      "[45]\teval-error:0.306818\ttrain-error:0.059629\n",
      "[46]\teval-error:0.306818\ttrain-error:0.059629\n",
      "[47]\teval-error:0.295455\ttrain-error:0.059629\n",
      "[48]\teval-error:0.306818\ttrain-error:0.059629\n",
      "[49]\teval-error:0.306818\ttrain-error:0.059629\n",
      "[50]\teval-error:0.295455\ttrain-error:0.059629\n",
      "[51]\teval-error:0.295455\ttrain-error:0.059629\n",
      "[52]\teval-error:0.295455\ttrain-error:0.059629\n",
      "[53]\teval-error:0.306818\ttrain-error:0.059629\n",
      "[54]\teval-error:0.295455\ttrain-error:0.059629\n",
      "[55]\teval-error:0.306818\ttrain-error:0.059629\n",
      "[56]\teval-error:0.284091\ttrain-error:0.059629\n",
      "[57]\teval-error:0.284091\ttrain-error:0.059629\n",
      "[58]\teval-error:0.284091\ttrain-error:0.059629\n",
      "[59]\teval-error:0.295455\ttrain-error:0.059629\n",
      "[60]\teval-error:0.295455\ttrain-error:0.059629\n",
      "[61]\teval-error:0.306818\ttrain-error:0.059629\n",
      "[62]\teval-error:0.306818\ttrain-error:0.059629\n",
      "[63]\teval-error:0.306818\ttrain-error:0.059629\n",
      "[64]\teval-error:0.306818\ttrain-error:0.059629\n",
      "[65]\teval-error:0.306818\ttrain-error:0.059629\n",
      "[66]\teval-error:0.306818\ttrain-error:0.059629\n",
      "[67]\teval-error:0.306818\ttrain-error:0.059629\n",
      "[68]\teval-error:0.306818\ttrain-error:0.059629\n",
      "[69]\teval-error:0.306818\ttrain-error:0.059629\n",
      "[0]\teval-error:0.329545\ttrain-error:0.118357\n",
      "[1]\teval-error:0.352273\ttrain-error:0.084669\n",
      "[2]\teval-error:0.340909\ttrain-error:0.071158\n",
      "[3]\teval-error:0.272727\ttrain-error:0.064313\n",
      "[4]\teval-error:0.215909\ttrain-error:0.06071\n",
      "[5]\teval-error:0.215909\ttrain-error:0.059809\n",
      "[6]\teval-error:0.204545\ttrain-error:0.058908\n",
      "[7]\teval-error:0.193182\ttrain-error:0.058548\n",
      "[8]\teval-error:0.215909\ttrain-error:0.058548\n",
      "[9]\teval-error:0.227273\ttrain-error:0.058548\n",
      "[10]\teval-error:0.193182\ttrain-error:0.058008\n",
      "[11]\teval-error:0.204545\ttrain-error:0.058008\n",
      "[12]\teval-error:0.215909\ttrain-error:0.057827\n",
      "[13]\teval-error:0.25\ttrain-error:0.057827\n",
      "[14]\teval-error:0.227273\ttrain-error:0.057827\n",
      "[15]\teval-error:0.227273\ttrain-error:0.057827\n",
      "[16]\teval-error:0.227273\ttrain-error:0.057827\n",
      "[17]\teval-error:0.215909\ttrain-error:0.057827\n",
      "[18]\teval-error:0.215909\ttrain-error:0.057827\n",
      "[19]\teval-error:0.204545\ttrain-error:0.057827\n",
      "[20]\teval-error:0.204545\ttrain-error:0.057827\n",
      "[21]\teval-error:0.204545\ttrain-error:0.057827\n",
      "[22]\teval-error:0.204545\ttrain-error:0.057827\n",
      "[23]\teval-error:0.204545\ttrain-error:0.057827\n",
      "[24]\teval-error:0.204545\ttrain-error:0.057827\n",
      "[25]\teval-error:0.204545\ttrain-error:0.057827\n",
      "[26]\teval-error:0.227273\ttrain-error:0.057827\n",
      "[27]\teval-error:0.227273\ttrain-error:0.057827\n",
      "[28]\teval-error:0.204545\ttrain-error:0.057827\n",
      "[29]\teval-error:0.227273\ttrain-error:0.057827\n",
      "[30]\teval-error:0.227273\ttrain-error:0.057827\n",
      "[31]\teval-error:0.227273\ttrain-error:0.057827\n",
      "[32]\teval-error:0.227273\ttrain-error:0.057827\n",
      "[33]\teval-error:0.227273\ttrain-error:0.057827\n",
      "[34]\teval-error:0.227273\ttrain-error:0.057827\n",
      "[35]\teval-error:0.227273\ttrain-error:0.057827\n",
      "[36]\teval-error:0.227273\ttrain-error:0.057827\n",
      "[37]\teval-error:0.227273\ttrain-error:0.057827\n",
      "[38]\teval-error:0.227273\ttrain-error:0.057827\n",
      "[39]\teval-error:0.227273\ttrain-error:0.057827\n",
      "[40]\teval-error:0.238636\ttrain-error:0.057827\n",
      "[41]\teval-error:0.238636\ttrain-error:0.057827\n",
      "[42]\teval-error:0.238636\ttrain-error:0.057827\n",
      "[43]\teval-error:0.238636\ttrain-error:0.057827\n",
      "[44]\teval-error:0.25\ttrain-error:0.057827\n",
      "[45]\teval-error:0.25\ttrain-error:0.057827\n",
      "[46]\teval-error:0.25\ttrain-error:0.057827\n",
      "[47]\teval-error:0.25\ttrain-error:0.057827\n",
      "[48]\teval-error:0.25\ttrain-error:0.057827\n",
      "[49]\teval-error:0.25\ttrain-error:0.057827\n",
      "[50]\teval-error:0.238636\ttrain-error:0.057827\n",
      "[51]\teval-error:0.238636\ttrain-error:0.057827\n",
      "[52]\teval-error:0.238636\ttrain-error:0.057827\n",
      "[53]\teval-error:0.238636\ttrain-error:0.057827\n",
      "[54]\teval-error:0.238636\ttrain-error:0.057827\n",
      "[55]\teval-error:0.238636\ttrain-error:0.057827\n",
      "[56]\teval-error:0.238636\ttrain-error:0.057827\n",
      "[57]\teval-error:0.238636\ttrain-error:0.057827\n",
      "[58]\teval-error:0.25\ttrain-error:0.057827\n",
      "[59]\teval-error:0.25\ttrain-error:0.057827\n",
      "[60]\teval-error:0.25\ttrain-error:0.057827\n",
      "[61]\teval-error:0.25\ttrain-error:0.057827\n",
      "[62]\teval-error:0.25\ttrain-error:0.057827\n",
      "[63]\teval-error:0.25\ttrain-error:0.057827\n",
      "[64]\teval-error:0.25\ttrain-error:0.057827\n",
      "[65]\teval-error:0.25\ttrain-error:0.057827\n",
      "[66]\teval-error:0.25\ttrain-error:0.057827\n",
      "[67]\teval-error:0.25\ttrain-error:0.057827\n",
      "[68]\teval-error:0.227273\ttrain-error:0.057827\n",
      "[69]\teval-error:0.25\ttrain-error:0.057827\n",
      "[0]\teval-error:0.340426\ttrain-error:0.114123\n",
      "[1]\teval-error:0.319149\ttrain-error:0.086367\n",
      "[2]\teval-error:0.340426\ttrain-error:0.073845\n",
      "[3]\teval-error:0.265957\ttrain-error:0.067906\n",
      "[4]\teval-error:0.276596\ttrain-error:0.065711\n",
      "[5]\teval-error:0.287234\ttrain-error:0.064937\n",
      "[6]\teval-error:0.276596\ttrain-error:0.064162\n",
      "[7]\teval-error:0.287234\ttrain-error:0.064291\n",
      "[8]\teval-error:0.287234\ttrain-error:0.064033\n",
      "[9]\teval-error:0.287234\ttrain-error:0.064033\n",
      "[10]\teval-error:0.287234\ttrain-error:0.064033\n",
      "[11]\teval-error:0.297872\ttrain-error:0.064033\n",
      "[12]\teval-error:0.287234\ttrain-error:0.064033\n",
      "[13]\teval-error:0.308511\ttrain-error:0.064033\n",
      "[14]\teval-error:0.287234\ttrain-error:0.064033\n",
      "[15]\teval-error:0.297872\ttrain-error:0.064033\n",
      "[16]\teval-error:0.287234\ttrain-error:0.064033\n",
      "[17]\teval-error:0.297872\ttrain-error:0.064033\n",
      "[18]\teval-error:0.319149\ttrain-error:0.064033\n",
      "[19]\teval-error:0.308511\ttrain-error:0.064033\n",
      "[20]\teval-error:0.308511\ttrain-error:0.064033\n",
      "[21]\teval-error:0.297872\ttrain-error:0.064033\n",
      "[22]\teval-error:0.319149\ttrain-error:0.064033\n",
      "[23]\teval-error:0.329787\ttrain-error:0.064033\n",
      "[24]\teval-error:0.308511\ttrain-error:0.064033\n",
      "[25]\teval-error:0.308511\ttrain-error:0.064033\n",
      "[26]\teval-error:0.308511\ttrain-error:0.064033\n",
      "[27]\teval-error:0.308511\ttrain-error:0.064033\n",
      "[28]\teval-error:0.329787\ttrain-error:0.064033\n",
      "[29]\teval-error:0.308511\ttrain-error:0.064033\n",
      "[30]\teval-error:0.308511\ttrain-error:0.064033\n",
      "[31]\teval-error:0.308511\ttrain-error:0.064033\n",
      "[32]\teval-error:0.308511\ttrain-error:0.064033\n",
      "[33]\teval-error:0.308511\ttrain-error:0.064033\n",
      "[34]\teval-error:0.308511\ttrain-error:0.064033\n",
      "[35]\teval-error:0.329787\ttrain-error:0.064033\n",
      "[36]\teval-error:0.308511\ttrain-error:0.064033\n",
      "[37]\teval-error:0.308511\ttrain-error:0.064033\n",
      "[38]\teval-error:0.308511\ttrain-error:0.064033\n",
      "[39]\teval-error:0.308511\ttrain-error:0.064033\n",
      "[40]\teval-error:0.329787\ttrain-error:0.064033\n",
      "[41]\teval-error:0.329787\ttrain-error:0.064033\n",
      "[42]\teval-error:0.308511\ttrain-error:0.064033\n",
      "[43]\teval-error:0.308511\ttrain-error:0.064033\n",
      "[44]\teval-error:0.308511\ttrain-error:0.064033\n",
      "[45]\teval-error:0.308511\ttrain-error:0.064033\n",
      "[46]\teval-error:0.308511\ttrain-error:0.064033\n",
      "[47]\teval-error:0.308511\ttrain-error:0.064033\n",
      "[48]\teval-error:0.308511\ttrain-error:0.064033\n",
      "[49]\teval-error:0.308511\ttrain-error:0.064033\n",
      "[50]\teval-error:0.308511\ttrain-error:0.064033\n",
      "[51]\teval-error:0.308511\ttrain-error:0.064033\n",
      "[52]\teval-error:0.308511\ttrain-error:0.064033\n",
      "[53]\teval-error:0.308511\ttrain-error:0.064033\n",
      "[54]\teval-error:0.308511\ttrain-error:0.064033\n",
      "[55]\teval-error:0.308511\ttrain-error:0.064033\n",
      "[56]\teval-error:0.329787\ttrain-error:0.064033\n",
      "[57]\teval-error:0.329787\ttrain-error:0.064033\n",
      "[58]\teval-error:0.329787\ttrain-error:0.064033\n",
      "[59]\teval-error:0.329787\ttrain-error:0.064033\n",
      "[60]\teval-error:0.329787\ttrain-error:0.064033\n",
      "[61]\teval-error:0.329787\ttrain-error:0.064033\n",
      "[62]\teval-error:0.329787\ttrain-error:0.064033\n",
      "[63]\teval-error:0.329787\ttrain-error:0.064033\n",
      "[64]\teval-error:0.329787\ttrain-error:0.064033\n",
      "[65]\teval-error:0.329787\ttrain-error:0.064033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[66]\teval-error:0.329787\ttrain-error:0.064033\n",
      "[67]\teval-error:0.329787\ttrain-error:0.064033\n",
      "[68]\teval-error:0.329787\ttrain-error:0.064033\n",
      "[69]\teval-error:0.329787\ttrain-error:0.064033\n",
      "[0]\teval-error:0.340426\ttrain-error:0.108314\n",
      "[1]\teval-error:0.37234\ttrain-error:0.085205\n",
      "[2]\teval-error:0.37234\ttrain-error:0.074619\n",
      "[3]\teval-error:0.340426\ttrain-error:0.068939\n",
      "[4]\teval-error:0.308511\ttrain-error:0.066486\n",
      "[5]\teval-error:0.319149\ttrain-error:0.065453\n",
      "[6]\teval-error:0.329787\ttrain-error:0.064808\n",
      "[7]\teval-error:0.287234\ttrain-error:0.064937\n",
      "[8]\teval-error:0.319149\ttrain-error:0.064679\n",
      "[9]\teval-error:0.308511\ttrain-error:0.064679\n",
      "[10]\teval-error:0.340426\ttrain-error:0.064679\n",
      "[11]\teval-error:0.329787\ttrain-error:0.064549\n",
      "[12]\teval-error:0.308511\ttrain-error:0.064549\n",
      "[13]\teval-error:0.329787\ttrain-error:0.064549\n",
      "[14]\teval-error:0.329787\ttrain-error:0.064549\n",
      "[15]\teval-error:0.319149\ttrain-error:0.064549\n",
      "[16]\teval-error:0.329787\ttrain-error:0.064549\n",
      "[17]\teval-error:0.329787\ttrain-error:0.064549\n",
      "[18]\teval-error:0.319149\ttrain-error:0.064549\n",
      "[19]\teval-error:0.297872\ttrain-error:0.064549\n",
      "[20]\teval-error:0.308511\ttrain-error:0.064549\n",
      "[21]\teval-error:0.287234\ttrain-error:0.064549\n",
      "[22]\teval-error:0.276596\ttrain-error:0.064549\n",
      "[23]\teval-error:0.297872\ttrain-error:0.064549\n",
      "[24]\teval-error:0.287234\ttrain-error:0.064549\n",
      "[25]\teval-error:0.297872\ttrain-error:0.064549\n",
      "[26]\teval-error:0.287234\ttrain-error:0.064549\n",
      "[27]\teval-error:0.297872\ttrain-error:0.064549\n",
      "[28]\teval-error:0.297872\ttrain-error:0.064549\n",
      "[29]\teval-error:0.297872\ttrain-error:0.064549\n",
      "[30]\teval-error:0.308511\ttrain-error:0.064549\n",
      "[31]\teval-error:0.308511\ttrain-error:0.064549\n",
      "[32]\teval-error:0.308511\ttrain-error:0.064549\n",
      "[33]\teval-error:0.308511\ttrain-error:0.064549\n",
      "[34]\teval-error:0.308511\ttrain-error:0.064549\n",
      "[35]\teval-error:0.308511\ttrain-error:0.064549\n",
      "[36]\teval-error:0.308511\ttrain-error:0.064549\n",
      "[37]\teval-error:0.308511\ttrain-error:0.064549\n",
      "[38]\teval-error:0.308511\ttrain-error:0.064549\n",
      "[39]\teval-error:0.308511\ttrain-error:0.064549\n",
      "[40]\teval-error:0.319149\ttrain-error:0.064549\n",
      "[41]\teval-error:0.308511\ttrain-error:0.064549\n",
      "[42]\teval-error:0.319149\ttrain-error:0.064549\n",
      "[43]\teval-error:0.319149\ttrain-error:0.064549\n",
      "[44]\teval-error:0.308511\ttrain-error:0.064549\n",
      "[45]\teval-error:0.297872\ttrain-error:0.064549\n",
      "[46]\teval-error:0.297872\ttrain-error:0.064549\n",
      "[47]\teval-error:0.297872\ttrain-error:0.064549\n",
      "[48]\teval-error:0.297872\ttrain-error:0.064549\n",
      "[49]\teval-error:0.297872\ttrain-error:0.064549\n",
      "[50]\teval-error:0.308511\ttrain-error:0.064549\n",
      "[51]\teval-error:0.308511\ttrain-error:0.064549\n",
      "[52]\teval-error:0.308511\ttrain-error:0.064549\n",
      "[53]\teval-error:0.308511\ttrain-error:0.064549\n",
      "[54]\teval-error:0.308511\ttrain-error:0.064549\n",
      "[55]\teval-error:0.308511\ttrain-error:0.064549\n",
      "[56]\teval-error:0.297872\ttrain-error:0.064549\n",
      "[57]\teval-error:0.297872\ttrain-error:0.064549\n",
      "[58]\teval-error:0.308511\ttrain-error:0.064549\n",
      "[59]\teval-error:0.297872\ttrain-error:0.064549\n",
      "[60]\teval-error:0.297872\ttrain-error:0.064549\n",
      "[61]\teval-error:0.297872\ttrain-error:0.064549\n",
      "[62]\teval-error:0.297872\ttrain-error:0.064549\n",
      "[63]\teval-error:0.308511\ttrain-error:0.064549\n",
      "[64]\teval-error:0.297872\ttrain-error:0.064549\n",
      "[65]\teval-error:0.297872\ttrain-error:0.064549\n",
      "[66]\teval-error:0.297872\ttrain-error:0.064549\n",
      "[67]\teval-error:0.297872\ttrain-error:0.064549\n",
      "[68]\teval-error:0.297872\ttrain-error:0.064549\n",
      "[69]\teval-error:0.297872\ttrain-error:0.064549\n",
      "[0]\teval-error:0.340426\ttrain-error:0.116447\n",
      "[1]\teval-error:0.329787\ttrain-error:0.090627\n",
      "[2]\teval-error:0.319149\ttrain-error:0.077201\n",
      "[3]\teval-error:0.319149\ttrain-error:0.070488\n",
      "[4]\teval-error:0.319149\ttrain-error:0.06739\n",
      "[5]\teval-error:0.308511\ttrain-error:0.066486\n",
      "[6]\teval-error:0.308511\ttrain-error:0.065582\n",
      "[7]\teval-error:0.297872\ttrain-error:0.065195\n",
      "[8]\teval-error:0.308511\ttrain-error:0.065195\n",
      "[9]\teval-error:0.287234\ttrain-error:0.065195\n",
      "[10]\teval-error:0.319149\ttrain-error:0.065066\n",
      "[11]\teval-error:0.319149\ttrain-error:0.065066\n",
      "[12]\teval-error:0.308511\ttrain-error:0.065066\n",
      "[13]\teval-error:0.319149\ttrain-error:0.065066\n",
      "[14]\teval-error:0.308511\ttrain-error:0.065066\n",
      "[15]\teval-error:0.308511\ttrain-error:0.065066\n",
      "[16]\teval-error:0.308511\ttrain-error:0.065066\n",
      "[17]\teval-error:0.297872\ttrain-error:0.065066\n",
      "[18]\teval-error:0.297872\ttrain-error:0.065066\n",
      "[19]\teval-error:0.297872\ttrain-error:0.065066\n",
      "[20]\teval-error:0.297872\ttrain-error:0.065066\n",
      "[21]\teval-error:0.287234\ttrain-error:0.064937\n",
      "[22]\teval-error:0.297872\ttrain-error:0.064937\n",
      "[23]\teval-error:0.297872\ttrain-error:0.065066\n",
      "[24]\teval-error:0.287234\ttrain-error:0.065066\n",
      "[25]\teval-error:0.308511\ttrain-error:0.065066\n",
      "[26]\teval-error:0.297872\ttrain-error:0.064937\n",
      "[27]\teval-error:0.297872\ttrain-error:0.065066\n",
      "[28]\teval-error:0.297872\ttrain-error:0.064937\n",
      "[29]\teval-error:0.297872\ttrain-error:0.064937\n",
      "[30]\teval-error:0.297872\ttrain-error:0.064937\n",
      "[31]\teval-error:0.308511\ttrain-error:0.064937\n",
      "[32]\teval-error:0.308511\ttrain-error:0.064937\n",
      "[33]\teval-error:0.308511\ttrain-error:0.064937\n",
      "[34]\teval-error:0.308511\ttrain-error:0.064937\n",
      "[35]\teval-error:0.308511\ttrain-error:0.064937\n",
      "[36]\teval-error:0.308511\ttrain-error:0.064937\n",
      "[37]\teval-error:0.308511\ttrain-error:0.064937\n",
      "[38]\teval-error:0.308511\ttrain-error:0.064937\n",
      "[39]\teval-error:0.308511\ttrain-error:0.064937\n",
      "[40]\teval-error:0.308511\ttrain-error:0.064937\n",
      "[41]\teval-error:0.308511\ttrain-error:0.064937\n",
      "[42]\teval-error:0.308511\ttrain-error:0.064937\n",
      "[43]\teval-error:0.308511\ttrain-error:0.064937\n",
      "[44]\teval-error:0.308511\ttrain-error:0.064937\n",
      "[45]\teval-error:0.308511\ttrain-error:0.064937\n",
      "[46]\teval-error:0.308511\ttrain-error:0.064937\n",
      "[47]\teval-error:0.308511\ttrain-error:0.064937\n",
      "[48]\teval-error:0.308511\ttrain-error:0.064937\n",
      "[49]\teval-error:0.308511\ttrain-error:0.064937\n",
      "[50]\teval-error:0.308511\ttrain-error:0.064937\n",
      "[51]\teval-error:0.308511\ttrain-error:0.064937\n",
      "[52]\teval-error:0.308511\ttrain-error:0.064937\n",
      "[53]\teval-error:0.308511\ttrain-error:0.064937\n",
      "[54]\teval-error:0.308511\ttrain-error:0.064937\n",
      "[55]\teval-error:0.308511\ttrain-error:0.064937\n",
      "[56]\teval-error:0.308511\ttrain-error:0.064937\n",
      "[57]\teval-error:0.308511\ttrain-error:0.064937\n",
      "[58]\teval-error:0.308511\ttrain-error:0.064937\n",
      "[59]\teval-error:0.308511\ttrain-error:0.064937\n",
      "[60]\teval-error:0.308511\ttrain-error:0.064937\n",
      "[61]\teval-error:0.308511\ttrain-error:0.064937\n",
      "[62]\teval-error:0.308511\ttrain-error:0.064937\n",
      "[63]\teval-error:0.308511\ttrain-error:0.064937\n",
      "[64]\teval-error:0.308511\ttrain-error:0.064937\n",
      "[65]\teval-error:0.308511\ttrain-error:0.064937\n",
      "[66]\teval-error:0.308511\ttrain-error:0.064937\n",
      "[67]\teval-error:0.308511\ttrain-error:0.064937\n",
      "[68]\teval-error:0.308511\ttrain-error:0.064937\n",
      "[69]\teval-error:0.308511\ttrain-error:0.064937\n",
      "[0]\teval-error:0.382979\ttrain-error:0.115285\n",
      "[1]\teval-error:0.340426\ttrain-error:0.085593\n",
      "[2]\teval-error:0.329787\ttrain-error:0.071908\n",
      "[3]\teval-error:0.319149\ttrain-error:0.068293\n",
      "[4]\teval-error:0.308511\ttrain-error:0.066615\n",
      "[5]\teval-error:0.308511\ttrain-error:0.065195\n",
      "[6]\teval-error:0.319149\ttrain-error:0.06442\n",
      "[7]\teval-error:0.308511\ttrain-error:0.064162\n",
      "[8]\teval-error:0.308511\ttrain-error:0.064033\n",
      "[9]\teval-error:0.308511\ttrain-error:0.064033\n",
      "[10]\teval-error:0.308511\ttrain-error:0.064033\n",
      "[11]\teval-error:0.297872\ttrain-error:0.064033\n",
      "[12]\teval-error:0.297872\ttrain-error:0.064033\n",
      "[13]\teval-error:0.287234\ttrain-error:0.064033\n",
      "[14]\teval-error:0.276596\ttrain-error:0.064033\n",
      "[15]\teval-error:0.297872\ttrain-error:0.064033\n",
      "[16]\teval-error:0.265957\ttrain-error:0.064033\n",
      "[17]\teval-error:0.265957\ttrain-error:0.064033\n",
      "[18]\teval-error:0.265957\ttrain-error:0.064033\n",
      "[19]\teval-error:0.265957\ttrain-error:0.064033\n",
      "[20]\teval-error:0.255319\ttrain-error:0.064033\n",
      "[21]\teval-error:0.255319\ttrain-error:0.064033\n",
      "[22]\teval-error:0.276596\ttrain-error:0.064033\n",
      "[23]\teval-error:0.276596\ttrain-error:0.064033\n",
      "[24]\teval-error:0.265957\ttrain-error:0.064033\n",
      "[25]\teval-error:0.276596\ttrain-error:0.064033\n",
      "[26]\teval-error:0.276596\ttrain-error:0.064033\n",
      "[27]\teval-error:0.276596\ttrain-error:0.064033\n",
      "[28]\teval-error:0.276596\ttrain-error:0.064033\n",
      "[29]\teval-error:0.276596\ttrain-error:0.064033\n",
      "[30]\teval-error:0.276596\ttrain-error:0.064033\n",
      "[31]\teval-error:0.276596\ttrain-error:0.064033\n",
      "[32]\teval-error:0.255319\ttrain-error:0.064033\n",
      "[33]\teval-error:0.255319\ttrain-error:0.064033\n",
      "[34]\teval-error:0.255319\ttrain-error:0.064033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35]\teval-error:0.255319\ttrain-error:0.064033\n",
      "[36]\teval-error:0.255319\ttrain-error:0.064033\n",
      "[37]\teval-error:0.255319\ttrain-error:0.064033\n",
      "[38]\teval-error:0.255319\ttrain-error:0.064033\n",
      "[39]\teval-error:0.276596\ttrain-error:0.064033\n",
      "[40]\teval-error:0.255319\ttrain-error:0.064033\n",
      "[41]\teval-error:0.265957\ttrain-error:0.064033\n",
      "[42]\teval-error:0.276596\ttrain-error:0.064033\n",
      "[43]\teval-error:0.276596\ttrain-error:0.064033\n",
      "[44]\teval-error:0.276596\ttrain-error:0.064033\n",
      "[45]\teval-error:0.255319\ttrain-error:0.064033\n",
      "[46]\teval-error:0.255319\ttrain-error:0.064033\n",
      "[47]\teval-error:0.265957\ttrain-error:0.064033\n",
      "[48]\teval-error:0.255319\ttrain-error:0.064033\n",
      "[49]\teval-error:0.255319\ttrain-error:0.064033\n",
      "[50]\teval-error:0.255319\ttrain-error:0.064033\n",
      "[51]\teval-error:0.255319\ttrain-error:0.064033\n",
      "[52]\teval-error:0.255319\ttrain-error:0.064033\n",
      "[53]\teval-error:0.265957\ttrain-error:0.064033\n",
      "[54]\teval-error:0.265957\ttrain-error:0.064033\n",
      "[55]\teval-error:0.276596\ttrain-error:0.064033\n",
      "[56]\teval-error:0.276596\ttrain-error:0.064033\n",
      "[57]\teval-error:0.276596\ttrain-error:0.064033\n",
      "[58]\teval-error:0.297872\ttrain-error:0.064033\n",
      "[59]\teval-error:0.297872\ttrain-error:0.064033\n",
      "[60]\teval-error:0.297872\ttrain-error:0.064033\n",
      "[61]\teval-error:0.297872\ttrain-error:0.064033\n",
      "[62]\teval-error:0.297872\ttrain-error:0.064033\n",
      "[63]\teval-error:0.276596\ttrain-error:0.064033\n",
      "[64]\teval-error:0.276596\ttrain-error:0.064033\n",
      "[65]\teval-error:0.276596\ttrain-error:0.064033\n",
      "[66]\teval-error:0.276596\ttrain-error:0.064033\n",
      "[67]\teval-error:0.276596\ttrain-error:0.064033\n",
      "[68]\teval-error:0.276596\ttrain-error:0.064033\n",
      "[69]\teval-error:0.276596\ttrain-error:0.064033\n",
      "[0]\teval-error:0.28626\ttrain-error:0.095629\n",
      "[1]\teval-error:0.259542\ttrain-error:0.080274\n",
      "[2]\teval-error:0.270992\ttrain-error:0.07199\n",
      "[3]\teval-error:0.267176\ttrain-error:0.068562\n",
      "[4]\teval-error:0.255725\ttrain-error:0.06699\n",
      "[5]\teval-error:0.274809\ttrain-error:0.065991\n",
      "[6]\teval-error:0.263359\ttrain-error:0.065419\n",
      "[7]\teval-error:0.267176\ttrain-error:0.065276\n",
      "[8]\teval-error:0.251908\ttrain-error:0.065062\n",
      "[9]\teval-error:0.255725\ttrain-error:0.064705\n",
      "[10]\teval-error:0.251908\ttrain-error:0.064705\n",
      "[11]\teval-error:0.259542\ttrain-error:0.064705\n",
      "[12]\teval-error:0.255725\ttrain-error:0.064705\n",
      "[13]\teval-error:0.251908\ttrain-error:0.064705\n",
      "[14]\teval-error:0.244275\ttrain-error:0.064705\n",
      "[15]\teval-error:0.240458\ttrain-error:0.064705\n",
      "[16]\teval-error:0.248092\ttrain-error:0.064705\n",
      "[17]\teval-error:0.240458\ttrain-error:0.064705\n",
      "[18]\teval-error:0.236641\ttrain-error:0.064705\n",
      "[19]\teval-error:0.236641\ttrain-error:0.064705\n",
      "[20]\teval-error:0.232824\ttrain-error:0.064705\n",
      "[21]\teval-error:0.244275\ttrain-error:0.064705\n",
      "[22]\teval-error:0.232824\ttrain-error:0.064705\n",
      "[23]\teval-error:0.244275\ttrain-error:0.064705\n",
      "[24]\teval-error:0.248092\ttrain-error:0.064705\n",
      "[25]\teval-error:0.244275\ttrain-error:0.064705\n",
      "[26]\teval-error:0.248092\ttrain-error:0.064705\n",
      "[27]\teval-error:0.232824\ttrain-error:0.064705\n",
      "[28]\teval-error:0.232824\ttrain-error:0.064705\n",
      "[29]\teval-error:0.236641\ttrain-error:0.064705\n",
      "[30]\teval-error:0.248092\ttrain-error:0.064705\n",
      "[31]\teval-error:0.244275\ttrain-error:0.064705\n",
      "[32]\teval-error:0.240458\ttrain-error:0.064705\n",
      "[33]\teval-error:0.244275\ttrain-error:0.064705\n",
      "[34]\teval-error:0.236641\ttrain-error:0.064705\n",
      "[35]\teval-error:0.236641\ttrain-error:0.064705\n",
      "[36]\teval-error:0.236641\ttrain-error:0.064705\n",
      "[37]\teval-error:0.240458\ttrain-error:0.064705\n",
      "[38]\teval-error:0.248092\ttrain-error:0.064705\n",
      "[39]\teval-error:0.236641\ttrain-error:0.064705\n",
      "[40]\teval-error:0.236641\ttrain-error:0.064634\n",
      "[41]\teval-error:0.244275\ttrain-error:0.064634\n",
      "[42]\teval-error:0.240458\ttrain-error:0.064634\n",
      "[43]\teval-error:0.244275\ttrain-error:0.064634\n",
      "[44]\teval-error:0.236641\ttrain-error:0.064634\n",
      "[45]\teval-error:0.236641\ttrain-error:0.064634\n",
      "[46]\teval-error:0.240458\ttrain-error:0.064634\n",
      "[47]\teval-error:0.248092\ttrain-error:0.064634\n",
      "[48]\teval-error:0.248092\ttrain-error:0.064634\n",
      "[49]\teval-error:0.244275\ttrain-error:0.064634\n",
      "[50]\teval-error:0.244275\ttrain-error:0.064634\n",
      "[51]\teval-error:0.240458\ttrain-error:0.064634\n",
      "[52]\teval-error:0.232824\ttrain-error:0.064634\n",
      "[53]\teval-error:0.236641\ttrain-error:0.064634\n",
      "[54]\teval-error:0.232824\ttrain-error:0.064634\n",
      "[55]\teval-error:0.232824\ttrain-error:0.064634\n",
      "[56]\teval-error:0.244275\ttrain-error:0.064634\n",
      "[57]\teval-error:0.240458\ttrain-error:0.064634\n",
      "[58]\teval-error:0.240458\ttrain-error:0.064634\n",
      "[59]\teval-error:0.248092\ttrain-error:0.064634\n",
      "[60]\teval-error:0.236641\ttrain-error:0.064634\n",
      "[61]\teval-error:0.232824\ttrain-error:0.064634\n",
      "[62]\teval-error:0.236641\ttrain-error:0.064634\n",
      "[63]\teval-error:0.236641\ttrain-error:0.064634\n",
      "[64]\teval-error:0.229008\ttrain-error:0.064634\n",
      "[65]\teval-error:0.236641\ttrain-error:0.064634\n",
      "[66]\teval-error:0.236641\ttrain-error:0.064634\n",
      "[67]\teval-error:0.232824\ttrain-error:0.064634\n",
      "[68]\teval-error:0.229008\ttrain-error:0.064634\n",
      "[69]\teval-error:0.232824\ttrain-error:0.064634\n",
      "[0]\teval-error:0.263359\ttrain-error:0.096772\n",
      "[1]\teval-error:0.217557\ttrain-error:0.079703\n",
      "[2]\teval-error:0.225191\ttrain-error:0.072847\n",
      "[3]\teval-error:0.232824\ttrain-error:0.069204\n",
      "[4]\teval-error:0.217557\ttrain-error:0.067062\n",
      "[5]\teval-error:0.198473\ttrain-error:0.066419\n",
      "[6]\teval-error:0.221374\ttrain-error:0.065991\n",
      "[7]\teval-error:0.20229\ttrain-error:0.065776\n",
      "[8]\teval-error:0.21374\ttrain-error:0.065491\n",
      "[9]\teval-error:0.21374\ttrain-error:0.065491\n",
      "[10]\teval-error:0.206107\ttrain-error:0.065348\n",
      "[11]\teval-error:0.21374\ttrain-error:0.065348\n",
      "[12]\teval-error:0.209924\ttrain-error:0.065348\n",
      "[13]\teval-error:0.21374\ttrain-error:0.065276\n",
      "[14]\teval-error:0.209924\ttrain-error:0.065276\n",
      "[15]\teval-error:0.206107\ttrain-error:0.065276\n",
      "[16]\teval-error:0.209924\ttrain-error:0.065276\n",
      "[17]\teval-error:0.206107\ttrain-error:0.065276\n",
      "[18]\teval-error:0.206107\ttrain-error:0.065348\n",
      "[19]\teval-error:0.206107\ttrain-error:0.065276\n",
      "[20]\teval-error:0.206107\ttrain-error:0.065276\n",
      "[21]\teval-error:0.209924\ttrain-error:0.065276\n",
      "[22]\teval-error:0.217557\ttrain-error:0.065276\n",
      "[23]\teval-error:0.217557\ttrain-error:0.065276\n",
      "[24]\teval-error:0.225191\ttrain-error:0.065276\n",
      "[25]\teval-error:0.221374\ttrain-error:0.065276\n",
      "[26]\teval-error:0.225191\ttrain-error:0.065276\n",
      "[27]\teval-error:0.225191\ttrain-error:0.065276\n",
      "[28]\teval-error:0.229008\ttrain-error:0.065276\n",
      "[29]\teval-error:0.225191\ttrain-error:0.065276\n",
      "[30]\teval-error:0.21374\ttrain-error:0.065276\n",
      "[31]\teval-error:0.221374\ttrain-error:0.065276\n",
      "[32]\teval-error:0.217557\ttrain-error:0.065276\n",
      "[33]\teval-error:0.21374\ttrain-error:0.065276\n",
      "[34]\teval-error:0.209924\ttrain-error:0.065276\n",
      "[35]\teval-error:0.209924\ttrain-error:0.065276\n",
      "[36]\teval-error:0.209924\ttrain-error:0.065276\n",
      "[37]\teval-error:0.209924\ttrain-error:0.065276\n",
      "[38]\teval-error:0.209924\ttrain-error:0.065276\n",
      "[39]\teval-error:0.209924\ttrain-error:0.065276\n",
      "[40]\teval-error:0.21374\ttrain-error:0.065276\n",
      "[41]\teval-error:0.20229\ttrain-error:0.065276\n",
      "[42]\teval-error:0.209924\ttrain-error:0.065276\n",
      "[43]\teval-error:0.20229\ttrain-error:0.065276\n",
      "[44]\teval-error:0.21374\ttrain-error:0.065276\n",
      "[45]\teval-error:0.206107\ttrain-error:0.065276\n",
      "[46]\teval-error:0.206107\ttrain-error:0.065276\n",
      "[47]\teval-error:0.20229\ttrain-error:0.065276\n",
      "[48]\teval-error:0.206107\ttrain-error:0.065276\n",
      "[49]\teval-error:0.206107\ttrain-error:0.065276\n",
      "[50]\teval-error:0.206107\ttrain-error:0.065276\n",
      "[51]\teval-error:0.209924\ttrain-error:0.065276\n",
      "[52]\teval-error:0.20229\ttrain-error:0.065276\n",
      "[53]\teval-error:0.20229\ttrain-error:0.065276\n",
      "[54]\teval-error:0.20229\ttrain-error:0.065276\n",
      "[55]\teval-error:0.206107\ttrain-error:0.065276\n",
      "[56]\teval-error:0.20229\ttrain-error:0.065276\n",
      "[57]\teval-error:0.198473\ttrain-error:0.065276\n",
      "[58]\teval-error:0.20229\ttrain-error:0.065276\n",
      "[59]\teval-error:0.198473\ttrain-error:0.065276\n",
      "[60]\teval-error:0.206107\ttrain-error:0.065276\n",
      "[61]\teval-error:0.209924\ttrain-error:0.065276\n",
      "[62]\teval-error:0.20229\ttrain-error:0.065276\n",
      "[63]\teval-error:0.198473\ttrain-error:0.065276\n",
      "[64]\teval-error:0.198473\ttrain-error:0.065276\n",
      "[65]\teval-error:0.209924\ttrain-error:0.065276\n",
      "[66]\teval-error:0.209924\ttrain-error:0.065276\n",
      "[67]\teval-error:0.206107\ttrain-error:0.065276\n",
      "[68]\teval-error:0.20229\ttrain-error:0.065276\n",
      "[69]\teval-error:0.209924\ttrain-error:0.065276\n",
      "[0]\teval-error:0.278626\ttrain-error:0.098986\n",
      "[1]\teval-error:0.255725\ttrain-error:0.083774\n",
      "[2]\teval-error:0.259542\ttrain-error:0.073918\n",
      "[3]\teval-error:0.274809\ttrain-error:0.069419\n",
      "[4]\teval-error:0.263359\ttrain-error:0.067276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\teval-error:0.263359\ttrain-error:0.066419\n",
      "[6]\teval-error:0.259542\ttrain-error:0.066205\n",
      "[7]\teval-error:0.251908\ttrain-error:0.065991\n",
      "[8]\teval-error:0.255725\ttrain-error:0.065919\n",
      "[9]\teval-error:0.255725\ttrain-error:0.065848\n",
      "[10]\teval-error:0.259542\ttrain-error:0.065776\n",
      "[11]\teval-error:0.274809\ttrain-error:0.065776\n",
      "[12]\teval-error:0.270992\ttrain-error:0.065776\n",
      "[13]\teval-error:0.263359\ttrain-error:0.065848\n",
      "[14]\teval-error:0.255725\ttrain-error:0.065776\n",
      "[15]\teval-error:0.267176\ttrain-error:0.065776\n",
      "[16]\teval-error:0.259542\ttrain-error:0.065776\n",
      "[17]\teval-error:0.255725\ttrain-error:0.065776\n",
      "[18]\teval-error:0.248092\ttrain-error:0.065776\n",
      "[19]\teval-error:0.251908\ttrain-error:0.065776\n",
      "[20]\teval-error:0.255725\ttrain-error:0.065776\n",
      "[21]\teval-error:0.255725\ttrain-error:0.065776\n",
      "[22]\teval-error:0.248092\ttrain-error:0.065776\n",
      "[23]\teval-error:0.270992\ttrain-error:0.065776\n",
      "[24]\teval-error:0.255725\ttrain-error:0.065776\n",
      "[25]\teval-error:0.259542\ttrain-error:0.065776\n",
      "[26]\teval-error:0.263359\ttrain-error:0.065776\n",
      "[27]\teval-error:0.267176\ttrain-error:0.065776\n",
      "[28]\teval-error:0.263359\ttrain-error:0.065776\n",
      "[29]\teval-error:0.251908\ttrain-error:0.065776\n",
      "[30]\teval-error:0.251908\ttrain-error:0.065776\n",
      "[31]\teval-error:0.248092\ttrain-error:0.065776\n",
      "[32]\teval-error:0.255725\ttrain-error:0.065776\n",
      "[33]\teval-error:0.263359\ttrain-error:0.065776\n",
      "[34]\teval-error:0.255725\ttrain-error:0.065776\n",
      "[35]\teval-error:0.255725\ttrain-error:0.065776\n",
      "[36]\teval-error:0.267176\ttrain-error:0.065776\n",
      "[37]\teval-error:0.259542\ttrain-error:0.065776\n",
      "[38]\teval-error:0.251908\ttrain-error:0.065776\n",
      "[39]\teval-error:0.263359\ttrain-error:0.065776\n",
      "[40]\teval-error:0.263359\ttrain-error:0.065776\n",
      "[41]\teval-error:0.255725\ttrain-error:0.065776\n",
      "[42]\teval-error:0.259542\ttrain-error:0.065776\n",
      "[43]\teval-error:0.263359\ttrain-error:0.065776\n",
      "[44]\teval-error:0.259542\ttrain-error:0.065776\n",
      "[45]\teval-error:0.263359\ttrain-error:0.065776\n",
      "[46]\teval-error:0.259542\ttrain-error:0.065776\n",
      "[47]\teval-error:0.263359\ttrain-error:0.065776\n",
      "[48]\teval-error:0.267176\ttrain-error:0.065776\n",
      "[49]\teval-error:0.248092\ttrain-error:0.065776\n",
      "[50]\teval-error:0.248092\ttrain-error:0.065776\n",
      "[51]\teval-error:0.251908\ttrain-error:0.065776\n",
      "[52]\teval-error:0.248092\ttrain-error:0.065776\n",
      "[53]\teval-error:0.248092\ttrain-error:0.065776\n",
      "[54]\teval-error:0.244275\ttrain-error:0.065776\n",
      "[55]\teval-error:0.251908\ttrain-error:0.065776\n",
      "[56]\teval-error:0.259542\ttrain-error:0.065776\n",
      "[57]\teval-error:0.259542\ttrain-error:0.065776\n",
      "[58]\teval-error:0.251908\ttrain-error:0.065776\n",
      "[59]\teval-error:0.251908\ttrain-error:0.065776\n",
      "[60]\teval-error:0.259542\ttrain-error:0.065776\n",
      "[61]\teval-error:0.259542\ttrain-error:0.065776\n",
      "[62]\teval-error:0.255725\ttrain-error:0.065776\n",
      "[63]\teval-error:0.248092\ttrain-error:0.065776\n",
      "[64]\teval-error:0.263359\ttrain-error:0.065776\n",
      "[65]\teval-error:0.255725\ttrain-error:0.065776\n",
      "[66]\teval-error:0.255725\ttrain-error:0.065776\n",
      "[67]\teval-error:0.251908\ttrain-error:0.065776\n",
      "[68]\teval-error:0.251908\ttrain-error:0.065776\n",
      "[69]\teval-error:0.255725\ttrain-error:0.065776\n",
      "[0]\teval-error:0.290076\ttrain-error:0.094915\n",
      "[1]\teval-error:0.267176\ttrain-error:0.078275\n",
      "[2]\teval-error:0.290076\ttrain-error:0.07099\n",
      "[3]\teval-error:0.28626\ttrain-error:0.067633\n",
      "[4]\teval-error:0.28626\ttrain-error:0.066133\n",
      "[5]\teval-error:0.251908\ttrain-error:0.065705\n",
      "[6]\teval-error:0.263359\ttrain-error:0.065419\n",
      "[7]\teval-error:0.267176\ttrain-error:0.064919\n",
      "[8]\teval-error:0.255725\ttrain-error:0.064848\n",
      "[9]\teval-error:0.267176\ttrain-error:0.064776\n",
      "[10]\teval-error:0.270992\ttrain-error:0.064776\n",
      "[11]\teval-error:0.270992\ttrain-error:0.064705\n",
      "[12]\teval-error:0.274809\ttrain-error:0.064705\n",
      "[13]\teval-error:0.274809\ttrain-error:0.064705\n",
      "[14]\teval-error:0.28626\ttrain-error:0.064705\n",
      "[15]\teval-error:0.278626\ttrain-error:0.064705\n",
      "[16]\teval-error:0.278626\ttrain-error:0.064705\n",
      "[17]\teval-error:0.270992\ttrain-error:0.064705\n",
      "[18]\teval-error:0.274809\ttrain-error:0.064705\n",
      "[19]\teval-error:0.274809\ttrain-error:0.064705\n",
      "[20]\teval-error:0.274809\ttrain-error:0.064705\n",
      "[21]\teval-error:0.278626\ttrain-error:0.064705\n",
      "[22]\teval-error:0.274809\ttrain-error:0.064705\n",
      "[23]\teval-error:0.278626\ttrain-error:0.064705\n",
      "[24]\teval-error:0.278626\ttrain-error:0.064705\n",
      "[25]\teval-error:0.274809\ttrain-error:0.064705\n",
      "[26]\teval-error:0.270992\ttrain-error:0.064705\n",
      "[27]\teval-error:0.270992\ttrain-error:0.064705\n",
      "[28]\teval-error:0.267176\ttrain-error:0.064705\n",
      "[29]\teval-error:0.270992\ttrain-error:0.064705\n",
      "[30]\teval-error:0.270992\ttrain-error:0.064705\n",
      "[31]\teval-error:0.278626\ttrain-error:0.064705\n",
      "[32]\teval-error:0.274809\ttrain-error:0.064705\n",
      "[33]\teval-error:0.278626\ttrain-error:0.064705\n",
      "[34]\teval-error:0.274809\ttrain-error:0.064705\n",
      "[35]\teval-error:0.270992\ttrain-error:0.064705\n",
      "[36]\teval-error:0.278626\ttrain-error:0.064705\n",
      "[37]\teval-error:0.263359\ttrain-error:0.064705\n",
      "[38]\teval-error:0.267176\ttrain-error:0.064705\n",
      "[39]\teval-error:0.270992\ttrain-error:0.064705\n",
      "[40]\teval-error:0.263359\ttrain-error:0.064634\n",
      "[41]\teval-error:0.263359\ttrain-error:0.064634\n",
      "[42]\teval-error:0.267176\ttrain-error:0.064634\n",
      "[43]\teval-error:0.267176\ttrain-error:0.064634\n",
      "[44]\teval-error:0.270992\ttrain-error:0.064634\n",
      "[45]\teval-error:0.270992\ttrain-error:0.064634\n",
      "[46]\teval-error:0.267176\ttrain-error:0.064634\n",
      "[47]\teval-error:0.270992\ttrain-error:0.064634\n",
      "[48]\teval-error:0.267176\ttrain-error:0.064634\n",
      "[49]\teval-error:0.270992\ttrain-error:0.064634\n",
      "[50]\teval-error:0.263359\ttrain-error:0.064634\n",
      "[51]\teval-error:0.263359\ttrain-error:0.064634\n",
      "[52]\teval-error:0.259542\ttrain-error:0.064634\n",
      "[53]\teval-error:0.263359\ttrain-error:0.064634\n",
      "[54]\teval-error:0.267176\ttrain-error:0.064634\n",
      "[55]\teval-error:0.267176\ttrain-error:0.064634\n",
      "[56]\teval-error:0.267176\ttrain-error:0.064634\n",
      "[57]\teval-error:0.259542\ttrain-error:0.064634\n",
      "[58]\teval-error:0.270992\ttrain-error:0.064634\n",
      "[59]\teval-error:0.270992\ttrain-error:0.064634\n",
      "[60]\teval-error:0.263359\ttrain-error:0.064634\n",
      "[61]\teval-error:0.270992\ttrain-error:0.064634\n",
      "[62]\teval-error:0.270992\ttrain-error:0.064634\n",
      "[63]\teval-error:0.263359\ttrain-error:0.064634\n",
      "[64]\teval-error:0.270992\ttrain-error:0.064634\n",
      "[65]\teval-error:0.270992\ttrain-error:0.064634\n",
      "[66]\teval-error:0.267176\ttrain-error:0.064634\n",
      "[67]\teval-error:0.267176\ttrain-error:0.064634\n",
      "[68]\teval-error:0.270992\ttrain-error:0.064634\n",
      "[69]\teval-error:0.259542\ttrain-error:0.064634\n",
      "[0]\teval-error:0.227273\ttrain-error:0.078004\n",
      "[1]\teval-error:0.215909\ttrain-error:0.037651\n",
      "[2]\teval-error:0.181818\ttrain-error:0.016033\n",
      "[3]\teval-error:0.193182\ttrain-error:0.006305\n",
      "[4]\teval-error:0.170455\ttrain-error:0.002882\n",
      "[5]\teval-error:0.181818\ttrain-error:0.00036\n",
      "[6]\teval-error:0.181818\ttrain-error:0.00036\n",
      "[7]\teval-error:0.170455\ttrain-error:0\n",
      "[8]\teval-error:0.181818\ttrain-error:0\n",
      "[9]\teval-error:0.170455\ttrain-error:0\n",
      "[10]\teval-error:0.159091\ttrain-error:0\n",
      "[11]\teval-error:0.170455\ttrain-error:0\n",
      "[12]\teval-error:0.170455\ttrain-error:0\n",
      "[13]\teval-error:0.170455\ttrain-error:0\n",
      "[14]\teval-error:0.170455\ttrain-error:0\n",
      "[15]\teval-error:0.159091\ttrain-error:0\n",
      "[16]\teval-error:0.159091\ttrain-error:0\n",
      "[17]\teval-error:0.159091\ttrain-error:0\n",
      "[18]\teval-error:0.159091\ttrain-error:0\n",
      "[19]\teval-error:0.159091\ttrain-error:0\n",
      "[20]\teval-error:0.159091\ttrain-error:0\n",
      "[21]\teval-error:0.159091\ttrain-error:0\n",
      "[22]\teval-error:0.159091\ttrain-error:0\n",
      "[23]\teval-error:0.159091\ttrain-error:0\n",
      "[24]\teval-error:0.159091\ttrain-error:0\n",
      "[25]\teval-error:0.159091\ttrain-error:0\n",
      "[26]\teval-error:0.159091\ttrain-error:0\n",
      "[27]\teval-error:0.159091\ttrain-error:0\n",
      "[28]\teval-error:0.159091\ttrain-error:0\n",
      "[29]\teval-error:0.159091\ttrain-error:0\n",
      "[30]\teval-error:0.159091\ttrain-error:0\n",
      "[31]\teval-error:0.159091\ttrain-error:0\n",
      "[32]\teval-error:0.159091\ttrain-error:0\n",
      "[33]\teval-error:0.159091\ttrain-error:0\n",
      "[34]\teval-error:0.159091\ttrain-error:0\n",
      "[35]\teval-error:0.159091\ttrain-error:0\n",
      "[36]\teval-error:0.159091\ttrain-error:0\n",
      "[37]\teval-error:0.159091\ttrain-error:0\n",
      "[38]\teval-error:0.159091\ttrain-error:0\n",
      "[39]\teval-error:0.159091\ttrain-error:0\n",
      "[40]\teval-error:0.159091\ttrain-error:0\n",
      "[41]\teval-error:0.159091\ttrain-error:0\n",
      "[42]\teval-error:0.159091\ttrain-error:0\n",
      "[43]\teval-error:0.159091\ttrain-error:0\n",
      "[44]\teval-error:0.159091\ttrain-error:0\n",
      "[45]\teval-error:0.159091\ttrain-error:0\n",
      "[46]\teval-error:0.159091\ttrain-error:0\n",
      "[47]\teval-error:0.159091\ttrain-error:0\n",
      "[48]\teval-error:0.159091\ttrain-error:0\n",
      "[49]\teval-error:0.159091\ttrain-error:0\n",
      "[50]\teval-error:0.159091\ttrain-error:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51]\teval-error:0.170455\ttrain-error:0\n",
      "[52]\teval-error:0.170455\ttrain-error:0\n",
      "[53]\teval-error:0.170455\ttrain-error:0\n",
      "[54]\teval-error:0.170455\ttrain-error:0\n",
      "[55]\teval-error:0.170455\ttrain-error:0\n",
      "[56]\teval-error:0.170455\ttrain-error:0\n",
      "[57]\teval-error:0.170455\ttrain-error:0\n",
      "[58]\teval-error:0.159091\ttrain-error:0\n",
      "[59]\teval-error:0.170455\ttrain-error:0\n",
      "[60]\teval-error:0.170455\ttrain-error:0\n",
      "[61]\teval-error:0.170455\ttrain-error:0\n",
      "[62]\teval-error:0.170455\ttrain-error:0\n",
      "[63]\teval-error:0.170455\ttrain-error:0\n",
      "[64]\teval-error:0.170455\ttrain-error:0\n",
      "[65]\teval-error:0.170455\ttrain-error:0\n",
      "[66]\teval-error:0.159091\ttrain-error:0\n",
      "[67]\teval-error:0.159091\ttrain-error:0\n",
      "[68]\teval-error:0.159091\ttrain-error:0\n",
      "[69]\teval-error:0.159091\ttrain-error:0\n",
      "[0]\teval-error:0.306818\ttrain-error:0.081787\n",
      "[1]\teval-error:0.261364\ttrain-error:0.033507\n",
      "[2]\teval-error:0.238636\ttrain-error:0.015673\n",
      "[3]\teval-error:0.204545\ttrain-error:0.005765\n",
      "[4]\teval-error:0.204545\ttrain-error:0.002342\n",
      "[5]\teval-error:0.204545\ttrain-error:0.00054\n",
      "[6]\teval-error:0.193182\ttrain-error:0.00018\n",
      "[7]\teval-error:0.204545\ttrain-error:0.00018\n",
      "[8]\teval-error:0.204545\ttrain-error:0\n",
      "[9]\teval-error:0.204545\ttrain-error:0\n",
      "[10]\teval-error:0.193182\ttrain-error:0\n",
      "[11]\teval-error:0.181818\ttrain-error:0\n",
      "[12]\teval-error:0.193182\ttrain-error:0\n",
      "[13]\teval-error:0.193182\ttrain-error:0\n",
      "[14]\teval-error:0.193182\ttrain-error:0\n",
      "[15]\teval-error:0.181818\ttrain-error:0\n",
      "[16]\teval-error:0.193182\ttrain-error:0\n",
      "[17]\teval-error:0.204545\ttrain-error:0\n",
      "[18]\teval-error:0.204545\ttrain-error:0\n",
      "[19]\teval-error:0.204545\ttrain-error:0\n",
      "[20]\teval-error:0.204545\ttrain-error:0\n",
      "[21]\teval-error:0.193182\ttrain-error:0\n",
      "[22]\teval-error:0.193182\ttrain-error:0\n",
      "[23]\teval-error:0.181818\ttrain-error:0\n",
      "[24]\teval-error:0.170455\ttrain-error:0\n",
      "[25]\teval-error:0.181818\ttrain-error:0\n",
      "[26]\teval-error:0.181818\ttrain-error:0\n",
      "[27]\teval-error:0.193182\ttrain-error:0\n",
      "[28]\teval-error:0.193182\ttrain-error:0\n",
      "[29]\teval-error:0.193182\ttrain-error:0\n",
      "[30]\teval-error:0.193182\ttrain-error:0\n",
      "[31]\teval-error:0.193182\ttrain-error:0\n",
      "[32]\teval-error:0.181818\ttrain-error:0\n",
      "[33]\teval-error:0.181818\ttrain-error:0\n",
      "[34]\teval-error:0.181818\ttrain-error:0\n",
      "[35]\teval-error:0.181818\ttrain-error:0\n",
      "[36]\teval-error:0.181818\ttrain-error:0\n",
      "[37]\teval-error:0.181818\ttrain-error:0\n",
      "[38]\teval-error:0.181818\ttrain-error:0\n",
      "[39]\teval-error:0.181818\ttrain-error:0\n",
      "[40]\teval-error:0.181818\ttrain-error:0\n",
      "[41]\teval-error:0.181818\ttrain-error:0\n",
      "[42]\teval-error:0.181818\ttrain-error:0\n",
      "[43]\teval-error:0.181818\ttrain-error:0\n",
      "[44]\teval-error:0.181818\ttrain-error:0\n",
      "[45]\teval-error:0.181818\ttrain-error:0\n",
      "[46]\teval-error:0.181818\ttrain-error:0\n",
      "[47]\teval-error:0.181818\ttrain-error:0\n",
      "[48]\teval-error:0.181818\ttrain-error:0\n",
      "[49]\teval-error:0.181818\ttrain-error:0\n",
      "[50]\teval-error:0.181818\ttrain-error:0\n",
      "[51]\teval-error:0.193182\ttrain-error:0\n",
      "[52]\teval-error:0.181818\ttrain-error:0\n",
      "[53]\teval-error:0.193182\ttrain-error:0\n",
      "[54]\teval-error:0.193182\ttrain-error:0\n",
      "[55]\teval-error:0.181818\ttrain-error:0\n",
      "[56]\teval-error:0.181818\ttrain-error:0\n",
      "[57]\teval-error:0.204545\ttrain-error:0\n",
      "[58]\teval-error:0.193182\ttrain-error:0\n",
      "[59]\teval-error:0.193182\ttrain-error:0\n",
      "[60]\teval-error:0.193182\ttrain-error:0\n",
      "[61]\teval-error:0.181818\ttrain-error:0\n",
      "[62]\teval-error:0.181818\ttrain-error:0\n",
      "[63]\teval-error:0.181818\ttrain-error:0\n",
      "[64]\teval-error:0.181818\ttrain-error:0\n",
      "[65]\teval-error:0.181818\ttrain-error:0\n",
      "[66]\teval-error:0.181818\ttrain-error:0\n",
      "[67]\teval-error:0.181818\ttrain-error:0\n",
      "[68]\teval-error:0.181818\ttrain-error:0\n",
      "[69]\teval-error:0.181818\ttrain-error:0\n",
      "[0]\teval-error:0.284091\ttrain-error:0.080166\n",
      "[1]\teval-error:0.25\ttrain-error:0.034408\n",
      "[2]\teval-error:0.295455\ttrain-error:0.014592\n",
      "[3]\teval-error:0.272727\ttrain-error:0.005044\n",
      "[4]\teval-error:0.284091\ttrain-error:0.001441\n",
      "[5]\teval-error:0.25\ttrain-error:0.00036\n",
      "[6]\teval-error:0.238636\ttrain-error:0.00036\n",
      "[7]\teval-error:0.227273\ttrain-error:0\n",
      "[8]\teval-error:0.227273\ttrain-error:0\n",
      "[9]\teval-error:0.227273\ttrain-error:0\n",
      "[10]\teval-error:0.227273\ttrain-error:0\n",
      "[11]\teval-error:0.227273\ttrain-error:0\n",
      "[12]\teval-error:0.227273\ttrain-error:0\n",
      "[13]\teval-error:0.238636\ttrain-error:0\n",
      "[14]\teval-error:0.238636\ttrain-error:0\n",
      "[15]\teval-error:0.215909\ttrain-error:0\n",
      "[16]\teval-error:0.193182\ttrain-error:0\n",
      "[17]\teval-error:0.193182\ttrain-error:0\n",
      "[18]\teval-error:0.204545\ttrain-error:0\n",
      "[19]\teval-error:0.215909\ttrain-error:0\n",
      "[20]\teval-error:0.215909\ttrain-error:0\n",
      "[21]\teval-error:0.238636\ttrain-error:0\n",
      "[22]\teval-error:0.227273\ttrain-error:0\n",
      "[23]\teval-error:0.215909\ttrain-error:0\n",
      "[24]\teval-error:0.238636\ttrain-error:0\n",
      "[25]\teval-error:0.238636\ttrain-error:0\n",
      "[26]\teval-error:0.215909\ttrain-error:0\n",
      "[27]\teval-error:0.227273\ttrain-error:0\n",
      "[28]\teval-error:0.204545\ttrain-error:0\n",
      "[29]\teval-error:0.238636\ttrain-error:0\n",
      "[30]\teval-error:0.215909\ttrain-error:0\n",
      "[31]\teval-error:0.227273\ttrain-error:0\n",
      "[32]\teval-error:0.227273\ttrain-error:0\n",
      "[33]\teval-error:0.227273\ttrain-error:0\n",
      "[34]\teval-error:0.215909\ttrain-error:0\n",
      "[35]\teval-error:0.215909\ttrain-error:0\n",
      "[36]\teval-error:0.204545\ttrain-error:0\n",
      "[37]\teval-error:0.204545\ttrain-error:0\n",
      "[38]\teval-error:0.215909\ttrain-error:0\n",
      "[39]\teval-error:0.204545\ttrain-error:0\n",
      "[40]\teval-error:0.193182\ttrain-error:0\n",
      "[41]\teval-error:0.193182\ttrain-error:0\n",
      "[42]\teval-error:0.204545\ttrain-error:0\n",
      "[43]\teval-error:0.215909\ttrain-error:0\n",
      "[44]\teval-error:0.215909\ttrain-error:0\n",
      "[45]\teval-error:0.215909\ttrain-error:0\n",
      "[46]\teval-error:0.215909\ttrain-error:0\n",
      "[47]\teval-error:0.215909\ttrain-error:0\n",
      "[48]\teval-error:0.215909\ttrain-error:0\n",
      "[49]\teval-error:0.215909\ttrain-error:0\n",
      "[50]\teval-error:0.215909\ttrain-error:0\n",
      "[51]\teval-error:0.227273\ttrain-error:0\n",
      "[52]\teval-error:0.227273\ttrain-error:0\n",
      "[53]\teval-error:0.227273\ttrain-error:0\n",
      "[54]\teval-error:0.227273\ttrain-error:0\n",
      "[55]\teval-error:0.227273\ttrain-error:0\n",
      "[56]\teval-error:0.227273\ttrain-error:0\n",
      "[57]\teval-error:0.227273\ttrain-error:0\n",
      "[58]\teval-error:0.227273\ttrain-error:0\n",
      "[59]\teval-error:0.227273\ttrain-error:0\n",
      "[60]\teval-error:0.215909\ttrain-error:0\n",
      "[61]\teval-error:0.215909\ttrain-error:0\n",
      "[62]\teval-error:0.227273\ttrain-error:0\n",
      "[63]\teval-error:0.227273\ttrain-error:0\n",
      "[64]\teval-error:0.227273\ttrain-error:0\n",
      "[65]\teval-error:0.215909\ttrain-error:0\n",
      "[66]\teval-error:0.215909\ttrain-error:0\n",
      "[67]\teval-error:0.215909\ttrain-error:0\n",
      "[68]\teval-error:0.215909\ttrain-error:0\n",
      "[69]\teval-error:0.215909\ttrain-error:0\n",
      "[0]\teval-error:0.272727\ttrain-error:0.076202\n",
      "[1]\teval-error:0.25\ttrain-error:0.03693\n",
      "[2]\teval-error:0.238636\ttrain-error:0.016754\n",
      "[3]\teval-error:0.238636\ttrain-error:0.007746\n",
      "[4]\teval-error:0.25\ttrain-error:0.002162\n",
      "[5]\teval-error:0.215909\ttrain-error:0.001261\n",
      "[6]\teval-error:0.215909\ttrain-error:0.00036\n",
      "[7]\teval-error:0.215909\ttrain-error:0.00018\n",
      "[8]\teval-error:0.215909\ttrain-error:0.00018\n",
      "[9]\teval-error:0.204545\ttrain-error:0\n",
      "[10]\teval-error:0.193182\ttrain-error:0\n",
      "[11]\teval-error:0.204545\ttrain-error:0\n",
      "[12]\teval-error:0.193182\ttrain-error:0\n",
      "[13]\teval-error:0.215909\ttrain-error:0\n",
      "[14]\teval-error:0.193182\ttrain-error:0\n",
      "[15]\teval-error:0.181818\ttrain-error:0\n",
      "[16]\teval-error:0.181818\ttrain-error:0\n",
      "[17]\teval-error:0.193182\ttrain-error:0\n",
      "[18]\teval-error:0.193182\ttrain-error:0\n",
      "[19]\teval-error:0.181818\ttrain-error:0\n",
      "[20]\teval-error:0.193182\ttrain-error:0\n",
      "[21]\teval-error:0.204545\ttrain-error:0\n",
      "[22]\teval-error:0.204545\ttrain-error:0\n",
      "[23]\teval-error:0.204545\ttrain-error:0\n",
      "[24]\teval-error:0.204545\ttrain-error:0\n",
      "[25]\teval-error:0.204545\ttrain-error:0\n",
      "[26]\teval-error:0.204545\ttrain-error:0\n",
      "[27]\teval-error:0.204545\ttrain-error:0\n",
      "[28]\teval-error:0.193182\ttrain-error:0\n",
      "[29]\teval-error:0.181818\ttrain-error:0\n",
      "[30]\teval-error:0.193182\ttrain-error:0\n",
      "[31]\teval-error:0.193182\ttrain-error:0\n",
      "[32]\teval-error:0.181818\ttrain-error:0\n",
      "[33]\teval-error:0.181818\ttrain-error:0\n",
      "[34]\teval-error:0.181818\ttrain-error:0\n",
      "[35]\teval-error:0.181818\ttrain-error:0\n",
      "[36]\teval-error:0.181818\ttrain-error:0\n",
      "[37]\teval-error:0.181818\ttrain-error:0\n",
      "[38]\teval-error:0.181818\ttrain-error:0\n",
      "[39]\teval-error:0.181818\ttrain-error:0\n",
      "[40]\teval-error:0.181818\ttrain-error:0\n",
      "[41]\teval-error:0.181818\ttrain-error:0\n",
      "[42]\teval-error:0.181818\ttrain-error:0\n",
      "[43]\teval-error:0.181818\ttrain-error:0\n",
      "[44]\teval-error:0.181818\ttrain-error:0\n",
      "[45]\teval-error:0.181818\ttrain-error:0\n",
      "[46]\teval-error:0.181818\ttrain-error:0\n",
      "[47]\teval-error:0.181818\ttrain-error:0\n",
      "[48]\teval-error:0.181818\ttrain-error:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49]\teval-error:0.181818\ttrain-error:0\n",
      "[50]\teval-error:0.181818\ttrain-error:0\n",
      "[51]\teval-error:0.181818\ttrain-error:0\n",
      "[52]\teval-error:0.181818\ttrain-error:0\n",
      "[53]\teval-error:0.181818\ttrain-error:0\n",
      "[54]\teval-error:0.193182\ttrain-error:0\n",
      "[55]\teval-error:0.193182\ttrain-error:0\n",
      "[56]\teval-error:0.193182\ttrain-error:0\n",
      "[57]\teval-error:0.193182\ttrain-error:0\n",
      "[58]\teval-error:0.193182\ttrain-error:0\n",
      "[59]\teval-error:0.181818\ttrain-error:0\n",
      "[60]\teval-error:0.193182\ttrain-error:0\n",
      "[61]\teval-error:0.193182\ttrain-error:0\n",
      "[62]\teval-error:0.193182\ttrain-error:0\n",
      "[63]\teval-error:0.204545\ttrain-error:0\n",
      "[64]\teval-error:0.193182\ttrain-error:0\n",
      "[65]\teval-error:0.193182\ttrain-error:0\n",
      "[66]\teval-error:0.193182\ttrain-error:0\n",
      "[67]\teval-error:0.193182\ttrain-error:0\n",
      "[68]\teval-error:0.193182\ttrain-error:0\n",
      "[69]\teval-error:0.204545\ttrain-error:0\n",
      "[0]\teval-error:0.244681\ttrain-error:0.072166\n",
      "[1]\teval-error:0.276596\ttrain-error:0.035889\n",
      "[2]\teval-error:0.234043\ttrain-error:0.018978\n",
      "[3]\teval-error:0.255319\ttrain-error:0.01007\n",
      "[4]\teval-error:0.244681\ttrain-error:0.006971\n",
      "[5]\teval-error:0.212766\ttrain-error:0.005164\n",
      "[6]\teval-error:0.234043\ttrain-error:0.004131\n",
      "[7]\teval-error:0.234043\ttrain-error:0.003486\n",
      "[8]\teval-error:0.234043\ttrain-error:0.003227\n",
      "[9]\teval-error:0.234043\ttrain-error:0.003227\n",
      "[10]\teval-error:0.244681\ttrain-error:0.003227\n",
      "[11]\teval-error:0.234043\ttrain-error:0.003227\n",
      "[12]\teval-error:0.234043\ttrain-error:0.003227\n",
      "[13]\teval-error:0.234043\ttrain-error:0.003227\n",
      "[14]\teval-error:0.244681\ttrain-error:0.003098\n",
      "[15]\teval-error:0.223404\ttrain-error:0.003098\n",
      "[16]\teval-error:0.223404\ttrain-error:0.003098\n",
      "[17]\teval-error:0.223404\ttrain-error:0.003098\n",
      "[18]\teval-error:0.223404\ttrain-error:0.003098\n",
      "[19]\teval-error:0.223404\ttrain-error:0.003098\n",
      "[20]\teval-error:0.223404\ttrain-error:0.003098\n",
      "[21]\teval-error:0.223404\ttrain-error:0.003098\n",
      "[22]\teval-error:0.223404\ttrain-error:0.003098\n",
      "[23]\teval-error:0.223404\ttrain-error:0.003098\n",
      "[24]\teval-error:0.223404\ttrain-error:0.003098\n",
      "[25]\teval-error:0.212766\ttrain-error:0.003098\n",
      "[26]\teval-error:0.212766\ttrain-error:0.003098\n",
      "[27]\teval-error:0.223404\ttrain-error:0.003098\n",
      "[28]\teval-error:0.202128\ttrain-error:0.003098\n",
      "[29]\teval-error:0.212766\ttrain-error:0.003098\n",
      "[30]\teval-error:0.212766\ttrain-error:0.003098\n",
      "[31]\teval-error:0.202128\ttrain-error:0.003098\n",
      "[32]\teval-error:0.202128\ttrain-error:0.003098\n",
      "[33]\teval-error:0.202128\ttrain-error:0.003098\n",
      "[34]\teval-error:0.202128\ttrain-error:0.003098\n",
      "[35]\teval-error:0.202128\ttrain-error:0.003098\n",
      "[36]\teval-error:0.202128\ttrain-error:0.003098\n",
      "[37]\teval-error:0.202128\ttrain-error:0.003098\n",
      "[38]\teval-error:0.212766\ttrain-error:0.003098\n",
      "[39]\teval-error:0.191489\ttrain-error:0.003098\n",
      "[40]\teval-error:0.191489\ttrain-error:0.003098\n",
      "[41]\teval-error:0.191489\ttrain-error:0.003098\n",
      "[42]\teval-error:0.191489\ttrain-error:0.003098\n",
      "[43]\teval-error:0.191489\ttrain-error:0.003098\n",
      "[44]\teval-error:0.202128\ttrain-error:0.003098\n",
      "[45]\teval-error:0.202128\ttrain-error:0.003098\n",
      "[46]\teval-error:0.191489\ttrain-error:0.003098\n",
      "[47]\teval-error:0.202128\ttrain-error:0.003098\n",
      "[48]\teval-error:0.202128\ttrain-error:0.003098\n",
      "[49]\teval-error:0.202128\ttrain-error:0.003098\n",
      "[50]\teval-error:0.202128\ttrain-error:0.003098\n",
      "[51]\teval-error:0.212766\ttrain-error:0.003098\n",
      "[52]\teval-error:0.212766\ttrain-error:0.003098\n",
      "[53]\teval-error:0.212766\ttrain-error:0.003098\n",
      "[54]\teval-error:0.202128\ttrain-error:0.003098\n",
      "[55]\teval-error:0.212766\ttrain-error:0.003098\n",
      "[56]\teval-error:0.212766\ttrain-error:0.003098\n",
      "[57]\teval-error:0.191489\ttrain-error:0.003098\n",
      "[58]\teval-error:0.191489\ttrain-error:0.003098\n",
      "[59]\teval-error:0.191489\ttrain-error:0.003098\n",
      "[60]\teval-error:0.202128\ttrain-error:0.003098\n",
      "[61]\teval-error:0.202128\ttrain-error:0.003098\n",
      "[62]\teval-error:0.202128\ttrain-error:0.003098\n",
      "[63]\teval-error:0.202128\ttrain-error:0.003098\n",
      "[64]\teval-error:0.202128\ttrain-error:0.003098\n",
      "[65]\teval-error:0.202128\ttrain-error:0.003098\n",
      "[66]\teval-error:0.202128\ttrain-error:0.003098\n",
      "[67]\teval-error:0.212766\ttrain-error:0.003098\n",
      "[68]\teval-error:0.212766\ttrain-error:0.003098\n",
      "[69]\teval-error:0.212766\ttrain-error:0.003098\n",
      "[0]\teval-error:0.308511\ttrain-error:0.073974\n",
      "[1]\teval-error:0.255319\ttrain-error:0.036535\n",
      "[2]\teval-error:0.255319\ttrain-error:0.017816\n",
      "[3]\teval-error:0.276596\ttrain-error:0.009295\n",
      "[4]\teval-error:0.276596\ttrain-error:0.005939\n",
      "[5]\teval-error:0.265957\ttrain-error:0.004389\n",
      "[6]\teval-error:0.265957\ttrain-error:0.003486\n",
      "[7]\teval-error:0.255319\ttrain-error:0.003227\n",
      "[8]\teval-error:0.265957\ttrain-error:0.003227\n",
      "[9]\teval-error:0.287234\ttrain-error:0.003227\n",
      "[10]\teval-error:0.287234\ttrain-error:0.003227\n",
      "[11]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[12]\teval-error:0.255319\ttrain-error:0.003098\n",
      "[13]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[14]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[15]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[16]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[17]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[18]\teval-error:0.255319\ttrain-error:0.003098\n",
      "[19]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[20]\teval-error:0.255319\ttrain-error:0.003098\n",
      "[21]\teval-error:0.244681\ttrain-error:0.003098\n",
      "[22]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[23]\teval-error:0.255319\ttrain-error:0.003098\n",
      "[24]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[25]\teval-error:0.255319\ttrain-error:0.003098\n",
      "[26]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[27]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[28]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[29]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[30]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[31]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[32]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[33]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[34]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[35]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[36]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[37]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[38]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[39]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[40]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[41]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[42]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[43]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[44]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[45]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[46]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[47]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[48]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[49]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[50]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[51]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[52]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[53]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[54]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[55]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[56]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[57]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[58]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[59]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[60]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[61]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[62]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[63]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[64]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[65]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[66]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[67]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[68]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[69]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[0]\teval-error:0.361702\ttrain-error:0.074361\n",
      "[1]\teval-error:0.319149\ttrain-error:0.037826\n",
      "[2]\teval-error:0.287234\ttrain-error:0.017557\n",
      "[3]\teval-error:0.297872\ttrain-error:0.009424\n",
      "[4]\teval-error:0.329787\ttrain-error:0.006713\n",
      "[5]\teval-error:0.329787\ttrain-error:0.004518\n",
      "[6]\teval-error:0.308511\ttrain-error:0.00426\n",
      "[7]\teval-error:0.297872\ttrain-error:0.003744\n",
      "[8]\teval-error:0.287234\ttrain-error:0.003486\n",
      "[9]\teval-error:0.308511\ttrain-error:0.003357\n",
      "[10]\teval-error:0.319149\ttrain-error:0.003227\n",
      "[11]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[12]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[13]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[14]\teval-error:0.308511\ttrain-error:0.003098\n",
      "[15]\teval-error:0.319149\ttrain-error:0.003098\n",
      "[16]\teval-error:0.319149\ttrain-error:0.003098\n",
      "[17]\teval-error:0.308511\ttrain-error:0.003098\n",
      "[18]\teval-error:0.329787\ttrain-error:0.003098\n",
      "[19]\teval-error:0.319149\ttrain-error:0.003098\n",
      "[20]\teval-error:0.308511\ttrain-error:0.003098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21]\teval-error:0.308511\ttrain-error:0.003098\n",
      "[22]\teval-error:0.308511\ttrain-error:0.003098\n",
      "[23]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[24]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[25]\teval-error:0.308511\ttrain-error:0.003098\n",
      "[26]\teval-error:0.308511\ttrain-error:0.003098\n",
      "[27]\teval-error:0.308511\ttrain-error:0.003098\n",
      "[28]\teval-error:0.308511\ttrain-error:0.003098\n",
      "[29]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[30]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[31]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[32]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[33]\teval-error:0.308511\ttrain-error:0.003098\n",
      "[34]\teval-error:0.308511\ttrain-error:0.003098\n",
      "[35]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[36]\teval-error:0.319149\ttrain-error:0.003098\n",
      "[37]\teval-error:0.308511\ttrain-error:0.003098\n",
      "[38]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[39]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[40]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[41]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[42]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[43]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[44]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[45]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[46]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[47]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[48]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[49]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[50]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[51]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[52]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[53]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[54]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[55]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[56]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[57]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[58]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[59]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[60]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[61]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[62]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[63]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[64]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[65]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[66]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[67]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[68]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[69]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[0]\teval-error:0.340426\ttrain-error:0.07307\n",
      "[1]\teval-error:0.329787\ttrain-error:0.034599\n",
      "[2]\teval-error:0.340426\ttrain-error:0.018461\n",
      "[3]\teval-error:0.340426\ttrain-error:0.009166\n",
      "[4]\teval-error:0.308511\ttrain-error:0.006197\n",
      "[5]\teval-error:0.287234\ttrain-error:0.004648\n",
      "[6]\teval-error:0.287234\ttrain-error:0.003873\n",
      "[7]\teval-error:0.265957\ttrain-error:0.003744\n",
      "[8]\teval-error:0.276596\ttrain-error:0.003357\n",
      "[9]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[10]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[11]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[12]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[13]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[14]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[15]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[16]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[17]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[18]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[19]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[20]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[21]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[22]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[23]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[24]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[25]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[26]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[27]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[28]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[29]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[30]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[31]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[32]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[33]\teval-error:0.308511\ttrain-error:0.003098\n",
      "[34]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[35]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[36]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[37]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[38]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[39]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[40]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[41]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[42]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[43]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[44]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[45]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[46]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[47]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[48]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[49]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[50]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[51]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[52]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[53]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[54]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[55]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[56]\teval-error:0.287234\ttrain-error:0.003098\n",
      "[57]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[58]\teval-error:0.297872\ttrain-error:0.003098\n",
      "[59]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[60]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[61]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[62]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[63]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[64]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[65]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[66]\teval-error:0.276596\ttrain-error:0.003098\n",
      "[67]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[68]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[69]\teval-error:0.265957\ttrain-error:0.003098\n",
      "[0]\teval-error:0.255725\ttrain-error:0.065491\n",
      "[1]\teval-error:0.240458\ttrain-error:0.040209\n",
      "[2]\teval-error:0.232824\ttrain-error:0.026139\n",
      "[3]\teval-error:0.244275\ttrain-error:0.016998\n",
      "[4]\teval-error:0.236641\ttrain-error:0.012998\n",
      "[5]\teval-error:0.240458\ttrain-error:0.01057\n",
      "[6]\teval-error:0.206107\ttrain-error:0.009142\n",
      "[7]\teval-error:0.225191\ttrain-error:0.00857\n",
      "[8]\teval-error:0.209924\ttrain-error:0.008285\n",
      "[9]\teval-error:0.217557\ttrain-error:0.008142\n",
      "[10]\teval-error:0.221374\ttrain-error:0.008213\n",
      "[11]\teval-error:0.229008\ttrain-error:0.008142\n",
      "[12]\teval-error:0.225191\ttrain-error:0.00807\n",
      "[13]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[14]\teval-error:0.225191\ttrain-error:0.007856\n",
      "[15]\teval-error:0.221374\ttrain-error:0.007856\n",
      "[16]\teval-error:0.217557\ttrain-error:0.007856\n",
      "[17]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[18]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[19]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[20]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[21]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[22]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[23]\teval-error:0.232824\ttrain-error:0.007785\n",
      "[24]\teval-error:0.229008\ttrain-error:0.007785\n",
      "[25]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[26]\teval-error:0.232824\ttrain-error:0.007785\n",
      "[27]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[28]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[29]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[30]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[31]\teval-error:0.206107\ttrain-error:0.007785\n",
      "[32]\teval-error:0.206107\ttrain-error:0.007785\n",
      "[33]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[34]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[35]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[36]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[37]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[38]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[39]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[40]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[41]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[42]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[43]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[44]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[45]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[46]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[47]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[48]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[49]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[50]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[51]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[52]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[53]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[54]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[55]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[56]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[57]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[58]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[59]\teval-error:0.217557\ttrain-error:0.007785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[61]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[62]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[63]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[64]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[65]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[66]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[67]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[68]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[69]\teval-error:0.206107\ttrain-error:0.007785\n",
      "[0]\teval-error:0.21374\ttrain-error:0.065562\n",
      "[1]\teval-error:0.206107\ttrain-error:0.038709\n",
      "[2]\teval-error:0.194656\ttrain-error:0.025068\n",
      "[3]\teval-error:0.21374\ttrain-error:0.016283\n",
      "[4]\teval-error:0.198473\ttrain-error:0.012498\n",
      "[5]\teval-error:0.20229\ttrain-error:0.010427\n",
      "[6]\teval-error:0.194656\ttrain-error:0.008784\n",
      "[7]\teval-error:0.198473\ttrain-error:0.008713\n",
      "[8]\teval-error:0.198473\ttrain-error:0.008427\n",
      "[9]\teval-error:0.209924\ttrain-error:0.008142\n",
      "[10]\teval-error:0.209924\ttrain-error:0.00807\n",
      "[11]\teval-error:0.20229\ttrain-error:0.007927\n",
      "[12]\teval-error:0.20229\ttrain-error:0.007927\n",
      "[13]\teval-error:0.209924\ttrain-error:0.007856\n",
      "[14]\teval-error:0.206107\ttrain-error:0.007856\n",
      "[15]\teval-error:0.206107\ttrain-error:0.007785\n",
      "[16]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[17]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[18]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[19]\teval-error:0.20229\ttrain-error:0.007785\n",
      "[20]\teval-error:0.198473\ttrain-error:0.007785\n",
      "[21]\teval-error:0.20229\ttrain-error:0.007785\n",
      "[22]\teval-error:0.206107\ttrain-error:0.007785\n",
      "[23]\teval-error:0.206107\ttrain-error:0.007785\n",
      "[24]\teval-error:0.206107\ttrain-error:0.007785\n",
      "[25]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[26]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[27]\teval-error:0.206107\ttrain-error:0.007785\n",
      "[28]\teval-error:0.206107\ttrain-error:0.007785\n",
      "[29]\teval-error:0.20229\ttrain-error:0.007785\n",
      "[30]\teval-error:0.20229\ttrain-error:0.007785\n",
      "[31]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[32]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[33]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[34]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[35]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[36]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[37]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[38]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[39]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[40]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[41]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[42]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[43]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[44]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[45]\teval-error:0.206107\ttrain-error:0.007785\n",
      "[46]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[47]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[48]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[49]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[50]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[51]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[52]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[53]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[54]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[55]\teval-error:0.206107\ttrain-error:0.007785\n",
      "[56]\teval-error:0.206107\ttrain-error:0.007785\n",
      "[57]\teval-error:0.20229\ttrain-error:0.007785\n",
      "[58]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[59]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[60]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[61]\teval-error:0.206107\ttrain-error:0.007785\n",
      "[62]\teval-error:0.206107\ttrain-error:0.007785\n",
      "[63]\teval-error:0.206107\ttrain-error:0.007785\n",
      "[64]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[65]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[66]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[67]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[68]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[69]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[0]\teval-error:0.270992\ttrain-error:0.066348\n",
      "[1]\teval-error:0.244275\ttrain-error:0.042137\n",
      "[2]\teval-error:0.274809\ttrain-error:0.025925\n",
      "[3]\teval-error:0.270992\ttrain-error:0.01664\n",
      "[4]\teval-error:0.251908\ttrain-error:0.01257\n",
      "[5]\teval-error:0.270992\ttrain-error:0.010141\n",
      "[6]\teval-error:0.267176\ttrain-error:0.008999\n",
      "[7]\teval-error:0.259542\ttrain-error:0.00857\n",
      "[8]\teval-error:0.248092\ttrain-error:0.008356\n",
      "[9]\teval-error:0.251908\ttrain-error:0.008213\n",
      "[10]\teval-error:0.240458\ttrain-error:0.007927\n",
      "[11]\teval-error:0.240458\ttrain-error:0.007927\n",
      "[12]\teval-error:0.244275\ttrain-error:0.007927\n",
      "[13]\teval-error:0.244275\ttrain-error:0.007927\n",
      "[14]\teval-error:0.251908\ttrain-error:0.007856\n",
      "[15]\teval-error:0.240458\ttrain-error:0.007785\n",
      "[16]\teval-error:0.248092\ttrain-error:0.007785\n",
      "[17]\teval-error:0.255725\ttrain-error:0.007785\n",
      "[18]\teval-error:0.251908\ttrain-error:0.007785\n",
      "[19]\teval-error:0.244275\ttrain-error:0.007785\n",
      "[20]\teval-error:0.251908\ttrain-error:0.007785\n",
      "[21]\teval-error:0.244275\ttrain-error:0.007785\n",
      "[22]\teval-error:0.248092\ttrain-error:0.007785\n",
      "[23]\teval-error:0.251908\ttrain-error:0.007785\n",
      "[24]\teval-error:0.251908\ttrain-error:0.007785\n",
      "[25]\teval-error:0.248092\ttrain-error:0.007785\n",
      "[26]\teval-error:0.248092\ttrain-error:0.007785\n",
      "[27]\teval-error:0.255725\ttrain-error:0.007785\n",
      "[28]\teval-error:0.248092\ttrain-error:0.007785\n",
      "[29]\teval-error:0.255725\ttrain-error:0.007785\n",
      "[30]\teval-error:0.255725\ttrain-error:0.007785\n",
      "[31]\teval-error:0.244275\ttrain-error:0.007785\n",
      "[32]\teval-error:0.236641\ttrain-error:0.007785\n",
      "[33]\teval-error:0.244275\ttrain-error:0.007785\n",
      "[34]\teval-error:0.244275\ttrain-error:0.007785\n",
      "[35]\teval-error:0.236641\ttrain-error:0.007785\n",
      "[36]\teval-error:0.240458\ttrain-error:0.007785\n",
      "[37]\teval-error:0.248092\ttrain-error:0.007785\n",
      "[38]\teval-error:0.244275\ttrain-error:0.007785\n",
      "[39]\teval-error:0.236641\ttrain-error:0.007785\n",
      "[40]\teval-error:0.240458\ttrain-error:0.007785\n",
      "[41]\teval-error:0.240458\ttrain-error:0.007785\n",
      "[42]\teval-error:0.240458\ttrain-error:0.007785\n",
      "[43]\teval-error:0.240458\ttrain-error:0.007785\n",
      "[44]\teval-error:0.244275\ttrain-error:0.007785\n",
      "[45]\teval-error:0.240458\ttrain-error:0.007785\n",
      "[46]\teval-error:0.244275\ttrain-error:0.007785\n",
      "[47]\teval-error:0.248092\ttrain-error:0.007785\n",
      "[48]\teval-error:0.251908\ttrain-error:0.007785\n",
      "[49]\teval-error:0.255725\ttrain-error:0.007785\n",
      "[50]\teval-error:0.248092\ttrain-error:0.007785\n",
      "[51]\teval-error:0.248092\ttrain-error:0.007785\n",
      "[52]\teval-error:0.248092\ttrain-error:0.007785\n",
      "[53]\teval-error:0.244275\ttrain-error:0.007785\n",
      "[54]\teval-error:0.244275\ttrain-error:0.007785\n",
      "[55]\teval-error:0.240458\ttrain-error:0.007785\n",
      "[56]\teval-error:0.240458\ttrain-error:0.007785\n",
      "[57]\teval-error:0.240458\ttrain-error:0.007785\n",
      "[58]\teval-error:0.240458\ttrain-error:0.007785\n",
      "[59]\teval-error:0.240458\ttrain-error:0.007785\n",
      "[60]\teval-error:0.236641\ttrain-error:0.007785\n",
      "[61]\teval-error:0.240458\ttrain-error:0.007785\n",
      "[62]\teval-error:0.244275\ttrain-error:0.007785\n",
      "[63]\teval-error:0.240458\ttrain-error:0.007785\n",
      "[64]\teval-error:0.240458\ttrain-error:0.007785\n",
      "[65]\teval-error:0.244275\ttrain-error:0.007785\n",
      "[66]\teval-error:0.244275\ttrain-error:0.007785\n",
      "[67]\teval-error:0.244275\ttrain-error:0.007785\n",
      "[68]\teval-error:0.244275\ttrain-error:0.007785\n",
      "[69]\teval-error:0.240458\ttrain-error:0.007785\n",
      "[0]\teval-error:0.244275\ttrain-error:0.064705\n",
      "[1]\teval-error:0.217557\ttrain-error:0.039923\n",
      "[2]\teval-error:0.225191\ttrain-error:0.025711\n",
      "[3]\teval-error:0.240458\ttrain-error:0.016141\n",
      "[4]\teval-error:0.229008\ttrain-error:0.012141\n",
      "[5]\teval-error:0.232824\ttrain-error:0.010213\n",
      "[6]\teval-error:0.236641\ttrain-error:0.009142\n",
      "[7]\teval-error:0.240458\ttrain-error:0.008427\n",
      "[8]\teval-error:0.251908\ttrain-error:0.008213\n",
      "[9]\teval-error:0.232824\ttrain-error:0.008142\n",
      "[10]\teval-error:0.244275\ttrain-error:0.007927\n",
      "[11]\teval-error:0.240458\ttrain-error:0.007856\n",
      "[12]\teval-error:0.229008\ttrain-error:0.007856\n",
      "[13]\teval-error:0.221374\ttrain-error:0.007856\n",
      "[14]\teval-error:0.225191\ttrain-error:0.007856\n",
      "[15]\teval-error:0.232824\ttrain-error:0.007856\n",
      "[16]\teval-error:0.225191\ttrain-error:0.007856\n",
      "[17]\teval-error:0.221374\ttrain-error:0.007856\n",
      "[18]\teval-error:0.217557\ttrain-error:0.007856\n",
      "[19]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[20]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[21]\teval-error:0.232824\ttrain-error:0.007785\n",
      "[22]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[23]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[24]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[25]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[26]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[27]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[28]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[29]\teval-error:0.217557\ttrain-error:0.007785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[31]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[32]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[33]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[34]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[35]\teval-error:0.206107\ttrain-error:0.007785\n",
      "[36]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[37]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[38]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[39]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[40]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[41]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[42]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[43]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[44]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[45]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[46]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[47]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[48]\teval-error:0.225191\ttrain-error:0.007785\n",
      "[49]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[50]\teval-error:0.221374\ttrain-error:0.007785\n",
      "[51]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[52]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[53]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[54]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[55]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[56]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[57]\teval-error:0.217557\ttrain-error:0.007785\n",
      "[58]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[59]\teval-error:0.21374\ttrain-error:0.007785\n",
      "[60]\teval-error:0.206107\ttrain-error:0.007785\n",
      "[61]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[62]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[63]\teval-error:0.206107\ttrain-error:0.007785\n",
      "[64]\teval-error:0.206107\ttrain-error:0.007785\n",
      "[65]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[66]\teval-error:0.209924\ttrain-error:0.007785\n",
      "[67]\teval-error:0.206107\ttrain-error:0.007785\n",
      "[68]\teval-error:0.20229\ttrain-error:0.007785\n",
      "[69]\teval-error:0.21374\ttrain-error:0.007785\n"
     ]
    }
   ],
   "source": [
    "results = [Result(fs, fs.fc, fs.agg,\n",
    "        precision_recall_fscore_support(*xgboost(*transform_feat_to_num(remove_labels_for_binary_df(fs.train), \n",
    "        remove_labels_for_binary_df(fs.test))), average='macro')) for fs in all_fc_datasets_dswp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.532468</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.530888</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.414634</td>\n",
       "      <td>0.459459</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.412500</td>\n",
       "      <td>0.445946</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.486329</td>\n",
       "      <td>0.491182</td>\n",
       "      <td>0.495174</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.637193</td>\n",
       "      <td>0.660256</td>\n",
       "      <td>0.626263</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.605042</td>\n",
       "      <td>0.652439</td>\n",
       "      <td>0.594697</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.563242</td>\n",
       "      <td>0.572574</td>\n",
       "      <td>0.559975</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.545731</td>\n",
       "      <td>0.559072</td>\n",
       "      <td>0.544192</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.620979</td>\n",
       "      <td>0.671484</td>\n",
       "      <td>0.606449</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.614258</td>\n",
       "      <td>0.655088</td>\n",
       "      <td>0.601595</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.583814</td>\n",
       "      <td>0.647173</td>\n",
       "      <td>0.575589</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.593414</td>\n",
       "      <td>0.634897</td>\n",
       "      <td>0.583738</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.532819</td>\n",
       "      <td>0.532819</td>\n",
       "      <td>0.532819</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.483932</td>\n",
       "      <td>0.484475</td>\n",
       "      <td>0.483591</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.443541</td>\n",
       "      <td>0.465392</td>\n",
       "      <td>0.444981</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.465278</td>\n",
       "      <td>0.469841</td>\n",
       "      <td>0.463320</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.601435</td>\n",
       "      <td>0.605405</td>\n",
       "      <td>0.598485</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.596265</td>\n",
       "      <td>0.632004</td>\n",
       "      <td>0.587753</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.562791</td>\n",
       "      <td>0.563927</td>\n",
       "      <td>0.561869</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.623095</td>\n",
       "      <td>0.625245</td>\n",
       "      <td>0.621212</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.669970</td>\n",
       "      <td>0.692099</td>\n",
       "      <td>0.656813</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.592125</td>\n",
       "      <td>0.598715</td>\n",
       "      <td>0.587899</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.576185</td>\n",
       "      <td>0.590167</td>\n",
       "      <td>0.570822</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.618324</td>\n",
       "      <td>0.622944</td>\n",
       "      <td>0.614684</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.566502</td>\n",
       "      <td>0.564840</td>\n",
       "      <td>0.568533</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.514706</td>\n",
       "      <td>0.523573</td>\n",
       "      <td>0.536680</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.503792</td>\n",
       "      <td>0.510033</td>\n",
       "      <td>0.514479</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.526098</td>\n",
       "      <td>0.526471</td>\n",
       "      <td>0.534749</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.604211</td>\n",
       "      <td>0.622596</td>\n",
       "      <td>0.596591</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.538246</td>\n",
       "      <td>0.547276</td>\n",
       "      <td>0.537247</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.519006</td>\n",
       "      <td>0.530357</td>\n",
       "      <td>0.521465</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.618919</td>\n",
       "      <td>0.630117</td>\n",
       "      <td>0.612374</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.565602</td>\n",
       "      <td>0.567076</td>\n",
       "      <td>0.564407</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.579811</td>\n",
       "      <td>0.590075</td>\n",
       "      <td>0.574896</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.582793</td>\n",
       "      <td>0.594722</td>\n",
       "      <td>0.577323</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.621558</td>\n",
       "      <td>0.627453</td>\n",
       "      <td>0.617112</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.517808</td>\n",
       "      <td>0.517361</td>\n",
       "      <td>0.519305</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.517808</td>\n",
       "      <td>0.517361</td>\n",
       "      <td>0.519305</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.554205</td>\n",
       "      <td>0.551883</td>\n",
       "      <td>0.570463</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.518541</td>\n",
       "      <td>0.520611</td>\n",
       "      <td>0.527992</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.467058</td>\n",
       "      <td>0.470427</td>\n",
       "      <td>0.482955</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.484388</td>\n",
       "      <td>0.508721</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.497326</td>\n",
       "      <td>0.509146</td>\n",
       "      <td>0.505682</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.587719</td>\n",
       "      <td>0.614286</td>\n",
       "      <td>0.580808</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.664743</td>\n",
       "      <td>0.691370</td>\n",
       "      <td>0.650312</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.585156</td>\n",
       "      <td>0.605851</td>\n",
       "      <td>0.578103</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.582793</td>\n",
       "      <td>0.594722</td>\n",
       "      <td>0.577323</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.638181</td>\n",
       "      <td>0.652688</td>\n",
       "      <td>0.629248</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.430670</td>\n",
       "      <td>0.458504</td>\n",
       "      <td>0.431467</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.391933</td>\n",
       "      <td>0.439084</td>\n",
       "      <td>0.390927</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.470234</td>\n",
       "      <td>0.497768</td>\n",
       "      <td>0.496139</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.437100</td>\n",
       "      <td>0.461905</td>\n",
       "      <td>0.438224</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.610282</td>\n",
       "      <td>0.606855</td>\n",
       "      <td>0.633838</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.584301</td>\n",
       "      <td>0.583165</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.629968</td>\n",
       "      <td>0.638518</td>\n",
       "      <td>0.690025</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.567641</td>\n",
       "      <td>0.570098</td>\n",
       "      <td>0.590278</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.565269</td>\n",
       "      <td>0.569369</td>\n",
       "      <td>0.562760</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.589274</td>\n",
       "      <td>0.586016</td>\n",
       "      <td>0.594487</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.548016</td>\n",
       "      <td>0.553069</td>\n",
       "      <td>0.571949</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.577186</td>\n",
       "      <td>0.578902</td>\n",
       "      <td>0.575763</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.498491</td>\n",
       "      <td>0.521687</td>\n",
       "      <td>0.508687</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.565789</td>\n",
       "      <td>0.579487</td>\n",
       "      <td>0.559846</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.496893</td>\n",
       "      <td>0.496923</td>\n",
       "      <td>0.497104</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.614035</td>\n",
       "      <td>0.635897</td>\n",
       "      <td>0.602317</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.632094</td>\n",
       "      <td>0.637162</td>\n",
       "      <td>0.628157</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.562791</td>\n",
       "      <td>0.563927</td>\n",
       "      <td>0.561869</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.632094</td>\n",
       "      <td>0.637162</td>\n",
       "      <td>0.628157</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.664025</td>\n",
       "      <td>0.661666</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.610904</td>\n",
       "      <td>0.627922</td>\n",
       "      <td>0.602462</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.604528</td>\n",
       "      <td>0.617389</td>\n",
       "      <td>0.597607</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.553641</td>\n",
       "      <td>0.565653</td>\n",
       "      <td>0.550537</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.549053</td>\n",
       "      <td>0.587313</td>\n",
       "      <td>0.548024</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.499645</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.523166</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.499645</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.523166</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.499645</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.523166</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.499645</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.523166</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.590886</td>\n",
       "      <td>0.600926</td>\n",
       "      <td>0.637626</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.590886</td>\n",
       "      <td>0.600926</td>\n",
       "      <td>0.637626</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.590886</td>\n",
       "      <td>0.600926</td>\n",
       "      <td>0.637626</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.590886</td>\n",
       "      <td>0.600926</td>\n",
       "      <td>0.637626</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.518714</td>\n",
       "      <td>0.521251</td>\n",
       "      <td>0.526439</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.518714</td>\n",
       "      <td>0.521251</td>\n",
       "      <td>0.526439</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.518714</td>\n",
       "      <td>0.521251</td>\n",
       "      <td>0.526439</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.518714</td>\n",
       "      <td>0.521251</td>\n",
       "      <td>0.526439</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.427083</td>\n",
       "      <td>0.434921</td>\n",
       "      <td>0.420849</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.499645</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.523166</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.500537</td>\n",
       "      <td>0.541340</td>\n",
       "      <td>0.576255</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.525217</td>\n",
       "      <td>0.524658</td>\n",
       "      <td>0.526062</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.636310</td>\n",
       "      <td>0.630656</td>\n",
       "      <td>0.645833</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.653042</td>\n",
       "      <td>0.646547</td>\n",
       "      <td>0.686237</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.573589</td>\n",
       "      <td>0.589744</td>\n",
       "      <td>0.623737</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.552381</td>\n",
       "      <td>0.550905</td>\n",
       "      <td>0.556818</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.528239</td>\n",
       "      <td>0.528668</td>\n",
       "      <td>0.527913</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.502902</td>\n",
       "      <td>0.509163</td>\n",
       "      <td>0.511876</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.477082</td>\n",
       "      <td>0.495265</td>\n",
       "      <td>0.493325</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.525230</td>\n",
       "      <td>0.528583</td>\n",
       "      <td>0.524619</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.557086</td>\n",
       "      <td>0.564935</td>\n",
       "      <td>0.553089</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.503590</td>\n",
       "      <td>0.504386</td>\n",
       "      <td>0.503861</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.474851</td>\n",
       "      <td>0.473277</td>\n",
       "      <td>0.481660</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.690359</td>\n",
       "      <td>0.685388</td>\n",
       "      <td>0.695946</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.571228</td>\n",
       "      <td>0.584936</td>\n",
       "      <td>0.566919</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.660299</td>\n",
       "      <td>0.680290</td>\n",
       "      <td>0.648990</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.632094</td>\n",
       "      <td>0.637162</td>\n",
       "      <td>0.628157</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.570776</td>\n",
       "      <td>0.573649</td>\n",
       "      <td>0.568813</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.636007</td>\n",
       "      <td>0.656277</td>\n",
       "      <td>0.625173</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.598352</td>\n",
       "      <td>0.613745</td>\n",
       "      <td>0.591106</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.538547</td>\n",
       "      <td>0.553220</td>\n",
       "      <td>0.537535</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.585156</td>\n",
       "      <td>0.605851</td>\n",
       "      <td>0.578103</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.503472</td>\n",
       "      <td>0.504762</td>\n",
       "      <td>0.505792</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.261268</td>\n",
       "      <td>0.475342</td>\n",
       "      <td>0.473938</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.517808</td>\n",
       "      <td>0.517361</td>\n",
       "      <td>0.519305</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.546032</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.563707</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.625842</td>\n",
       "      <td>0.622619</td>\n",
       "      <td>0.630051</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.339229</td>\n",
       "      <td>0.507310</td>\n",
       "      <td>0.506313</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433026</td>\n",
       "      <td>0.544562</td>\n",
       "      <td>0.553030</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.627828</td>\n",
       "      <td>0.621875</td>\n",
       "      <td>0.647727</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.521326</td>\n",
       "      <td>0.520961</td>\n",
       "      <td>0.522278</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.370146</td>\n",
       "      <td>0.563013</td>\n",
       "      <td>0.560506</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.417021</td>\n",
       "      <td>0.505998</td>\n",
       "      <td>0.508322</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.580878</td>\n",
       "      <td>0.579254</td>\n",
       "      <td>0.602722</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.486329</td>\n",
       "      <td>0.491182</td>\n",
       "      <td>0.495174</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.532468</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.530888</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.424837</td>\n",
       "      <td>0.411392</td>\n",
       "      <td>0.439189</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.540641</td>\n",
       "      <td>0.542051</td>\n",
       "      <td>0.539575</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.553363</td>\n",
       "      <td>0.572321</td>\n",
       "      <td>0.551136</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.596265</td>\n",
       "      <td>0.632004</td>\n",
       "      <td>0.587753</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.526050</td>\n",
       "      <td>0.542735</td>\n",
       "      <td>0.528409</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.570776</td>\n",
       "      <td>0.573649</td>\n",
       "      <td>0.568813</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.614350</td>\n",
       "      <td>0.643063</td>\n",
       "      <td>0.603242</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.597542</td>\n",
       "      <td>0.630676</td>\n",
       "      <td>0.587812</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.600723</td>\n",
       "      <td>0.637753</td>\n",
       "      <td>0.590239</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.579147</td>\n",
       "      <td>0.595158</td>\n",
       "      <td>0.573249</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.517544</td>\n",
       "      <td>0.523077</td>\n",
       "      <td>0.517375</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.557086</td>\n",
       "      <td>0.564935</td>\n",
       "      <td>0.553089</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.442646</td>\n",
       "      <td>0.444292</td>\n",
       "      <td>0.441120</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.469298</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.474903</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.516509</td>\n",
       "      <td>0.518246</td>\n",
       "      <td>0.516414</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.555405</td>\n",
       "      <td>0.561404</td>\n",
       "      <td>0.553030</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.619008</td>\n",
       "      <td>0.614183</td>\n",
       "      <td>0.640783</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.625842</td>\n",
       "      <td>0.622619</td>\n",
       "      <td>0.630051</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.601277</td>\n",
       "      <td>0.627327</td>\n",
       "      <td>0.591886</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.646424</td>\n",
       "      <td>0.675693</td>\n",
       "      <td>0.632455</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.562059</td>\n",
       "      <td>0.580120</td>\n",
       "      <td>0.557819</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.566394</td>\n",
       "      <td>0.580405</td>\n",
       "      <td>0.561893</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.566807</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.557915</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.486329</td>\n",
       "      <td>0.491182</td>\n",
       "      <td>0.495174</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.492308</td>\n",
       "      <td>0.504065</td>\n",
       "      <td>0.501931</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.623397</td>\n",
       "      <td>0.704762</td>\n",
       "      <td>0.608586</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.478529</td>\n",
       "      <td>0.493464</td>\n",
       "      <td>0.496843</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.578895</td>\n",
       "      <td>0.584211</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.595595</td>\n",
       "      <td>0.608480</td>\n",
       "      <td>0.589646</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.655666</td>\n",
       "      <td>0.683671</td>\n",
       "      <td>0.641383</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.652092</td>\n",
       "      <td>0.676912</td>\n",
       "      <td>0.638956</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.581098</td>\n",
       "      <td>0.607489</td>\n",
       "      <td>0.574029</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.630837</td>\n",
       "      <td>0.665929</td>\n",
       "      <td>0.617025</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                agg  \\\n",
       "0                                                                                                                              mean   \n",
       "1                                                                                                                               max   \n",
       "2                                                                                                                               min   \n",
       "3                                                                                                                     weighted_mean   \n",
       "4                                                                                                                              mean   \n",
       "5                                                                                                                               max   \n",
       "6                                                                                                                               min   \n",
       "7                                                                                                                     weighted_mean   \n",
       "8                                                                                                                              mean   \n",
       "9                                                                                                                               max   \n",
       "10                                                                                                                              min   \n",
       "11                                                                                                                    weighted_mean   \n",
       "12                                                                                                                             mean   \n",
       "13                                                                                                                              max   \n",
       "14                                                                                                                              min   \n",
       "15                                                                                                                    weighted_mean   \n",
       "16                                                                                                                             mean   \n",
       "17                                                                                                                              max   \n",
       "18                                                                                                                              min   \n",
       "19                                                                                                                    weighted_mean   \n",
       "20                                                                                                                             mean   \n",
       "21                                                                                                                              max   \n",
       "22                                                                                                                              min   \n",
       "23                                                                                                                    weighted_mean   \n",
       "24                                                                                                                             mean   \n",
       "25                                                                                                                              max   \n",
       "26                                                                                                                              min   \n",
       "27                                                                                                                    weighted_mean   \n",
       "28                                                                                                                             mean   \n",
       "29                                                                                                                              max   \n",
       "30                                                                                                                              min   \n",
       "31                                                                                                                    weighted_mean   \n",
       "32                                                                                                                             mean   \n",
       "33                                                                                                                              max   \n",
       "34                                                                                                                              min   \n",
       "35                                                                                                                    weighted_mean   \n",
       "36                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "37                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "38                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "39                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "40                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "41                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "42                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "43                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "44                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "45                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "46                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "47                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "48                                                                                                                             mean   \n",
       "49                                                                                                                              max   \n",
       "50                                                                                                                              min   \n",
       "51                                                                                                                    weighted_mean   \n",
       "52                                                                                                                             mean   \n",
       "53                                                                                                                              max   \n",
       "54                                                                                                                              min   \n",
       "55                                                                                                                    weighted_mean   \n",
       "56                                                                                                                             mean   \n",
       "57                                                                                                                              max   \n",
       "58                                                                                                                              min   \n",
       "59                                                                                                                    weighted_mean   \n",
       "60                                                                                                                             mean   \n",
       "61                                                                                                                              max   \n",
       "62                                                                                                                              min   \n",
       "63                                                                                                                    weighted_mean   \n",
       "64                                                                                                                             mean   \n",
       "65                                                                                                                              max   \n",
       "66                                                                                                                              min   \n",
       "67                                                                                                                    weighted_mean   \n",
       "68                                                                                                                             mean   \n",
       "69                                                                                                                              max   \n",
       "70                                                                                                                              min   \n",
       "71                                                                                                                    weighted_mean   \n",
       "72                                                                                                                             mean   \n",
       "73                                                                                                                              max   \n",
       "74                                                                                                                              min   \n",
       "75                                                                                                                    weighted_mean   \n",
       "76                                                                                                                             mean   \n",
       "77                                                                                                                              max   \n",
       "78                                                                                                                              min   \n",
       "79                                                                                                                    weighted_mean   \n",
       "80                                                                                                                             mean   \n",
       "81                                                                                                                              max   \n",
       "82                                                                                                                              min   \n",
       "83                                                                                                                    weighted_mean   \n",
       "84                                                                                                                             mean   \n",
       "85                                                                                                                              max   \n",
       "86                                                                                                                              min   \n",
       "87                                                                                                                    weighted_mean   \n",
       "88                                                                                                                             mean   \n",
       "89                                                                                                                              max   \n",
       "90                                                                                                                              min   \n",
       "91                                                                                                                    weighted_mean   \n",
       "92                                                                                                                             mean   \n",
       "93                                                                                                                              max   \n",
       "94                                                                                                                              min   \n",
       "95                                                                                                                    weighted_mean   \n",
       "96                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "97                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "98                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "99                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "100                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "101                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "102                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "103                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "104                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "105                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "106                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "107                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "108                                                                                                                            mean   \n",
       "109                                                                                                                             max   \n",
       "110                                                                                                                             min   \n",
       "111                                                                                                                   weighted_mean   \n",
       "112                                                                                                                            mean   \n",
       "113                                                                                                                             max   \n",
       "114                                                                                                                             min   \n",
       "115                                                                                                                   weighted_mean   \n",
       "116                                                                                                                            mean   \n",
       "117                                                                                                                             max   \n",
       "118                                                                                                                             min   \n",
       "119                                                                                                                   weighted_mean   \n",
       "120  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "121            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "122            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "123        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "124  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "125            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "126            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "127        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "128  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "129            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "130            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "131        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "132                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "133                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "134                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "135                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "136                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "137                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "138                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "139                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "140                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "141                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "142                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "143                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "144                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "145                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "146                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "147                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "148                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "149                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "150                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "151                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "152                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "153                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "154                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "155                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "\n",
       "       dataset        f1      prec       rec                        zc  \n",
       "0    Wikipedia  0.532468  0.550000  0.530888                linguistic  \n",
       "1    Wikipedia  0.435897  0.414634  0.459459                linguistic  \n",
       "2    Wikipedia  0.428571  0.412500  0.445946                linguistic  \n",
       "3    Wikipedia  0.486329  0.491182  0.495174                linguistic  \n",
       "4     WikiNews  0.637193  0.660256  0.626263                linguistic  \n",
       "5     WikiNews  0.605042  0.652439  0.594697                linguistic  \n",
       "6     WikiNews  0.563242  0.572574  0.559975                linguistic  \n",
       "7     WikiNews  0.545731  0.559072  0.544192                linguistic  \n",
       "8         News  0.620979  0.671484  0.606449                linguistic  \n",
       "9         News  0.614258  0.655088  0.601595                linguistic  \n",
       "10        News  0.583814  0.647173  0.575589                linguistic  \n",
       "11        News  0.593414  0.634897  0.583738                linguistic  \n",
       "12   Wikipedia  0.532819  0.532819  0.532819                 frequency  \n",
       "13   Wikipedia  0.483932  0.484475  0.483591                 frequency  \n",
       "14   Wikipedia  0.443541  0.465392  0.444981                 frequency  \n",
       "15   Wikipedia  0.465278  0.469841  0.463320                 frequency  \n",
       "16    WikiNews  0.601435  0.605405  0.598485                 frequency  \n",
       "17    WikiNews  0.596265  0.632004  0.587753                 frequency  \n",
       "18    WikiNews  0.562791  0.563927  0.561869                 frequency  \n",
       "19    WikiNews  0.623095  0.625245  0.621212                 frequency  \n",
       "20        News  0.669970  0.692099  0.656813                 frequency  \n",
       "21        News  0.592125  0.598715  0.587899                 frequency  \n",
       "22        News  0.576185  0.590167  0.570822                 frequency  \n",
       "23        News  0.618324  0.622944  0.614684                 frequency  \n",
       "24   Wikipedia  0.566502  0.564840  0.568533            language_model  \n",
       "25   Wikipedia  0.514706  0.523573  0.536680            language_model  \n",
       "26   Wikipedia  0.503792  0.510033  0.514479            language_model  \n",
       "27   Wikipedia  0.526098  0.526471  0.534749            language_model  \n",
       "28    WikiNews  0.604211  0.622596  0.596591            language_model  \n",
       "29    WikiNews  0.538246  0.547276  0.537247            language_model  \n",
       "30    WikiNews  0.519006  0.530357  0.521465            language_model  \n",
       "31    WikiNews  0.618919  0.630117  0.612374            language_model  \n",
       "32        News  0.565602  0.567076  0.564407            language_model  \n",
       "33        News  0.579811  0.590075  0.574896            language_model  \n",
       "34        News  0.582793  0.594722  0.577323            language_model  \n",
       "35        News  0.621558  0.627453  0.617112            language_model  \n",
       "36   Wikipedia  0.517808  0.517361  0.519305                    corpus  \n",
       "37   Wikipedia  0.517808  0.517361  0.519305                    corpus  \n",
       "38   Wikipedia  0.554205  0.551883  0.570463                    corpus  \n",
       "39   Wikipedia  0.518541  0.520611  0.527992                    corpus  \n",
       "40    WikiNews  0.467058  0.470427  0.482955                    corpus  \n",
       "41    WikiNews  0.484388  0.508721  0.503788                    corpus  \n",
       "42    WikiNews  0.497326  0.509146  0.505682                    corpus  \n",
       "43    WikiNews  0.587719  0.614286  0.580808                    corpus  \n",
       "44        News  0.664743  0.691370  0.650312                    corpus  \n",
       "45        News  0.585156  0.605851  0.578103                    corpus  \n",
       "46        News  0.582793  0.594722  0.577323                    corpus  \n",
       "47        News  0.638181  0.652688  0.629248                    corpus  \n",
       "48   Wikipedia  0.430670  0.458504  0.431467          psycholinguistic  \n",
       "49   Wikipedia  0.391933  0.439084  0.390927          psycholinguistic  \n",
       "50   Wikipedia  0.470234  0.497768  0.496139          psycholinguistic  \n",
       "51   Wikipedia  0.437100  0.461905  0.438224          psycholinguistic  \n",
       "52    WikiNews  0.610282  0.606855  0.633838          psycholinguistic  \n",
       "53    WikiNews  0.584301  0.583165  0.604167          psycholinguistic  \n",
       "54    WikiNews  0.629968  0.638518  0.690025          psycholinguistic  \n",
       "55    WikiNews  0.567641  0.570098  0.590278          psycholinguistic  \n",
       "56        News  0.565269  0.569369  0.562760          psycholinguistic  \n",
       "57        News  0.589274  0.586016  0.594487          psycholinguistic  \n",
       "58        News  0.548016  0.553069  0.571949          psycholinguistic  \n",
       "59        News  0.577186  0.578902  0.575763          psycholinguistic  \n",
       "60   Wikipedia  0.498491  0.521687  0.508687                   wordnet  \n",
       "61   Wikipedia  0.565789  0.579487  0.559846                   wordnet  \n",
       "62   Wikipedia  0.496893  0.496923  0.497104                   wordnet  \n",
       "63   Wikipedia  0.614035  0.635897  0.602317                   wordnet  \n",
       "64    WikiNews  0.632094  0.637162  0.628157                   wordnet  \n",
       "65    WikiNews  0.562791  0.563927  0.561869                   wordnet  \n",
       "66    WikiNews  0.632094  0.637162  0.628157                   wordnet  \n",
       "67    WikiNews  0.664025  0.661666  0.666667                   wordnet  \n",
       "68        News  0.610904  0.627922  0.602462                   wordnet  \n",
       "69        News  0.604528  0.617389  0.597607                   wordnet  \n",
       "70        News  0.553641  0.565653  0.550537                   wordnet  \n",
       "71        News  0.549053  0.587313  0.548024                   wordnet  \n",
       "72   Wikipedia  0.499645  0.514286  0.523166                   dbpedia  \n",
       "73   Wikipedia  0.499645  0.514286  0.523166                   dbpedia  \n",
       "74   Wikipedia  0.499645  0.514286  0.523166                   dbpedia  \n",
       "75   Wikipedia  0.499645  0.514286  0.523166                   dbpedia  \n",
       "76    WikiNews  0.590886  0.600926  0.637626                   dbpedia  \n",
       "77    WikiNews  0.590886  0.600926  0.637626                   dbpedia  \n",
       "78    WikiNews  0.590886  0.600926  0.637626                   dbpedia  \n",
       "79    WikiNews  0.590886  0.600926  0.637626                   dbpedia  \n",
       "80        News  0.518714  0.521251  0.526439                   dbpedia  \n",
       "81        News  0.518714  0.521251  0.526439                   dbpedia  \n",
       "82        News  0.518714  0.521251  0.526439                   dbpedia  \n",
       "83        News  0.518714  0.521251  0.526439                   dbpedia  \n",
       "84   Wikipedia  0.427083  0.434921  0.420849          brown_clustering  \n",
       "85   Wikipedia  0.499645  0.514286  0.523166          brown_clustering  \n",
       "86   Wikipedia  0.500537  0.541340  0.576255          brown_clustering  \n",
       "87   Wikipedia  0.525217  0.524658  0.526062          brown_clustering  \n",
       "88    WikiNews  0.636310  0.630656  0.645833          brown_clustering  \n",
       "89    WikiNews  0.653042  0.646547  0.686237          brown_clustering  \n",
       "90    WikiNews  0.573589  0.589744  0.623737          brown_clustering  \n",
       "91    WikiNews  0.552381  0.550905  0.556818          brown_clustering  \n",
       "92        News  0.528239  0.528668  0.527913          brown_clustering  \n",
       "93        News  0.502902  0.509163  0.511876          brown_clustering  \n",
       "94        News  0.477082  0.495265  0.493325          brown_clustering  \n",
       "95        News  0.525230  0.528583  0.524619          brown_clustering  \n",
       "96   Wikipedia  0.557086  0.564935  0.553089                  semantic  \n",
       "97   Wikipedia  0.503590  0.504386  0.503861                  semantic  \n",
       "98   Wikipedia  0.474851  0.473277  0.481660                  semantic  \n",
       "99   Wikipedia  0.690359  0.685388  0.695946                  semantic  \n",
       "100   WikiNews  0.571228  0.584936  0.566919                  semantic  \n",
       "101   WikiNews  0.660299  0.680290  0.648990                  semantic  \n",
       "102   WikiNews  0.632094  0.637162  0.628157                  semantic  \n",
       "103   WikiNews  0.570776  0.573649  0.568813                  semantic  \n",
       "104       News  0.636007  0.656277  0.625173                  semantic  \n",
       "105       News  0.598352  0.613745  0.591106                  semantic  \n",
       "106       News  0.538547  0.553220  0.537535                  semantic  \n",
       "107       News  0.585156  0.605851  0.578103                  semantic  \n",
       "108  Wikipedia  0.503472  0.504762  0.505792                dictionary  \n",
       "109  Wikipedia  0.261268  0.475342  0.473938                dictionary  \n",
       "110  Wikipedia  0.517808  0.517361  0.519305                dictionary  \n",
       "111  Wikipedia  0.546032  0.545455  0.563707                dictionary  \n",
       "112   WikiNews  0.625842  0.622619  0.630051                dictionary  \n",
       "113   WikiNews  0.339229  0.507310  0.506313                dictionary  \n",
       "114   WikiNews  0.433026  0.544562  0.553030                dictionary  \n",
       "115   WikiNews  0.627828  0.621875  0.647727                dictionary  \n",
       "116       News  0.521326  0.520961  0.522278                dictionary  \n",
       "117       News  0.370146  0.563013  0.560506                dictionary  \n",
       "118       News  0.417021  0.505998  0.508322                dictionary  \n",
       "119       News  0.580878  0.579254  0.602722                dictionary  \n",
       "120  Wikipedia  0.486329  0.491182  0.495174           corpus+semantic  \n",
       "121  Wikipedia  0.532468  0.550000  0.530888           corpus+semantic  \n",
       "122  Wikipedia  0.424837  0.411392  0.439189           corpus+semantic  \n",
       "123  Wikipedia  0.540641  0.542051  0.539575           corpus+semantic  \n",
       "124   WikiNews  0.553363  0.572321  0.551136           corpus+semantic  \n",
       "125   WikiNews  0.596265  0.632004  0.587753           corpus+semantic  \n",
       "126   WikiNews  0.526050  0.542735  0.528409           corpus+semantic  \n",
       "127   WikiNews  0.570776  0.573649  0.568813           corpus+semantic  \n",
       "128       News  0.614350  0.643063  0.603242           corpus+semantic  \n",
       "129       News  0.597542  0.630676  0.587812           corpus+semantic  \n",
       "130       News  0.600723  0.637753  0.590239           corpus+semantic  \n",
       "131       News  0.579147  0.595158  0.573249           corpus+semantic  \n",
       "132  Wikipedia  0.517544  0.523077  0.517375  wordnet+psycholinguistic  \n",
       "133  Wikipedia  0.557086  0.564935  0.553089  wordnet+psycholinguistic  \n",
       "134  Wikipedia  0.442646  0.444292  0.441120  wordnet+psycholinguistic  \n",
       "135  Wikipedia  0.469298  0.466667  0.474903  wordnet+psycholinguistic  \n",
       "136   WikiNews  0.516509  0.518246  0.516414  wordnet+psycholinguistic  \n",
       "137   WikiNews  0.555405  0.561404  0.553030  wordnet+psycholinguistic  \n",
       "138   WikiNews  0.619008  0.614183  0.640783  wordnet+psycholinguistic  \n",
       "139   WikiNews  0.625842  0.622619  0.630051  wordnet+psycholinguistic  \n",
       "140       News  0.601277  0.627327  0.591886  wordnet+psycholinguistic  \n",
       "141       News  0.646424  0.675693  0.632455  wordnet+psycholinguistic  \n",
       "142       News  0.562059  0.580120  0.557819  wordnet+psycholinguistic  \n",
       "143       News  0.566394  0.580405  0.561893  wordnet+psycholinguistic  \n",
       "144  Wikipedia  0.566807  0.678571  0.557915                       all  \n",
       "145  Wikipedia  0.450000  0.418605  0.486486                       all  \n",
       "146  Wikipedia  0.486329  0.491182  0.495174                       all  \n",
       "147  Wikipedia  0.492308  0.504065  0.501931                       all  \n",
       "148   WikiNews  0.623397  0.704762  0.608586                       all  \n",
       "149   WikiNews  0.478529  0.493464  0.496843                       all  \n",
       "150   WikiNews  0.578895  0.584211  0.575758                       all  \n",
       "151   WikiNews  0.595595  0.608480  0.589646                       all  \n",
       "152       News  0.655666  0.683671  0.641383                       all  \n",
       "153       News  0.652092  0.676912  0.638956                       all  \n",
       "154       News  0.581098  0.607489  0.574029                       all  \n",
       "155       News  0.630837  0.665929  0.617025                       all  "
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_eval_data_xg_dswp = create_eval_df_from_results_macro(results, False)\n",
    "feature_eval_data_xg_dswp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.669970</td>\n",
       "      <td>0.692099</td>\n",
       "      <td>0.656813</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.664025</td>\n",
       "      <td>0.661666</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.690359</td>\n",
       "      <td>0.685388</td>\n",
       "      <td>0.695946</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           agg    dataset  \\\n",
       "20                                                        mean       News   \n",
       "67                                               weighted_mean   WikiNews   \n",
       "99  (weighted_mean, <function <lambda> at 0x000000FF52232E18>)  Wikipedia   \n",
       "\n",
       "          f1      prec       rec         zc  \n",
       "20  0.669970  0.692099  0.656813  frequency  \n",
       "67  0.664025  0.661666  0.666667    wordnet  \n",
       "99  0.690359  0.685388  0.695946   semantic  "
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = feature_eval_data_xg_dswp.groupby(['dataset'])['f1'].transform(max) == feature_eval_data_xg_dswp['f1']\n",
    "feature_eval_data_xg_dswp[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.2.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    4.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    3.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    3.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    5.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    5.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    5.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    5.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    6.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   10.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    8.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    7.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   14.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    4.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    3.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    3.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    4.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    5.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    5.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    4.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    6.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   13.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   10.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   10.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   13.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    4.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    4.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    4.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    4.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    5.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    5.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    5.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    5.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   12.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   10.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   10.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   12.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    5.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    5.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    4.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    6.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    7.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    6.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    6.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    7.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   17.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   14.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   13.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   17.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    2.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    3.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    3.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    2.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    4.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    7.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    5.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    3.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    8.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    5.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    3.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    3.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    6.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    6.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    5.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    4.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    8.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   15.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   10.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    8.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   18.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    2.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    3.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    3.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    3.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    3.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    6.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    6.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    6.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    6.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    2.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    3.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    3.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    4.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    6.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    5.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    4.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    8.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    9.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    4.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    4.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    6.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    7.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    6.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    5.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    8.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   16.6s finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   13.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   11.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   19.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    1.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    3.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    8.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    7.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    6.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    9.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   10.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    9.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    8.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   11.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   24.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   20.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   17.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   27.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    5.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    4.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    3.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    7.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    7.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    5.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    4.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    9.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   17.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   12.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    9.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   21.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    8.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    7.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    6.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   10.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   11.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    9.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    8.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   13.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   27.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   21.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   19.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   33.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "results = [Result(fs, fs.fc, fs.agg,\n",
    "        precision_recall_fscore_support(*random_forest(*transform_feat_to_num(remove_labels_for_binary_df(fs.train), \n",
    "        remove_labels_for_binary_df(fs.test))), average='macro')) for fs in all_fc_datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.511654</td>\n",
       "      <td>0.590196</td>\n",
       "      <td>0.522201</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.504923</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.515444</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.518750</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.528958</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.538393</td>\n",
       "      <td>0.642045</td>\n",
       "      <td>0.547348</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.580357</td>\n",
       "      <td>0.731061</td>\n",
       "      <td>0.577020</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.502646</td>\n",
       "      <td>0.587640</td>\n",
       "      <td>0.524621</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.502646</td>\n",
       "      <td>0.587640</td>\n",
       "      <td>0.524621</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.574391</td>\n",
       "      <td>0.751776</td>\n",
       "      <td>0.570648</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.613569</td>\n",
       "      <td>0.785569</td>\n",
       "      <td>0.597434</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.557700</td>\n",
       "      <td>0.711307</td>\n",
       "      <td>0.559293</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.531957</td>\n",
       "      <td>0.700794</td>\n",
       "      <td>0.543863</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.498491</td>\n",
       "      <td>0.521687</td>\n",
       "      <td>0.508687</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.511654</td>\n",
       "      <td>0.590196</td>\n",
       "      <td>0.522201</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.492308</td>\n",
       "      <td>0.504065</td>\n",
       "      <td>0.501931</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.526294</td>\n",
       "      <td>0.925287</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.563713</td>\n",
       "      <td>0.645349</td>\n",
       "      <td>0.563131</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.563713</td>\n",
       "      <td>0.645349</td>\n",
       "      <td>0.563131</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.416149</td>\n",
       "      <td>0.376404</td>\n",
       "      <td>0.465278</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.571893</td>\n",
       "      <td>0.682266</td>\n",
       "      <td>0.570076</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.618768</td>\n",
       "      <td>0.753628</td>\n",
       "      <td>0.601508</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.540965</td>\n",
       "      <td>0.611761</td>\n",
       "      <td>0.544730</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.487781</td>\n",
       "      <td>0.646484</td>\n",
       "      <td>0.519504</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.611784</td>\n",
       "      <td>0.720312</td>\n",
       "      <td>0.596654</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.479730</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.498491</td>\n",
       "      <td>0.521687</td>\n",
       "      <td>0.508687</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.498491</td>\n",
       "      <td>0.521687</td>\n",
       "      <td>0.508687</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.479730</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.469448</td>\n",
       "      <td>0.551282</td>\n",
       "      <td>0.508838</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.490349</td>\n",
       "      <td>0.527915</td>\n",
       "      <td>0.510732</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.515583</td>\n",
       "      <td>0.723443</td>\n",
       "      <td>0.538510</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.469448</td>\n",
       "      <td>0.551282</td>\n",
       "      <td>0.508838</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.611784</td>\n",
       "      <td>0.720312</td>\n",
       "      <td>0.596654</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.581336</td>\n",
       "      <td>0.668544</td>\n",
       "      <td>0.573942</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.572086</td>\n",
       "      <td>0.668508</td>\n",
       "      <td>0.567441</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.578228</td>\n",
       "      <td>0.700240</td>\n",
       "      <td>0.572295</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.479730</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.504923</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.515444</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.540399</td>\n",
       "      <td>0.568783</td>\n",
       "      <td>0.537645</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.479730</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.502646</td>\n",
       "      <td>0.587640</td>\n",
       "      <td>0.524621</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.509022</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.531566</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.423313</td>\n",
       "      <td>0.379121</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.509022</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.531566</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.594144</td>\n",
       "      <td>0.731693</td>\n",
       "      <td>0.583651</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.598417</td>\n",
       "      <td>0.669283</td>\n",
       "      <td>0.586945</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.606607</td>\n",
       "      <td>0.743169</td>\n",
       "      <td>0.592580</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.603207</td>\n",
       "      <td>0.725254</td>\n",
       "      <td>0.590153</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.511654</td>\n",
       "      <td>0.590196</td>\n",
       "      <td>0.522201</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.504923</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.515444</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.555784</td>\n",
       "      <td>0.616340</td>\n",
       "      <td>0.556187</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.545894</td>\n",
       "      <td>0.693258</td>\n",
       "      <td>0.554293</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.633039</td>\n",
       "      <td>0.739216</td>\n",
       "      <td>0.615530</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.553656</td>\n",
       "      <td>0.769444</td>\n",
       "      <td>0.561237</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.541010</td>\n",
       "      <td>0.670837</td>\n",
       "      <td>0.547937</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.563604</td>\n",
       "      <td>0.768019</td>\n",
       "      <td>0.564147</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.529377</td>\n",
       "      <td>0.673126</td>\n",
       "      <td>0.541436</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.517010</td>\n",
       "      <td>0.676987</td>\n",
       "      <td>0.534934</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.509022</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.531566</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.589142</td>\n",
       "      <td>0.798876</td>\n",
       "      <td>0.583965</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.545894</td>\n",
       "      <td>0.693258</td>\n",
       "      <td>0.554293</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.515583</td>\n",
       "      <td>0.723443</td>\n",
       "      <td>0.538510</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.541010</td>\n",
       "      <td>0.670837</td>\n",
       "      <td>0.547937</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.571303</td>\n",
       "      <td>0.726671</td>\n",
       "      <td>0.568221</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.508338</td>\n",
       "      <td>0.798833</td>\n",
       "      <td>0.533287</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.469488</td>\n",
       "      <td>0.561198</td>\n",
       "      <td>0.508148</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.474593</td>\n",
       "      <td>0.635870</td>\n",
       "      <td>0.515783</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.474593</td>\n",
       "      <td>0.635870</td>\n",
       "      <td>0.515783</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.474593</td>\n",
       "      <td>0.635870</td>\n",
       "      <td>0.515783</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.474593</td>\n",
       "      <td>0.635870</td>\n",
       "      <td>0.515783</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.476543</td>\n",
       "      <td>0.896154</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.476543</td>\n",
       "      <td>0.896154</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.476543</td>\n",
       "      <td>0.896154</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.476543</td>\n",
       "      <td>0.896154</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.518750</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.528958</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.504923</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.515444</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.430303</td>\n",
       "      <td>0.381720</td>\n",
       "      <td>0.493056</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.449577</td>\n",
       "      <td>0.450739</td>\n",
       "      <td>0.481061</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.522358</td>\n",
       "      <td>0.891304</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.416149</td>\n",
       "      <td>0.376404</td>\n",
       "      <td>0.465278</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.481884</td>\n",
       "      <td>0.561924</td>\n",
       "      <td>0.512223</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.451194</td>\n",
       "      <td>0.475911</td>\n",
       "      <td>0.496793</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.549164</td>\n",
       "      <td>0.652439</td>\n",
       "      <td>0.552011</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.451194</td>\n",
       "      <td>0.475911</td>\n",
       "      <td>0.496793</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.526294</td>\n",
       "      <td>0.925287</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.492308</td>\n",
       "      <td>0.504065</td>\n",
       "      <td>0.501931</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.469448</td>\n",
       "      <td>0.551282</td>\n",
       "      <td>0.508838</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.580357</td>\n",
       "      <td>0.731061</td>\n",
       "      <td>0.577020</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.515583</td>\n",
       "      <td>0.723443</td>\n",
       "      <td>0.538510</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.515583</td>\n",
       "      <td>0.723443</td>\n",
       "      <td>0.538510</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.560630</td>\n",
       "      <td>0.737333</td>\n",
       "      <td>0.561720</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.577528</td>\n",
       "      <td>0.781000</td>\n",
       "      <td>0.573076</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.552137</td>\n",
       "      <td>0.792051</td>\n",
       "      <td>0.557646</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.489785</td>\n",
       "      <td>0.696887</td>\n",
       "      <td>0.521931</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.538393</td>\n",
       "      <td>0.642045</td>\n",
       "      <td>0.547348</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.459397</td>\n",
       "      <td>0.482022</td>\n",
       "      <td>0.494949</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.487781</td>\n",
       "      <td>0.646484</td>\n",
       "      <td>0.519504</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.437768</td>\n",
       "      <td>0.392308</td>\n",
       "      <td>0.495146</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.472973</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.498491</td>\n",
       "      <td>0.521687</td>\n",
       "      <td>0.508687</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.479730</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.472973</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.474593</td>\n",
       "      <td>0.635870</td>\n",
       "      <td>0.515783</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.545894</td>\n",
       "      <td>0.693258</td>\n",
       "      <td>0.554293</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.464387</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.501894</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.509022</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.531566</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.597468</td>\n",
       "      <td>0.752287</td>\n",
       "      <td>0.586078</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.597468</td>\n",
       "      <td>0.752287</td>\n",
       "      <td>0.586078</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.587791</td>\n",
       "      <td>0.764401</td>\n",
       "      <td>0.579577</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.571303</td>\n",
       "      <td>0.726671</td>\n",
       "      <td>0.568221</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.518750</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.528958</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.538393</td>\n",
       "      <td>0.642045</td>\n",
       "      <td>0.547348</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.589142</td>\n",
       "      <td>0.798876</td>\n",
       "      <td>0.583965</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.515583</td>\n",
       "      <td>0.723443</td>\n",
       "      <td>0.538510</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.553656</td>\n",
       "      <td>0.769444</td>\n",
       "      <td>0.561237</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.587791</td>\n",
       "      <td>0.764401</td>\n",
       "      <td>0.579577</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.557700</td>\n",
       "      <td>0.711307</td>\n",
       "      <td>0.559293</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.577528</td>\n",
       "      <td>0.781000</td>\n",
       "      <td>0.573076</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.474759</td>\n",
       "      <td>0.729086</td>\n",
       "      <td>0.515430</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.504923</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.515444</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.479730</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.472973</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.545894</td>\n",
       "      <td>0.693258</td>\n",
       "      <td>0.554293</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.515583</td>\n",
       "      <td>0.723443</td>\n",
       "      <td>0.538510</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.469448</td>\n",
       "      <td>0.551282</td>\n",
       "      <td>0.508838</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.474593</td>\n",
       "      <td>0.635870</td>\n",
       "      <td>0.515783</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.613569</td>\n",
       "      <td>0.785569</td>\n",
       "      <td>0.597434</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.646029</td>\n",
       "      <td>0.790289</td>\n",
       "      <td>0.621793</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.600846</td>\n",
       "      <td>0.775574</td>\n",
       "      <td>0.588506</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.591081</td>\n",
       "      <td>0.792246</td>\n",
       "      <td>0.582004</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                agg  \\\n",
       "0                                                                                                                              mean   \n",
       "1                                                                                                                               max   \n",
       "2                                                                                                                               min   \n",
       "3                                                                                                                     weighted_mean   \n",
       "4                                                                                                                              mean   \n",
       "5                                                                                                                               max   \n",
       "6                                                                                                                               min   \n",
       "7                                                                                                                     weighted_mean   \n",
       "8                                                                                                                              mean   \n",
       "9                                                                                                                               max   \n",
       "10                                                                                                                              min   \n",
       "11                                                                                                                    weighted_mean   \n",
       "12                                                                                                                             mean   \n",
       "13                                                                                                                              max   \n",
       "14                                                                                                                              min   \n",
       "15                                                                                                                    weighted_mean   \n",
       "16                                                                                                                             mean   \n",
       "17                                                                                                                              max   \n",
       "18                                                                                                                              min   \n",
       "19                                                                                                                    weighted_mean   \n",
       "20                                                                                                                             mean   \n",
       "21                                                                                                                              max   \n",
       "22                                                                                                                              min   \n",
       "23                                                                                                                    weighted_mean   \n",
       "24                                                                                                                             mean   \n",
       "25                                                                                                                              max   \n",
       "26                                                                                                                              min   \n",
       "27                                                                                                                    weighted_mean   \n",
       "28                                                                                                                             mean   \n",
       "29                                                                                                                              max   \n",
       "30                                                                                                                              min   \n",
       "31                                                                                                                    weighted_mean   \n",
       "32                                                                                                                             mean   \n",
       "33                                                                                                                              max   \n",
       "34                                                                                                                              min   \n",
       "35                                                                                                                    weighted_mean   \n",
       "36                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "37                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "38                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "39                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "40                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "41                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "42                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "43                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "44                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "45                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "46                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "47                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "48                                                                                                                             mean   \n",
       "49                                                                                                                              max   \n",
       "50                                                                                                                              min   \n",
       "51                                                                                                                    weighted_mean   \n",
       "52                                                                                                                             mean   \n",
       "53                                                                                                                              max   \n",
       "54                                                                                                                              min   \n",
       "55                                                                                                                    weighted_mean   \n",
       "56                                                                                                                             mean   \n",
       "57                                                                                                                              max   \n",
       "58                                                                                                                              min   \n",
       "59                                                                                                                    weighted_mean   \n",
       "60                                                                                                                             mean   \n",
       "61                                                                                                                              max   \n",
       "62                                                                                                                              min   \n",
       "63                                                                                                                    weighted_mean   \n",
       "64                                                                                                                             mean   \n",
       "65                                                                                                                              max   \n",
       "66                                                                                                                              min   \n",
       "67                                                                                                                    weighted_mean   \n",
       "68                                                                                                                             mean   \n",
       "69                                                                                                                              max   \n",
       "70                                                                                                                              min   \n",
       "71                                                                                                                    weighted_mean   \n",
       "72                                                                                                                             mean   \n",
       "73                                                                                                                              max   \n",
       "74                                                                                                                              min   \n",
       "75                                                                                                                    weighted_mean   \n",
       "76                                                                                                                             mean   \n",
       "77                                                                                                                              max   \n",
       "78                                                                                                                              min   \n",
       "79                                                                                                                    weighted_mean   \n",
       "80                                                                                                                             mean   \n",
       "81                                                                                                                              max   \n",
       "82                                                                                                                              min   \n",
       "83                                                                                                                    weighted_mean   \n",
       "84                                                                                                                             mean   \n",
       "85                                                                                                                              max   \n",
       "86                                                                                                                              min   \n",
       "87                                                                                                                    weighted_mean   \n",
       "88                                                                                                                             mean   \n",
       "89                                                                                                                              max   \n",
       "90                                                                                                                              min   \n",
       "91                                                                                                                    weighted_mean   \n",
       "92                                                                                                                             mean   \n",
       "93                                                                                                                              max   \n",
       "94                                                                                                                              min   \n",
       "95                                                                                                                    weighted_mean   \n",
       "96                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "97                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "98                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "99                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "100                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "101                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "102                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "103                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "104                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "105                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "106                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "107                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "108                                                                                                                            mean   \n",
       "109                                                                                                                             max   \n",
       "110                                                                                                                             min   \n",
       "111                                                                                                                   weighted_mean   \n",
       "112                                                                                                                            mean   \n",
       "113                                                                                                                             max   \n",
       "114                                                                                                                             min   \n",
       "115                                                                                                                   weighted_mean   \n",
       "116                                                                                                                            mean   \n",
       "117                                                                                                                             max   \n",
       "118                                                                                                                             min   \n",
       "119                                                                                                                   weighted_mean   \n",
       "120  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "121            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "122            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "123        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "124  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "125            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "126            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "127        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "128  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "129            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "130            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "131        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "132                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "133                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "134                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "135                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "136                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "137                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "138                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "139                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "140                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "141                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "142                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "143                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "144                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "145                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "146                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "147                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "148                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "149                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "150                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "151                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "152                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "153                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "154                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "155                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "\n",
       "       dataset        f1      prec       rec                        zc  \n",
       "0    Wikipedia  0.453416  0.419540  0.493243                linguistic  \n",
       "1    Wikipedia  0.511654  0.590196  0.522201                linguistic  \n",
       "2    Wikipedia  0.504923  0.547619  0.515444                linguistic  \n",
       "3    Wikipedia  0.518750  0.674419  0.528958                linguistic  \n",
       "4     WikiNews  0.538393  0.642045  0.547348                linguistic  \n",
       "5     WikiNews  0.580357  0.731061  0.577020                linguistic  \n",
       "6     WikiNews  0.502646  0.587640  0.524621                linguistic  \n",
       "7     WikiNews  0.502646  0.587640  0.524621                linguistic  \n",
       "8         News  0.574391  0.751776  0.570648                linguistic  \n",
       "9         News  0.613569  0.785569  0.597434                linguistic  \n",
       "10        News  0.557700  0.711307  0.559293                linguistic  \n",
       "11        News  0.531957  0.700794  0.543863                linguistic  \n",
       "12   Wikipedia  0.498491  0.521687  0.508687                 frequency  \n",
       "13   Wikipedia  0.511654  0.590196  0.522201                 frequency  \n",
       "14   Wikipedia  0.492308  0.504065  0.501931                 frequency  \n",
       "15   Wikipedia  0.526294  0.925287  0.535714                 frequency  \n",
       "16    WikiNews  0.563713  0.645349  0.563131                 frequency  \n",
       "17    WikiNews  0.563713  0.645349  0.563131                 frequency  \n",
       "18    WikiNews  0.416149  0.376404  0.465278                 frequency  \n",
       "19    WikiNews  0.571893  0.682266  0.570076                 frequency  \n",
       "20        News  0.618768  0.753628  0.601508                 frequency  \n",
       "21        News  0.540965  0.611761  0.544730                 frequency  \n",
       "22        News  0.487781  0.646484  0.519504                 frequency  \n",
       "23        News  0.611784  0.720312  0.596654                 frequency  \n",
       "24   Wikipedia  0.446541  0.417647  0.479730            language_model  \n",
       "25   Wikipedia  0.498491  0.521687  0.508687            language_model  \n",
       "26   Wikipedia  0.498491  0.521687  0.508687            language_model  \n",
       "27   Wikipedia  0.446541  0.417647  0.479730            language_model  \n",
       "28    WikiNews  0.469448  0.551282  0.508838            language_model  \n",
       "29    WikiNews  0.490349  0.527915  0.510732            language_model  \n",
       "30    WikiNews  0.515583  0.723443  0.538510            language_model  \n",
       "31    WikiNews  0.469448  0.551282  0.508838            language_model  \n",
       "32        News  0.611784  0.720312  0.596654            language_model  \n",
       "33        News  0.581336  0.668544  0.573942            language_model  \n",
       "34        News  0.572086  0.668508  0.567441            language_model  \n",
       "35        News  0.578228  0.700240  0.572295            language_model  \n",
       "36   Wikipedia  0.446541  0.417647  0.479730                    corpus  \n",
       "37   Wikipedia  0.504923  0.547619  0.515444                    corpus  \n",
       "38   Wikipedia  0.540399  0.568783  0.537645                    corpus  \n",
       "39   Wikipedia  0.446541  0.417647  0.479730                    corpus  \n",
       "40    WikiNews  0.502646  0.587640  0.524621                    corpus  \n",
       "41    WikiNews  0.509022  0.638889  0.531566                    corpus  \n",
       "42    WikiNews  0.423313  0.379121  0.479167                    corpus  \n",
       "43    WikiNews  0.509022  0.638889  0.531566                    corpus  \n",
       "44        News  0.594144  0.731693  0.583651                    corpus  \n",
       "45        News  0.598417  0.669283  0.586945                    corpus  \n",
       "46        News  0.606607  0.743169  0.592580                    corpus  \n",
       "47        News  0.603207  0.725254  0.590153                    corpus  \n",
       "48   Wikipedia  0.511654  0.590196  0.522201          psycholinguistic  \n",
       "49   Wikipedia  0.504923  0.547619  0.515444          psycholinguistic  \n",
       "50   Wikipedia  0.456790  0.420455  0.500000          psycholinguistic  \n",
       "51   Wikipedia  0.453416  0.419540  0.493243          psycholinguistic  \n",
       "52    WikiNews  0.555784  0.616340  0.556187          psycholinguistic  \n",
       "53    WikiNews  0.545894  0.693258  0.554293          psycholinguistic  \n",
       "54    WikiNews  0.633039  0.739216  0.615530          psycholinguistic  \n",
       "55    WikiNews  0.553656  0.769444  0.561237          psycholinguistic  \n",
       "56        News  0.541010  0.670837  0.547937          psycholinguistic  \n",
       "57        News  0.563604  0.768019  0.564147          psycholinguistic  \n",
       "58        News  0.529377  0.673126  0.541436          psycholinguistic  \n",
       "59        News  0.517010  0.676987  0.534934          psycholinguistic  \n",
       "60   Wikipedia  0.456790  0.420455  0.500000                   wordnet  \n",
       "61   Wikipedia  0.456790  0.420455  0.500000                   wordnet  \n",
       "62   Wikipedia  0.453416  0.419540  0.493243                   wordnet  \n",
       "63   Wikipedia  0.456790  0.420455  0.500000                   wordnet  \n",
       "64    WikiNews  0.509022  0.638889  0.531566                   wordnet  \n",
       "65    WikiNews  0.589142  0.798876  0.583965                   wordnet  \n",
       "66    WikiNews  0.545894  0.693258  0.554293                   wordnet  \n",
       "67    WikiNews  0.515583  0.723443  0.538510                   wordnet  \n",
       "68        News  0.541010  0.670837  0.547937                   wordnet  \n",
       "69        News  0.571303  0.726671  0.568221                   wordnet  \n",
       "70        News  0.508338  0.798833  0.533287                   wordnet  \n",
       "71        News  0.469488  0.561198  0.508148                   wordnet  \n",
       "72   Wikipedia  0.450000  0.418605  0.486486                   dbpedia  \n",
       "73   Wikipedia  0.450000  0.418605  0.486486                   dbpedia  \n",
       "74   Wikipedia  0.450000  0.418605  0.486486                   dbpedia  \n",
       "75   Wikipedia  0.450000  0.418605  0.486486                   dbpedia  \n",
       "76    WikiNews  0.474593  0.635870  0.515783                   dbpedia  \n",
       "77    WikiNews  0.474593  0.635870  0.515783                   dbpedia  \n",
       "78    WikiNews  0.474593  0.635870  0.515783                   dbpedia  \n",
       "79    WikiNews  0.474593  0.635870  0.515783                   dbpedia  \n",
       "80        News  0.476543  0.896154  0.517857                   dbpedia  \n",
       "81        News  0.476543  0.896154  0.517857                   dbpedia  \n",
       "82        News  0.476543  0.896154  0.517857                   dbpedia  \n",
       "83        News  0.476543  0.896154  0.517857                   dbpedia  \n",
       "84   Wikipedia  0.518750  0.674419  0.528958          brown_clustering  \n",
       "85   Wikipedia  0.450000  0.418605  0.486486          brown_clustering  \n",
       "86   Wikipedia  0.504923  0.547619  0.515444          brown_clustering  \n",
       "87   Wikipedia  0.456790  0.420455  0.500000          brown_clustering  \n",
       "88    WikiNews  0.430303  0.381720  0.493056          brown_clustering  \n",
       "89    WikiNews  0.449577  0.450739  0.481061          brown_clustering  \n",
       "90    WikiNews  0.522358  0.891304  0.545455          brown_clustering  \n",
       "91    WikiNews  0.416149  0.376404  0.465278          brown_clustering  \n",
       "92        News  0.481884  0.561924  0.512223          brown_clustering  \n",
       "93        News  0.451194  0.475911  0.496793          brown_clustering  \n",
       "94        News  0.549164  0.652439  0.552011          brown_clustering  \n",
       "95        News  0.451194  0.475911  0.496793          brown_clustering  \n",
       "96   Wikipedia  0.456790  0.420455  0.500000                  semantic  \n",
       "97   Wikipedia  0.526294  0.925287  0.535714                  semantic  \n",
       "98   Wikipedia  0.492308  0.504065  0.501931                  semantic  \n",
       "99   Wikipedia  0.453416  0.419540  0.493243                  semantic  \n",
       "100   WikiNews  0.469448  0.551282  0.508838                  semantic  \n",
       "101   WikiNews  0.580357  0.731061  0.577020                  semantic  \n",
       "102   WikiNews  0.515583  0.723443  0.538510                  semantic  \n",
       "103   WikiNews  0.515583  0.723443  0.538510                  semantic  \n",
       "104       News  0.560630  0.737333  0.561720                  semantic  \n",
       "105       News  0.577528  0.781000  0.573076                  semantic  \n",
       "106       News  0.552137  0.792051  0.557646                  semantic  \n",
       "107       News  0.489785  0.696887  0.521931                  semantic  \n",
       "108  Wikipedia  0.456790  0.420455  0.500000                dictionary  \n",
       "109  Wikipedia  0.456790  0.420455  0.500000                dictionary  \n",
       "110  Wikipedia  0.456790  0.420455  0.500000                dictionary  \n",
       "111  Wikipedia  0.450000  0.418605  0.486486                dictionary  \n",
       "112   WikiNews  0.538393  0.642045  0.547348                dictionary  \n",
       "113   WikiNews  0.433735  0.382979  0.500000                dictionary  \n",
       "114   WikiNews  0.433735  0.382979  0.500000                dictionary  \n",
       "115   WikiNews  0.459397  0.482022  0.494949                dictionary  \n",
       "116       News  0.487781  0.646484  0.519504                dictionary  \n",
       "117       News  0.440171  0.393130  0.500000                dictionary  \n",
       "118       News  0.440171  0.393130  0.500000                dictionary  \n",
       "119       News  0.437768  0.392308  0.495146                dictionary  \n",
       "120  Wikipedia  0.443038  0.416667  0.472973           corpus+semantic  \n",
       "121  Wikipedia  0.498491  0.521687  0.508687           corpus+semantic  \n",
       "122  Wikipedia  0.446541  0.417647  0.479730           corpus+semantic  \n",
       "123  Wikipedia  0.443038  0.416667  0.472973           corpus+semantic  \n",
       "124   WikiNews  0.474593  0.635870  0.515783           corpus+semantic  \n",
       "125   WikiNews  0.545894  0.693258  0.554293           corpus+semantic  \n",
       "126   WikiNews  0.464387  0.508333  0.501894           corpus+semantic  \n",
       "127   WikiNews  0.509022  0.638889  0.531566           corpus+semantic  \n",
       "128       News  0.597468  0.752287  0.586078           corpus+semantic  \n",
       "129       News  0.597468  0.752287  0.586078           corpus+semantic  \n",
       "130       News  0.587791  0.764401  0.579577           corpus+semantic  \n",
       "131       News  0.571303  0.726671  0.568221           corpus+semantic  \n",
       "132  Wikipedia  0.453416  0.419540  0.493243  wordnet+psycholinguistic  \n",
       "133  Wikipedia  0.518750  0.674419  0.528958  wordnet+psycholinguistic  \n",
       "134  Wikipedia  0.456790  0.420455  0.500000  wordnet+psycholinguistic  \n",
       "135  Wikipedia  0.456790  0.420455  0.500000  wordnet+psycholinguistic  \n",
       "136   WikiNews  0.538393  0.642045  0.547348  wordnet+psycholinguistic  \n",
       "137   WikiNews  0.589142  0.798876  0.583965  wordnet+psycholinguistic  \n",
       "138   WikiNews  0.515583  0.723443  0.538510  wordnet+psycholinguistic  \n",
       "139   WikiNews  0.553656  0.769444  0.561237  wordnet+psycholinguistic  \n",
       "140       News  0.587791  0.764401  0.579577  wordnet+psycholinguistic  \n",
       "141       News  0.557700  0.711307  0.559293  wordnet+psycholinguistic  \n",
       "142       News  0.577528  0.781000  0.573076  wordnet+psycholinguistic  \n",
       "143       News  0.474759  0.729086  0.515430  wordnet+psycholinguistic  \n",
       "144  Wikipedia  0.450000  0.418605  0.486486                       all  \n",
       "145  Wikipedia  0.504923  0.547619  0.515444                       all  \n",
       "146  Wikipedia  0.446541  0.417647  0.479730                       all  \n",
       "147  Wikipedia  0.443038  0.416667  0.472973                       all  \n",
       "148   WikiNews  0.545894  0.693258  0.554293                       all  \n",
       "149   WikiNews  0.515583  0.723443  0.538510                       all  \n",
       "150   WikiNews  0.469448  0.551282  0.508838                       all  \n",
       "151   WikiNews  0.474593  0.635870  0.515783                       all  \n",
       "152       News  0.613569  0.785569  0.597434                       all  \n",
       "153       News  0.646029  0.790289  0.621793                       all  \n",
       "154       News  0.600846  0.775574  0.588506                       all  \n",
       "155       News  0.591081  0.792246  0.582004                       all  "
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_eval_data_rf = create_eval_df_from_results_macro(results, False)\n",
    "feature_eval_data_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.540399</td>\n",
       "      <td>0.568783</td>\n",
       "      <td>0.537645</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.633039</td>\n",
       "      <td>0.739216</td>\n",
       "      <td>0.615530</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.646029</td>\n",
       "      <td>0.790289</td>\n",
       "      <td>0.621793</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          agg    dataset  \\\n",
       "38   (min, <function agg_feat_num_min at 0x000000FF52232950>)  Wikipedia   \n",
       "54                                                        min   WikiNews   \n",
       "153  (max, <function agg_feat_num_max at 0x000000FF522328C8>)       News   \n",
       "\n",
       "           f1      prec       rec                zc  \n",
       "38   0.540399  0.568783  0.537645            corpus  \n",
       "54   0.633039  0.739216  0.615530  psycholinguistic  \n",
       "153  0.646029  0.790289  0.621793               all  "
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = feature_eval_data_rf.groupby(['dataset'])['f1'].transform(max) == feature_eval_data_rf['f1']\n",
    "feature_eval_data_rf[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   22.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   21.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   21.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   23.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   30.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   28.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   28.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   31.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   54.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   51.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   51.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   58.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   29.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   28.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   30.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   29.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   39.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   37.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   36.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   40.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.0min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   59.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   58.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   31.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   29.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   29.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   31.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   40.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   38.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   37.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   40.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   57.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   57.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.0min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   40.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   39.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   38.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   43.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   56.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   54.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   52.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   57.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.5min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.4min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.4min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.5min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   12.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   11.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   10.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   13.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   16.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   14.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   14.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   17.6s finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   28.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   24.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   24.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   31.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   32.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   27.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   26.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   32.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   40.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   37.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   35.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   42.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.0min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   59.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   14.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   14.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   14.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   14.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   20.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   19.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   19.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   19.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   32.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   32.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   32.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   32.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   13.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   12.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   12.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   14.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   18.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   17.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   16.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   20.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   32.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   28.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   27.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   35.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   35.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   34.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   33.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   36.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   48.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   46.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   44.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   51.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.4min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.5min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    3.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    3.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    3.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    4.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    5.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    4.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    4.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    6.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    9.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    8.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    8.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   11.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   58.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   59.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   54.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   54.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.5min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.6min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.5min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  2.7min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  2.6min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  2.5min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  2.8min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   41.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   38.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   37.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   44.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   56.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   52.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   50.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:   60.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.5min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.4min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.8min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.8min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  1.8min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  2.8min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  2.7min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  2.7min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:  3.0min finished\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "results = [Result(fs, fs.fc, fs.agg,\n",
    "        precision_recall_fscore_support(*random_forest(*transform_feat_to_num(remove_labels_for_binary_df(fs.train), \n",
    "        remove_labels_for_binary_df(fs.test))), average='macro')) for fs in all_fc_datasets_dswp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.479730</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.469448</td>\n",
       "      <td>0.551282</td>\n",
       "      <td>0.508838</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.426829</td>\n",
       "      <td>0.380435</td>\n",
       "      <td>0.486111</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.469448</td>\n",
       "      <td>0.551282</td>\n",
       "      <td>0.508838</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.426829</td>\n",
       "      <td>0.380435</td>\n",
       "      <td>0.486111</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.489785</td>\n",
       "      <td>0.696887</td>\n",
       "      <td>0.521931</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.506075</td>\n",
       "      <td>0.731771</td>\n",
       "      <td>0.530860</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.539924</td>\n",
       "      <td>0.830532</td>\n",
       "      <td>0.551144</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.476543</td>\n",
       "      <td>0.896154</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.565789</td>\n",
       "      <td>0.579487</td>\n",
       "      <td>0.559846</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.469298</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.474903</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.607788</td>\n",
       "      <td>0.605023</td>\n",
       "      <td>0.611004</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.557991</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.561776</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.579381</td>\n",
       "      <td>0.598734</td>\n",
       "      <td>0.573864</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.587719</td>\n",
       "      <td>0.614286</td>\n",
       "      <td>0.580808</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.592943</td>\n",
       "      <td>0.594586</td>\n",
       "      <td>0.591540</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.594411</td>\n",
       "      <td>0.677778</td>\n",
       "      <td>0.585859</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.620979</td>\n",
       "      <td>0.671484</td>\n",
       "      <td>0.606449</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.557296</td>\n",
       "      <td>0.579979</td>\n",
       "      <td>0.553745</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.591791</td>\n",
       "      <td>0.594597</td>\n",
       "      <td>0.589546</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.617598</td>\n",
       "      <td>0.663043</td>\n",
       "      <td>0.604022</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.510567</td>\n",
       "      <td>0.510771</td>\n",
       "      <td>0.512548</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.503590</td>\n",
       "      <td>0.504386</td>\n",
       "      <td>0.503861</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.557991</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.561776</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.409396</td>\n",
       "      <td>0.406667</td>\n",
       "      <td>0.412162</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.641281</td>\n",
       "      <td>0.650175</td>\n",
       "      <td>0.635101</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.683400</td>\n",
       "      <td>0.686562</td>\n",
       "      <td>0.680556</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.553363</td>\n",
       "      <td>0.572321</td>\n",
       "      <td>0.551136</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.660299</td>\n",
       "      <td>0.680290</td>\n",
       "      <td>0.648990</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.582793</td>\n",
       "      <td>0.594722</td>\n",
       "      <td>0.577323</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.593367</td>\n",
       "      <td>0.591759</td>\n",
       "      <td>0.595267</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.554501</td>\n",
       "      <td>0.574512</td>\n",
       "      <td>0.551318</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.568759</td>\n",
       "      <td>0.605172</td>\n",
       "      <td>0.563454</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.548718</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.546332</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.486329</td>\n",
       "      <td>0.491182</td>\n",
       "      <td>0.495174</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.539683</td>\n",
       "      <td>0.548263</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.496893</td>\n",
       "      <td>0.496923</td>\n",
       "      <td>0.497104</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.553363</td>\n",
       "      <td>0.572321</td>\n",
       "      <td>0.551136</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.614076</td>\n",
       "      <td>0.676342</td>\n",
       "      <td>0.601641</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.577322</td>\n",
       "      <td>0.624863</td>\n",
       "      <td>0.571970</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.605042</td>\n",
       "      <td>0.652439</td>\n",
       "      <td>0.594697</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.606316</td>\n",
       "      <td>0.669898</td>\n",
       "      <td>0.593447</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.603941</td>\n",
       "      <td>0.645245</td>\n",
       "      <td>0.592666</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.607196</td>\n",
       "      <td>0.653191</td>\n",
       "      <td>0.595094</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.583814</td>\n",
       "      <td>0.647173</td>\n",
       "      <td>0.575589</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.514706</td>\n",
       "      <td>0.523573</td>\n",
       "      <td>0.536680</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.468372</td>\n",
       "      <td>0.488095</td>\n",
       "      <td>0.480695</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.489425</td>\n",
       "      <td>0.500635</td>\n",
       "      <td>0.500965</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.503792</td>\n",
       "      <td>0.510033</td>\n",
       "      <td>0.514479</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.617133</td>\n",
       "      <td>0.619014</td>\n",
       "      <td>0.658460</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.591304</td>\n",
       "      <td>0.587662</td>\n",
       "      <td>0.602273</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.678742</td>\n",
       "      <td>0.669896</td>\n",
       "      <td>0.715909</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.608333</td>\n",
       "      <td>0.612782</td>\n",
       "      <td>0.651515</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.553641</td>\n",
       "      <td>0.565653</td>\n",
       "      <td>0.550537</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.476470</td>\n",
       "      <td>0.478664</td>\n",
       "      <td>0.476075</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.538768</td>\n",
       "      <td>0.539507</td>\n",
       "      <td>0.549150</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.498994</td>\n",
       "      <td>0.504916</td>\n",
       "      <td>0.503467</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.518750</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.528958</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.479730</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.498491</td>\n",
       "      <td>0.521687</td>\n",
       "      <td>0.508687</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.587719</td>\n",
       "      <td>0.614286</td>\n",
       "      <td>0.580808</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.623397</td>\n",
       "      <td>0.704762</td>\n",
       "      <td>0.608586</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.680329</td>\n",
       "      <td>0.717722</td>\n",
       "      <td>0.662879</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.622076</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.610480</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.497287</td>\n",
       "      <td>0.596825</td>\n",
       "      <td>0.521151</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.488861</td>\n",
       "      <td>0.538018</td>\n",
       "      <td>0.511442</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.470510</td>\n",
       "      <td>0.492713</td>\n",
       "      <td>0.497660</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.483832</td>\n",
       "      <td>0.583169</td>\n",
       "      <td>0.514650</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.505747</td>\n",
       "      <td>0.509653</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.505747</td>\n",
       "      <td>0.509653</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.505747</td>\n",
       "      <td>0.509653</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.505747</td>\n",
       "      <td>0.509653</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.532338</td>\n",
       "      <td>0.535786</td>\n",
       "      <td>0.544823</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.532338</td>\n",
       "      <td>0.535786</td>\n",
       "      <td>0.544823</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.532338</td>\n",
       "      <td>0.535786</td>\n",
       "      <td>0.544823</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.532338</td>\n",
       "      <td>0.535786</td>\n",
       "      <td>0.544823</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.468490</td>\n",
       "      <td>0.468292</td>\n",
       "      <td>0.468707</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.468490</td>\n",
       "      <td>0.468292</td>\n",
       "      <td>0.468707</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.468490</td>\n",
       "      <td>0.468292</td>\n",
       "      <td>0.468707</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.468490</td>\n",
       "      <td>0.468292</td>\n",
       "      <td>0.468707</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.471413</td>\n",
       "      <td>0.474316</td>\n",
       "      <td>0.470077</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.475339</td>\n",
       "      <td>0.492107</td>\n",
       "      <td>0.487452</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.416188</td>\n",
       "      <td>0.480879</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.401361</td>\n",
       "      <td>0.404110</td>\n",
       "      <td>0.398649</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.645797</td>\n",
       "      <td>0.638528</td>\n",
       "      <td>0.661616</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.619008</td>\n",
       "      <td>0.614183</td>\n",
       "      <td>0.640783</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.461996</td>\n",
       "      <td>0.558972</td>\n",
       "      <td>0.573864</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.560284</td>\n",
       "      <td>0.558551</td>\n",
       "      <td>0.563763</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.493720</td>\n",
       "      <td>0.493679</td>\n",
       "      <td>0.493845</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.500318</td>\n",
       "      <td>0.501093</td>\n",
       "      <td>0.501214</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.435239</td>\n",
       "      <td>0.488546</td>\n",
       "      <td>0.483010</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.456226</td>\n",
       "      <td>0.453503</td>\n",
       "      <td>0.461338</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.472973</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.504923</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.515444</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.472973</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.492308</td>\n",
       "      <td>0.504065</td>\n",
       "      <td>0.501931</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.633039</td>\n",
       "      <td>0.739216</td>\n",
       "      <td>0.615530</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.594411</td>\n",
       "      <td>0.677778</td>\n",
       "      <td>0.585859</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.631373</td>\n",
       "      <td>0.676638</td>\n",
       "      <td>0.617424</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.676853</td>\n",
       "      <td>0.747967</td>\n",
       "      <td>0.654040</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.500860</td>\n",
       "      <td>0.552591</td>\n",
       "      <td>0.517944</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.480721</td>\n",
       "      <td>0.504554</td>\n",
       "      <td>0.501734</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.478723</td>\n",
       "      <td>0.498267</td>\n",
       "      <td>0.499307</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.490938</td>\n",
       "      <td>0.549428</td>\n",
       "      <td>0.513870</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.532819</td>\n",
       "      <td>0.532819</td>\n",
       "      <td>0.532819</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.261268</td>\n",
       "      <td>0.475342</td>\n",
       "      <td>0.473938</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.517808</td>\n",
       "      <td>0.517361</td>\n",
       "      <td>0.519305</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.539683</td>\n",
       "      <td>0.548263</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.599765</td>\n",
       "      <td>0.595633</td>\n",
       "      <td>0.609217</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.382699</td>\n",
       "      <td>0.517262</td>\n",
       "      <td>0.518308</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.508768</td>\n",
       "      <td>0.581555</td>\n",
       "      <td>0.608586</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.592943</td>\n",
       "      <td>0.594586</td>\n",
       "      <td>0.591540</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.475160</td>\n",
       "      <td>0.474634</td>\n",
       "      <td>0.475988</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.362363</td>\n",
       "      <td>0.559660</td>\n",
       "      <td>0.555652</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.420215</td>\n",
       "      <td>0.507715</td>\n",
       "      <td>0.510749</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.475160</td>\n",
       "      <td>0.474634</td>\n",
       "      <td>0.475988</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.439490</td>\n",
       "      <td>0.415663</td>\n",
       "      <td>0.466216</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.439490</td>\n",
       "      <td>0.415663</td>\n",
       "      <td>0.466216</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.584416</td>\n",
       "      <td>0.618750</td>\n",
       "      <td>0.573359</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.424837</td>\n",
       "      <td>0.411392</td>\n",
       "      <td>0.439189</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.594411</td>\n",
       "      <td>0.677778</td>\n",
       "      <td>0.585859</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.622321</td>\n",
       "      <td>0.820076</td>\n",
       "      <td>0.606692</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.650831</td>\n",
       "      <td>0.727820</td>\n",
       "      <td>0.631313</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.643038</td>\n",
       "      <td>0.781977</td>\n",
       "      <td>0.622475</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.590101</td>\n",
       "      <td>0.668819</td>\n",
       "      <td>0.580444</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.601689</td>\n",
       "      <td>0.680497</td>\n",
       "      <td>0.589372</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.569211</td>\n",
       "      <td>0.625063</td>\n",
       "      <td>0.564234</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.584469</td>\n",
       "      <td>0.682025</td>\n",
       "      <td>0.576370</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.492308</td>\n",
       "      <td>0.504065</td>\n",
       "      <td>0.501931</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.548718</td>\n",
       "      <td>0.593496</td>\n",
       "      <td>0.544402</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.472973</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.548718</td>\n",
       "      <td>0.593496</td>\n",
       "      <td>0.544402</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.587162</td>\n",
       "      <td>0.595760</td>\n",
       "      <td>0.582702</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.623397</td>\n",
       "      <td>0.704762</td>\n",
       "      <td>0.608586</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.660299</td>\n",
       "      <td>0.680290</td>\n",
       "      <td>0.648990</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.660299</td>\n",
       "      <td>0.680290</td>\n",
       "      <td>0.648990</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.500860</td>\n",
       "      <td>0.552591</td>\n",
       "      <td>0.517944</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.512192</td>\n",
       "      <td>0.564208</td>\n",
       "      <td>0.524445</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.463163</td>\n",
       "      <td>0.469894</td>\n",
       "      <td>0.487951</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.460923</td>\n",
       "      <td>0.483339</td>\n",
       "      <td>0.496012</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.479730</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.553656</td>\n",
       "      <td>0.769444</td>\n",
       "      <td>0.561237</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.553656</td>\n",
       "      <td>0.769444</td>\n",
       "      <td>0.561237</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.603376</td>\n",
       "      <td>0.713663</td>\n",
       "      <td>0.592803</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.545894</td>\n",
       "      <td>0.693258</td>\n",
       "      <td>0.554293</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.535678</td>\n",
       "      <td>0.634143</td>\n",
       "      <td>0.543083</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.596560</td>\n",
       "      <td>0.694428</td>\n",
       "      <td>0.585298</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.584554</td>\n",
       "      <td>0.740216</td>\n",
       "      <td>0.577150</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.574391</td>\n",
       "      <td>0.751776</td>\n",
       "      <td>0.570648</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                agg  \\\n",
       "0                                                                                                                              mean   \n",
       "1                                                                                                                               max   \n",
       "2                                                                                                                               min   \n",
       "3                                                                                                                     weighted_mean   \n",
       "4                                                                                                                              mean   \n",
       "5                                                                                                                               max   \n",
       "6                                                                                                                               min   \n",
       "7                                                                                                                     weighted_mean   \n",
       "8                                                                                                                              mean   \n",
       "9                                                                                                                               max   \n",
       "10                                                                                                                              min   \n",
       "11                                                                                                                    weighted_mean   \n",
       "12                                                                                                                             mean   \n",
       "13                                                                                                                              max   \n",
       "14                                                                                                                              min   \n",
       "15                                                                                                                    weighted_mean   \n",
       "16                                                                                                                             mean   \n",
       "17                                                                                                                              max   \n",
       "18                                                                                                                              min   \n",
       "19                                                                                                                    weighted_mean   \n",
       "20                                                                                                                             mean   \n",
       "21                                                                                                                              max   \n",
       "22                                                                                                                              min   \n",
       "23                                                                                                                    weighted_mean   \n",
       "24                                                                                                                             mean   \n",
       "25                                                                                                                              max   \n",
       "26                                                                                                                              min   \n",
       "27                                                                                                                    weighted_mean   \n",
       "28                                                                                                                             mean   \n",
       "29                                                                                                                              max   \n",
       "30                                                                                                                              min   \n",
       "31                                                                                                                    weighted_mean   \n",
       "32                                                                                                                             mean   \n",
       "33                                                                                                                              max   \n",
       "34                                                                                                                              min   \n",
       "35                                                                                                                    weighted_mean   \n",
       "36                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "37                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "38                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "39                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "40                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "41                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "42                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "43                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "44                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "45                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "46                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "47                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "48                                                                                                                             mean   \n",
       "49                                                                                                                              max   \n",
       "50                                                                                                                              min   \n",
       "51                                                                                                                    weighted_mean   \n",
       "52                                                                                                                             mean   \n",
       "53                                                                                                                              max   \n",
       "54                                                                                                                              min   \n",
       "55                                                                                                                    weighted_mean   \n",
       "56                                                                                                                             mean   \n",
       "57                                                                                                                              max   \n",
       "58                                                                                                                              min   \n",
       "59                                                                                                                    weighted_mean   \n",
       "60                                                                                                                             mean   \n",
       "61                                                                                                                              max   \n",
       "62                                                                                                                              min   \n",
       "63                                                                                                                    weighted_mean   \n",
       "64                                                                                                                             mean   \n",
       "65                                                                                                                              max   \n",
       "66                                                                                                                              min   \n",
       "67                                                                                                                    weighted_mean   \n",
       "68                                                                                                                             mean   \n",
       "69                                                                                                                              max   \n",
       "70                                                                                                                              min   \n",
       "71                                                                                                                    weighted_mean   \n",
       "72                                                                                                                             mean   \n",
       "73                                                                                                                              max   \n",
       "74                                                                                                                              min   \n",
       "75                                                                                                                    weighted_mean   \n",
       "76                                                                                                                             mean   \n",
       "77                                                                                                                              max   \n",
       "78                                                                                                                              min   \n",
       "79                                                                                                                    weighted_mean   \n",
       "80                                                                                                                             mean   \n",
       "81                                                                                                                              max   \n",
       "82                                                                                                                              min   \n",
       "83                                                                                                                    weighted_mean   \n",
       "84                                                                                                                             mean   \n",
       "85                                                                                                                              max   \n",
       "86                                                                                                                              min   \n",
       "87                                                                                                                    weighted_mean   \n",
       "88                                                                                                                             mean   \n",
       "89                                                                                                                              max   \n",
       "90                                                                                                                              min   \n",
       "91                                                                                                                    weighted_mean   \n",
       "92                                                                                                                             mean   \n",
       "93                                                                                                                              max   \n",
       "94                                                                                                                              min   \n",
       "95                                                                                                                    weighted_mean   \n",
       "96                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "97                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "98                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "99                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "100                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "101                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "102                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "103                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "104                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "105                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "106                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "107                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "108                                                                                                                            mean   \n",
       "109                                                                                                                             max   \n",
       "110                                                                                                                             min   \n",
       "111                                                                                                                   weighted_mean   \n",
       "112                                                                                                                            mean   \n",
       "113                                                                                                                             max   \n",
       "114                                                                                                                             min   \n",
       "115                                                                                                                   weighted_mean   \n",
       "116                                                                                                                            mean   \n",
       "117                                                                                                                             max   \n",
       "118                                                                                                                             min   \n",
       "119                                                                                                                   weighted_mean   \n",
       "120  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "121            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "122            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "123        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "124  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "125            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "126            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "127        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "128  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "129            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "130            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "131        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "132                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "133                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "134                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "135                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "136                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "137                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "138                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "139                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "140                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "141                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "142                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "143                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "144                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "145                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "146                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "147                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "148                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "149                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "150                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "151                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "152                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "153                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "154                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "155                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "\n",
       "       dataset        f1      prec       rec                        zc  \n",
       "0    Wikipedia  0.453416  0.419540  0.493243                linguistic  \n",
       "1    Wikipedia  0.456790  0.420455  0.500000                linguistic  \n",
       "2    Wikipedia  0.446541  0.417647  0.479730                linguistic  \n",
       "3    Wikipedia  0.450000  0.418605  0.486486                linguistic  \n",
       "4     WikiNews  0.469448  0.551282  0.508838                linguistic  \n",
       "5     WikiNews  0.426829  0.380435  0.486111                linguistic  \n",
       "6     WikiNews  0.469448  0.551282  0.508838                linguistic  \n",
       "7     WikiNews  0.426829  0.380435  0.486111                linguistic  \n",
       "8         News  0.489785  0.696887  0.521931                linguistic  \n",
       "9         News  0.506075  0.731771  0.530860                linguistic  \n",
       "10        News  0.539924  0.830532  0.551144                linguistic  \n",
       "11        News  0.476543  0.896154  0.517857                linguistic  \n",
       "12   Wikipedia  0.565789  0.579487  0.559846                 frequency  \n",
       "13   Wikipedia  0.469298  0.466667  0.474903                 frequency  \n",
       "14   Wikipedia  0.607788  0.605023  0.611004                 frequency  \n",
       "15   Wikipedia  0.557991  0.555556  0.561776                 frequency  \n",
       "16    WikiNews  0.579381  0.598734  0.573864                 frequency  \n",
       "17    WikiNews  0.587719  0.614286  0.580808                 frequency  \n",
       "18    WikiNews  0.592943  0.594586  0.591540                 frequency  \n",
       "19    WikiNews  0.594411  0.677778  0.585859                 frequency  \n",
       "20        News  0.620979  0.671484  0.606449                 frequency  \n",
       "21        News  0.557296  0.579979  0.553745                 frequency  \n",
       "22        News  0.591791  0.594597  0.589546                 frequency  \n",
       "23        News  0.617598  0.663043  0.604022                 frequency  \n",
       "24   Wikipedia  0.510567  0.510771  0.512548            language_model  \n",
       "25   Wikipedia  0.503590  0.504386  0.503861            language_model  \n",
       "26   Wikipedia  0.557991  0.555556  0.561776            language_model  \n",
       "27   Wikipedia  0.409396  0.406667  0.412162            language_model  \n",
       "28    WikiNews  0.641281  0.650175  0.635101            language_model  \n",
       "29    WikiNews  0.683400  0.686562  0.680556            language_model  \n",
       "30    WikiNews  0.553363  0.572321  0.551136            language_model  \n",
       "31    WikiNews  0.660299  0.680290  0.648990            language_model  \n",
       "32        News  0.582793  0.594722  0.577323            language_model  \n",
       "33        News  0.593367  0.591759  0.595267            language_model  \n",
       "34        News  0.554501  0.574512  0.551318            language_model  \n",
       "35        News  0.568759  0.605172  0.563454            language_model  \n",
       "36   Wikipedia  0.548718  0.552632  0.546332                    corpus  \n",
       "37   Wikipedia  0.486329  0.491182  0.495174                    corpus  \n",
       "38   Wikipedia  0.541667  0.539683  0.548263                    corpus  \n",
       "39   Wikipedia  0.496893  0.496923  0.497104                    corpus  \n",
       "40    WikiNews  0.553363  0.572321  0.551136                    corpus  \n",
       "41    WikiNews  0.614076  0.676342  0.601641                    corpus  \n",
       "42    WikiNews  0.577322  0.624863  0.571970                    corpus  \n",
       "43    WikiNews  0.605042  0.652439  0.594697                    corpus  \n",
       "44        News  0.606316  0.669898  0.593447                    corpus  \n",
       "45        News  0.603941  0.645245  0.592666                    corpus  \n",
       "46        News  0.607196  0.653191  0.595094                    corpus  \n",
       "47        News  0.583814  0.647173  0.575589                    corpus  \n",
       "48   Wikipedia  0.514706  0.523573  0.536680          psycholinguistic  \n",
       "49   Wikipedia  0.468372  0.488095  0.480695          psycholinguistic  \n",
       "50   Wikipedia  0.489425  0.500635  0.500965          psycholinguistic  \n",
       "51   Wikipedia  0.503792  0.510033  0.514479          psycholinguistic  \n",
       "52    WikiNews  0.617133  0.619014  0.658460          psycholinguistic  \n",
       "53    WikiNews  0.591304  0.587662  0.602273          psycholinguistic  \n",
       "54    WikiNews  0.678742  0.669896  0.715909          psycholinguistic  \n",
       "55    WikiNews  0.608333  0.612782  0.651515          psycholinguistic  \n",
       "56        News  0.553641  0.565653  0.550537          psycholinguistic  \n",
       "57        News  0.476470  0.478664  0.476075          psycholinguistic  \n",
       "58        News  0.538768  0.539507  0.549150          psycholinguistic  \n",
       "59        News  0.498994  0.504916  0.503467          psycholinguistic  \n",
       "60   Wikipedia  0.456790  0.420455  0.500000                   wordnet  \n",
       "61   Wikipedia  0.518750  0.674419  0.528958                   wordnet  \n",
       "62   Wikipedia  0.446541  0.417647  0.479730                   wordnet  \n",
       "63   Wikipedia  0.498491  0.521687  0.508687                   wordnet  \n",
       "64    WikiNews  0.587719  0.614286  0.580808                   wordnet  \n",
       "65    WikiNews  0.623397  0.704762  0.608586                   wordnet  \n",
       "66    WikiNews  0.680329  0.717722  0.662879                   wordnet  \n",
       "67    WikiNews  0.622076  0.656250  0.610480                   wordnet  \n",
       "68        News  0.497287  0.596825  0.521151                   wordnet  \n",
       "69        News  0.488861  0.538018  0.511442                   wordnet  \n",
       "70        News  0.470510  0.492713  0.497660                   wordnet  \n",
       "71        News  0.483832  0.583169  0.514650                   wordnet  \n",
       "72   Wikipedia  0.484848  0.505747  0.509653                   dbpedia  \n",
       "73   Wikipedia  0.484848  0.505747  0.509653                   dbpedia  \n",
       "74   Wikipedia  0.484848  0.505747  0.509653                   dbpedia  \n",
       "75   Wikipedia  0.484848  0.505747  0.509653                   dbpedia  \n",
       "76    WikiNews  0.532338  0.535786  0.544823                   dbpedia  \n",
       "77    WikiNews  0.532338  0.535786  0.544823                   dbpedia  \n",
       "78    WikiNews  0.532338  0.535786  0.544823                   dbpedia  \n",
       "79    WikiNews  0.532338  0.535786  0.544823                   dbpedia  \n",
       "80        News  0.468490  0.468292  0.468707                   dbpedia  \n",
       "81        News  0.468490  0.468292  0.468707                   dbpedia  \n",
       "82        News  0.468490  0.468292  0.468707                   dbpedia  \n",
       "83        News  0.468490  0.468292  0.468707                   dbpedia  \n",
       "84   Wikipedia  0.471413  0.474316  0.470077          brown_clustering  \n",
       "85   Wikipedia  0.475339  0.492107  0.487452          brown_clustering  \n",
       "86   Wikipedia  0.416188  0.480879  0.464286          brown_clustering  \n",
       "87   Wikipedia  0.401361  0.404110  0.398649          brown_clustering  \n",
       "88    WikiNews  0.645797  0.638528  0.661616          brown_clustering  \n",
       "89    WikiNews  0.619008  0.614183  0.640783          brown_clustering  \n",
       "90    WikiNews  0.461996  0.558972  0.573864          brown_clustering  \n",
       "91    WikiNews  0.560284  0.558551  0.563763          brown_clustering  \n",
       "92        News  0.493720  0.493679  0.493845          brown_clustering  \n",
       "93        News  0.500318  0.501093  0.501214          brown_clustering  \n",
       "94        News  0.435239  0.488546  0.483010          brown_clustering  \n",
       "95        News  0.456226  0.453503  0.461338          brown_clustering  \n",
       "96   Wikipedia  0.443038  0.416667  0.472973                  semantic  \n",
       "97   Wikipedia  0.504923  0.547619  0.515444                  semantic  \n",
       "98   Wikipedia  0.443038  0.416667  0.472973                  semantic  \n",
       "99   Wikipedia  0.492308  0.504065  0.501931                  semantic  \n",
       "100   WikiNews  0.633039  0.739216  0.615530                  semantic  \n",
       "101   WikiNews  0.594411  0.677778  0.585859                  semantic  \n",
       "102   WikiNews  0.631373  0.676638  0.617424                  semantic  \n",
       "103   WikiNews  0.676853  0.747967  0.654040                  semantic  \n",
       "104       News  0.500860  0.552591  0.517944                  semantic  \n",
       "105       News  0.480721  0.504554  0.501734                  semantic  \n",
       "106       News  0.478723  0.498267  0.499307                  semantic  \n",
       "107       News  0.490938  0.549428  0.513870                  semantic  \n",
       "108  Wikipedia  0.532819  0.532819  0.532819                dictionary  \n",
       "109  Wikipedia  0.261268  0.475342  0.473938                dictionary  \n",
       "110  Wikipedia  0.517808  0.517361  0.519305                dictionary  \n",
       "111  Wikipedia  0.541667  0.539683  0.548263                dictionary  \n",
       "112   WikiNews  0.599765  0.595633  0.609217                dictionary  \n",
       "113   WikiNews  0.382699  0.517262  0.518308                dictionary  \n",
       "114   WikiNews  0.508768  0.581555  0.608586                dictionary  \n",
       "115   WikiNews  0.592943  0.594586  0.591540                dictionary  \n",
       "116       News  0.475160  0.474634  0.475988                dictionary  \n",
       "117       News  0.362363  0.559660  0.555652                dictionary  \n",
       "118       News  0.420215  0.507715  0.510749                dictionary  \n",
       "119       News  0.475160  0.474634  0.475988                dictionary  \n",
       "120  Wikipedia  0.439490  0.415663  0.466216           corpus+semantic  \n",
       "121  Wikipedia  0.439490  0.415663  0.466216           corpus+semantic  \n",
       "122  Wikipedia  0.584416  0.618750  0.573359           corpus+semantic  \n",
       "123  Wikipedia  0.424837  0.411392  0.439189           corpus+semantic  \n",
       "124   WikiNews  0.594411  0.677778  0.585859           corpus+semantic  \n",
       "125   WikiNews  0.622321  0.820076  0.606692           corpus+semantic  \n",
       "126   WikiNews  0.650831  0.727820  0.631313           corpus+semantic  \n",
       "127   WikiNews  0.643038  0.781977  0.622475           corpus+semantic  \n",
       "128       News  0.590101  0.668819  0.580444           corpus+semantic  \n",
       "129       News  0.601689  0.680497  0.589372           corpus+semantic  \n",
       "130       News  0.569211  0.625063  0.564234           corpus+semantic  \n",
       "131       News  0.584469  0.682025  0.576370           corpus+semantic  \n",
       "132  Wikipedia  0.492308  0.504065  0.501931  wordnet+psycholinguistic  \n",
       "133  Wikipedia  0.548718  0.593496  0.544402  wordnet+psycholinguistic  \n",
       "134  Wikipedia  0.443038  0.416667  0.472973  wordnet+psycholinguistic  \n",
       "135  Wikipedia  0.548718  0.593496  0.544402  wordnet+psycholinguistic  \n",
       "136   WikiNews  0.587162  0.595760  0.582702  wordnet+psycholinguistic  \n",
       "137   WikiNews  0.623397  0.704762  0.608586  wordnet+psycholinguistic  \n",
       "138   WikiNews  0.660299  0.680290  0.648990  wordnet+psycholinguistic  \n",
       "139   WikiNews  0.660299  0.680290  0.648990  wordnet+psycholinguistic  \n",
       "140       News  0.500860  0.552591  0.517944  wordnet+psycholinguistic  \n",
       "141       News  0.512192  0.564208  0.524445  wordnet+psycholinguistic  \n",
       "142       News  0.463163  0.469894  0.487951  wordnet+psycholinguistic  \n",
       "143       News  0.460923  0.483339  0.496012  wordnet+psycholinguistic  \n",
       "144  Wikipedia  0.450000  0.418605  0.486486                       all  \n",
       "145  Wikipedia  0.453416  0.419540  0.493243                       all  \n",
       "146  Wikipedia  0.446541  0.417647  0.479730                       all  \n",
       "147  Wikipedia  0.450000  0.418605  0.486486                       all  \n",
       "148   WikiNews  0.553656  0.769444  0.561237                       all  \n",
       "149   WikiNews  0.553656  0.769444  0.561237                       all  \n",
       "150   WikiNews  0.603376  0.713663  0.592803                       all  \n",
       "151   WikiNews  0.545894  0.693258  0.554293                       all  \n",
       "152       News  0.535678  0.634143  0.543083                       all  \n",
       "153       News  0.596560  0.694428  0.585298                       all  \n",
       "154       News  0.584554  0.740216  0.577150                       all  \n",
       "155       News  0.574391  0.751776  0.570648                       all  "
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_eval_data_rf_dswp = create_eval_df_from_results_macro(results, False)\n",
    "feature_eval_data_rf_dswp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.607788</td>\n",
       "      <td>0.605023</td>\n",
       "      <td>0.611004</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.620979</td>\n",
       "      <td>0.671484</td>\n",
       "      <td>0.606449</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.683400</td>\n",
       "      <td>0.686562</td>\n",
       "      <td>0.680556</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     agg    dataset        f1      prec       rec              zc\n",
       "14   min  Wikipedia  0.607788  0.605023  0.611004       frequency\n",
       "20  mean       News  0.620979  0.671484  0.606449       frequency\n",
       "29   max   WikiNews  0.683400  0.686562  0.680556  language_model"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = feature_eval_data_rf_dswp.groupby(['dataset'])['f1'].transform(max) == feature_eval_data_rf_dswp['f1']\n",
    "feature_eval_data_rf_dswp[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.2.3 Random Forest (Extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "results = [Result(fs, fs.fc, fs.agg,\n",
    "        precision_recall_fscore_support(*random_forest_extra(*transform_feat_to_num(remove_labels_for_binary_df(fs.train), \n",
    "        remove_labels_for_binary_df(fs.test))), average='macro')) for fs in all_fc_datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.518750</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.528958</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.479730</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.469448</td>\n",
       "      <td>0.551282</td>\n",
       "      <td>0.508838</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.469448</td>\n",
       "      <td>0.551282</td>\n",
       "      <td>0.508838</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.469448</td>\n",
       "      <td>0.551282</td>\n",
       "      <td>0.508838</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.430303</td>\n",
       "      <td>0.381720</td>\n",
       "      <td>0.493056</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.521881</td>\n",
       "      <td>0.757143</td>\n",
       "      <td>0.539788</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.534573</td>\n",
       "      <td>0.734519</td>\n",
       "      <td>0.546290</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.552137</td>\n",
       "      <td>0.792051</td>\n",
       "      <td>0.557646</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.526892</td>\n",
       "      <td>0.900778</td>\n",
       "      <td>0.544643</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.464387</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.501894</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.464387</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.501894</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.584554</td>\n",
       "      <td>0.740216</td>\n",
       "      <td>0.577150</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.554814</td>\n",
       "      <td>0.688940</td>\n",
       "      <td>0.556865</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.458657</td>\n",
       "      <td>0.894636</td>\n",
       "      <td>0.508929</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.552137</td>\n",
       "      <td>0.792051</td>\n",
       "      <td>0.557646</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.479730</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.479730</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.479730</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.479730</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.479842</td>\n",
       "      <td>0.887097</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.451194</td>\n",
       "      <td>0.475911</td>\n",
       "      <td>0.496793</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.512264</td>\n",
       "      <td>0.625679</td>\n",
       "      <td>0.530080</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.474759</td>\n",
       "      <td>0.729086</td>\n",
       "      <td>0.515430</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.508338</td>\n",
       "      <td>0.798833</td>\n",
       "      <td>0.533287</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.479730</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.479730</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.479730</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.479730</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.430303</td>\n",
       "      <td>0.381720</td>\n",
       "      <td>0.493056</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.557700</td>\n",
       "      <td>0.711307</td>\n",
       "      <td>0.559293</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.551970</td>\n",
       "      <td>0.669501</td>\n",
       "      <td>0.554438</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.560630</td>\n",
       "      <td>0.737333</td>\n",
       "      <td>0.561720</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.546491</td>\n",
       "      <td>0.720572</td>\n",
       "      <td>0.552791</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.479842</td>\n",
       "      <td>0.887097</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.561718</td>\n",
       "      <td>0.895604</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.489785</td>\n",
       "      <td>0.696887</td>\n",
       "      <td>0.521931</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.458657</td>\n",
       "      <td>0.894636</td>\n",
       "      <td>0.508929</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.474759</td>\n",
       "      <td>0.729086</td>\n",
       "      <td>0.515430</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.438972</td>\n",
       "      <td>0.392720</td>\n",
       "      <td>0.497573</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.430303</td>\n",
       "      <td>0.381720</td>\n",
       "      <td>0.493056</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.479842</td>\n",
       "      <td>0.887097</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.479842</td>\n",
       "      <td>0.887097</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.430303</td>\n",
       "      <td>0.381720</td>\n",
       "      <td>0.493056</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.455659</td>\n",
       "      <td>0.560489</td>\n",
       "      <td>0.504074</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.438972</td>\n",
       "      <td>0.392720</td>\n",
       "      <td>0.497573</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.458657</td>\n",
       "      <td>0.894636</td>\n",
       "      <td>0.508929</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.545894</td>\n",
       "      <td>0.693258</td>\n",
       "      <td>0.554293</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.545894</td>\n",
       "      <td>0.693258</td>\n",
       "      <td>0.554293</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.545894</td>\n",
       "      <td>0.693258</td>\n",
       "      <td>0.554293</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.545894</td>\n",
       "      <td>0.693258</td>\n",
       "      <td>0.554293</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.458657</td>\n",
       "      <td>0.894636</td>\n",
       "      <td>0.508929</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.458657</td>\n",
       "      <td>0.894636</td>\n",
       "      <td>0.508929</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.458657</td>\n",
       "      <td>0.894636</td>\n",
       "      <td>0.508929</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.458657</td>\n",
       "      <td>0.894636</td>\n",
       "      <td>0.508929</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.489785</td>\n",
       "      <td>0.696887</td>\n",
       "      <td>0.521931</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.479842</td>\n",
       "      <td>0.887097</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.479842</td>\n",
       "      <td>0.887097</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.472989</td>\n",
       "      <td>0.645349</td>\n",
       "      <td>0.513003</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.438972</td>\n",
       "      <td>0.392720</td>\n",
       "      <td>0.497573</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.458657</td>\n",
       "      <td>0.894636</td>\n",
       "      <td>0.508929</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.458657</td>\n",
       "      <td>0.894636</td>\n",
       "      <td>0.508929</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.479842</td>\n",
       "      <td>0.887097</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.438972</td>\n",
       "      <td>0.392720</td>\n",
       "      <td>0.497573</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.474593</td>\n",
       "      <td>0.635870</td>\n",
       "      <td>0.515783</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.479842</td>\n",
       "      <td>0.887097</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.546491</td>\n",
       "      <td>0.720572</td>\n",
       "      <td>0.552791</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.529377</td>\n",
       "      <td>0.673126</td>\n",
       "      <td>0.541436</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.577528</td>\n",
       "      <td>0.781000</td>\n",
       "      <td>0.573076</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.549292</td>\n",
       "      <td>0.752778</td>\n",
       "      <td>0.555218</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.430303</td>\n",
       "      <td>0.381720</td>\n",
       "      <td>0.493056</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.598291</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.479842</td>\n",
       "      <td>0.887097</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.430303</td>\n",
       "      <td>0.381720</td>\n",
       "      <td>0.493056</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.489785</td>\n",
       "      <td>0.696887</td>\n",
       "      <td>0.521931</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.457156</td>\n",
       "      <td>0.644231</td>\n",
       "      <td>0.506501</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.476543</td>\n",
       "      <td>0.896154</td>\n",
       "      <td>0.517857</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.458657</td>\n",
       "      <td>0.894636</td>\n",
       "      <td>0.508929</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.479842</td>\n",
       "      <td>0.887097</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.522358</td>\n",
       "      <td>0.891304</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.474593</td>\n",
       "      <td>0.635870</td>\n",
       "      <td>0.515783</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.430303</td>\n",
       "      <td>0.381720</td>\n",
       "      <td>0.493056</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.560630</td>\n",
       "      <td>0.737333</td>\n",
       "      <td>0.561720</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.584554</td>\n",
       "      <td>0.740216</td>\n",
       "      <td>0.577150</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.600846</td>\n",
       "      <td>0.775574</td>\n",
       "      <td>0.588506</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.569700</td>\n",
       "      <td>0.849583</td>\n",
       "      <td>0.569001</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                agg  \\\n",
       "0                                                                                                                              mean   \n",
       "1                                                                                                                               max   \n",
       "2                                                                                                                               min   \n",
       "3                                                                                                                     weighted_mean   \n",
       "4                                                                                                                              mean   \n",
       "5                                                                                                                               max   \n",
       "6                                                                                                                               min   \n",
       "7                                                                                                                     weighted_mean   \n",
       "8                                                                                                                              mean   \n",
       "9                                                                                                                               max   \n",
       "10                                                                                                                              min   \n",
       "11                                                                                                                    weighted_mean   \n",
       "12                                                                                                                             mean   \n",
       "13                                                                                                                              max   \n",
       "14                                                                                                                              min   \n",
       "15                                                                                                                    weighted_mean   \n",
       "16                                                                                                                             mean   \n",
       "17                                                                                                                              max   \n",
       "18                                                                                                                              min   \n",
       "19                                                                                                                    weighted_mean   \n",
       "20                                                                                                                             mean   \n",
       "21                                                                                                                              max   \n",
       "22                                                                                                                              min   \n",
       "23                                                                                                                    weighted_mean   \n",
       "24                                                                                                                             mean   \n",
       "25                                                                                                                              max   \n",
       "26                                                                                                                              min   \n",
       "27                                                                                                                    weighted_mean   \n",
       "28                                                                                                                             mean   \n",
       "29                                                                                                                              max   \n",
       "30                                                                                                                              min   \n",
       "31                                                                                                                    weighted_mean   \n",
       "32                                                                                                                             mean   \n",
       "33                                                                                                                              max   \n",
       "34                                                                                                                              min   \n",
       "35                                                                                                                    weighted_mean   \n",
       "36                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "37                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "38                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "39                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "40                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "41                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "42                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "43                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "44                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "45                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "46                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "47                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "48                                                                                                                             mean   \n",
       "49                                                                                                                              max   \n",
       "50                                                                                                                              min   \n",
       "51                                                                                                                    weighted_mean   \n",
       "52                                                                                                                             mean   \n",
       "53                                                                                                                              max   \n",
       "54                                                                                                                              min   \n",
       "55                                                                                                                    weighted_mean   \n",
       "56                                                                                                                             mean   \n",
       "57                                                                                                                              max   \n",
       "58                                                                                                                              min   \n",
       "59                                                                                                                    weighted_mean   \n",
       "60                                                                                                                             mean   \n",
       "61                                                                                                                              max   \n",
       "62                                                                                                                              min   \n",
       "63                                                                                                                    weighted_mean   \n",
       "64                                                                                                                             mean   \n",
       "65                                                                                                                              max   \n",
       "66                                                                                                                              min   \n",
       "67                                                                                                                    weighted_mean   \n",
       "68                                                                                                                             mean   \n",
       "69                                                                                                                              max   \n",
       "70                                                                                                                              min   \n",
       "71                                                                                                                    weighted_mean   \n",
       "72                                                                                                                             mean   \n",
       "73                                                                                                                              max   \n",
       "74                                                                                                                              min   \n",
       "75                                                                                                                    weighted_mean   \n",
       "76                                                                                                                             mean   \n",
       "77                                                                                                                              max   \n",
       "78                                                                                                                              min   \n",
       "79                                                                                                                    weighted_mean   \n",
       "80                                                                                                                             mean   \n",
       "81                                                                                                                              max   \n",
       "82                                                                                                                              min   \n",
       "83                                                                                                                    weighted_mean   \n",
       "84                                                                                                                             mean   \n",
       "85                                                                                                                              max   \n",
       "86                                                                                                                              min   \n",
       "87                                                                                                                    weighted_mean   \n",
       "88                                                                                                                             mean   \n",
       "89                                                                                                                              max   \n",
       "90                                                                                                                              min   \n",
       "91                                                                                                                    weighted_mean   \n",
       "92                                                                                                                             mean   \n",
       "93                                                                                                                              max   \n",
       "94                                                                                                                              min   \n",
       "95                                                                                                                    weighted_mean   \n",
       "96                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "97                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "98                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "99                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "100                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "101                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "102                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "103                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "104                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "105                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "106                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "107                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "108                                                                                                                            mean   \n",
       "109                                                                                                                             max   \n",
       "110                                                                                                                             min   \n",
       "111                                                                                                                   weighted_mean   \n",
       "112                                                                                                                            mean   \n",
       "113                                                                                                                             max   \n",
       "114                                                                                                                             min   \n",
       "115                                                                                                                   weighted_mean   \n",
       "116                                                                                                                            mean   \n",
       "117                                                                                                                             max   \n",
       "118                                                                                                                             min   \n",
       "119                                                                                                                   weighted_mean   \n",
       "120  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "121            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "122            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "123        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "124  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "125            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "126            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "127        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "128  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "129            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "130            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "131        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "132                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "133                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "134                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "135                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "136                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "137                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "138                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "139                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "140                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "141                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "142                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "143                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "144                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "145                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "146                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "147                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "148                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "149                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "150                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "151                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "152                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "153                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "154                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "155                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "\n",
       "       dataset        f1      prec       rec                        zc  \n",
       "0    Wikipedia  0.456790  0.420455  0.500000                linguistic  \n",
       "1    Wikipedia  0.518750  0.674419  0.528958                linguistic  \n",
       "2    Wikipedia  0.446541  0.417647  0.479730                linguistic  \n",
       "3    Wikipedia  0.456790  0.420455  0.500000                linguistic  \n",
       "4     WikiNews  0.469448  0.551282  0.508838                linguistic  \n",
       "5     WikiNews  0.469448  0.551282  0.508838                linguistic  \n",
       "6     WikiNews  0.469448  0.551282  0.508838                linguistic  \n",
       "7     WikiNews  0.430303  0.381720  0.493056                linguistic  \n",
       "8         News  0.521881  0.757143  0.539788                linguistic  \n",
       "9         News  0.534573  0.734519  0.546290                linguistic  \n",
       "10        News  0.552137  0.792051  0.557646                linguistic  \n",
       "11        News  0.526892  0.900778  0.544643                linguistic  \n",
       "12   Wikipedia  0.456790  0.420455  0.500000                 frequency  \n",
       "13   Wikipedia  0.456790  0.420455  0.500000                 frequency  \n",
       "14   Wikipedia  0.456790  0.420455  0.500000                 frequency  \n",
       "15   Wikipedia  0.456790  0.420455  0.500000                 frequency  \n",
       "16    WikiNews  0.464387  0.508333  0.501894                 frequency  \n",
       "17    WikiNews  0.464387  0.508333  0.501894                 frequency  \n",
       "18    WikiNews  0.433735  0.382979  0.500000                 frequency  \n",
       "19    WikiNews  0.433735  0.382979  0.500000                 frequency  \n",
       "20        News  0.584554  0.740216  0.577150                 frequency  \n",
       "21        News  0.554814  0.688940  0.556865                 frequency  \n",
       "22        News  0.458657  0.894636  0.508929                 frequency  \n",
       "23        News  0.552137  0.792051  0.557646                 frequency  \n",
       "24   Wikipedia  0.446541  0.417647  0.479730            language_model  \n",
       "25   Wikipedia  0.446541  0.417647  0.479730            language_model  \n",
       "26   Wikipedia  0.446541  0.417647  0.479730            language_model  \n",
       "27   Wikipedia  0.446541  0.417647  0.479730            language_model  \n",
       "28    WikiNews  0.433735  0.382979  0.500000            language_model  \n",
       "29    WikiNews  0.479842  0.887097  0.522727            language_model  \n",
       "30    WikiNews  0.433735  0.382979  0.500000            language_model  \n",
       "31    WikiNews  0.433735  0.382979  0.500000            language_model  \n",
       "32        News  0.451194  0.475911  0.496793            language_model  \n",
       "33        News  0.512264  0.625679  0.530080            language_model  \n",
       "34        News  0.474759  0.729086  0.515430            language_model  \n",
       "35        News  0.508338  0.798833  0.533287            language_model  \n",
       "36   Wikipedia  0.446541  0.417647  0.479730                    corpus  \n",
       "37   Wikipedia  0.446541  0.417647  0.479730                    corpus  \n",
       "38   Wikipedia  0.446541  0.417647  0.479730                    corpus  \n",
       "39   Wikipedia  0.446541  0.417647  0.479730                    corpus  \n",
       "40    WikiNews  0.433735  0.382979  0.500000                    corpus  \n",
       "41    WikiNews  0.430303  0.381720  0.493056                    corpus  \n",
       "42    WikiNews  0.433735  0.382979  0.500000                    corpus  \n",
       "43    WikiNews  0.433735  0.382979  0.500000                    corpus  \n",
       "44        News  0.557700  0.711307  0.559293                    corpus  \n",
       "45        News  0.551970  0.669501  0.554438                    corpus  \n",
       "46        News  0.560630  0.737333  0.561720                    corpus  \n",
       "47        News  0.546491  0.720572  0.552791                    corpus  \n",
       "48   Wikipedia  0.456790  0.420455  0.500000          psycholinguistic  \n",
       "49   Wikipedia  0.456790  0.420455  0.500000          psycholinguistic  \n",
       "50   Wikipedia  0.456790  0.420455  0.500000          psycholinguistic  \n",
       "51   Wikipedia  0.456790  0.420455  0.500000          psycholinguistic  \n",
       "52    WikiNews  0.433735  0.382979  0.500000          psycholinguistic  \n",
       "53    WikiNews  0.479842  0.887097  0.522727          psycholinguistic  \n",
       "54    WikiNews  0.561718  0.895604  0.568182          psycholinguistic  \n",
       "55    WikiNews  0.433735  0.382979  0.500000          psycholinguistic  \n",
       "56        News  0.489785  0.696887  0.521931          psycholinguistic  \n",
       "57        News  0.458657  0.894636  0.508929          psycholinguistic  \n",
       "58        News  0.474759  0.729086  0.515430          psycholinguistic  \n",
       "59        News  0.438972  0.392720  0.497573          psycholinguistic  \n",
       "60   Wikipedia  0.456790  0.420455  0.500000                   wordnet  \n",
       "61   Wikipedia  0.456790  0.420455  0.500000                   wordnet  \n",
       "62   Wikipedia  0.456790  0.420455  0.500000                   wordnet  \n",
       "63   Wikipedia  0.456790  0.420455  0.500000                   wordnet  \n",
       "64    WikiNews  0.430303  0.381720  0.493056                   wordnet  \n",
       "65    WikiNews  0.479842  0.887097  0.522727                   wordnet  \n",
       "66    WikiNews  0.479842  0.887097  0.522727                   wordnet  \n",
       "67    WikiNews  0.430303  0.381720  0.493056                   wordnet  \n",
       "68        News  0.455659  0.560489  0.504074                   wordnet  \n",
       "69        News  0.438972  0.392720  0.497573                   wordnet  \n",
       "70        News  0.458657  0.894636  0.508929                   wordnet  \n",
       "71        News  0.440171  0.393130  0.500000                   wordnet  \n",
       "72   Wikipedia  0.456790  0.420455  0.500000                   dbpedia  \n",
       "73   Wikipedia  0.456790  0.420455  0.500000                   dbpedia  \n",
       "74   Wikipedia  0.456790  0.420455  0.500000                   dbpedia  \n",
       "75   Wikipedia  0.456790  0.420455  0.500000                   dbpedia  \n",
       "76    WikiNews  0.545894  0.693258  0.554293                   dbpedia  \n",
       "77    WikiNews  0.545894  0.693258  0.554293                   dbpedia  \n",
       "78    WikiNews  0.545894  0.693258  0.554293                   dbpedia  \n",
       "79    WikiNews  0.545894  0.693258  0.554293                   dbpedia  \n",
       "80        News  0.458657  0.894636  0.508929                   dbpedia  \n",
       "81        News  0.458657  0.894636  0.508929                   dbpedia  \n",
       "82        News  0.458657  0.894636  0.508929                   dbpedia  \n",
       "83        News  0.458657  0.894636  0.508929                   dbpedia  \n",
       "84   Wikipedia  0.456790  0.420455  0.500000          brown_clustering  \n",
       "85   Wikipedia  0.456790  0.420455  0.500000          brown_clustering  \n",
       "86   Wikipedia  0.456790  0.420455  0.500000          brown_clustering  \n",
       "87   Wikipedia  0.456790  0.420455  0.500000          brown_clustering  \n",
       "88    WikiNews  0.433735  0.382979  0.500000          brown_clustering  \n",
       "89    WikiNews  0.433735  0.382979  0.500000          brown_clustering  \n",
       "90    WikiNews  0.433735  0.382979  0.500000          brown_clustering  \n",
       "91    WikiNews  0.433735  0.382979  0.500000          brown_clustering  \n",
       "92        News  0.440171  0.393130  0.500000          brown_clustering  \n",
       "93        News  0.440171  0.393130  0.500000          brown_clustering  \n",
       "94        News  0.489785  0.696887  0.521931          brown_clustering  \n",
       "95        News  0.440171  0.393130  0.500000          brown_clustering  \n",
       "96   Wikipedia  0.456790  0.420455  0.500000                  semantic  \n",
       "97   Wikipedia  0.456790  0.420455  0.500000                  semantic  \n",
       "98   Wikipedia  0.456790  0.420455  0.500000                  semantic  \n",
       "99   Wikipedia  0.456790  0.420455  0.500000                  semantic  \n",
       "100   WikiNews  0.433735  0.382979  0.500000                  semantic  \n",
       "101   WikiNews  0.479842  0.887097  0.522727                  semantic  \n",
       "102   WikiNews  0.479842  0.887097  0.522727                  semantic  \n",
       "103   WikiNews  0.433735  0.382979  0.500000                  semantic  \n",
       "104       News  0.472989  0.645349  0.513003                  semantic  \n",
       "105       News  0.438972  0.392720  0.497573                  semantic  \n",
       "106       News  0.458657  0.894636  0.508929                  semantic  \n",
       "107       News  0.458657  0.894636  0.508929                  semantic  \n",
       "108  Wikipedia  0.456790  0.420455  0.500000                dictionary  \n",
       "109  Wikipedia  0.456790  0.420455  0.500000                dictionary  \n",
       "110  Wikipedia  0.456790  0.420455  0.500000                dictionary  \n",
       "111  Wikipedia  0.456790  0.420455  0.500000                dictionary  \n",
       "112   WikiNews  0.479842  0.887097  0.522727                dictionary  \n",
       "113   WikiNews  0.433735  0.382979  0.500000                dictionary  \n",
       "114   WikiNews  0.433735  0.382979  0.500000                dictionary  \n",
       "115   WikiNews  0.433735  0.382979  0.500000                dictionary  \n",
       "116       News  0.438972  0.392720  0.497573                dictionary  \n",
       "117       News  0.440171  0.393130  0.500000                dictionary  \n",
       "118       News  0.440171  0.393130  0.500000                dictionary  \n",
       "119       News  0.440171  0.393130  0.500000                dictionary  \n",
       "120  Wikipedia  0.450000  0.418605  0.486486           corpus+semantic  \n",
       "121  Wikipedia  0.450000  0.418605  0.486486           corpus+semantic  \n",
       "122  Wikipedia  0.450000  0.418605  0.486486           corpus+semantic  \n",
       "123  Wikipedia  0.450000  0.418605  0.486486           corpus+semantic  \n",
       "124   WikiNews  0.433735  0.382979  0.500000           corpus+semantic  \n",
       "125   WikiNews  0.474593  0.635870  0.515783           corpus+semantic  \n",
       "126   WikiNews  0.479842  0.887097  0.522727           corpus+semantic  \n",
       "127   WikiNews  0.433735  0.382979  0.500000           corpus+semantic  \n",
       "128       News  0.546491  0.720572  0.552791           corpus+semantic  \n",
       "129       News  0.529377  0.673126  0.541436           corpus+semantic  \n",
       "130       News  0.577528  0.781000  0.573076           corpus+semantic  \n",
       "131       News  0.549292  0.752778  0.555218           corpus+semantic  \n",
       "132  Wikipedia  0.456790  0.420455  0.500000  wordnet+psycholinguistic  \n",
       "133  Wikipedia  0.456790  0.420455  0.500000  wordnet+psycholinguistic  \n",
       "134  Wikipedia  0.456790  0.420455  0.500000  wordnet+psycholinguistic  \n",
       "135  Wikipedia  0.456790  0.420455  0.500000  wordnet+psycholinguistic  \n",
       "136   WikiNews  0.430303  0.381720  0.493056  wordnet+psycholinguistic  \n",
       "137   WikiNews  0.598291  0.900000  0.590909  wordnet+psycholinguistic  \n",
       "138   WikiNews  0.479842  0.887097  0.522727  wordnet+psycholinguistic  \n",
       "139   WikiNews  0.430303  0.381720  0.493056  wordnet+psycholinguistic  \n",
       "140       News  0.489785  0.696887  0.521931  wordnet+psycholinguistic  \n",
       "141       News  0.457156  0.644231  0.506501  wordnet+psycholinguistic  \n",
       "142       News  0.476543  0.896154  0.517857  wordnet+psycholinguistic  \n",
       "143       News  0.458657  0.894636  0.508929  wordnet+psycholinguistic  \n",
       "144  Wikipedia  0.450000  0.418605  0.486486                       all  \n",
       "145  Wikipedia  0.450000  0.418605  0.486486                       all  \n",
       "146  Wikipedia  0.450000  0.418605  0.486486                       all  \n",
       "147  Wikipedia  0.450000  0.418605  0.486486                       all  \n",
       "148   WikiNews  0.479842  0.887097  0.522727                       all  \n",
       "149   WikiNews  0.522358  0.891304  0.545455                       all  \n",
       "150   WikiNews  0.474593  0.635870  0.515783                       all  \n",
       "151   WikiNews  0.430303  0.381720  0.493056                       all  \n",
       "152       News  0.560630  0.737333  0.561720                       all  \n",
       "153       News  0.584554  0.740216  0.577150                       all  \n",
       "154       News  0.600846  0.775574  0.588506                       all  \n",
       "155       News  0.569700  0.849583  0.569001                       all  "
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_eval_data_rfe = create_eval_df_from_results_macro(results, False)\n",
    "feature_eval_data_rfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.518750</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.528958</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.598291</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.600846</td>\n",
       "      <td>0.775574</td>\n",
       "      <td>0.588506</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          agg    dataset  \\\n",
       "1                                                         max  Wikipedia   \n",
       "137  (max, <function agg_feat_num_max at 0x000000FF522328C8>)   WikiNews   \n",
       "154  (min, <function agg_feat_num_min at 0x000000FF52232950>)       News   \n",
       "\n",
       "           f1      prec       rec                        zc  \n",
       "1    0.518750  0.674419  0.528958                linguistic  \n",
       "137  0.598291  0.900000  0.590909  wordnet+psycholinguistic  \n",
       "154  0.600846  0.775574  0.588506                       all  "
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = feature_eval_data_rfe.groupby(['dataset'])['f1'].transform(max) == feature_eval_data_rfe['f1']\n",
    "feature_eval_data_rfe[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "results = [Result(fs, fs.fc, fs.agg,\n",
    "        precision_recall_fscore_support(*random_forest_extra(*transform_feat_to_num(remove_labels_for_binary_df(fs.train), \n",
    "        remove_labels_for_binary_df(fs.test))), average='macro')) for fs in all_fc_datasets_dswp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.545894</td>\n",
       "      <td>0.693258</td>\n",
       "      <td>0.554293</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.545894</td>\n",
       "      <td>0.693258</td>\n",
       "      <td>0.554293</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.502646</td>\n",
       "      <td>0.587640</td>\n",
       "      <td>0.524621</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.538393</td>\n",
       "      <td>0.642045</td>\n",
       "      <td>0.547348</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.466033</td>\n",
       "      <td>0.518701</td>\n",
       "      <td>0.503294</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.481884</td>\n",
       "      <td>0.561924</td>\n",
       "      <td>0.512223</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.501630</td>\n",
       "      <td>0.647638</td>\n",
       "      <td>0.526006</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.434125</td>\n",
       "      <td>0.391051</td>\n",
       "      <td>0.487864</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.557991</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.561776</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.503590</td>\n",
       "      <td>0.504386</td>\n",
       "      <td>0.503861</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.510567</td>\n",
       "      <td>0.510771</td>\n",
       "      <td>0.512548</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.538934</td>\n",
       "      <td>0.545537</td>\n",
       "      <td>0.572394</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.570776</td>\n",
       "      <td>0.573649</td>\n",
       "      <td>0.568813</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.587162</td>\n",
       "      <td>0.595760</td>\n",
       "      <td>0.582702</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.560284</td>\n",
       "      <td>0.558551</td>\n",
       "      <td>0.563763</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.596265</td>\n",
       "      <td>0.632004</td>\n",
       "      <td>0.587753</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.633566</td>\n",
       "      <td>0.660630</td>\n",
       "      <td>0.621099</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.570712</td>\n",
       "      <td>0.596878</td>\n",
       "      <td>0.565101</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.597031</td>\n",
       "      <td>0.593167</td>\n",
       "      <td>0.603415</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.604653</td>\n",
       "      <td>0.624662</td>\n",
       "      <td>0.595960</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.563692</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.592664</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.538934</td>\n",
       "      <td>0.545537</td>\n",
       "      <td>0.572394</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.563692</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.592664</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.562189</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.608108</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.540117</td>\n",
       "      <td>0.541892</td>\n",
       "      <td>0.539141</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.609750</td>\n",
       "      <td>0.609443</td>\n",
       "      <td>0.642677</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.584596</td>\n",
       "      <td>0.584596</td>\n",
       "      <td>0.584596</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.673759</td>\n",
       "      <td>0.667536</td>\n",
       "      <td>0.682449</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.527904</td>\n",
       "      <td>0.541536</td>\n",
       "      <td>0.528606</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.561139</td>\n",
       "      <td>0.558854</td>\n",
       "      <td>0.568568</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.530377</td>\n",
       "      <td>0.535446</td>\n",
       "      <td>0.529473</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.528135</td>\n",
       "      <td>0.536149</td>\n",
       "      <td>0.527826</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.565789</td>\n",
       "      <td>0.579487</td>\n",
       "      <td>0.559846</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.524865</td>\n",
       "      <td>0.535162</td>\n",
       "      <td>0.524131</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.539683</td>\n",
       "      <td>0.548263</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.579861</td>\n",
       "      <td>0.574603</td>\n",
       "      <td>0.590734</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.530890</td>\n",
       "      <td>0.536669</td>\n",
       "      <td>0.530303</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.548077</td>\n",
       "      <td>0.592857</td>\n",
       "      <td>0.549242</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.578895</td>\n",
       "      <td>0.584211</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.578895</td>\n",
       "      <td>0.584211</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.610958</td>\n",
       "      <td>0.647575</td>\n",
       "      <td>0.599168</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.587188</td>\n",
       "      <td>0.620418</td>\n",
       "      <td>0.578883</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.601386</td>\n",
       "      <td>0.612442</td>\n",
       "      <td>0.595180</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.579631</td>\n",
       "      <td>0.616604</td>\n",
       "      <td>0.572382</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.505747</td>\n",
       "      <td>0.509653</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.475339</td>\n",
       "      <td>0.492107</td>\n",
       "      <td>0.487452</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.522994</td>\n",
       "      <td>0.535652</td>\n",
       "      <td>0.558880</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.514706</td>\n",
       "      <td>0.523573</td>\n",
       "      <td>0.536680</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.559375</td>\n",
       "      <td>0.568609</td>\n",
       "      <td>0.592172</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.645283</td>\n",
       "      <td>0.638249</td>\n",
       "      <td>0.670455</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.645283</td>\n",
       "      <td>0.638249</td>\n",
       "      <td>0.670455</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.601640</td>\n",
       "      <td>0.599851</td>\n",
       "      <td>0.626894</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.572192</td>\n",
       "      <td>0.590461</td>\n",
       "      <td>0.566748</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.506718</td>\n",
       "      <td>0.506677</td>\n",
       "      <td>0.506848</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.490963</td>\n",
       "      <td>0.494274</td>\n",
       "      <td>0.493152</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.535913</td>\n",
       "      <td>0.548649</td>\n",
       "      <td>0.535107</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.518750</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.528958</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.518750</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.528958</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.612665</td>\n",
       "      <td>0.759442</td>\n",
       "      <td>0.599747</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.603376</td>\n",
       "      <td>0.713663</td>\n",
       "      <td>0.592803</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.687586</td>\n",
       "      <td>0.779299</td>\n",
       "      <td>0.660985</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.633039</td>\n",
       "      <td>0.739216</td>\n",
       "      <td>0.615530</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.466033</td>\n",
       "      <td>0.518701</td>\n",
       "      <td>0.503294</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.443809</td>\n",
       "      <td>0.435893</td>\n",
       "      <td>0.484657</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.472373</td>\n",
       "      <td>0.500288</td>\n",
       "      <td>0.500087</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.483832</td>\n",
       "      <td>0.583169</td>\n",
       "      <td>0.514650</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.505062</td>\n",
       "      <td>0.533962</td>\n",
       "      <td>0.560811</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.505062</td>\n",
       "      <td>0.533962</td>\n",
       "      <td>0.560811</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.505062</td>\n",
       "      <td>0.533962</td>\n",
       "      <td>0.560811</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.505062</td>\n",
       "      <td>0.533962</td>\n",
       "      <td>0.560811</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.470098</td>\n",
       "      <td>0.504982</td>\n",
       "      <td>0.506944</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.470098</td>\n",
       "      <td>0.504982</td>\n",
       "      <td>0.506944</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.470098</td>\n",
       "      <td>0.504982</td>\n",
       "      <td>0.506944</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.470098</td>\n",
       "      <td>0.504982</td>\n",
       "      <td>0.506944</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.452099</td>\n",
       "      <td>0.457968</td>\n",
       "      <td>0.450156</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.452099</td>\n",
       "      <td>0.457968</td>\n",
       "      <td>0.450156</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.452099</td>\n",
       "      <td>0.457968</td>\n",
       "      <td>0.450156</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.452099</td>\n",
       "      <td>0.457968</td>\n",
       "      <td>0.450156</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.458067</td>\n",
       "      <td>0.443050</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.496568</td>\n",
       "      <td>0.505208</td>\n",
       "      <td>0.507722</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.418391</td>\n",
       "      <td>0.492754</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.477626</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>0.476834</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.584301</td>\n",
       "      <td>0.583165</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.636754</td>\n",
       "      <td>0.629973</td>\n",
       "      <td>0.654672</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.480902</td>\n",
       "      <td>0.568137</td>\n",
       "      <td>0.587753</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.584596</td>\n",
       "      <td>0.584596</td>\n",
       "      <td>0.584596</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.543565</td>\n",
       "      <td>0.555191</td>\n",
       "      <td>0.541609</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.534990</td>\n",
       "      <td>0.534323</td>\n",
       "      <td>0.536061</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.449327</td>\n",
       "      <td>0.482874</td>\n",
       "      <td>0.474775</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.492445</td>\n",
       "      <td>0.499074</td>\n",
       "      <td>0.499393</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.548718</td>\n",
       "      <td>0.593496</td>\n",
       "      <td>0.544402</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.557492</td>\n",
       "      <td>0.627711</td>\n",
       "      <td>0.551158</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.557492</td>\n",
       "      <td>0.627711</td>\n",
       "      <td>0.551158</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.585737</td>\n",
       "      <td>0.648810</td>\n",
       "      <td>0.578914</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.623397</td>\n",
       "      <td>0.704762</td>\n",
       "      <td>0.608586</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.653437</td>\n",
       "      <td>0.836617</td>\n",
       "      <td>0.629419</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.650831</td>\n",
       "      <td>0.727820</td>\n",
       "      <td>0.631313</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.460923</td>\n",
       "      <td>0.483339</td>\n",
       "      <td>0.496012</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.474247</td>\n",
       "      <td>0.508959</td>\n",
       "      <td>0.502514</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.459236</td>\n",
       "      <td>0.475333</td>\n",
       "      <td>0.493585</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.459236</td>\n",
       "      <td>0.475333</td>\n",
       "      <td>0.493585</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.532819</td>\n",
       "      <td>0.532819</td>\n",
       "      <td>0.532819</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.261268</td>\n",
       "      <td>0.475342</td>\n",
       "      <td>0.473938</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.517808</td>\n",
       "      <td>0.517361</td>\n",
       "      <td>0.519305</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.566502</td>\n",
       "      <td>0.564840</td>\n",
       "      <td>0.568533</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.634810</td>\n",
       "      <td>0.632884</td>\n",
       "      <td>0.636995</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.382699</td>\n",
       "      <td>0.517262</td>\n",
       "      <td>0.518308</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.508768</td>\n",
       "      <td>0.581555</td>\n",
       "      <td>0.608586</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.634810</td>\n",
       "      <td>0.632884</td>\n",
       "      <td>0.636995</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.489141</td>\n",
       "      <td>0.489034</td>\n",
       "      <td>0.489771</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.362363</td>\n",
       "      <td>0.559660</td>\n",
       "      <td>0.555652</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.420215</td>\n",
       "      <td>0.507715</td>\n",
       "      <td>0.510749</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.475357</td>\n",
       "      <td>0.475524</td>\n",
       "      <td>0.475208</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.480519</td>\n",
       "      <td>0.481250</td>\n",
       "      <td>0.488417</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.492308</td>\n",
       "      <td>0.504065</td>\n",
       "      <td>0.501931</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.532468</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.530888</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.517544</td>\n",
       "      <td>0.523077</td>\n",
       "      <td>0.517375</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.585737</td>\n",
       "      <td>0.648810</td>\n",
       "      <td>0.578914</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.622321</td>\n",
       "      <td>0.820076</td>\n",
       "      <td>0.606692</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.661058</td>\n",
       "      <td>0.760714</td>\n",
       "      <td>0.638258</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.650831</td>\n",
       "      <td>0.727820</td>\n",
       "      <td>0.631313</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.542680</td>\n",
       "      <td>0.588761</td>\n",
       "      <td>0.543949</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.572185</td>\n",
       "      <td>0.634629</td>\n",
       "      <td>0.566661</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.520422</td>\n",
       "      <td>0.546020</td>\n",
       "      <td>0.525312</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.540965</td>\n",
       "      <td>0.611761</td>\n",
       "      <td>0.544730</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.472973</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.479730</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.472973</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.540399</td>\n",
       "      <td>0.568783</td>\n",
       "      <td>0.537645</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.585737</td>\n",
       "      <td>0.648810</td>\n",
       "      <td>0.578914</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.623397</td>\n",
       "      <td>0.704762</td>\n",
       "      <td>0.608586</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.661058</td>\n",
       "      <td>0.760714</td>\n",
       "      <td>0.638258</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.633039</td>\n",
       "      <td>0.739216</td>\n",
       "      <td>0.615530</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.446757</td>\n",
       "      <td>0.446860</td>\n",
       "      <td>0.489511</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.462617</td>\n",
       "      <td>0.492857</td>\n",
       "      <td>0.498440</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.486802</td>\n",
       "      <td>0.528070</td>\n",
       "      <td>0.509015</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.462617</td>\n",
       "      <td>0.492857</td>\n",
       "      <td>0.498440</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.479730</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.580357</td>\n",
       "      <td>0.731061</td>\n",
       "      <td>0.577020</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.589142</td>\n",
       "      <td>0.798876</td>\n",
       "      <td>0.583965</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.603376</td>\n",
       "      <td>0.713663</td>\n",
       "      <td>0.592803</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.612665</td>\n",
       "      <td>0.759442</td>\n",
       "      <td>0.599747</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.549164</td>\n",
       "      <td>0.652439</td>\n",
       "      <td>0.552011</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.566112</td>\n",
       "      <td>0.642660</td>\n",
       "      <td>0.562587</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.590101</td>\n",
       "      <td>0.668819</td>\n",
       "      <td>0.580444</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.554814</td>\n",
       "      <td>0.688940</td>\n",
       "      <td>0.556865</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                agg  \\\n",
       "0                                                                                                                              mean   \n",
       "1                                                                                                                               max   \n",
       "2                                                                                                                               min   \n",
       "3                                                                                                                     weighted_mean   \n",
       "4                                                                                                                              mean   \n",
       "5                                                                                                                               max   \n",
       "6                                                                                                                               min   \n",
       "7                                                                                                                     weighted_mean   \n",
       "8                                                                                                                              mean   \n",
       "9                                                                                                                               max   \n",
       "10                                                                                                                              min   \n",
       "11                                                                                                                    weighted_mean   \n",
       "12                                                                                                                             mean   \n",
       "13                                                                                                                              max   \n",
       "14                                                                                                                              min   \n",
       "15                                                                                                                    weighted_mean   \n",
       "16                                                                                                                             mean   \n",
       "17                                                                                                                              max   \n",
       "18                                                                                                                              min   \n",
       "19                                                                                                                    weighted_mean   \n",
       "20                                                                                                                             mean   \n",
       "21                                                                                                                              max   \n",
       "22                                                                                                                              min   \n",
       "23                                                                                                                    weighted_mean   \n",
       "24                                                                                                                             mean   \n",
       "25                                                                                                                              max   \n",
       "26                                                                                                                              min   \n",
       "27                                                                                                                    weighted_mean   \n",
       "28                                                                                                                             mean   \n",
       "29                                                                                                                              max   \n",
       "30                                                                                                                              min   \n",
       "31                                                                                                                    weighted_mean   \n",
       "32                                                                                                                             mean   \n",
       "33                                                                                                                              max   \n",
       "34                                                                                                                              min   \n",
       "35                                                                                                                    weighted_mean   \n",
       "36                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "37                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "38                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "39                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "40                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "41                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "42                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "43                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "44                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "45                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "46                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "47                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "48                                                                                                                             mean   \n",
       "49                                                                                                                              max   \n",
       "50                                                                                                                              min   \n",
       "51                                                                                                                    weighted_mean   \n",
       "52                                                                                                                             mean   \n",
       "53                                                                                                                              max   \n",
       "54                                                                                                                              min   \n",
       "55                                                                                                                    weighted_mean   \n",
       "56                                                                                                                             mean   \n",
       "57                                                                                                                              max   \n",
       "58                                                                                                                              min   \n",
       "59                                                                                                                    weighted_mean   \n",
       "60                                                                                                                             mean   \n",
       "61                                                                                                                              max   \n",
       "62                                                                                                                              min   \n",
       "63                                                                                                                    weighted_mean   \n",
       "64                                                                                                                             mean   \n",
       "65                                                                                                                              max   \n",
       "66                                                                                                                              min   \n",
       "67                                                                                                                    weighted_mean   \n",
       "68                                                                                                                             mean   \n",
       "69                                                                                                                              max   \n",
       "70                                                                                                                              min   \n",
       "71                                                                                                                    weighted_mean   \n",
       "72                                                                                                                             mean   \n",
       "73                                                                                                                              max   \n",
       "74                                                                                                                              min   \n",
       "75                                                                                                                    weighted_mean   \n",
       "76                                                                                                                             mean   \n",
       "77                                                                                                                              max   \n",
       "78                                                                                                                              min   \n",
       "79                                                                                                                    weighted_mean   \n",
       "80                                                                                                                             mean   \n",
       "81                                                                                                                              max   \n",
       "82                                                                                                                              min   \n",
       "83                                                                                                                    weighted_mean   \n",
       "84                                                                                                                             mean   \n",
       "85                                                                                                                              max   \n",
       "86                                                                                                                              min   \n",
       "87                                                                                                                    weighted_mean   \n",
       "88                                                                                                                             mean   \n",
       "89                                                                                                                              max   \n",
       "90                                                                                                                              min   \n",
       "91                                                                                                                    weighted_mean   \n",
       "92                                                                                                                             mean   \n",
       "93                                                                                                                              max   \n",
       "94                                                                                                                              min   \n",
       "95                                                                                                                    weighted_mean   \n",
       "96                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "97                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "98                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "99                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "100                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "101                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "102                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "103                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "104                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "105                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "106                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "107                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "108                                                                                                                            mean   \n",
       "109                                                                                                                             max   \n",
       "110                                                                                                                             min   \n",
       "111                                                                                                                   weighted_mean   \n",
       "112                                                                                                                            mean   \n",
       "113                                                                                                                             max   \n",
       "114                                                                                                                             min   \n",
       "115                                                                                                                   weighted_mean   \n",
       "116                                                                                                                            mean   \n",
       "117                                                                                                                             max   \n",
       "118                                                                                                                             min   \n",
       "119                                                                                                                   weighted_mean   \n",
       "120  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "121            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "122            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "123        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "124  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "125            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "126            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "127        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "128  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "129            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "130            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "131        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "132                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "133                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "134                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "135                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "136                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "137                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "138                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "139                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "140                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "141                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "142                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "143                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "144                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "145                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "146                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "147                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "148                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "149                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "150                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "151                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "152                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "153                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "154                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "155                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "\n",
       "       dataset        f1      prec       rec                        zc  \n",
       "0    Wikipedia  0.456790  0.420455  0.500000                linguistic  \n",
       "1    Wikipedia  0.456790  0.420455  0.500000                linguistic  \n",
       "2    Wikipedia  0.450000  0.418605  0.486486                linguistic  \n",
       "3    Wikipedia  0.456790  0.420455  0.500000                linguistic  \n",
       "4     WikiNews  0.545894  0.693258  0.554293                linguistic  \n",
       "5     WikiNews  0.545894  0.693258  0.554293                linguistic  \n",
       "6     WikiNews  0.502646  0.587640  0.524621                linguistic  \n",
       "7     WikiNews  0.538393  0.642045  0.547348                linguistic  \n",
       "8         News  0.466033  0.518701  0.503294                linguistic  \n",
       "9         News  0.481884  0.561924  0.512223                linguistic  \n",
       "10        News  0.501630  0.647638  0.526006                linguistic  \n",
       "11        News  0.434125  0.391051  0.487864                linguistic  \n",
       "12   Wikipedia  0.557991  0.555556  0.561776                 frequency  \n",
       "13   Wikipedia  0.503590  0.504386  0.503861                 frequency  \n",
       "14   Wikipedia  0.510567  0.510771  0.512548                 frequency  \n",
       "15   Wikipedia  0.538934  0.545537  0.572394                 frequency  \n",
       "16    WikiNews  0.570776  0.573649  0.568813                 frequency  \n",
       "17    WikiNews  0.587162  0.595760  0.582702                 frequency  \n",
       "18    WikiNews  0.560284  0.558551  0.563763                 frequency  \n",
       "19    WikiNews  0.596265  0.632004  0.587753                 frequency  \n",
       "20        News  0.633566  0.660630  0.621099                 frequency  \n",
       "21        News  0.570712  0.596878  0.565101                 frequency  \n",
       "22        News  0.597031  0.593167  0.603415                 frequency  \n",
       "23        News  0.604653  0.624662  0.595960                 frequency  \n",
       "24   Wikipedia  0.563692  0.562500  0.592664            language_model  \n",
       "25   Wikipedia  0.538934  0.545537  0.572394            language_model  \n",
       "26   Wikipedia  0.563692  0.562500  0.592664            language_model  \n",
       "27   Wikipedia  0.562189  0.566667  0.608108            language_model  \n",
       "28    WikiNews  0.540117  0.541892  0.539141            language_model  \n",
       "29    WikiNews  0.609750  0.609443  0.642677            language_model  \n",
       "30    WikiNews  0.584596  0.584596  0.584596            language_model  \n",
       "31    WikiNews  0.673759  0.667536  0.682449            language_model  \n",
       "32        News  0.527904  0.541536  0.528606            language_model  \n",
       "33        News  0.561139  0.558854  0.568568            language_model  \n",
       "34        News  0.530377  0.535446  0.529473            language_model  \n",
       "35        News  0.528135  0.536149  0.527826            language_model  \n",
       "36   Wikipedia  0.565789  0.579487  0.559846                    corpus  \n",
       "37   Wikipedia  0.524865  0.535162  0.524131                    corpus  \n",
       "38   Wikipedia  0.541667  0.539683  0.548263                    corpus  \n",
       "39   Wikipedia  0.579861  0.574603  0.590734                    corpus  \n",
       "40    WikiNews  0.530890  0.536669  0.530303                    corpus  \n",
       "41    WikiNews  0.548077  0.592857  0.549242                    corpus  \n",
       "42    WikiNews  0.578895  0.584211  0.575758                    corpus  \n",
       "43    WikiNews  0.578895  0.584211  0.575758                    corpus  \n",
       "44        News  0.610958  0.647575  0.599168                    corpus  \n",
       "45        News  0.587188  0.620418  0.578883                    corpus  \n",
       "46        News  0.601386  0.612442  0.595180                    corpus  \n",
       "47        News  0.579631  0.616604  0.572382                    corpus  \n",
       "48   Wikipedia  0.484848  0.505747  0.509653          psycholinguistic  \n",
       "49   Wikipedia  0.475339  0.492107  0.487452          psycholinguistic  \n",
       "50   Wikipedia  0.522994  0.535652  0.558880          psycholinguistic  \n",
       "51   Wikipedia  0.514706  0.523573  0.536680          psycholinguistic  \n",
       "52    WikiNews  0.559375  0.568609  0.592172          psycholinguistic  \n",
       "53    WikiNews  0.645283  0.638249  0.670455          psycholinguistic  \n",
       "54    WikiNews  0.645283  0.638249  0.670455          psycholinguistic  \n",
       "55    WikiNews  0.601640  0.599851  0.626894          psycholinguistic  \n",
       "56        News  0.572192  0.590461  0.566748          psycholinguistic  \n",
       "57        News  0.506718  0.506677  0.506848          psycholinguistic  \n",
       "58        News  0.490963  0.494274  0.493152          psycholinguistic  \n",
       "59        News  0.535913  0.548649  0.535107          psycholinguistic  \n",
       "60   Wikipedia  0.453416  0.419540  0.493243                   wordnet  \n",
       "61   Wikipedia  0.518750  0.674419  0.528958                   wordnet  \n",
       "62   Wikipedia  0.450000  0.418605  0.486486                   wordnet  \n",
       "63   Wikipedia  0.518750  0.674419  0.528958                   wordnet  \n",
       "64    WikiNews  0.612665  0.759442  0.599747                   wordnet  \n",
       "65    WikiNews  0.603376  0.713663  0.592803                   wordnet  \n",
       "66    WikiNews  0.687586  0.779299  0.660985                   wordnet  \n",
       "67    WikiNews  0.633039  0.739216  0.615530                   wordnet  \n",
       "68        News  0.466033  0.518701  0.503294                   wordnet  \n",
       "69        News  0.443809  0.435893  0.484657                   wordnet  \n",
       "70        News  0.472373  0.500288  0.500087                   wordnet  \n",
       "71        News  0.483832  0.583169  0.514650                   wordnet  \n",
       "72   Wikipedia  0.505062  0.533962  0.560811                   dbpedia  \n",
       "73   Wikipedia  0.505062  0.533962  0.560811                   dbpedia  \n",
       "74   Wikipedia  0.505062  0.533962  0.560811                   dbpedia  \n",
       "75   Wikipedia  0.505062  0.533962  0.560811                   dbpedia  \n",
       "76    WikiNews  0.470098  0.504982  0.506944                   dbpedia  \n",
       "77    WikiNews  0.470098  0.504982  0.506944                   dbpedia  \n",
       "78    WikiNews  0.470098  0.504982  0.506944                   dbpedia  \n",
       "79    WikiNews  0.470098  0.504982  0.506944                   dbpedia  \n",
       "80        News  0.452099  0.457968  0.450156                   dbpedia  \n",
       "81        News  0.452099  0.457968  0.450156                   dbpedia  \n",
       "82        News  0.452099  0.457968  0.450156                   dbpedia  \n",
       "83        News  0.452099  0.457968  0.450156                   dbpedia  \n",
       "84   Wikipedia  0.447214  0.458067  0.443050          brown_clustering  \n",
       "85   Wikipedia  0.496568  0.505208  0.507722          brown_clustering  \n",
       "86   Wikipedia  0.418391  0.492754  0.486486          brown_clustering  \n",
       "87   Wikipedia  0.477626  0.479167  0.476834          brown_clustering  \n",
       "88    WikiNews  0.584301  0.583165  0.604167          brown_clustering  \n",
       "89    WikiNews  0.636754  0.629973  0.654672          brown_clustering  \n",
       "90    WikiNews  0.480902  0.568137  0.587753          brown_clustering  \n",
       "91    WikiNews  0.584596  0.584596  0.584596          brown_clustering  \n",
       "92        News  0.543565  0.555191  0.541609          brown_clustering  \n",
       "93        News  0.534990  0.534323  0.536061          brown_clustering  \n",
       "94        News  0.449327  0.482874  0.474775          brown_clustering  \n",
       "95        News  0.492445  0.499074  0.499393          brown_clustering  \n",
       "96   Wikipedia  0.548718  0.593496  0.544402                  semantic  \n",
       "97   Wikipedia  0.557492  0.627711  0.551158                  semantic  \n",
       "98   Wikipedia  0.450000  0.418605  0.486486                  semantic  \n",
       "99   Wikipedia  0.557492  0.627711  0.551158                  semantic  \n",
       "100   WikiNews  0.585737  0.648810  0.578914                  semantic  \n",
       "101   WikiNews  0.623397  0.704762  0.608586                  semantic  \n",
       "102   WikiNews  0.653437  0.836617  0.629419                  semantic  \n",
       "103   WikiNews  0.650831  0.727820  0.631313                  semantic  \n",
       "104       News  0.460923  0.483339  0.496012                  semantic  \n",
       "105       News  0.474247  0.508959  0.502514                  semantic  \n",
       "106       News  0.459236  0.475333  0.493585                  semantic  \n",
       "107       News  0.459236  0.475333  0.493585                  semantic  \n",
       "108  Wikipedia  0.532819  0.532819  0.532819                dictionary  \n",
       "109  Wikipedia  0.261268  0.475342  0.473938                dictionary  \n",
       "110  Wikipedia  0.517808  0.517361  0.519305                dictionary  \n",
       "111  Wikipedia  0.566502  0.564840  0.568533                dictionary  \n",
       "112   WikiNews  0.634810  0.632884  0.636995                dictionary  \n",
       "113   WikiNews  0.382699  0.517262  0.518308                dictionary  \n",
       "114   WikiNews  0.508768  0.581555  0.608586                dictionary  \n",
       "115   WikiNews  0.634810  0.632884  0.636995                dictionary  \n",
       "116       News  0.489141  0.489034  0.489771                dictionary  \n",
       "117       News  0.362363  0.559660  0.555652                dictionary  \n",
       "118       News  0.420215  0.507715  0.510749                dictionary  \n",
       "119       News  0.475357  0.475524  0.475208                dictionary  \n",
       "120  Wikipedia  0.480519  0.481250  0.488417           corpus+semantic  \n",
       "121  Wikipedia  0.492308  0.504065  0.501931           corpus+semantic  \n",
       "122  Wikipedia  0.532468  0.550000  0.530888           corpus+semantic  \n",
       "123  Wikipedia  0.517544  0.523077  0.517375           corpus+semantic  \n",
       "124   WikiNews  0.585737  0.648810  0.578914           corpus+semantic  \n",
       "125   WikiNews  0.622321  0.820076  0.606692           corpus+semantic  \n",
       "126   WikiNews  0.661058  0.760714  0.638258           corpus+semantic  \n",
       "127   WikiNews  0.650831  0.727820  0.631313           corpus+semantic  \n",
       "128       News  0.542680  0.588761  0.543949           corpus+semantic  \n",
       "129       News  0.572185  0.634629  0.566661           corpus+semantic  \n",
       "130       News  0.520422  0.546020  0.525312           corpus+semantic  \n",
       "131       News  0.540965  0.611761  0.544730           corpus+semantic  \n",
       "132  Wikipedia  0.443038  0.416667  0.472973  wordnet+psycholinguistic  \n",
       "133  Wikipedia  0.446541  0.417647  0.479730  wordnet+psycholinguistic  \n",
       "134  Wikipedia  0.443038  0.416667  0.472973  wordnet+psycholinguistic  \n",
       "135  Wikipedia  0.540399  0.568783  0.537645  wordnet+psycholinguistic  \n",
       "136   WikiNews  0.585737  0.648810  0.578914  wordnet+psycholinguistic  \n",
       "137   WikiNews  0.623397  0.704762  0.608586  wordnet+psycholinguistic  \n",
       "138   WikiNews  0.661058  0.760714  0.638258  wordnet+psycholinguistic  \n",
       "139   WikiNews  0.633039  0.739216  0.615530  wordnet+psycholinguistic  \n",
       "140       News  0.446757  0.446860  0.489511  wordnet+psycholinguistic  \n",
       "141       News  0.462617  0.492857  0.498440  wordnet+psycholinguistic  \n",
       "142       News  0.486802  0.528070  0.509015  wordnet+psycholinguistic  \n",
       "143       News  0.462617  0.492857  0.498440  wordnet+psycholinguistic  \n",
       "144  Wikipedia  0.453416  0.419540  0.493243                       all  \n",
       "145  Wikipedia  0.453416  0.419540  0.493243                       all  \n",
       "146  Wikipedia  0.450000  0.418605  0.486486                       all  \n",
       "147  Wikipedia  0.446541  0.417647  0.479730                       all  \n",
       "148   WikiNews  0.580357  0.731061  0.577020                       all  \n",
       "149   WikiNews  0.589142  0.798876  0.583965                       all  \n",
       "150   WikiNews  0.603376  0.713663  0.592803                       all  \n",
       "151   WikiNews  0.612665  0.759442  0.599747                       all  \n",
       "152       News  0.549164  0.652439  0.552011                       all  \n",
       "153       News  0.566112  0.642660  0.562587                       all  \n",
       "154       News  0.590101  0.668819  0.580444                       all  \n",
       "155       News  0.554814  0.688940  0.556865                       all  "
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_eval_data_rfe_dswp = create_eval_df_from_results_macro(results, False)\n",
    "feature_eval_data_rfe_dswp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.633566</td>\n",
       "      <td>0.660630</td>\n",
       "      <td>0.621099</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.579861</td>\n",
       "      <td>0.574603</td>\n",
       "      <td>0.590734</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.687586</td>\n",
       "      <td>0.779299</td>\n",
       "      <td>0.660985</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           agg    dataset  \\\n",
       "20                                                        mean       News   \n",
       "39  (weighted_mean, <function <lambda> at 0x000000FF52232E18>)  Wikipedia   \n",
       "66                                                         min   WikiNews   \n",
       "\n",
       "          f1      prec       rec         zc  \n",
       "20  0.633566  0.660630  0.621099  frequency  \n",
       "39  0.579861  0.574603  0.590734     corpus  \n",
       "66  0.687586  0.779299  0.660985    wordnet  "
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = feature_eval_data_rfe_dswp.groupby(['dataset'])['f1'].transform(max) == feature_eval_data_rfe_dswp['f1']\n",
    "feature_eval_data_rfe_dswp[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.2.4 AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "results = [Result(fs, fs.fc, fs.agg,\n",
    "        precision_recall_fscore_support(*adaboost(*transform_feat_to_num(remove_labels_for_binary_df(fs.train), \n",
    "        remove_labels_for_binary_df(fs.test))), average='macro')) for fs in all_fc_datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.447876</td>\n",
       "      <td>0.447876</td>\n",
       "      <td>0.447876</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.503590</td>\n",
       "      <td>0.504386</td>\n",
       "      <td>0.503861</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453144</td>\n",
       "      <td>0.451795</td>\n",
       "      <td>0.454633</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.447876</td>\n",
       "      <td>0.447876</td>\n",
       "      <td>0.447876</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.539498</td>\n",
       "      <td>0.538690</td>\n",
       "      <td>0.541035</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.570776</td>\n",
       "      <td>0.573649</td>\n",
       "      <td>0.568813</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.578895</td>\n",
       "      <td>0.584211</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.693412</td>\n",
       "      <td>0.700676</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.624373</td>\n",
       "      <td>0.664474</td>\n",
       "      <td>0.610524</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.604653</td>\n",
       "      <td>0.624662</td>\n",
       "      <td>0.595960</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.570712</td>\n",
       "      <td>0.596878</td>\n",
       "      <td>0.565101</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.576856</td>\n",
       "      <td>0.585612</td>\n",
       "      <td>0.572469</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.557086</td>\n",
       "      <td>0.564935</td>\n",
       "      <td>0.553089</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.549722</td>\n",
       "      <td>0.547225</td>\n",
       "      <td>0.555019</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.483932</td>\n",
       "      <td>0.484475</td>\n",
       "      <td>0.483591</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.478764</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.517949</td>\n",
       "      <td>0.517759</td>\n",
       "      <td>0.518308</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.503812</td>\n",
       "      <td>0.521906</td>\n",
       "      <td>0.512626</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.489355</td>\n",
       "      <td>0.491708</td>\n",
       "      <td>0.490530</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.562791</td>\n",
       "      <td>0.563927</td>\n",
       "      <td>0.561869</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.617653</td>\n",
       "      <td>0.649828</td>\n",
       "      <td>0.605669</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.616117</td>\n",
       "      <td>0.624221</td>\n",
       "      <td>0.610610</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.525362</td>\n",
       "      <td>0.537119</td>\n",
       "      <td>0.526179</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.617407</td>\n",
       "      <td>0.639414</td>\n",
       "      <td>0.607316</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.510464</td>\n",
       "      <td>0.512987</td>\n",
       "      <td>0.510618</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.539683</td>\n",
       "      <td>0.548263</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.548718</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.546332</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.557991</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.561776</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.576379</td>\n",
       "      <td>0.575321</td>\n",
       "      <td>0.577652</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.517949</td>\n",
       "      <td>0.517759</td>\n",
       "      <td>0.518308</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.553363</td>\n",
       "      <td>0.572321</td>\n",
       "      <td>0.551136</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.478800</td>\n",
       "      <td>0.478378</td>\n",
       "      <td>0.479798</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.631255</td>\n",
       "      <td>0.680842</td>\n",
       "      <td>0.615378</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.631255</td>\n",
       "      <td>0.680842</td>\n",
       "      <td>0.615378</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.585743</td>\n",
       "      <td>0.631863</td>\n",
       "      <td>0.577236</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.599786</td>\n",
       "      <td>0.651251</td>\n",
       "      <td>0.588592</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.510464</td>\n",
       "      <td>0.512987</td>\n",
       "      <td>0.510618</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.532819</td>\n",
       "      <td>0.532819</td>\n",
       "      <td>0.532819</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.517808</td>\n",
       "      <td>0.517361</td>\n",
       "      <td>0.519305</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.580952</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>0.606178</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.547702</td>\n",
       "      <td>0.551228</td>\n",
       "      <td>0.546086</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.484649</td>\n",
       "      <td>0.488393</td>\n",
       "      <td>0.491793</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.498537</td>\n",
       "      <td>0.500764</td>\n",
       "      <td>0.500631</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.538246</td>\n",
       "      <td>0.547276</td>\n",
       "      <td>0.537247</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.620993</td>\n",
       "      <td>0.656954</td>\n",
       "      <td>0.608096</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.614138</td>\n",
       "      <td>0.633539</td>\n",
       "      <td>0.604889</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.616887</td>\n",
       "      <td>0.631047</td>\n",
       "      <td>0.608963</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.617407</td>\n",
       "      <td>0.639414</td>\n",
       "      <td>0.607316</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.503590</td>\n",
       "      <td>0.504386</td>\n",
       "      <td>0.503861</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.480519</td>\n",
       "      <td>0.481250</td>\n",
       "      <td>0.488417</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.540399</td>\n",
       "      <td>0.568783</td>\n",
       "      <td>0.537645</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.584390</td>\n",
       "      <td>0.587179</td>\n",
       "      <td>0.582046</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.579381</td>\n",
       "      <td>0.598734</td>\n",
       "      <td>0.573864</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.526050</td>\n",
       "      <td>0.542735</td>\n",
       "      <td>0.528409</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.604211</td>\n",
       "      <td>0.622596</td>\n",
       "      <td>0.596591</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.484649</td>\n",
       "      <td>0.488393</td>\n",
       "      <td>0.491793</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.567799</td>\n",
       "      <td>0.591001</td>\n",
       "      <td>0.562673</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.593414</td>\n",
       "      <td>0.634897</td>\n",
       "      <td>0.583738</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.563369</td>\n",
       "      <td>0.607959</td>\n",
       "      <td>0.559379</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.537358</td>\n",
       "      <td>0.573501</td>\n",
       "      <td>0.539095</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.566502</td>\n",
       "      <td>0.564840</td>\n",
       "      <td>0.568533</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.571096</td>\n",
       "      <td>0.566362</td>\n",
       "      <td>0.583977</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.548718</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.546332</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.549722</td>\n",
       "      <td>0.547225</td>\n",
       "      <td>0.555019</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.623095</td>\n",
       "      <td>0.625245</td>\n",
       "      <td>0.621212</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.650676</td>\n",
       "      <td>0.664474</td>\n",
       "      <td>0.642045</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.485316</td>\n",
       "      <td>0.485263</td>\n",
       "      <td>0.486742</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.530890</td>\n",
       "      <td>0.536669</td>\n",
       "      <td>0.530303</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.607695</td>\n",
       "      <td>0.640466</td>\n",
       "      <td>0.596741</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.546264</td>\n",
       "      <td>0.559680</td>\n",
       "      <td>0.544036</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.528135</td>\n",
       "      <td>0.536149</td>\n",
       "      <td>0.527826</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.584127</td>\n",
       "      <td>0.613777</td>\n",
       "      <td>0.576456</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.562552</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.577220</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.580952</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>0.606178</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.483932</td>\n",
       "      <td>0.484475</td>\n",
       "      <td>0.483591</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.465278</td>\n",
       "      <td>0.469841</td>\n",
       "      <td>0.463320</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.650831</td>\n",
       "      <td>0.727820</td>\n",
       "      <td>0.631313</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.631373</td>\n",
       "      <td>0.676638</td>\n",
       "      <td>0.617424</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.612665</td>\n",
       "      <td>0.759442</td>\n",
       "      <td>0.599747</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.622321</td>\n",
       "      <td>0.820076</td>\n",
       "      <td>0.606692</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.505349</td>\n",
       "      <td>0.575749</td>\n",
       "      <td>0.522798</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.527928</td>\n",
       "      <td>0.565826</td>\n",
       "      <td>0.532594</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.493034</td>\n",
       "      <td>0.562667</td>\n",
       "      <td>0.516297</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.512192</td>\n",
       "      <td>0.564208</td>\n",
       "      <td>0.524445</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.517544</td>\n",
       "      <td>0.523077</td>\n",
       "      <td>0.517375</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.498491</td>\n",
       "      <td>0.521687</td>\n",
       "      <td>0.508687</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.614035</td>\n",
       "      <td>0.635897</td>\n",
       "      <td>0.602317</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.469298</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.474903</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.519006</td>\n",
       "      <td>0.530357</td>\n",
       "      <td>0.521465</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.540567</td>\n",
       "      <td>0.573384</td>\n",
       "      <td>0.542298</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.563713</td>\n",
       "      <td>0.645349</td>\n",
       "      <td>0.563131</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.461421</td>\n",
       "      <td>0.461382</td>\n",
       "      <td>0.476010</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.535655</td>\n",
       "      <td>0.556250</td>\n",
       "      <td>0.535888</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.513176</td>\n",
       "      <td>0.542892</td>\n",
       "      <td>0.521238</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.488861</td>\n",
       "      <td>0.538018</td>\n",
       "      <td>0.511442</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.525402</td>\n",
       "      <td>0.558734</td>\n",
       "      <td>0.530166</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.588246</td>\n",
       "      <td>0.584444</td>\n",
       "      <td>0.628378</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.562552</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.577220</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.539683</td>\n",
       "      <td>0.548263</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.546032</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.563707</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.587719</td>\n",
       "      <td>0.614286</td>\n",
       "      <td>0.580808</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.640947</td>\n",
       "      <td>0.700203</td>\n",
       "      <td>0.624369</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.613030</td>\n",
       "      <td>0.638397</td>\n",
       "      <td>0.603535</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.519006</td>\n",
       "      <td>0.530357</td>\n",
       "      <td>0.521465</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.601277</td>\n",
       "      <td>0.627327</td>\n",
       "      <td>0.591886</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.571017</td>\n",
       "      <td>0.577190</td>\n",
       "      <td>0.567614</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.593414</td>\n",
       "      <td>0.634897</td>\n",
       "      <td>0.583738</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.573249</td>\n",
       "      <td>0.585390</td>\n",
       "      <td>0.568395</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.532468</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.530888</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.472756</td>\n",
       "      <td>0.480952</td>\n",
       "      <td>0.489899</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.510417</td>\n",
       "      <td>0.536905</td>\n",
       "      <td>0.519571</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.552137</td>\n",
       "      <td>0.792051</td>\n",
       "      <td>0.557646</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.599859</td>\n",
       "      <td>0.709091</td>\n",
       "      <td>0.587725</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.483932</td>\n",
       "      <td>0.484475</td>\n",
       "      <td>0.483591</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.572234</td>\n",
       "      <td>0.568896</td>\n",
       "      <td>0.599421</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.565789</td>\n",
       "      <td>0.579487</td>\n",
       "      <td>0.559846</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.593846</td>\n",
       "      <td>0.600877</td>\n",
       "      <td>0.588803</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.585737</td>\n",
       "      <td>0.648810</td>\n",
       "      <td>0.578914</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.579381</td>\n",
       "      <td>0.598734</td>\n",
       "      <td>0.573864</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.656433</td>\n",
       "      <td>0.698214</td>\n",
       "      <td>0.640152</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.497326</td>\n",
       "      <td>0.509146</td>\n",
       "      <td>0.505682</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.591284</td>\n",
       "      <td>0.617625</td>\n",
       "      <td>0.582958</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.577179</td>\n",
       "      <td>0.581920</td>\n",
       "      <td>0.574116</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.563534</td>\n",
       "      <td>0.575709</td>\n",
       "      <td>0.559466</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.607852</td>\n",
       "      <td>0.630505</td>\n",
       "      <td>0.598388</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.618056</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>0.633205</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.526098</td>\n",
       "      <td>0.526471</td>\n",
       "      <td>0.534749</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.510464</td>\n",
       "      <td>0.512987</td>\n",
       "      <td>0.510618</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.566502</td>\n",
       "      <td>0.564840</td>\n",
       "      <td>0.568533</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.570776</td>\n",
       "      <td>0.573649</td>\n",
       "      <td>0.568813</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.627947</td>\n",
       "      <td>0.644385</td>\n",
       "      <td>0.619318</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.532638</td>\n",
       "      <td>0.533268</td>\n",
       "      <td>0.532197</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.523649</td>\n",
       "      <td>0.527047</td>\n",
       "      <td>0.523359</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.595247</td>\n",
       "      <td>0.608633</td>\n",
       "      <td>0.588679</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.576185</td>\n",
       "      <td>0.590167</td>\n",
       "      <td>0.570822</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.538231</td>\n",
       "      <td>0.546794</td>\n",
       "      <td>0.536755</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.562059</td>\n",
       "      <td>0.580120</td>\n",
       "      <td>0.557819</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.503590</td>\n",
       "      <td>0.504386</td>\n",
       "      <td>0.503861</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.598174</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.604247</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.548718</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.546332</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.548718</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.546332</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.618919</td>\n",
       "      <td>0.630117</td>\n",
       "      <td>0.612374</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.578895</td>\n",
       "      <td>0.584211</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.571228</td>\n",
       "      <td>0.584936</td>\n",
       "      <td>0.566919</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.618919</td>\n",
       "      <td>0.630117</td>\n",
       "      <td>0.612374</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.677394</td>\n",
       "      <td>0.705827</td>\n",
       "      <td>0.661668</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.651375</td>\n",
       "      <td>0.652394</td>\n",
       "      <td>0.650399</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.607269</td>\n",
       "      <td>0.616081</td>\n",
       "      <td>0.601682</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.601386</td>\n",
       "      <td>0.612442</td>\n",
       "      <td>0.595180</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                agg  \\\n",
       "0                                                                                                                              mean   \n",
       "1                                                                                                                               max   \n",
       "2                                                                                                                               min   \n",
       "3                                                                                                                     weighted_mean   \n",
       "4                                                                                                                              mean   \n",
       "5                                                                                                                               max   \n",
       "6                                                                                                                               min   \n",
       "7                                                                                                                     weighted_mean   \n",
       "8                                                                                                                              mean   \n",
       "9                                                                                                                               max   \n",
       "10                                                                                                                              min   \n",
       "11                                                                                                                    weighted_mean   \n",
       "12                                                                                                                             mean   \n",
       "13                                                                                                                              max   \n",
       "14                                                                                                                              min   \n",
       "15                                                                                                                    weighted_mean   \n",
       "16                                                                                                                             mean   \n",
       "17                                                                                                                              max   \n",
       "18                                                                                                                              min   \n",
       "19                                                                                                                    weighted_mean   \n",
       "20                                                                                                                             mean   \n",
       "21                                                                                                                              max   \n",
       "22                                                                                                                              min   \n",
       "23                                                                                                                    weighted_mean   \n",
       "24                                                                                                                             mean   \n",
       "25                                                                                                                              max   \n",
       "26                                                                                                                              min   \n",
       "27                                                                                                                    weighted_mean   \n",
       "28                                                                                                                             mean   \n",
       "29                                                                                                                              max   \n",
       "30                                                                                                                              min   \n",
       "31                                                                                                                    weighted_mean   \n",
       "32                                                                                                                             mean   \n",
       "33                                                                                                                              max   \n",
       "34                                                                                                                              min   \n",
       "35                                                                                                                    weighted_mean   \n",
       "36                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "37                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "38                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "39                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "40                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "41                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "42                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "43                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "44                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "45                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "46                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "47                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "48                                                                                                                             mean   \n",
       "49                                                                                                                              max   \n",
       "50                                                                                                                              min   \n",
       "51                                                                                                                    weighted_mean   \n",
       "52                                                                                                                             mean   \n",
       "53                                                                                                                              max   \n",
       "54                                                                                                                              min   \n",
       "55                                                                                                                    weighted_mean   \n",
       "56                                                                                                                             mean   \n",
       "57                                                                                                                              max   \n",
       "58                                                                                                                              min   \n",
       "59                                                                                                                    weighted_mean   \n",
       "60                                                                                                                             mean   \n",
       "61                                                                                                                              max   \n",
       "62                                                                                                                              min   \n",
       "63                                                                                                                    weighted_mean   \n",
       "64                                                                                                                             mean   \n",
       "65                                                                                                                              max   \n",
       "66                                                                                                                              min   \n",
       "67                                                                                                                    weighted_mean   \n",
       "68                                                                                                                             mean   \n",
       "69                                                                                                                              max   \n",
       "70                                                                                                                              min   \n",
       "71                                                                                                                    weighted_mean   \n",
       "72                                                                                                                             mean   \n",
       "73                                                                                                                              max   \n",
       "74                                                                                                                              min   \n",
       "75                                                                                                                    weighted_mean   \n",
       "76                                                                                                                             mean   \n",
       "77                                                                                                                              max   \n",
       "78                                                                                                                              min   \n",
       "79                                                                                                                    weighted_mean   \n",
       "80                                                                                                                             mean   \n",
       "81                                                                                                                              max   \n",
       "82                                                                                                                              min   \n",
       "83                                                                                                                    weighted_mean   \n",
       "84                                                                                                                             mean   \n",
       "85                                                                                                                              max   \n",
       "86                                                                                                                              min   \n",
       "87                                                                                                                    weighted_mean   \n",
       "88                                                                                                                             mean   \n",
       "89                                                                                                                              max   \n",
       "90                                                                                                                              min   \n",
       "91                                                                                                                    weighted_mean   \n",
       "92                                                                                                                             mean   \n",
       "93                                                                                                                              max   \n",
       "94                                                                                                                              min   \n",
       "95                                                                                                                    weighted_mean   \n",
       "96                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "97                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "98                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "99                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "100                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "101                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "102                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "103                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "104                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "105                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "106                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "107                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "108                                                                                                                            mean   \n",
       "109                                                                                                                             max   \n",
       "110                                                                                                                             min   \n",
       "111                                                                                                                   weighted_mean   \n",
       "112                                                                                                                            mean   \n",
       "113                                                                                                                             max   \n",
       "114                                                                                                                             min   \n",
       "115                                                                                                                   weighted_mean   \n",
       "116                                                                                                                            mean   \n",
       "117                                                                                                                             max   \n",
       "118                                                                                                                             min   \n",
       "119                                                                                                                   weighted_mean   \n",
       "120  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "121            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "122            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "123        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "124  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "125            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "126            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "127        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "128  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "129            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "130            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "131        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "132                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "133                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "134                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "135                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "136                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "137                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "138                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "139                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "140                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "141                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "142                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "143                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "144                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "145                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "146                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "147                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "148                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "149                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "150                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "151                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "152                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "153                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "154                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "155                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "\n",
       "       dataset        f1      prec       rec                        zc  \n",
       "0    Wikipedia  0.447876  0.447876  0.447876                linguistic  \n",
       "1    Wikipedia  0.503590  0.504386  0.503861                linguistic  \n",
       "2    Wikipedia  0.453144  0.451795  0.454633                linguistic  \n",
       "3    Wikipedia  0.447876  0.447876  0.447876                linguistic  \n",
       "4     WikiNews  0.539498  0.538690  0.541035                linguistic  \n",
       "5     WikiNews  0.570776  0.573649  0.568813                linguistic  \n",
       "6     WikiNews  0.578895  0.584211  0.575758                linguistic  \n",
       "7     WikiNews  0.693412  0.700676  0.687500                linguistic  \n",
       "8         News  0.624373  0.664474  0.610524                linguistic  \n",
       "9         News  0.604653  0.624662  0.595960                linguistic  \n",
       "10        News  0.570712  0.596878  0.565101                linguistic  \n",
       "11        News  0.576856  0.585612  0.572469                linguistic  \n",
       "12   Wikipedia  0.557086  0.564935  0.553089                 frequency  \n",
       "13   Wikipedia  0.549722  0.547225  0.555019                 frequency  \n",
       "14   Wikipedia  0.483932  0.484475  0.483591                 frequency  \n",
       "15   Wikipedia  0.476190  0.484848  0.478764                 frequency  \n",
       "16    WikiNews  0.517949  0.517759  0.518308                 frequency  \n",
       "17    WikiNews  0.503812  0.521906  0.512626                 frequency  \n",
       "18    WikiNews  0.489355  0.491708  0.490530                 frequency  \n",
       "19    WikiNews  0.562791  0.563927  0.561869                 frequency  \n",
       "20        News  0.617653  0.649828  0.605669                 frequency  \n",
       "21        News  0.616117  0.624221  0.610610                 frequency  \n",
       "22        News  0.525362  0.537119  0.526179                 frequency  \n",
       "23        News  0.617407  0.639414  0.607316                 frequency  \n",
       "24   Wikipedia  0.510464  0.512987  0.510618            language_model  \n",
       "25   Wikipedia  0.541667  0.539683  0.548263            language_model  \n",
       "26   Wikipedia  0.548718  0.552632  0.546332            language_model  \n",
       "27   Wikipedia  0.557991  0.555556  0.561776            language_model  \n",
       "28    WikiNews  0.576379  0.575321  0.577652            language_model  \n",
       "29    WikiNews  0.517949  0.517759  0.518308            language_model  \n",
       "30    WikiNews  0.553363  0.572321  0.551136            language_model  \n",
       "31    WikiNews  0.478800  0.478378  0.479798            language_model  \n",
       "32        News  0.631255  0.680842  0.615378            language_model  \n",
       "33        News  0.631255  0.680842  0.615378            language_model  \n",
       "34        News  0.585743  0.631863  0.577236            language_model  \n",
       "35        News  0.599786  0.651251  0.588592            language_model  \n",
       "36   Wikipedia  0.510464  0.512987  0.510618                    corpus  \n",
       "37   Wikipedia  0.532819  0.532819  0.532819                    corpus  \n",
       "38   Wikipedia  0.517808  0.517361  0.519305                    corpus  \n",
       "39   Wikipedia  0.580952  0.575758  0.606178                    corpus  \n",
       "40    WikiNews  0.547702  0.551228  0.546086                    corpus  \n",
       "41    WikiNews  0.484649  0.488393  0.491793                    corpus  \n",
       "42    WikiNews  0.498537  0.500764  0.500631                    corpus  \n",
       "43    WikiNews  0.538246  0.547276  0.537247                    corpus  \n",
       "44        News  0.620993  0.656954  0.608096                    corpus  \n",
       "45        News  0.614138  0.633539  0.604889                    corpus  \n",
       "46        News  0.616887  0.631047  0.608963                    corpus  \n",
       "47        News  0.617407  0.639414  0.607316                    corpus  \n",
       "48   Wikipedia  0.503590  0.504386  0.503861          psycholinguistic  \n",
       "49   Wikipedia  0.480519  0.481250  0.488417          psycholinguistic  \n",
       "50   Wikipedia  0.540399  0.568783  0.537645          psycholinguistic  \n",
       "51   Wikipedia  0.584390  0.587179  0.582046          psycholinguistic  \n",
       "52    WikiNews  0.579381  0.598734  0.573864          psycholinguistic  \n",
       "53    WikiNews  0.526050  0.542735  0.528409          psycholinguistic  \n",
       "54    WikiNews  0.604211  0.622596  0.596591          psycholinguistic  \n",
       "55    WikiNews  0.484649  0.488393  0.491793          psycholinguistic  \n",
       "56        News  0.567799  0.591001  0.562673          psycholinguistic  \n",
       "57        News  0.593414  0.634897  0.583738          psycholinguistic  \n",
       "58        News  0.563369  0.607959  0.559379          psycholinguistic  \n",
       "59        News  0.537358  0.573501  0.539095          psycholinguistic  \n",
       "60   Wikipedia  0.566502  0.564840  0.568533                   wordnet  \n",
       "61   Wikipedia  0.571096  0.566362  0.583977                   wordnet  \n",
       "62   Wikipedia  0.548718  0.552632  0.546332                   wordnet  \n",
       "63   Wikipedia  0.549722  0.547225  0.555019                   wordnet  \n",
       "64    WikiNews  0.623095  0.625245  0.621212                   wordnet  \n",
       "65    WikiNews  0.650676  0.664474  0.642045                   wordnet  \n",
       "66    WikiNews  0.485316  0.485263  0.486742                   wordnet  \n",
       "67    WikiNews  0.530890  0.536669  0.530303                   wordnet  \n",
       "68        News  0.607695  0.640466  0.596741                   wordnet  \n",
       "69        News  0.546264  0.559680  0.544036                   wordnet  \n",
       "70        News  0.528135  0.536149  0.527826                   wordnet  \n",
       "71        News  0.584127  0.613777  0.576456                   wordnet  \n",
       "72   Wikipedia  0.562552  0.558824  0.577220                   dbpedia  \n",
       "73   Wikipedia  0.580952  0.575758  0.606178                   dbpedia  \n",
       "74   Wikipedia  0.483932  0.484475  0.483591                   dbpedia  \n",
       "75   Wikipedia  0.465278  0.469841  0.463320                   dbpedia  \n",
       "76    WikiNews  0.650831  0.727820  0.631313                   dbpedia  \n",
       "77    WikiNews  0.631373  0.676638  0.617424                   dbpedia  \n",
       "78    WikiNews  0.612665  0.759442  0.599747                   dbpedia  \n",
       "79    WikiNews  0.622321  0.820076  0.606692                   dbpedia  \n",
       "80        News  0.505349  0.575749  0.522798                   dbpedia  \n",
       "81        News  0.527928  0.565826  0.532594                   dbpedia  \n",
       "82        News  0.493034  0.562667  0.516297                   dbpedia  \n",
       "83        News  0.512192  0.564208  0.524445                   dbpedia  \n",
       "84   Wikipedia  0.517544  0.523077  0.517375          brown_clustering  \n",
       "85   Wikipedia  0.498491  0.521687  0.508687          brown_clustering  \n",
       "86   Wikipedia  0.614035  0.635897  0.602317          brown_clustering  \n",
       "87   Wikipedia  0.469298  0.466667  0.474903          brown_clustering  \n",
       "88    WikiNews  0.519006  0.530357  0.521465          brown_clustering  \n",
       "89    WikiNews  0.540567  0.573384  0.542298          brown_clustering  \n",
       "90    WikiNews  0.563713  0.645349  0.563131          brown_clustering  \n",
       "91    WikiNews  0.461421  0.461382  0.476010          brown_clustering  \n",
       "92        News  0.535655  0.556250  0.535888          brown_clustering  \n",
       "93        News  0.513176  0.542892  0.521238          brown_clustering  \n",
       "94        News  0.488861  0.538018  0.511442          brown_clustering  \n",
       "95        News  0.525402  0.558734  0.530166          brown_clustering  \n",
       "96   Wikipedia  0.588246  0.584444  0.628378                  semantic  \n",
       "97   Wikipedia  0.562552  0.558824  0.577220                  semantic  \n",
       "98   Wikipedia  0.541667  0.539683  0.548263                  semantic  \n",
       "99   Wikipedia  0.546032  0.545455  0.563707                  semantic  \n",
       "100   WikiNews  0.587719  0.614286  0.580808                  semantic  \n",
       "101   WikiNews  0.640947  0.700203  0.624369                  semantic  \n",
       "102   WikiNews  0.613030  0.638397  0.603535                  semantic  \n",
       "103   WikiNews  0.519006  0.530357  0.521465                  semantic  \n",
       "104       News  0.601277  0.627327  0.591886                  semantic  \n",
       "105       News  0.571017  0.577190  0.567614                  semantic  \n",
       "106       News  0.593414  0.634897  0.583738                  semantic  \n",
       "107       News  0.573249  0.585390  0.568395                  semantic  \n",
       "108  Wikipedia  0.450000  0.418605  0.486486                dictionary  \n",
       "109  Wikipedia  0.456790  0.420455  0.500000                dictionary  \n",
       "110  Wikipedia  0.456790  0.420455  0.500000                dictionary  \n",
       "111  Wikipedia  0.532468  0.550000  0.530888                dictionary  \n",
       "112   WikiNews  0.472756  0.480952  0.489899                dictionary  \n",
       "113   WikiNews  0.433735  0.382979  0.500000                dictionary  \n",
       "114   WikiNews  0.433735  0.382979  0.500000                dictionary  \n",
       "115   WikiNews  0.510417  0.536905  0.519571                dictionary  \n",
       "116       News  0.552137  0.792051  0.557646                dictionary  \n",
       "117       News  0.440171  0.393130  0.500000                dictionary  \n",
       "118       News  0.440171  0.393130  0.500000                dictionary  \n",
       "119       News  0.599859  0.709091  0.587725                dictionary  \n",
       "120  Wikipedia  0.483932  0.484475  0.483591           corpus+semantic  \n",
       "121  Wikipedia  0.572234  0.568896  0.599421           corpus+semantic  \n",
       "122  Wikipedia  0.565789  0.579487  0.559846           corpus+semantic  \n",
       "123  Wikipedia  0.593846  0.600877  0.588803           corpus+semantic  \n",
       "124   WikiNews  0.585737  0.648810  0.578914           corpus+semantic  \n",
       "125   WikiNews  0.579381  0.598734  0.573864           corpus+semantic  \n",
       "126   WikiNews  0.656433  0.698214  0.640152           corpus+semantic  \n",
       "127   WikiNews  0.497326  0.509146  0.505682           corpus+semantic  \n",
       "128       News  0.591284  0.617625  0.582958           corpus+semantic  \n",
       "129       News  0.577179  0.581920  0.574116           corpus+semantic  \n",
       "130       News  0.563534  0.575709  0.559466           corpus+semantic  \n",
       "131       News  0.607852  0.630505  0.598388           corpus+semantic  \n",
       "132  Wikipedia  0.618056  0.609524  0.633205  wordnet+psycholinguistic  \n",
       "133  Wikipedia  0.526098  0.526471  0.534749  wordnet+psycholinguistic  \n",
       "134  Wikipedia  0.510464  0.512987  0.510618  wordnet+psycholinguistic  \n",
       "135  Wikipedia  0.566502  0.564840  0.568533  wordnet+psycholinguistic  \n",
       "136   WikiNews  0.570776  0.573649  0.568813  wordnet+psycholinguistic  \n",
       "137   WikiNews  0.627947  0.644385  0.619318  wordnet+psycholinguistic  \n",
       "138   WikiNews  0.532638  0.533268  0.532197  wordnet+psycholinguistic  \n",
       "139   WikiNews  0.523649  0.527047  0.523359  wordnet+psycholinguistic  \n",
       "140       News  0.595247  0.608633  0.588679  wordnet+psycholinguistic  \n",
       "141       News  0.576185  0.590167  0.570822  wordnet+psycholinguistic  \n",
       "142       News  0.538231  0.546794  0.536755  wordnet+psycholinguistic  \n",
       "143       News  0.562059  0.580120  0.557819  wordnet+psycholinguistic  \n",
       "144  Wikipedia  0.503590  0.504386  0.503861                       all  \n",
       "145  Wikipedia  0.598174  0.593750  0.604247                       all  \n",
       "146  Wikipedia  0.548718  0.552632  0.546332                       all  \n",
       "147  Wikipedia  0.548718  0.552632  0.546332                       all  \n",
       "148   WikiNews  0.618919  0.630117  0.612374                       all  \n",
       "149   WikiNews  0.578895  0.584211  0.575758                       all  \n",
       "150   WikiNews  0.571228  0.584936  0.566919                       all  \n",
       "151   WikiNews  0.618919  0.630117  0.612374                       all  \n",
       "152       News  0.677394  0.705827  0.661668                       all  \n",
       "153       News  0.651375  0.652394  0.650399                       all  \n",
       "154       News  0.607269  0.616081  0.601682                       all  \n",
       "155       News  0.601386  0.612442  0.595180                       all  "
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_eval_data_ada = create_eval_df_from_results_macro(results, False)\n",
    "feature_eval_data_ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.693412</td>\n",
       "      <td>0.700676</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.618056</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>0.633205</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.677394</td>\n",
       "      <td>0.705827</td>\n",
       "      <td>0.661668</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               agg    dataset  \\\n",
       "7                                                    weighted_mean   WikiNews   \n",
       "132  (mean, <function agg_feat_num_average at 0x000000FF52232D90>)  Wikipedia   \n",
       "152  (mean, <function agg_feat_num_average at 0x000000FF52232D90>)       News   \n",
       "\n",
       "           f1      prec       rec                        zc  \n",
       "7    0.693412  0.700676  0.687500                linguistic  \n",
       "132  0.618056  0.609524  0.633205  wordnet+psycholinguistic  \n",
       "152  0.677394  0.705827  0.661668                       all  "
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = feature_eval_data_ada.groupby(['dataset'])['f1'].transform(max) == feature_eval_data_ada['f1']\n",
    "feature_eval_data_ada[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [Result(fs, fs.fc, fs.agg,\n",
    "        precision_recall_fscore_support(*adaboost(*transform_feat_to_num(remove_labels_for_binary_df(fs.train), \n",
    "        remove_labels_for_binary_df(fs.test))), average='macro')) for fs in all_fc_datasets_dswp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_eval_data_ada_dswp = create_eval_df_from_results_macro(results)\n",
    "feature_eval_data_ada_dswp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = feature_eval_data_ada_dswp.groupby(['dataset'])['f1'].transform(max) == feature_eval_data_ada_dswp['f1']\n",
    "feature_eval_data_ada_dswp[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.2.5 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "results = [Result(fs, fs.fc, fs.agg,\n",
    "        precision_recall_fscore_support(*decision_tree(*transform_feat_to_num(remove_labels_for_binary_df(fs.train), \n",
    "        remove_labels_for_binary_df(fs.test))), average='macro')) for fs in all_fc_datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456485</td>\n",
       "      <td>0.472698</td>\n",
       "      <td>0.458494</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.526098</td>\n",
       "      <td>0.526471</td>\n",
       "      <td>0.534749</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.503472</td>\n",
       "      <td>0.504762</td>\n",
       "      <td>0.505792</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.496568</td>\n",
       "      <td>0.505208</td>\n",
       "      <td>0.507722</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.645797</td>\n",
       "      <td>0.638528</td>\n",
       "      <td>0.661616</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.572163</td>\n",
       "      <td>0.569652</td>\n",
       "      <td>0.579545</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.540117</td>\n",
       "      <td>0.541892</td>\n",
       "      <td>0.539141</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.605594</td>\n",
       "      <td>0.604103</td>\n",
       "      <td>0.607323</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.574253</td>\n",
       "      <td>0.578019</td>\n",
       "      <td>0.571689</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.583092</td>\n",
       "      <td>0.586423</td>\n",
       "      <td>0.580617</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.553358</td>\n",
       "      <td>0.556817</td>\n",
       "      <td>0.551404</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.591193</td>\n",
       "      <td>0.591193</td>\n",
       "      <td>0.591193</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.458067</td>\n",
       "      <td>0.443050</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.526098</td>\n",
       "      <td>0.526471</td>\n",
       "      <td>0.534749</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.628031</td>\n",
       "      <td>0.620133</td>\n",
       "      <td>0.639961</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.526098</td>\n",
       "      <td>0.526471</td>\n",
       "      <td>0.534749</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.532338</td>\n",
       "      <td>0.535786</td>\n",
       "      <td>0.544823</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.627367</td>\n",
       "      <td>0.621614</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.554924</td>\n",
       "      <td>0.554924</td>\n",
       "      <td>0.554924</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.552381</td>\n",
       "      <td>0.550905</td>\n",
       "      <td>0.556818</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.559933</td>\n",
       "      <td>0.560343</td>\n",
       "      <td>0.559553</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.588183</td>\n",
       "      <td>0.587634</td>\n",
       "      <td>0.588766</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.597277</td>\n",
       "      <td>0.598647</td>\n",
       "      <td>0.596047</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.597906</td>\n",
       "      <td>0.602736</td>\n",
       "      <td>0.594400</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.530130</td>\n",
       "      <td>0.533854</td>\n",
       "      <td>0.550193</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.505747</td>\n",
       "      <td>0.509653</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.557991</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.561776</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.489425</td>\n",
       "      <td>0.500635</td>\n",
       "      <td>0.500965</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.540117</td>\n",
       "      <td>0.541892</td>\n",
       "      <td>0.539141</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.529126</td>\n",
       "      <td>0.530239</td>\n",
       "      <td>0.535985</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.608333</td>\n",
       "      <td>0.604072</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.548077</td>\n",
       "      <td>0.548438</td>\n",
       "      <td>0.558712</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.582226</td>\n",
       "      <td>0.580822</td>\n",
       "      <td>0.583911</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.610749</td>\n",
       "      <td>0.610056</td>\n",
       "      <td>0.611477</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.589121</td>\n",
       "      <td>0.599027</td>\n",
       "      <td>0.583825</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.618324</td>\n",
       "      <td>0.622944</td>\n",
       "      <td>0.614684</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.546032</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.563707</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.427083</td>\n",
       "      <td>0.434921</td>\n",
       "      <td>0.420849</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.522366</td>\n",
       "      <td>0.528571</td>\n",
       "      <td>0.543436</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.489644</td>\n",
       "      <td>0.494118</td>\n",
       "      <td>0.492278</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.568279</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.570707</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.536812</td>\n",
       "      <td>0.536797</td>\n",
       "      <td>0.542929</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.568279</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.570707</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.574661</td>\n",
       "      <td>0.572917</td>\n",
       "      <td>0.588384</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.540351</td>\n",
       "      <td>0.539892</td>\n",
       "      <td>0.540915</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.573924</td>\n",
       "      <td>0.581320</td>\n",
       "      <td>0.570042</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.568465</td>\n",
       "      <td>0.570604</td>\n",
       "      <td>0.566834</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.579811</td>\n",
       "      <td>0.590075</td>\n",
       "      <td>0.574896</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.566502</td>\n",
       "      <td>0.564840</td>\n",
       "      <td>0.568533</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.421911</td>\n",
       "      <td>0.432113</td>\n",
       "      <td>0.414093</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.532819</td>\n",
       "      <td>0.532819</td>\n",
       "      <td>0.532819</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.588877</td>\n",
       "      <td>0.583679</td>\n",
       "      <td>0.597490</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.498537</td>\n",
       "      <td>0.500764</td>\n",
       "      <td>0.500631</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.578895</td>\n",
       "      <td>0.584211</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.610088</td>\n",
       "      <td>0.617193</td>\n",
       "      <td>0.605429</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.485316</td>\n",
       "      <td>0.485263</td>\n",
       "      <td>0.486742</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.519266</td>\n",
       "      <td>0.519771</td>\n",
       "      <td>0.518984</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.567657</td>\n",
       "      <td>0.565341</td>\n",
       "      <td>0.571775</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.567453</td>\n",
       "      <td>0.576418</td>\n",
       "      <td>0.563540</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.607617</td>\n",
       "      <td>0.606322</td>\n",
       "      <td>0.609050</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.554205</td>\n",
       "      <td>0.551883</td>\n",
       "      <td>0.570463</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.459207</td>\n",
       "      <td>0.465675</td>\n",
       "      <td>0.456564</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.469570</td>\n",
       "      <td>0.480602</td>\n",
       "      <td>0.472008</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.557991</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.561776</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.645797</td>\n",
       "      <td>0.638528</td>\n",
       "      <td>0.661616</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.597061</td>\n",
       "      <td>0.594643</td>\n",
       "      <td>0.600379</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.547702</td>\n",
       "      <td>0.551228</td>\n",
       "      <td>0.546086</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.584596</td>\n",
       "      <td>0.584596</td>\n",
       "      <td>0.584596</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.545770</td>\n",
       "      <td>0.545770</td>\n",
       "      <td>0.545770</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.639945</td>\n",
       "      <td>0.640887</td>\n",
       "      <td>0.639043</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.537072</td>\n",
       "      <td>0.537330</td>\n",
       "      <td>0.536841</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.511702</td>\n",
       "      <td>0.511702</td>\n",
       "      <td>0.511702</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453190</td>\n",
       "      <td>0.461765</td>\n",
       "      <td>0.449807</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.441270</td>\n",
       "      <td>0.482906</td>\n",
       "      <td>0.469112</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.458067</td>\n",
       "      <td>0.443050</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.503472</td>\n",
       "      <td>0.504762</td>\n",
       "      <td>0.505792</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.596265</td>\n",
       "      <td>0.632004</td>\n",
       "      <td>0.587753</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.604211</td>\n",
       "      <td>0.622596</td>\n",
       "      <td>0.596591</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.643038</td>\n",
       "      <td>0.781977</td>\n",
       "      <td>0.622475</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.614268</td>\n",
       "      <td>0.614268</td>\n",
       "      <td>0.614268</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.462772</td>\n",
       "      <td>0.483880</td>\n",
       "      <td>0.477115</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.464362</td>\n",
       "      <td>0.462661</td>\n",
       "      <td>0.467840</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.483626</td>\n",
       "      <td>0.490110</td>\n",
       "      <td>0.487517</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.536317</td>\n",
       "      <td>0.539971</td>\n",
       "      <td>0.552445</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.409396</td>\n",
       "      <td>0.406667</td>\n",
       "      <td>0.412162</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.557086</td>\n",
       "      <td>0.564935</td>\n",
       "      <td>0.553089</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.510567</td>\n",
       "      <td>0.510771</td>\n",
       "      <td>0.512548</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.510567</td>\n",
       "      <td>0.510771</td>\n",
       "      <td>0.512548</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.491892</td>\n",
       "      <td>0.492690</td>\n",
       "      <td>0.493687</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.512082</td>\n",
       "      <td>0.519409</td>\n",
       "      <td>0.514520</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.570776</td>\n",
       "      <td>0.573649</td>\n",
       "      <td>0.568813</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.472334</td>\n",
       "      <td>0.471950</td>\n",
       "      <td>0.472854</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.556848</td>\n",
       "      <td>0.555003</td>\n",
       "      <td>0.560420</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.515121</td>\n",
       "      <td>0.516226</td>\n",
       "      <td>0.514910</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.532977</td>\n",
       "      <td>0.539078</td>\n",
       "      <td>0.531900</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.551562</td>\n",
       "      <td>0.550963</td>\n",
       "      <td>0.552271</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.589868</td>\n",
       "      <td>0.583156</td>\n",
       "      <td>0.612934</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.598174</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.604247</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.427083</td>\n",
       "      <td>0.434921</td>\n",
       "      <td>0.420849</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.608392</td>\n",
       "      <td>0.599924</td>\n",
       "      <td>0.626448</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.692652</td>\n",
       "      <td>0.716196</td>\n",
       "      <td>0.678662</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.643939</td>\n",
       "      <td>0.643939</td>\n",
       "      <td>0.643939</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.605594</td>\n",
       "      <td>0.604103</td>\n",
       "      <td>0.607323</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.566457</td>\n",
       "      <td>0.566052</td>\n",
       "      <td>0.581439</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.571363</td>\n",
       "      <td>0.571849</td>\n",
       "      <td>0.570908</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.657516</td>\n",
       "      <td>0.651870</td>\n",
       "      <td>0.665049</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.577186</td>\n",
       "      <td>0.578902</td>\n",
       "      <td>0.575763</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.536896</td>\n",
       "      <td>0.537077</td>\n",
       "      <td>0.545076</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.474851</td>\n",
       "      <td>0.473277</td>\n",
       "      <td>0.481660</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.503472</td>\n",
       "      <td>0.504762</td>\n",
       "      <td>0.505792</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.497326</td>\n",
       "      <td>0.509146</td>\n",
       "      <td>0.505682</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.533231</td>\n",
       "      <td>0.556911</td>\n",
       "      <td>0.535354</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.559399</td>\n",
       "      <td>0.653689</td>\n",
       "      <td>0.558512</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.437768</td>\n",
       "      <td>0.392308</td>\n",
       "      <td>0.495146</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.438972</td>\n",
       "      <td>0.392720</td>\n",
       "      <td>0.497573</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.631437</td>\n",
       "      <td>0.642009</td>\n",
       "      <td>0.624393</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.465278</td>\n",
       "      <td>0.469841</td>\n",
       "      <td>0.463320</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.514706</td>\n",
       "      <td>0.523573</td>\n",
       "      <td>0.536680</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.525217</td>\n",
       "      <td>0.524658</td>\n",
       "      <td>0.526062</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.447668</td>\n",
       "      <td>0.476797</td>\n",
       "      <td>0.460425</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.562791</td>\n",
       "      <td>0.563927</td>\n",
       "      <td>0.561869</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.547702</td>\n",
       "      <td>0.551228</td>\n",
       "      <td>0.546086</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.567731</td>\n",
       "      <td>0.574443</td>\n",
       "      <td>0.599116</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.578895</td>\n",
       "      <td>0.584211</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.598354</td>\n",
       "      <td>0.595751</td>\n",
       "      <td>0.601768</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.585195</td>\n",
       "      <td>0.584178</td>\n",
       "      <td>0.586338</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.565342</td>\n",
       "      <td>0.563698</td>\n",
       "      <td>0.567701</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.582226</td>\n",
       "      <td>0.580822</td>\n",
       "      <td>0.583911</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.483932</td>\n",
       "      <td>0.484475</td>\n",
       "      <td>0.483591</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.539683</td>\n",
       "      <td>0.548263</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.471413</td>\n",
       "      <td>0.474316</td>\n",
       "      <td>0.470077</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.496893</td>\n",
       "      <td>0.496923</td>\n",
       "      <td>0.497104</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.714189</td>\n",
       "      <td>0.733187</td>\n",
       "      <td>0.701389</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.601435</td>\n",
       "      <td>0.605405</td>\n",
       "      <td>0.598485</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.524405</td>\n",
       "      <td>0.524321</td>\n",
       "      <td>0.527146</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.618551</td>\n",
       "      <td>0.613095</td>\n",
       "      <td>0.631944</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.567657</td>\n",
       "      <td>0.565341</td>\n",
       "      <td>0.571775</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.591791</td>\n",
       "      <td>0.594597</td>\n",
       "      <td>0.589546</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.537072</td>\n",
       "      <td>0.537330</td>\n",
       "      <td>0.536841</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.533476</td>\n",
       "      <td>0.534615</td>\n",
       "      <td>0.532767</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.395881</td>\n",
       "      <td>0.419271</td>\n",
       "      <td>0.380309</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.522366</td>\n",
       "      <td>0.528571</td>\n",
       "      <td>0.543436</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.546032</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.563707</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.539683</td>\n",
       "      <td>0.548263</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.552381</td>\n",
       "      <td>0.550905</td>\n",
       "      <td>0.556818</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.510716</td>\n",
       "      <td>0.510714</td>\n",
       "      <td>0.511364</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>0.613043</td>\n",
       "      <td>0.623106</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.569137</td>\n",
       "      <td>0.604675</td>\n",
       "      <td>0.565025</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.651375</td>\n",
       "      <td>0.652394</td>\n",
       "      <td>0.650399</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.583092</td>\n",
       "      <td>0.586423</td>\n",
       "      <td>0.580617</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.608783</td>\n",
       "      <td>0.610310</td>\n",
       "      <td>0.607403</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.607617</td>\n",
       "      <td>0.606322</td>\n",
       "      <td>0.609050</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                agg  \\\n",
       "0                                                                                                                              mean   \n",
       "1                                                                                                                               max   \n",
       "2                                                                                                                               min   \n",
       "3                                                                                                                     weighted_mean   \n",
       "4                                                                                                                              mean   \n",
       "5                                                                                                                               max   \n",
       "6                                                                                                                               min   \n",
       "7                                                                                                                     weighted_mean   \n",
       "8                                                                                                                              mean   \n",
       "9                                                                                                                               max   \n",
       "10                                                                                                                              min   \n",
       "11                                                                                                                    weighted_mean   \n",
       "12                                                                                                                             mean   \n",
       "13                                                                                                                              max   \n",
       "14                                                                                                                              min   \n",
       "15                                                                                                                    weighted_mean   \n",
       "16                                                                                                                             mean   \n",
       "17                                                                                                                              max   \n",
       "18                                                                                                                              min   \n",
       "19                                                                                                                    weighted_mean   \n",
       "20                                                                                                                             mean   \n",
       "21                                                                                                                              max   \n",
       "22                                                                                                                              min   \n",
       "23                                                                                                                    weighted_mean   \n",
       "24                                                                                                                             mean   \n",
       "25                                                                                                                              max   \n",
       "26                                                                                                                              min   \n",
       "27                                                                                                                    weighted_mean   \n",
       "28                                                                                                                             mean   \n",
       "29                                                                                                                              max   \n",
       "30                                                                                                                              min   \n",
       "31                                                                                                                    weighted_mean   \n",
       "32                                                                                                                             mean   \n",
       "33                                                                                                                              max   \n",
       "34                                                                                                                              min   \n",
       "35                                                                                                                    weighted_mean   \n",
       "36                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "37                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "38                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "39                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "40                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "41                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "42                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "43                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "44                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "45                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "46                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "47                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "48                                                                                                                             mean   \n",
       "49                                                                                                                              max   \n",
       "50                                                                                                                              min   \n",
       "51                                                                                                                    weighted_mean   \n",
       "52                                                                                                                             mean   \n",
       "53                                                                                                                              max   \n",
       "54                                                                                                                              min   \n",
       "55                                                                                                                    weighted_mean   \n",
       "56                                                                                                                             mean   \n",
       "57                                                                                                                              max   \n",
       "58                                                                                                                              min   \n",
       "59                                                                                                                    weighted_mean   \n",
       "60                                                                                                                             mean   \n",
       "61                                                                                                                              max   \n",
       "62                                                                                                                              min   \n",
       "63                                                                                                                    weighted_mean   \n",
       "64                                                                                                                             mean   \n",
       "65                                                                                                                              max   \n",
       "66                                                                                                                              min   \n",
       "67                                                                                                                    weighted_mean   \n",
       "68                                                                                                                             mean   \n",
       "69                                                                                                                              max   \n",
       "70                                                                                                                              min   \n",
       "71                                                                                                                    weighted_mean   \n",
       "72                                                                                                                             mean   \n",
       "73                                                                                                                              max   \n",
       "74                                                                                                                              min   \n",
       "75                                                                                                                    weighted_mean   \n",
       "76                                                                                                                             mean   \n",
       "77                                                                                                                              max   \n",
       "78                                                                                                                              min   \n",
       "79                                                                                                                    weighted_mean   \n",
       "80                                                                                                                             mean   \n",
       "81                                                                                                                              max   \n",
       "82                                                                                                                              min   \n",
       "83                                                                                                                    weighted_mean   \n",
       "84                                                                                                                             mean   \n",
       "85                                                                                                                              max   \n",
       "86                                                                                                                              min   \n",
       "87                                                                                                                    weighted_mean   \n",
       "88                                                                                                                             mean   \n",
       "89                                                                                                                              max   \n",
       "90                                                                                                                              min   \n",
       "91                                                                                                                    weighted_mean   \n",
       "92                                                                                                                             mean   \n",
       "93                                                                                                                              max   \n",
       "94                                                                                                                              min   \n",
       "95                                                                                                                    weighted_mean   \n",
       "96                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "97                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "98                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "99                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "100                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "101                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "102                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "103                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "104                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "105                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "106                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "107                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "108                                                                                                                            mean   \n",
       "109                                                                                                                             max   \n",
       "110                                                                                                                             min   \n",
       "111                                                                                                                   weighted_mean   \n",
       "112                                                                                                                            mean   \n",
       "113                                                                                                                             max   \n",
       "114                                                                                                                             min   \n",
       "115                                                                                                                   weighted_mean   \n",
       "116                                                                                                                            mean   \n",
       "117                                                                                                                             max   \n",
       "118                                                                                                                             min   \n",
       "119                                                                                                                   weighted_mean   \n",
       "120  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "121            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "122            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "123        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "124  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "125            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "126            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "127        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "128  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "129            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "130            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "131        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "132                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "133                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "134                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "135                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "136                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "137                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "138                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "139                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "140                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "141                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "142                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "143                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "144                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "145                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "146                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "147                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "148                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "149                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "150                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "151                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "152                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "153                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "154                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "155                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "\n",
       "       dataset        f1      prec       rec                        zc  \n",
       "0    Wikipedia  0.456485  0.472698  0.458494                linguistic  \n",
       "1    Wikipedia  0.526098  0.526471  0.534749                linguistic  \n",
       "2    Wikipedia  0.503472  0.504762  0.505792                linguistic  \n",
       "3    Wikipedia  0.496568  0.505208  0.507722                linguistic  \n",
       "4     WikiNews  0.645797  0.638528  0.661616                linguistic  \n",
       "5     WikiNews  0.572163  0.569652  0.579545                linguistic  \n",
       "6     WikiNews  0.540117  0.541892  0.539141                linguistic  \n",
       "7     WikiNews  0.605594  0.604103  0.607323                linguistic  \n",
       "8         News  0.574253  0.578019  0.571689                linguistic  \n",
       "9         News  0.583092  0.586423  0.580617                linguistic  \n",
       "10        News  0.553358  0.556817  0.551404                linguistic  \n",
       "11        News  0.591193  0.591193  0.591193                linguistic  \n",
       "12   Wikipedia  0.447214  0.458067  0.443050                 frequency  \n",
       "13   Wikipedia  0.526098  0.526471  0.534749                 frequency  \n",
       "14   Wikipedia  0.628031  0.620133  0.639961                 frequency  \n",
       "15   Wikipedia  0.526098  0.526471  0.534749                 frequency  \n",
       "16    WikiNews  0.532338  0.535786  0.544823                 frequency  \n",
       "17    WikiNews  0.627367  0.621614  0.638889                 frequency  \n",
       "18    WikiNews  0.554924  0.554924  0.554924                 frequency  \n",
       "19    WikiNews  0.552381  0.550905  0.556818                 frequency  \n",
       "20        News  0.559933  0.560343  0.559553                 frequency  \n",
       "21        News  0.588183  0.587634  0.588766                 frequency  \n",
       "22        News  0.597277  0.598647  0.596047                 frequency  \n",
       "23        News  0.597906  0.602736  0.594400                 frequency  \n",
       "24   Wikipedia  0.530130  0.533854  0.550193            language_model  \n",
       "25   Wikipedia  0.484848  0.505747  0.509653            language_model  \n",
       "26   Wikipedia  0.557991  0.555556  0.561776            language_model  \n",
       "27   Wikipedia  0.489425  0.500635  0.500965            language_model  \n",
       "28    WikiNews  0.540117  0.541892  0.539141            language_model  \n",
       "29    WikiNews  0.529126  0.530239  0.535985            language_model  \n",
       "30    WikiNews  0.608333  0.604072  0.616162            language_model  \n",
       "31    WikiNews  0.548077  0.548438  0.558712            language_model  \n",
       "32        News  0.582226  0.580822  0.583911            language_model  \n",
       "33        News  0.610749  0.610056  0.611477            language_model  \n",
       "34        News  0.589121  0.599027  0.583825            language_model  \n",
       "35        News  0.618324  0.622944  0.614684            language_model  \n",
       "36   Wikipedia  0.546032  0.545455  0.563707                    corpus  \n",
       "37   Wikipedia  0.427083  0.434921  0.420849                    corpus  \n",
       "38   Wikipedia  0.522366  0.528571  0.543436                    corpus  \n",
       "39   Wikipedia  0.489644  0.494118  0.492278                    corpus  \n",
       "40    WikiNews  0.568279  0.566667  0.570707                    corpus  \n",
       "41    WikiNews  0.536812  0.536797  0.542929                    corpus  \n",
       "42    WikiNews  0.568279  0.566667  0.570707                    corpus  \n",
       "43    WikiNews  0.574661  0.572917  0.588384                    corpus  \n",
       "44        News  0.540351  0.539892  0.540915                    corpus  \n",
       "45        News  0.573924  0.581320  0.570042                    corpus  \n",
       "46        News  0.568465  0.570604  0.566834                    corpus  \n",
       "47        News  0.579811  0.590075  0.574896                    corpus  \n",
       "48   Wikipedia  0.566502  0.564840  0.568533          psycholinguistic  \n",
       "49   Wikipedia  0.421911  0.432113  0.414093          psycholinguistic  \n",
       "50   Wikipedia  0.532819  0.532819  0.532819          psycholinguistic  \n",
       "51   Wikipedia  0.588877  0.583679  0.597490          psycholinguistic  \n",
       "52    WikiNews  0.498537  0.500764  0.500631          psycholinguistic  \n",
       "53    WikiNews  0.578895  0.584211  0.575758          psycholinguistic  \n",
       "54    WikiNews  0.610088  0.617193  0.605429          psycholinguistic  \n",
       "55    WikiNews  0.485316  0.485263  0.486742          psycholinguistic  \n",
       "56        News  0.519266  0.519771  0.518984          psycholinguistic  \n",
       "57        News  0.567657  0.565341  0.571775          psycholinguistic  \n",
       "58        News  0.567453  0.576418  0.563540          psycholinguistic  \n",
       "59        News  0.607617  0.606322  0.609050          psycholinguistic  \n",
       "60   Wikipedia  0.554205  0.551883  0.570463                   wordnet  \n",
       "61   Wikipedia  0.459207  0.465675  0.456564                   wordnet  \n",
       "62   Wikipedia  0.469570  0.480602  0.472008                   wordnet  \n",
       "63   Wikipedia  0.557991  0.555556  0.561776                   wordnet  \n",
       "64    WikiNews  0.645797  0.638528  0.661616                   wordnet  \n",
       "65    WikiNews  0.597061  0.594643  0.600379                   wordnet  \n",
       "66    WikiNews  0.547702  0.551228  0.546086                   wordnet  \n",
       "67    WikiNews  0.584596  0.584596  0.584596                   wordnet  \n",
       "68        News  0.545770  0.545770  0.545770                   wordnet  \n",
       "69        News  0.639945  0.640887  0.639043                   wordnet  \n",
       "70        News  0.537072  0.537330  0.536841                   wordnet  \n",
       "71        News  0.511702  0.511702  0.511702                   wordnet  \n",
       "72   Wikipedia  0.453190  0.461765  0.449807                   dbpedia  \n",
       "73   Wikipedia  0.441270  0.482906  0.469112                   dbpedia  \n",
       "74   Wikipedia  0.447214  0.458067  0.443050                   dbpedia  \n",
       "75   Wikipedia  0.503472  0.504762  0.505792                   dbpedia  \n",
       "76    WikiNews  0.596265  0.632004  0.587753                   dbpedia  \n",
       "77    WikiNews  0.604211  0.622596  0.596591                   dbpedia  \n",
       "78    WikiNews  0.643038  0.781977  0.622475                   dbpedia  \n",
       "79    WikiNews  0.614268  0.614268  0.614268                   dbpedia  \n",
       "80        News  0.462772  0.483880  0.477115                   dbpedia  \n",
       "81        News  0.464362  0.462661  0.467840                   dbpedia  \n",
       "82        News  0.483626  0.490110  0.487517                   dbpedia  \n",
       "83        News  0.536317  0.539971  0.552445                   dbpedia  \n",
       "84   Wikipedia  0.409396  0.406667  0.412162          brown_clustering  \n",
       "85   Wikipedia  0.557086  0.564935  0.553089          brown_clustering  \n",
       "86   Wikipedia  0.510567  0.510771  0.512548          brown_clustering  \n",
       "87   Wikipedia  0.510567  0.510771  0.512548          brown_clustering  \n",
       "88    WikiNews  0.491892  0.492690  0.493687          brown_clustering  \n",
       "89    WikiNews  0.512082  0.519409  0.514520          brown_clustering  \n",
       "90    WikiNews  0.570776  0.573649  0.568813          brown_clustering  \n",
       "91    WikiNews  0.472334  0.471950  0.472854          brown_clustering  \n",
       "92        News  0.556848  0.555003  0.560420          brown_clustering  \n",
       "93        News  0.515121  0.516226  0.514910          brown_clustering  \n",
       "94        News  0.532977  0.539078  0.531900          brown_clustering  \n",
       "95        News  0.551562  0.550963  0.552271          brown_clustering  \n",
       "96   Wikipedia  0.589868  0.583156  0.612934                  semantic  \n",
       "97   Wikipedia  0.598174  0.593750  0.604247                  semantic  \n",
       "98   Wikipedia  0.427083  0.434921  0.420849                  semantic  \n",
       "99   Wikipedia  0.608392  0.599924  0.626448                  semantic  \n",
       "100   WikiNews  0.692652  0.716196  0.678662                  semantic  \n",
       "101   WikiNews  0.643939  0.643939  0.643939                  semantic  \n",
       "102   WikiNews  0.605594  0.604103  0.607323                  semantic  \n",
       "103   WikiNews  0.566457  0.566052  0.581439                  semantic  \n",
       "104       News  0.571363  0.571849  0.570908                  semantic  \n",
       "105       News  0.657516  0.651870  0.665049                  semantic  \n",
       "106       News  0.577186  0.578902  0.575763                  semantic  \n",
       "107       News  0.536896  0.537077  0.545076                  semantic  \n",
       "108  Wikipedia  0.474851  0.473277  0.481660                dictionary  \n",
       "109  Wikipedia  0.450000  0.418605  0.486486                dictionary  \n",
       "110  Wikipedia  0.450000  0.418605  0.486486                dictionary  \n",
       "111  Wikipedia  0.503472  0.504762  0.505792                dictionary  \n",
       "112   WikiNews  0.497326  0.509146  0.505682                dictionary  \n",
       "113   WikiNews  0.433735  0.382979  0.500000                dictionary  \n",
       "114   WikiNews  0.433735  0.382979  0.500000                dictionary  \n",
       "115   WikiNews  0.533231  0.556911  0.535354                dictionary  \n",
       "116       News  0.559399  0.653689  0.558512                dictionary  \n",
       "117       News  0.437768  0.392308  0.495146                dictionary  \n",
       "118       News  0.438972  0.392720  0.497573                dictionary  \n",
       "119       News  0.631437  0.642009  0.624393                dictionary  \n",
       "120  Wikipedia  0.465278  0.469841  0.463320           corpus+semantic  \n",
       "121  Wikipedia  0.514706  0.523573  0.536680           corpus+semantic  \n",
       "122  Wikipedia  0.525217  0.524658  0.526062           corpus+semantic  \n",
       "123  Wikipedia  0.447668  0.476797  0.460425           corpus+semantic  \n",
       "124   WikiNews  0.562791  0.563927  0.561869           corpus+semantic  \n",
       "125   WikiNews  0.547702  0.551228  0.546086           corpus+semantic  \n",
       "126   WikiNews  0.567731  0.574443  0.599116           corpus+semantic  \n",
       "127   WikiNews  0.578895  0.584211  0.575758           corpus+semantic  \n",
       "128       News  0.598354  0.595751  0.601768           corpus+semantic  \n",
       "129       News  0.585195  0.584178  0.586338           corpus+semantic  \n",
       "130       News  0.565342  0.563698  0.567701           corpus+semantic  \n",
       "131       News  0.582226  0.580822  0.583911           corpus+semantic  \n",
       "132  Wikipedia  0.483932  0.484475  0.483591  wordnet+psycholinguistic  \n",
       "133  Wikipedia  0.541667  0.539683  0.548263  wordnet+psycholinguistic  \n",
       "134  Wikipedia  0.471413  0.474316  0.470077  wordnet+psycholinguistic  \n",
       "135  Wikipedia  0.496893  0.496923  0.497104  wordnet+psycholinguistic  \n",
       "136   WikiNews  0.714189  0.733187  0.701389  wordnet+psycholinguistic  \n",
       "137   WikiNews  0.601435  0.605405  0.598485  wordnet+psycholinguistic  \n",
       "138   WikiNews  0.524405  0.524321  0.527146  wordnet+psycholinguistic  \n",
       "139   WikiNews  0.618551  0.613095  0.631944  wordnet+psycholinguistic  \n",
       "140       News  0.567657  0.565341  0.571775  wordnet+psycholinguistic  \n",
       "141       News  0.591791  0.594597  0.589546  wordnet+psycholinguistic  \n",
       "142       News  0.537072  0.537330  0.536841  wordnet+psycholinguistic  \n",
       "143       News  0.533476  0.534615  0.532767  wordnet+psycholinguistic  \n",
       "144  Wikipedia  0.395881  0.419271  0.380309                       all  \n",
       "145  Wikipedia  0.522366  0.528571  0.543436                       all  \n",
       "146  Wikipedia  0.546032  0.545455  0.563707                       all  \n",
       "147  Wikipedia  0.541667  0.539683  0.548263                       all  \n",
       "148   WikiNews  0.552381  0.550905  0.556818                       all  \n",
       "149   WikiNews  0.510716  0.510714  0.511364                       all  \n",
       "150   WikiNews  0.617021  0.613043  0.623106                       all  \n",
       "151   WikiNews  0.569137  0.604675  0.565025                       all  \n",
       "152       News  0.651375  0.652394  0.650399                       all  \n",
       "153       News  0.583092  0.586423  0.580617                       all  \n",
       "154       News  0.608783  0.610310  0.607403                       all  \n",
       "155       News  0.607617  0.606322  0.609050                       all  "
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_eval_data_dt = create_eval_df_from_results_macro(results, False)\n",
    "feature_eval_data_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.628031</td>\n",
       "      <td>0.620133</td>\n",
       "      <td>0.639961</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.657516</td>\n",
       "      <td>0.651870</td>\n",
       "      <td>0.665049</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.714189</td>\n",
       "      <td>0.733187</td>\n",
       "      <td>0.701389</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               agg    dataset  \\\n",
       "14                                                             min  Wikipedia   \n",
       "105       (max, <function agg_feat_num_max at 0x000000FF522328C8>)       News   \n",
       "136  (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   WikiNews   \n",
       "\n",
       "           f1      prec       rec                        zc  \n",
       "14   0.628031  0.620133  0.639961                 frequency  \n",
       "105  0.657516  0.651870  0.665049                  semantic  \n",
       "136  0.714189  0.733187  0.701389  wordnet+psycholinguistic  "
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = feature_eval_data_dt.groupby(['dataset'])['f1'].transform(max) == feature_eval_data_dt['f1']\n",
    "feature_eval_data_dt[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [Result(fs, fs.fc, fs.agg,\n",
    "        precision_recall_fscore_support(*decision_tree(*transform_feat_to_num(remove_labels_for_binary_df(fs.train), \n",
    "        remove_labels_for_binary_df(fs.test))), average='macro')) for fs in all_fc_datasets_dswp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.533800</td>\n",
       "      <td>0.532799</td>\n",
       "      <td>0.541506</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.525217</td>\n",
       "      <td>0.524658</td>\n",
       "      <td>0.526062</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.378378</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.571096</td>\n",
       "      <td>0.566362</td>\n",
       "      <td>0.583977</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.563242</td>\n",
       "      <td>0.572574</td>\n",
       "      <td>0.559975</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.582940</td>\n",
       "      <td>0.580106</td>\n",
       "      <td>0.595328</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.521493</td>\n",
       "      <td>0.523958</td>\n",
       "      <td>0.529040</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.623095</td>\n",
       "      <td>0.625245</td>\n",
       "      <td>0.621212</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.596406</td>\n",
       "      <td>0.595250</td>\n",
       "      <td>0.597694</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.537863</td>\n",
       "      <td>0.536892</td>\n",
       "      <td>0.541782</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.610749</td>\n",
       "      <td>0.610056</td>\n",
       "      <td>0.611477</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.538231</td>\n",
       "      <td>0.546794</td>\n",
       "      <td>0.536755</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.522994</td>\n",
       "      <td>0.535652</td>\n",
       "      <td>0.558880</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.429443</td>\n",
       "      <td>0.447917</td>\n",
       "      <td>0.422780</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.469206</td>\n",
       "      <td>0.506410</td>\n",
       "      <td>0.511583</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.447668</td>\n",
       "      <td>0.476797</td>\n",
       "      <td>0.460425</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.584572</td>\n",
       "      <td>0.586683</td>\n",
       "      <td>0.613005</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.588652</td>\n",
       "      <td>0.585797</td>\n",
       "      <td>0.593434</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.532338</td>\n",
       "      <td>0.535786</td>\n",
       "      <td>0.544823</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.592732</td>\n",
       "      <td>0.590118</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.615366</td>\n",
       "      <td>0.610312</td>\n",
       "      <td>0.623700</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.556848</td>\n",
       "      <td>0.555003</td>\n",
       "      <td>0.560420</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.586285</td>\n",
       "      <td>0.582936</td>\n",
       "      <td>0.592060</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.592281</td>\n",
       "      <td>0.589176</td>\n",
       "      <td>0.596914</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.514706</td>\n",
       "      <td>0.523573</td>\n",
       "      <td>0.536680</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.588246</td>\n",
       "      <td>0.584444</td>\n",
       "      <td>0.628378</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456485</td>\n",
       "      <td>0.472698</td>\n",
       "      <td>0.458494</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.398437</td>\n",
       "      <td>0.442266</td>\n",
       "      <td>0.397683</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.625842</td>\n",
       "      <td>0.622619</td>\n",
       "      <td>0.630051</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.584301</td>\n",
       "      <td>0.583165</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.556033</td>\n",
       "      <td>0.555172</td>\n",
       "      <td>0.565657</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.584596</td>\n",
       "      <td>0.584596</td>\n",
       "      <td>0.584596</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.603154</td>\n",
       "      <td>0.599625</td>\n",
       "      <td>0.608270</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.606015</td>\n",
       "      <td>0.600849</td>\n",
       "      <td>0.616418</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.543301</td>\n",
       "      <td>0.542015</td>\n",
       "      <td>0.546637</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.565814</td>\n",
       "      <td>0.563304</td>\n",
       "      <td>0.575069</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.463005</td>\n",
       "      <td>0.476562</td>\n",
       "      <td>0.465251</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.468372</td>\n",
       "      <td>0.488095</td>\n",
       "      <td>0.480695</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.503792</td>\n",
       "      <td>0.510033</td>\n",
       "      <td>0.514479</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>0.455172</td>\n",
       "      <td>0.424710</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.609847</td>\n",
       "      <td>0.605040</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.568279</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.570707</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.575940</td>\n",
       "      <td>0.576503</td>\n",
       "      <td>0.597222</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.525253</td>\n",
       "      <td>0.525253</td>\n",
       "      <td>0.525253</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.640014</td>\n",
       "      <td>0.634088</td>\n",
       "      <td>0.648838</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.612230</td>\n",
       "      <td>0.607080</td>\n",
       "      <td>0.621273</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.601606</td>\n",
       "      <td>0.597053</td>\n",
       "      <td>0.609917</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.604682</td>\n",
       "      <td>0.600186</td>\n",
       "      <td>0.612344</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.391933</td>\n",
       "      <td>0.439084</td>\n",
       "      <td>0.390927</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.461444</td>\n",
       "      <td>0.484220</td>\n",
       "      <td>0.473938</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.505062</td>\n",
       "      <td>0.533962</td>\n",
       "      <td>0.560811</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.468372</td>\n",
       "      <td>0.488095</td>\n",
       "      <td>0.480695</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.486730</td>\n",
       "      <td>0.508744</td>\n",
       "      <td>0.511995</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.609750</td>\n",
       "      <td>0.609443</td>\n",
       "      <td>0.642677</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.653841</td>\n",
       "      <td>0.660147</td>\n",
       "      <td>0.719697</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.653042</td>\n",
       "      <td>0.646547</td>\n",
       "      <td>0.686237</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.584802</td>\n",
       "      <td>0.581140</td>\n",
       "      <td>0.599428</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.515764</td>\n",
       "      <td>0.523385</td>\n",
       "      <td>0.531380</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.531648</td>\n",
       "      <td>0.543601</td>\n",
       "      <td>0.561460</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.557252</td>\n",
       "      <td>0.555758</td>\n",
       "      <td>0.567788</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.549722</td>\n",
       "      <td>0.547225</td>\n",
       "      <td>0.555019</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.589868</td>\n",
       "      <td>0.583156</td>\n",
       "      <td>0.612934</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.507379</td>\n",
       "      <td>0.526599</td>\n",
       "      <td>0.545367</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.514706</td>\n",
       "      <td>0.523573</td>\n",
       "      <td>0.536680</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.547164</td>\n",
       "      <td>0.546540</td>\n",
       "      <td>0.547980</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.539498</td>\n",
       "      <td>0.538690</td>\n",
       "      <td>0.541035</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.618551</td>\n",
       "      <td>0.613095</td>\n",
       "      <td>0.631944</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.599765</td>\n",
       "      <td>0.595633</td>\n",
       "      <td>0.609217</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.535161</td>\n",
       "      <td>0.534415</td>\n",
       "      <td>0.539355</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.589274</td>\n",
       "      <td>0.586016</td>\n",
       "      <td>0.594487</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.562495</td>\n",
       "      <td>0.560726</td>\n",
       "      <td>0.565274</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.553358</td>\n",
       "      <td>0.556817</td>\n",
       "      <td>0.551404</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.492219</td>\n",
       "      <td>0.509936</td>\n",
       "      <td>0.516409</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.516863</td>\n",
       "      <td>0.549285</td>\n",
       "      <td>0.589768</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.327163</td>\n",
       "      <td>0.448500</td>\n",
       "      <td>0.412162</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.481390</td>\n",
       "      <td>0.522105</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.576422</td>\n",
       "      <td>0.606383</td>\n",
       "      <td>0.648359</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.453982</td>\n",
       "      <td>0.475291</td>\n",
       "      <td>0.466540</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.530630</td>\n",
       "      <td>0.563830</td>\n",
       "      <td>0.589015</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.579732</td>\n",
       "      <td>0.600455</td>\n",
       "      <td>0.639520</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.481041</td>\n",
       "      <td>0.495733</td>\n",
       "      <td>0.494105</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.478251</td>\n",
       "      <td>0.482892</td>\n",
       "      <td>0.479369</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.502047</td>\n",
       "      <td>0.510188</td>\n",
       "      <td>0.513523</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.490690</td>\n",
       "      <td>0.510643</td>\n",
       "      <td>0.515257</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.411743</td>\n",
       "      <td>0.438676</td>\n",
       "      <td>0.402510</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.473539</td>\n",
       "      <td>0.518315</td>\n",
       "      <td>0.533784</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.507246</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.447668</td>\n",
       "      <td>0.476797</td>\n",
       "      <td>0.460425</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.592432</td>\n",
       "      <td>0.596728</td>\n",
       "      <td>0.628788</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.575319</td>\n",
       "      <td>0.584848</td>\n",
       "      <td>0.614899</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.530630</td>\n",
       "      <td>0.563830</td>\n",
       "      <td>0.589015</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.534896</td>\n",
       "      <td>0.546523</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.549863</td>\n",
       "      <td>0.548807</td>\n",
       "      <td>0.558859</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.543047</td>\n",
       "      <td>0.551296</td>\n",
       "      <td>0.571169</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.464810</td>\n",
       "      <td>0.489806</td>\n",
       "      <td>0.485264</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.542977</td>\n",
       "      <td>0.541826</td>\n",
       "      <td>0.548284</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.588246</td>\n",
       "      <td>0.584444</td>\n",
       "      <td>0.628378</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.489644</td>\n",
       "      <td>0.494118</td>\n",
       "      <td>0.492278</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.530130</td>\n",
       "      <td>0.533854</td>\n",
       "      <td>0.550193</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.503792</td>\n",
       "      <td>0.510033</td>\n",
       "      <td>0.514479</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.558320</td>\n",
       "      <td>0.559476</td>\n",
       "      <td>0.574495</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.510603</td>\n",
       "      <td>0.530373</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.575940</td>\n",
       "      <td>0.576503</td>\n",
       "      <td>0.597222</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.516775</td>\n",
       "      <td>0.524020</td>\n",
       "      <td>0.530934</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.494145</td>\n",
       "      <td>0.504627</td>\n",
       "      <td>0.506241</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.443424</td>\n",
       "      <td>0.496201</td>\n",
       "      <td>0.494365</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.524041</td>\n",
       "      <td>0.525538</td>\n",
       "      <td>0.531293</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.561944</td>\n",
       "      <td>0.559678</td>\n",
       "      <td>0.566921</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.496568</td>\n",
       "      <td>0.505208</td>\n",
       "      <td>0.507722</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.261268</td>\n",
       "      <td>0.475342</td>\n",
       "      <td>0.473938</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.517808</td>\n",
       "      <td>0.517361</td>\n",
       "      <td>0.519305</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.530130</td>\n",
       "      <td>0.533854</td>\n",
       "      <td>0.550193</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.574661</td>\n",
       "      <td>0.572917</td>\n",
       "      <td>0.588384</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.339229</td>\n",
       "      <td>0.507310</td>\n",
       "      <td>0.506313</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433026</td>\n",
       "      <td>0.544562</td>\n",
       "      <td>0.553030</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.610282</td>\n",
       "      <td>0.606855</td>\n",
       "      <td>0.633838</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.527239</td>\n",
       "      <td>0.526866</td>\n",
       "      <td>0.530426</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.370146</td>\n",
       "      <td>0.563013</td>\n",
       "      <td>0.560506</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.417021</td>\n",
       "      <td>0.505998</td>\n",
       "      <td>0.508322</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.521199</td>\n",
       "      <td>0.527299</td>\n",
       "      <td>0.536234</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.530130</td>\n",
       "      <td>0.533854</td>\n",
       "      <td>0.550193</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.496568</td>\n",
       "      <td>0.505208</td>\n",
       "      <td>0.507722</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.381940</td>\n",
       "      <td>0.424107</td>\n",
       "      <td>0.368726</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453190</td>\n",
       "      <td>0.461765</td>\n",
       "      <td>0.449807</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.524405</td>\n",
       "      <td>0.524321</td>\n",
       "      <td>0.527146</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.568279</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.570707</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.554924</td>\n",
       "      <td>0.554924</td>\n",
       "      <td>0.554924</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.475177</td>\n",
       "      <td>0.476812</td>\n",
       "      <td>0.474747</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.554018</td>\n",
       "      <td>0.555250</td>\n",
       "      <td>0.553051</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.594929</td>\n",
       "      <td>0.590407</td>\n",
       "      <td>0.610784</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.591908</td>\n",
       "      <td>0.587750</td>\n",
       "      <td>0.608356</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.524041</td>\n",
       "      <td>0.525538</td>\n",
       "      <td>0.531293</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.458067</td>\n",
       "      <td>0.443050</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.503472</td>\n",
       "      <td>0.504762</td>\n",
       "      <td>0.505792</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.458067</td>\n",
       "      <td>0.443050</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.514706</td>\n",
       "      <td>0.523573</td>\n",
       "      <td>0.536680</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.564058</td>\n",
       "      <td>0.562229</td>\n",
       "      <td>0.572601</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.529126</td>\n",
       "      <td>0.530239</td>\n",
       "      <td>0.535985</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.609847</td>\n",
       "      <td>0.605040</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.634810</td>\n",
       "      <td>0.632884</td>\n",
       "      <td>0.636995</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.520487</td>\n",
       "      <td>0.520368</td>\n",
       "      <td>0.520631</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.571085</td>\n",
       "      <td>0.569884</td>\n",
       "      <td>0.572555</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.540620</td>\n",
       "      <td>0.539597</td>\n",
       "      <td>0.542562</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.557126</td>\n",
       "      <td>0.557126</td>\n",
       "      <td>0.557126</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.525217</td>\n",
       "      <td>0.524658</td>\n",
       "      <td>0.526062</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.489644</td>\n",
       "      <td>0.494118</td>\n",
       "      <td>0.492278</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.572234</td>\n",
       "      <td>0.568896</td>\n",
       "      <td>0.599421</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.503472</td>\n",
       "      <td>0.504762</td>\n",
       "      <td>0.505792</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.532638</td>\n",
       "      <td>0.533268</td>\n",
       "      <td>0.532197</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.547164</td>\n",
       "      <td>0.546540</td>\n",
       "      <td>0.547980</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>0.613043</td>\n",
       "      <td>0.623106</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.498537</td>\n",
       "      <td>0.500764</td>\n",
       "      <td>0.500631</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.571017</td>\n",
       "      <td>0.577190</td>\n",
       "      <td>0.567614</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.607269</td>\n",
       "      <td>0.616081</td>\n",
       "      <td>0.601682</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.559933</td>\n",
       "      <td>0.560343</td>\n",
       "      <td>0.559553</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.579837</td>\n",
       "      <td>0.579837</td>\n",
       "      <td>0.579837</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                agg  \\\n",
       "0                                                                                                                              mean   \n",
       "1                                                                                                                               max   \n",
       "2                                                                                                                               min   \n",
       "3                                                                                                                     weighted_mean   \n",
       "4                                                                                                                              mean   \n",
       "5                                                                                                                               max   \n",
       "6                                                                                                                               min   \n",
       "7                                                                                                                     weighted_mean   \n",
       "8                                                                                                                              mean   \n",
       "9                                                                                                                               max   \n",
       "10                                                                                                                              min   \n",
       "11                                                                                                                    weighted_mean   \n",
       "12                                                                                                                             mean   \n",
       "13                                                                                                                              max   \n",
       "14                                                                                                                              min   \n",
       "15                                                                                                                    weighted_mean   \n",
       "16                                                                                                                             mean   \n",
       "17                                                                                                                              max   \n",
       "18                                                                                                                              min   \n",
       "19                                                                                                                    weighted_mean   \n",
       "20                                                                                                                             mean   \n",
       "21                                                                                                                              max   \n",
       "22                                                                                                                              min   \n",
       "23                                                                                                                    weighted_mean   \n",
       "24                                                                                                                             mean   \n",
       "25                                                                                                                              max   \n",
       "26                                                                                                                              min   \n",
       "27                                                                                                                    weighted_mean   \n",
       "28                                                                                                                             mean   \n",
       "29                                                                                                                              max   \n",
       "30                                                                                                                              min   \n",
       "31                                                                                                                    weighted_mean   \n",
       "32                                                                                                                             mean   \n",
       "33                                                                                                                              max   \n",
       "34                                                                                                                              min   \n",
       "35                                                                                                                    weighted_mean   \n",
       "36                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "37                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "38                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "39                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "40                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "41                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "42                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "43                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "44                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "45                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "46                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "47                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "48                                                                                                                             mean   \n",
       "49                                                                                                                              max   \n",
       "50                                                                                                                              min   \n",
       "51                                                                                                                    weighted_mean   \n",
       "52                                                                                                                             mean   \n",
       "53                                                                                                                              max   \n",
       "54                                                                                                                              min   \n",
       "55                                                                                                                    weighted_mean   \n",
       "56                                                                                                                             mean   \n",
       "57                                                                                                                              max   \n",
       "58                                                                                                                              min   \n",
       "59                                                                                                                    weighted_mean   \n",
       "60                                                                                                                             mean   \n",
       "61                                                                                                                              max   \n",
       "62                                                                                                                              min   \n",
       "63                                                                                                                    weighted_mean   \n",
       "64                                                                                                                             mean   \n",
       "65                                                                                                                              max   \n",
       "66                                                                                                                              min   \n",
       "67                                                                                                                    weighted_mean   \n",
       "68                                                                                                                             mean   \n",
       "69                                                                                                                              max   \n",
       "70                                                                                                                              min   \n",
       "71                                                                                                                    weighted_mean   \n",
       "72                                                                                                                             mean   \n",
       "73                                                                                                                              max   \n",
       "74                                                                                                                              min   \n",
       "75                                                                                                                    weighted_mean   \n",
       "76                                                                                                                             mean   \n",
       "77                                                                                                                              max   \n",
       "78                                                                                                                              min   \n",
       "79                                                                                                                    weighted_mean   \n",
       "80                                                                                                                             mean   \n",
       "81                                                                                                                              max   \n",
       "82                                                                                                                              min   \n",
       "83                                                                                                                    weighted_mean   \n",
       "84                                                                                                                             mean   \n",
       "85                                                                                                                              max   \n",
       "86                                                                                                                              min   \n",
       "87                                                                                                                    weighted_mean   \n",
       "88                                                                                                                             mean   \n",
       "89                                                                                                                              max   \n",
       "90                                                                                                                              min   \n",
       "91                                                                                                                    weighted_mean   \n",
       "92                                                                                                                             mean   \n",
       "93                                                                                                                              max   \n",
       "94                                                                                                                              min   \n",
       "95                                                                                                                    weighted_mean   \n",
       "96                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "97                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "98                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "99                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "100                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "101                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "102                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "103                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "104                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "105                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "106                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "107                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "108                                                                                                                            mean   \n",
       "109                                                                                                                             max   \n",
       "110                                                                                                                             min   \n",
       "111                                                                                                                   weighted_mean   \n",
       "112                                                                                                                            mean   \n",
       "113                                                                                                                             max   \n",
       "114                                                                                                                             min   \n",
       "115                                                                                                                   weighted_mean   \n",
       "116                                                                                                                            mean   \n",
       "117                                                                                                                             max   \n",
       "118                                                                                                                             min   \n",
       "119                                                                                                                   weighted_mean   \n",
       "120  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "121            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "122            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "123        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "124  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "125            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "126            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "127        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "128  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "129            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "130            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "131        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "132                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "133                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "134                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "135                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "136                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "137                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "138                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "139                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "140                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "141                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "142                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "143                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "144                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "145                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "146                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "147                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "148                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "149                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "150                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "151                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "152                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "153                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "154                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "155                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "\n",
       "       dataset        f1      prec       rec                        zc  \n",
       "0    Wikipedia  0.533800  0.532799  0.541506                linguistic  \n",
       "1    Wikipedia  0.525217  0.524658  0.526062                linguistic  \n",
       "2    Wikipedia  0.388889  0.400000  0.378378                linguistic  \n",
       "3    Wikipedia  0.571096  0.566362  0.583977                linguistic  \n",
       "4     WikiNews  0.563242  0.572574  0.559975                linguistic  \n",
       "5     WikiNews  0.582940  0.580106  0.595328                linguistic  \n",
       "6     WikiNews  0.521493  0.523958  0.529040                linguistic  \n",
       "7     WikiNews  0.623095  0.625245  0.621212                linguistic  \n",
       "8         News  0.596406  0.595250  0.597694                linguistic  \n",
       "9         News  0.537863  0.536892  0.541782                linguistic  \n",
       "10        News  0.610749  0.610056  0.611477                linguistic  \n",
       "11        News  0.538231  0.546794  0.536755                linguistic  \n",
       "12   Wikipedia  0.522994  0.535652  0.558880                 frequency  \n",
       "13   Wikipedia  0.429443  0.447917  0.422780                 frequency  \n",
       "14   Wikipedia  0.469206  0.506410  0.511583                 frequency  \n",
       "15   Wikipedia  0.447668  0.476797  0.460425                 frequency  \n",
       "16    WikiNews  0.584572  0.586683  0.613005                 frequency  \n",
       "17    WikiNews  0.588652  0.585797  0.593434                 frequency  \n",
       "18    WikiNews  0.532338  0.535786  0.544823                 frequency  \n",
       "19    WikiNews  0.592732  0.590118  0.611111                 frequency  \n",
       "20        News  0.615366  0.610312  0.623700                 frequency  \n",
       "21        News  0.556848  0.555003  0.560420                 frequency  \n",
       "22        News  0.586285  0.582936  0.592060                 frequency  \n",
       "23        News  0.592281  0.589176  0.596914                 frequency  \n",
       "24   Wikipedia  0.514706  0.523573  0.536680            language_model  \n",
       "25   Wikipedia  0.588246  0.584444  0.628378            language_model  \n",
       "26   Wikipedia  0.456485  0.472698  0.458494            language_model  \n",
       "27   Wikipedia  0.398437  0.442266  0.397683            language_model  \n",
       "28    WikiNews  0.625842  0.622619  0.630051            language_model  \n",
       "29    WikiNews  0.584301  0.583165  0.604167            language_model  \n",
       "30    WikiNews  0.556033  0.555172  0.565657            language_model  \n",
       "31    WikiNews  0.584596  0.584596  0.584596            language_model  \n",
       "32        News  0.603154  0.599625  0.608270            language_model  \n",
       "33        News  0.606015  0.600849  0.616418            language_model  \n",
       "34        News  0.543301  0.542015  0.546637            language_model  \n",
       "35        News  0.565814  0.563304  0.575069            language_model  \n",
       "36   Wikipedia  0.463005  0.476562  0.465251                    corpus  \n",
       "37   Wikipedia  0.468372  0.488095  0.480695                    corpus  \n",
       "38   Wikipedia  0.503792  0.510033  0.514479                    corpus  \n",
       "39   Wikipedia  0.424242  0.455172  0.424710                    corpus  \n",
       "40    WikiNews  0.609847  0.605040  0.625000                    corpus  \n",
       "41    WikiNews  0.568279  0.566667  0.570707                    corpus  \n",
       "42    WikiNews  0.575940  0.576503  0.597222                    corpus  \n",
       "43    WikiNews  0.525253  0.525253  0.525253                    corpus  \n",
       "44        News  0.640014  0.634088  0.648838                    corpus  \n",
       "45        News  0.612230  0.607080  0.621273                    corpus  \n",
       "46        News  0.601606  0.597053  0.609917                    corpus  \n",
       "47        News  0.604682  0.600186  0.612344                    corpus  \n",
       "48   Wikipedia  0.391933  0.439084  0.390927          psycholinguistic  \n",
       "49   Wikipedia  0.461444  0.484220  0.473938          psycholinguistic  \n",
       "50   Wikipedia  0.505062  0.533962  0.560811          psycholinguistic  \n",
       "51   Wikipedia  0.468372  0.488095  0.480695          psycholinguistic  \n",
       "52    WikiNews  0.486730  0.508744  0.511995          psycholinguistic  \n",
       "53    WikiNews  0.609750  0.609443  0.642677          psycholinguistic  \n",
       "54    WikiNews  0.653841  0.660147  0.719697          psycholinguistic  \n",
       "55    WikiNews  0.653042  0.646547  0.686237          psycholinguistic  \n",
       "56        News  0.584802  0.581140  0.599428          psycholinguistic  \n",
       "57        News  0.515764  0.523385  0.531380          psycholinguistic  \n",
       "58        News  0.531648  0.543601  0.561460          psycholinguistic  \n",
       "59        News  0.557252  0.555758  0.567788          psycholinguistic  \n",
       "60   Wikipedia  0.549722  0.547225  0.555019                   wordnet  \n",
       "61   Wikipedia  0.589868  0.583156  0.612934                   wordnet  \n",
       "62   Wikipedia  0.507379  0.526599  0.545367                   wordnet  \n",
       "63   Wikipedia  0.514706  0.523573  0.536680                   wordnet  \n",
       "64    WikiNews  0.547164  0.546540  0.547980                   wordnet  \n",
       "65    WikiNews  0.539498  0.538690  0.541035                   wordnet  \n",
       "66    WikiNews  0.618551  0.613095  0.631944                   wordnet  \n",
       "67    WikiNews  0.599765  0.595633  0.609217                   wordnet  \n",
       "68        News  0.535161  0.534415  0.539355                   wordnet  \n",
       "69        News  0.589274  0.586016  0.594487                   wordnet  \n",
       "70        News  0.562495  0.560726  0.565274                   wordnet  \n",
       "71        News  0.553358  0.556817  0.551404                   wordnet  \n",
       "72   Wikipedia  0.492219  0.509936  0.516409                   dbpedia  \n",
       "73   Wikipedia  0.516863  0.549285  0.589768                   dbpedia  \n",
       "74   Wikipedia  0.327163  0.448500  0.412162                   dbpedia  \n",
       "75   Wikipedia  0.481390  0.522105  0.540541                   dbpedia  \n",
       "76    WikiNews  0.576422  0.606383  0.648359                   dbpedia  \n",
       "77    WikiNews  0.453982  0.475291  0.466540                   dbpedia  \n",
       "78    WikiNews  0.530630  0.563830  0.589015                   dbpedia  \n",
       "79    WikiNews  0.579732  0.600455  0.639520                   dbpedia  \n",
       "80        News  0.481041  0.495733  0.494105                   dbpedia  \n",
       "81        News  0.478251  0.482892  0.479369                   dbpedia  \n",
       "82        News  0.502047  0.510188  0.513523                   dbpedia  \n",
       "83        News  0.490690  0.510643  0.515257                   dbpedia  \n",
       "84   Wikipedia  0.411743  0.438676  0.402510          brown_clustering  \n",
       "85   Wikipedia  0.473539  0.518315  0.533784          brown_clustering  \n",
       "86   Wikipedia  0.450000  0.507246  0.513514          brown_clustering  \n",
       "87   Wikipedia  0.447668  0.476797  0.460425          brown_clustering  \n",
       "88    WikiNews  0.592432  0.596728  0.628788          brown_clustering  \n",
       "89    WikiNews  0.575319  0.584848  0.614899          brown_clustering  \n",
       "90    WikiNews  0.530630  0.563830  0.589015          brown_clustering  \n",
       "91    WikiNews  0.534896  0.546523  0.562500          brown_clustering  \n",
       "92        News  0.549863  0.548807  0.558859          brown_clustering  \n",
       "93        News  0.543047  0.551296  0.571169          brown_clustering  \n",
       "94        News  0.464810  0.489806  0.485264          brown_clustering  \n",
       "95        News  0.542977  0.541826  0.548284          brown_clustering  \n",
       "96   Wikipedia  0.588246  0.584444  0.628378                  semantic  \n",
       "97   Wikipedia  0.489644  0.494118  0.492278                  semantic  \n",
       "98   Wikipedia  0.530130  0.533854  0.550193                  semantic  \n",
       "99   Wikipedia  0.503792  0.510033  0.514479                  semantic  \n",
       "100   WikiNews  0.558320  0.559476  0.574495                  semantic  \n",
       "101   WikiNews  0.510603  0.530373  0.541667                  semantic  \n",
       "102   WikiNews  0.575940  0.576503  0.597222                  semantic  \n",
       "103   WikiNews  0.516775  0.524020  0.530934                  semantic  \n",
       "104       News  0.494145  0.504627  0.506241                  semantic  \n",
       "105       News  0.443424  0.496201  0.494365                  semantic  \n",
       "106       News  0.524041  0.525538  0.531293                  semantic  \n",
       "107       News  0.561944  0.559678  0.566921                  semantic  \n",
       "108  Wikipedia  0.496568  0.505208  0.507722                dictionary  \n",
       "109  Wikipedia  0.261268  0.475342  0.473938                dictionary  \n",
       "110  Wikipedia  0.517808  0.517361  0.519305                dictionary  \n",
       "111  Wikipedia  0.530130  0.533854  0.550193                dictionary  \n",
       "112   WikiNews  0.574661  0.572917  0.588384                dictionary  \n",
       "113   WikiNews  0.339229  0.507310  0.506313                dictionary  \n",
       "114   WikiNews  0.433026  0.544562  0.553030                dictionary  \n",
       "115   WikiNews  0.610282  0.606855  0.633838                dictionary  \n",
       "116       News  0.527239  0.526866  0.530426                dictionary  \n",
       "117       News  0.370146  0.563013  0.560506                dictionary  \n",
       "118       News  0.417021  0.505998  0.508322                dictionary  \n",
       "119       News  0.521199  0.527299  0.536234                dictionary  \n",
       "120  Wikipedia  0.530130  0.533854  0.550193           corpus+semantic  \n",
       "121  Wikipedia  0.496568  0.505208  0.507722           corpus+semantic  \n",
       "122  Wikipedia  0.381940  0.424107  0.368726           corpus+semantic  \n",
       "123  Wikipedia  0.453190  0.461765  0.449807           corpus+semantic  \n",
       "124   WikiNews  0.524405  0.524321  0.527146           corpus+semantic  \n",
       "125   WikiNews  0.568279  0.566667  0.570707           corpus+semantic  \n",
       "126   WikiNews  0.554924  0.554924  0.554924           corpus+semantic  \n",
       "127   WikiNews  0.475177  0.476812  0.474747           corpus+semantic  \n",
       "128       News  0.554018  0.555250  0.553051           corpus+semantic  \n",
       "129       News  0.594929  0.590407  0.610784           corpus+semantic  \n",
       "130       News  0.591908  0.587750  0.608356           corpus+semantic  \n",
       "131       News  0.524041  0.525538  0.531293           corpus+semantic  \n",
       "132  Wikipedia  0.447214  0.458067  0.443050  wordnet+psycholinguistic  \n",
       "133  Wikipedia  0.503472  0.504762  0.505792  wordnet+psycholinguistic  \n",
       "134  Wikipedia  0.447214  0.458067  0.443050  wordnet+psycholinguistic  \n",
       "135  Wikipedia  0.514706  0.523573  0.536680  wordnet+psycholinguistic  \n",
       "136   WikiNews  0.564058  0.562229  0.572601  wordnet+psycholinguistic  \n",
       "137   WikiNews  0.529126  0.530239  0.535985  wordnet+psycholinguistic  \n",
       "138   WikiNews  0.609847  0.605040  0.625000  wordnet+psycholinguistic  \n",
       "139   WikiNews  0.634810  0.632884  0.636995  wordnet+psycholinguistic  \n",
       "140       News  0.520487  0.520368  0.520631  wordnet+psycholinguistic  \n",
       "141       News  0.571085  0.569884  0.572555  wordnet+psycholinguistic  \n",
       "142       News  0.540620  0.539597  0.542562  wordnet+psycholinguistic  \n",
       "143       News  0.557126  0.557126  0.557126  wordnet+psycholinguistic  \n",
       "144  Wikipedia  0.525217  0.524658  0.526062                       all  \n",
       "145  Wikipedia  0.489644  0.494118  0.492278                       all  \n",
       "146  Wikipedia  0.572234  0.568896  0.599421                       all  \n",
       "147  Wikipedia  0.503472  0.504762  0.505792                       all  \n",
       "148   WikiNews  0.532638  0.533268  0.532197                       all  \n",
       "149   WikiNews  0.547164  0.546540  0.547980                       all  \n",
       "150   WikiNews  0.617021  0.613043  0.623106                       all  \n",
       "151   WikiNews  0.498537  0.500764  0.500631                       all  \n",
       "152       News  0.571017  0.577190  0.567614                       all  \n",
       "153       News  0.607269  0.616081  0.601682                       all  \n",
       "154       News  0.559933  0.560343  0.559553                       all  \n",
       "155       News  0.579837  0.579837  0.579837                       all  "
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_eval_data_dt_dswp = create_eval_df_from_results_macro(results, False)\n",
    "feature_eval_data_dt_dswp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.640014</td>\n",
       "      <td>0.634088</td>\n",
       "      <td>0.648838</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.653841</td>\n",
       "      <td>0.660147</td>\n",
       "      <td>0.719697</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.589868</td>\n",
       "      <td>0.583156</td>\n",
       "      <td>0.612934</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              agg    dataset  \\\n",
       "44  (mean, <function agg_feat_num_average at 0x000000FF52232D90>)       News   \n",
       "54                                                            min   WikiNews   \n",
       "61                                                            max  Wikipedia   \n",
       "\n",
       "          f1      prec       rec                zc  \n",
       "44  0.640014  0.634088  0.648838            corpus  \n",
       "54  0.653841  0.660147  0.719697  psycholinguistic  \n",
       "61  0.589868  0.583156  0.612934           wordnet  "
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = feature_eval_data_dt_dswp.groupby(['dataset'])['f1'].transform(max) == feature_eval_data_dt_dswp['f1']\n",
    "feature_eval_data_dt_dswp[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.2.6 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "results = [Result(fs, fs.fc, fs.agg,\n",
    "        precision_recall_fscore_support(*logistic_regression(*transform_feat_to_num(remove_labels_for_binary_df(fs.train), \n",
    "        remove_labels_for_binary_df(fs.test))), average='macro')) for fs in all_fc_datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.518750</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.528958</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.548718</td>\n",
       "      <td>0.593496</td>\n",
       "      <td>0.544402</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.504923</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.515444</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.511654</td>\n",
       "      <td>0.590196</td>\n",
       "      <td>0.522201</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.509022</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.531566</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.545894</td>\n",
       "      <td>0.693258</td>\n",
       "      <td>0.554293</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.469448</td>\n",
       "      <td>0.551282</td>\n",
       "      <td>0.508838</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.545894</td>\n",
       "      <td>0.693258</td>\n",
       "      <td>0.554293</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.587791</td>\n",
       "      <td>0.764401</td>\n",
       "      <td>0.579577</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.584554</td>\n",
       "      <td>0.740216</td>\n",
       "      <td>0.577150</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.554814</td>\n",
       "      <td>0.688940</td>\n",
       "      <td>0.556865</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.610060</td>\n",
       "      <td>0.763145</td>\n",
       "      <td>0.595007</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.511654</td>\n",
       "      <td>0.590196</td>\n",
       "      <td>0.522201</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.464387</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.501894</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.563713</td>\n",
       "      <td>0.645349</td>\n",
       "      <td>0.563131</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.524319</td>\n",
       "      <td>0.630368</td>\n",
       "      <td>0.536581</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.493034</td>\n",
       "      <td>0.562667</td>\n",
       "      <td>0.516297</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.472989</td>\n",
       "      <td>0.645349</td>\n",
       "      <td>0.513003</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.479730</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.479730</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.479730</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.479730</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.438972</td>\n",
       "      <td>0.392720</td>\n",
       "      <td>0.497573</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.518750</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.528958</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.464387</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.501894</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.538393</td>\n",
       "      <td>0.642045</td>\n",
       "      <td>0.547348</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.509933</td>\n",
       "      <td>0.606333</td>\n",
       "      <td>0.527653</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.514622</td>\n",
       "      <td>0.648810</td>\n",
       "      <td>0.532507</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.472989</td>\n",
       "      <td>0.645349</td>\n",
       "      <td>0.513003</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.509022</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.531566</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.479842</td>\n",
       "      <td>0.887097</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.561718</td>\n",
       "      <td>0.895604</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.501630</td>\n",
       "      <td>0.647638</td>\n",
       "      <td>0.526006</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.457156</td>\n",
       "      <td>0.644231</td>\n",
       "      <td>0.506501</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.438972</td>\n",
       "      <td>0.392720</td>\n",
       "      <td>0.497573</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.474593</td>\n",
       "      <td>0.635870</td>\n",
       "      <td>0.515783</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.455659</td>\n",
       "      <td>0.560489</td>\n",
       "      <td>0.504074</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.454167</td>\n",
       "      <td>0.518411</td>\n",
       "      <td>0.501647</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.438972</td>\n",
       "      <td>0.392720</td>\n",
       "      <td>0.497573</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.438972</td>\n",
       "      <td>0.392720</td>\n",
       "      <td>0.497573</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.438972</td>\n",
       "      <td>0.392720</td>\n",
       "      <td>0.497573</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.438972</td>\n",
       "      <td>0.392720</td>\n",
       "      <td>0.497573</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.438972</td>\n",
       "      <td>0.392720</td>\n",
       "      <td>0.497573</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.436559</td>\n",
       "      <td>0.391892</td>\n",
       "      <td>0.492718</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.451194</td>\n",
       "      <td>0.475911</td>\n",
       "      <td>0.496793</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.499447</td>\n",
       "      <td>0.619455</td>\n",
       "      <td>0.523578</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.436559</td>\n",
       "      <td>0.391892</td>\n",
       "      <td>0.492718</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.509022</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.531566</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.603376</td>\n",
       "      <td>0.713663</td>\n",
       "      <td>0.592803</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.479842</td>\n",
       "      <td>0.887097</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.452679</td>\n",
       "      <td>0.492996</td>\n",
       "      <td>0.499220</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.549164</td>\n",
       "      <td>0.652439</td>\n",
       "      <td>0.552011</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.438972</td>\n",
       "      <td>0.392720</td>\n",
       "      <td>0.497573</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.472989</td>\n",
       "      <td>0.645349</td>\n",
       "      <td>0.513003</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.553656</td>\n",
       "      <td>0.769444</td>\n",
       "      <td>0.561237</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.438972</td>\n",
       "      <td>0.392720</td>\n",
       "      <td>0.497573</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.455659</td>\n",
       "      <td>0.560489</td>\n",
       "      <td>0.504074</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.437768</td>\n",
       "      <td>0.392308</td>\n",
       "      <td>0.495146</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.515583</td>\n",
       "      <td>0.723443</td>\n",
       "      <td>0.538510</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.546491</td>\n",
       "      <td>0.720572</td>\n",
       "      <td>0.552791</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.559399</td>\n",
       "      <td>0.653689</td>\n",
       "      <td>0.558512</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.438972</td>\n",
       "      <td>0.392720</td>\n",
       "      <td>0.497573</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.472989</td>\n",
       "      <td>0.645349</td>\n",
       "      <td>0.513003</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                agg  \\\n",
       "0                                                                                                                              mean   \n",
       "1                                                                                                                               max   \n",
       "2                                                                                                                               min   \n",
       "3                                                                                                                     weighted_mean   \n",
       "4                                                                                                                              mean   \n",
       "5                                                                                                                               max   \n",
       "6                                                                                                                               min   \n",
       "7                                                                                                                     weighted_mean   \n",
       "8                                                                                                                              mean   \n",
       "9                                                                                                                               max   \n",
       "10                                                                                                                              min   \n",
       "11                                                                                                                    weighted_mean   \n",
       "12                                                                                                                             mean   \n",
       "13                                                                                                                              max   \n",
       "14                                                                                                                              min   \n",
       "15                                                                                                                    weighted_mean   \n",
       "16                                                                                                                             mean   \n",
       "17                                                                                                                              max   \n",
       "18                                                                                                                              min   \n",
       "19                                                                                                                    weighted_mean   \n",
       "20                                                                                                                             mean   \n",
       "21                                                                                                                              max   \n",
       "22                                                                                                                              min   \n",
       "23                                                                                                                    weighted_mean   \n",
       "24                                                                                                                             mean   \n",
       "25                                                                                                                              max   \n",
       "26                                                                                                                              min   \n",
       "27                                                                                                                    weighted_mean   \n",
       "28                                                                                                                             mean   \n",
       "29                                                                                                                              max   \n",
       "30                                                                                                                              min   \n",
       "31                                                                                                                    weighted_mean   \n",
       "32                                                                                                                             mean   \n",
       "33                                                                                                                              max   \n",
       "34                                                                                                                              min   \n",
       "35                                                                                                                    weighted_mean   \n",
       "36                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "37                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "38                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "39                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "40                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "41                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "42                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "43                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "44                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "45                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "46                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "47                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "48                                                                                                                             mean   \n",
       "49                                                                                                                              max   \n",
       "50                                                                                                                              min   \n",
       "51                                                                                                                    weighted_mean   \n",
       "52                                                                                                                             mean   \n",
       "53                                                                                                                              max   \n",
       "54                                                                                                                              min   \n",
       "55                                                                                                                    weighted_mean   \n",
       "56                                                                                                                             mean   \n",
       "57                                                                                                                              max   \n",
       "58                                                                                                                              min   \n",
       "59                                                                                                                    weighted_mean   \n",
       "60                                                                                                                             mean   \n",
       "61                                                                                                                              max   \n",
       "62                                                                                                                              min   \n",
       "63                                                                                                                    weighted_mean   \n",
       "64                                                                                                                             mean   \n",
       "65                                                                                                                              max   \n",
       "66                                                                                                                              min   \n",
       "67                                                                                                                    weighted_mean   \n",
       "68                                                                                                                             mean   \n",
       "69                                                                                                                              max   \n",
       "70                                                                                                                              min   \n",
       "71                                                                                                                    weighted_mean   \n",
       "72                                                                                                                             mean   \n",
       "73                                                                                                                              max   \n",
       "74                                                                                                                              min   \n",
       "75                                                                                                                    weighted_mean   \n",
       "76                                                                                                                             mean   \n",
       "77                                                                                                                              max   \n",
       "78                                                                                                                              min   \n",
       "79                                                                                                                    weighted_mean   \n",
       "80                                                                                                                             mean   \n",
       "81                                                                                                                              max   \n",
       "82                                                                                                                              min   \n",
       "83                                                                                                                    weighted_mean   \n",
       "84                                                                                                                             mean   \n",
       "85                                                                                                                              max   \n",
       "86                                                                                                                              min   \n",
       "87                                                                                                                    weighted_mean   \n",
       "88                                                                                                                             mean   \n",
       "89                                                                                                                              max   \n",
       "90                                                                                                                              min   \n",
       "91                                                                                                                    weighted_mean   \n",
       "92                                                                                                                             mean   \n",
       "93                                                                                                                              max   \n",
       "94                                                                                                                              min   \n",
       "95                                                                                                                    weighted_mean   \n",
       "96                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "97                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "98                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "99                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "100                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "101                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "102                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "103                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "104                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "105                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "106                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "107                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "108                                                                                                                            mean   \n",
       "109                                                                                                                             max   \n",
       "110                                                                                                                             min   \n",
       "111                                                                                                                   weighted_mean   \n",
       "112                                                                                                                            mean   \n",
       "113                                                                                                                             max   \n",
       "114                                                                                                                             min   \n",
       "115                                                                                                                   weighted_mean   \n",
       "116                                                                                                                            mean   \n",
       "117                                                                                                                             max   \n",
       "118                                                                                                                             min   \n",
       "119                                                                                                                   weighted_mean   \n",
       "120  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "121            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "122            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "123        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "124  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "125            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "126            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "127        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "128  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "129            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "130            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "131        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "132                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "133                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "134                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "135                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "136                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "137                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "138                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "139                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "140                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "141                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "142                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "143                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "144                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "145                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "146                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "147                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "148                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "149                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "150                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "151                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "152                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "153                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "154                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "155                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "\n",
       "       dataset        f1      prec       rec                        zc  \n",
       "0    Wikipedia  0.518750  0.674419  0.528958                linguistic  \n",
       "1    Wikipedia  0.548718  0.593496  0.544402                linguistic  \n",
       "2    Wikipedia  0.504923  0.547619  0.515444                linguistic  \n",
       "3    Wikipedia  0.511654  0.590196  0.522201                linguistic  \n",
       "4     WikiNews  0.509022  0.638889  0.531566                linguistic  \n",
       "5     WikiNews  0.545894  0.693258  0.554293                linguistic  \n",
       "6     WikiNews  0.469448  0.551282  0.508838                linguistic  \n",
       "7     WikiNews  0.545894  0.693258  0.554293                linguistic  \n",
       "8         News  0.587791  0.764401  0.579577                linguistic  \n",
       "9         News  0.584554  0.740216  0.577150                linguistic  \n",
       "10        News  0.554814  0.688940  0.556865                linguistic  \n",
       "11        News  0.610060  0.763145  0.595007                linguistic  \n",
       "12   Wikipedia  0.511654  0.590196  0.522201                 frequency  \n",
       "13   Wikipedia  0.456790  0.420455  0.500000                 frequency  \n",
       "14   Wikipedia  0.453416  0.419540  0.493243                 frequency  \n",
       "15   Wikipedia  0.453416  0.419540  0.493243                 frequency  \n",
       "16    WikiNews  0.464387  0.508333  0.501894                 frequency  \n",
       "17    WikiNews  0.563713  0.645349  0.563131                 frequency  \n",
       "18    WikiNews  0.433735  0.382979  0.500000                 frequency  \n",
       "19    WikiNews  0.433735  0.382979  0.500000                 frequency  \n",
       "20        News  0.524319  0.630368  0.536581                 frequency  \n",
       "21        News  0.493034  0.562667  0.516297                 frequency  \n",
       "22        News  0.440171  0.393130  0.500000                 frequency  \n",
       "23        News  0.472989  0.645349  0.513003                 frequency  \n",
       "24   Wikipedia  0.446541  0.417647  0.479730            language_model  \n",
       "25   Wikipedia  0.446541  0.417647  0.479730            language_model  \n",
       "26   Wikipedia  0.446541  0.417647  0.479730            language_model  \n",
       "27   Wikipedia  0.446541  0.417647  0.479730            language_model  \n",
       "28    WikiNews  0.433735  0.382979  0.500000            language_model  \n",
       "29    WikiNews  0.433735  0.382979  0.500000            language_model  \n",
       "30    WikiNews  0.433735  0.382979  0.500000            language_model  \n",
       "31    WikiNews  0.433735  0.382979  0.500000            language_model  \n",
       "32        News  0.440171  0.393130  0.500000            language_model  \n",
       "33        News  0.438972  0.392720  0.497573            language_model  \n",
       "34        News  0.440171  0.393130  0.500000            language_model  \n",
       "35        News  0.440171  0.393130  0.500000            language_model  \n",
       "36   Wikipedia  0.518750  0.674419  0.528958                    corpus  \n",
       "37   Wikipedia  0.456790  0.420455  0.500000                    corpus  \n",
       "38   Wikipedia  0.453416  0.419540  0.493243                    corpus  \n",
       "39   Wikipedia  0.453416  0.419540  0.493243                    corpus  \n",
       "40    WikiNews  0.464387  0.508333  0.501894                    corpus  \n",
       "41    WikiNews  0.538393  0.642045  0.547348                    corpus  \n",
       "42    WikiNews  0.433735  0.382979  0.500000                    corpus  \n",
       "43    WikiNews  0.433735  0.382979  0.500000                    corpus  \n",
       "44        News  0.509933  0.606333  0.527653                    corpus  \n",
       "45        News  0.514622  0.648810  0.532507                    corpus  \n",
       "46        News  0.440171  0.393130  0.500000                    corpus  \n",
       "47        News  0.472989  0.645349  0.513003                    corpus  \n",
       "48   Wikipedia  0.456790  0.420455  0.500000          psycholinguistic  \n",
       "49   Wikipedia  0.456790  0.420455  0.500000          psycholinguistic  \n",
       "50   Wikipedia  0.456790  0.420455  0.500000          psycholinguistic  \n",
       "51   Wikipedia  0.456790  0.420455  0.500000          psycholinguistic  \n",
       "52    WikiNews  0.509022  0.638889  0.531566          psycholinguistic  \n",
       "53    WikiNews  0.479842  0.887097  0.522727          psycholinguistic  \n",
       "54    WikiNews  0.561718  0.895604  0.568182          psycholinguistic  \n",
       "55    WikiNews  0.433735  0.382979  0.500000          psycholinguistic  \n",
       "56        News  0.501630  0.647638  0.526006          psycholinguistic  \n",
       "57        News  0.440171  0.393130  0.500000          psycholinguistic  \n",
       "58        News  0.457156  0.644231  0.506501          psycholinguistic  \n",
       "59        News  0.438972  0.392720  0.497573          psycholinguistic  \n",
       "60   Wikipedia  0.456790  0.420455  0.500000                   wordnet  \n",
       "61   Wikipedia  0.456790  0.420455  0.500000                   wordnet  \n",
       "62   Wikipedia  0.456790  0.420455  0.500000                   wordnet  \n",
       "63   Wikipedia  0.456790  0.420455  0.500000                   wordnet  \n",
       "64    WikiNews  0.433735  0.382979  0.500000                   wordnet  \n",
       "65    WikiNews  0.433735  0.382979  0.500000                   wordnet  \n",
       "66    WikiNews  0.474593  0.635870  0.515783                   wordnet  \n",
       "67    WikiNews  0.433735  0.382979  0.500000                   wordnet  \n",
       "68        News  0.455659  0.560489  0.504074                   wordnet  \n",
       "69        News  0.454167  0.518411  0.501647                   wordnet  \n",
       "70        News  0.440171  0.393130  0.500000                   wordnet  \n",
       "71        News  0.438972  0.392720  0.497573                   wordnet  \n",
       "72   Wikipedia  0.456790  0.420455  0.500000                   dbpedia  \n",
       "73   Wikipedia  0.456790  0.420455  0.500000                   dbpedia  \n",
       "74   Wikipedia  0.456790  0.420455  0.500000                   dbpedia  \n",
       "75   Wikipedia  0.456790  0.420455  0.500000                   dbpedia  \n",
       "76    WikiNews  0.433735  0.382979  0.500000                   dbpedia  \n",
       "77    WikiNews  0.433735  0.382979  0.500000                   dbpedia  \n",
       "78    WikiNews  0.433735  0.382979  0.500000                   dbpedia  \n",
       "79    WikiNews  0.433735  0.382979  0.500000                   dbpedia  \n",
       "80        News  0.438972  0.392720  0.497573                   dbpedia  \n",
       "81        News  0.438972  0.392720  0.497573                   dbpedia  \n",
       "82        News  0.438972  0.392720  0.497573                   dbpedia  \n",
       "83        News  0.438972  0.392720  0.497573                   dbpedia  \n",
       "84   Wikipedia  0.456790  0.420455  0.500000          brown_clustering  \n",
       "85   Wikipedia  0.456790  0.420455  0.500000          brown_clustering  \n",
       "86   Wikipedia  0.453416  0.419540  0.493243          brown_clustering  \n",
       "87   Wikipedia  0.456790  0.420455  0.500000          brown_clustering  \n",
       "88    WikiNews  0.433735  0.382979  0.500000          brown_clustering  \n",
       "89    WikiNews  0.433735  0.382979  0.500000          brown_clustering  \n",
       "90    WikiNews  0.433735  0.382979  0.500000          brown_clustering  \n",
       "91    WikiNews  0.433735  0.382979  0.500000          brown_clustering  \n",
       "92        News  0.440171  0.393130  0.500000          brown_clustering  \n",
       "93        News  0.440171  0.393130  0.500000          brown_clustering  \n",
       "94        News  0.440171  0.393130  0.500000          brown_clustering  \n",
       "95        News  0.440171  0.393130  0.500000          brown_clustering  \n",
       "96   Wikipedia  0.456790  0.420455  0.500000                  semantic  \n",
       "97   Wikipedia  0.456790  0.420455  0.500000                  semantic  \n",
       "98   Wikipedia  0.456790  0.420455  0.500000                  semantic  \n",
       "99   Wikipedia  0.456790  0.420455  0.500000                  semantic  \n",
       "100   WikiNews  0.433735  0.382979  0.500000                  semantic  \n",
       "101   WikiNews  0.433735  0.382979  0.500000                  semantic  \n",
       "102   WikiNews  0.433735  0.382979  0.500000                  semantic  \n",
       "103   WikiNews  0.433735  0.382979  0.500000                  semantic  \n",
       "104       News  0.436559  0.391892  0.492718                  semantic  \n",
       "105       News  0.451194  0.475911  0.496793                  semantic  \n",
       "106       News  0.499447  0.619455  0.523578                  semantic  \n",
       "107       News  0.436559  0.391892  0.492718                  semantic  \n",
       "108  Wikipedia  0.456790  0.420455  0.500000                dictionary  \n",
       "109  Wikipedia  0.456790  0.420455  0.500000                dictionary  \n",
       "110  Wikipedia  0.456790  0.420455  0.500000                dictionary  \n",
       "111  Wikipedia  0.456790  0.420455  0.500000                dictionary  \n",
       "112   WikiNews  0.433735  0.382979  0.500000                dictionary  \n",
       "113   WikiNews  0.433735  0.382979  0.500000                dictionary  \n",
       "114   WikiNews  0.433735  0.382979  0.500000                dictionary  \n",
       "115   WikiNews  0.433735  0.382979  0.500000                dictionary  \n",
       "116       News  0.440171  0.393130  0.500000                dictionary  \n",
       "117       News  0.440171  0.393130  0.500000                dictionary  \n",
       "118       News  0.440171  0.393130  0.500000                dictionary  \n",
       "119       News  0.440171  0.393130  0.500000                dictionary  \n",
       "120  Wikipedia  0.453416  0.419540  0.493243           corpus+semantic  \n",
       "121  Wikipedia  0.456790  0.420455  0.500000           corpus+semantic  \n",
       "122  Wikipedia  0.453416  0.419540  0.493243           corpus+semantic  \n",
       "123  Wikipedia  0.453416  0.419540  0.493243           corpus+semantic  \n",
       "124   WikiNews  0.509022  0.638889  0.531566           corpus+semantic  \n",
       "125   WikiNews  0.603376  0.713663  0.592803           corpus+semantic  \n",
       "126   WikiNews  0.479842  0.887097  0.522727           corpus+semantic  \n",
       "127   WikiNews  0.433735  0.382979  0.500000           corpus+semantic  \n",
       "128       News  0.452679  0.492996  0.499220           corpus+semantic  \n",
       "129       News  0.549164  0.652439  0.552011           corpus+semantic  \n",
       "130       News  0.438972  0.392720  0.497573           corpus+semantic  \n",
       "131       News  0.472989  0.645349  0.513003           corpus+semantic  \n",
       "132  Wikipedia  0.456790  0.420455  0.500000  wordnet+psycholinguistic  \n",
       "133  Wikipedia  0.456790  0.420455  0.500000  wordnet+psycholinguistic  \n",
       "134  Wikipedia  0.456790  0.420455  0.500000  wordnet+psycholinguistic  \n",
       "135  Wikipedia  0.456790  0.420455  0.500000  wordnet+psycholinguistic  \n",
       "136   WikiNews  0.433735  0.382979  0.500000  wordnet+psycholinguistic  \n",
       "137   WikiNews  0.433735  0.382979  0.500000  wordnet+psycholinguistic  \n",
       "138   WikiNews  0.553656  0.769444  0.561237  wordnet+psycholinguistic  \n",
       "139   WikiNews  0.433735  0.382979  0.500000  wordnet+psycholinguistic  \n",
       "140       News  0.438972  0.392720  0.497573  wordnet+psycholinguistic  \n",
       "141       News  0.455659  0.560489  0.504074  wordnet+psycholinguistic  \n",
       "142       News  0.440171  0.393130  0.500000  wordnet+psycholinguistic  \n",
       "143       News  0.437768  0.392308  0.495146  wordnet+psycholinguistic  \n",
       "144  Wikipedia  0.453416  0.419540  0.493243                       all  \n",
       "145  Wikipedia  0.456790  0.420455  0.500000                       all  \n",
       "146  Wikipedia  0.453416  0.419540  0.493243                       all  \n",
       "147  Wikipedia  0.453416  0.419540  0.493243                       all  \n",
       "148   WikiNews  0.433735  0.382979  0.500000                       all  \n",
       "149   WikiNews  0.515583  0.723443  0.538510                       all  \n",
       "150   WikiNews  0.433735  0.382979  0.500000                       all  \n",
       "151   WikiNews  0.433735  0.382979  0.500000                       all  \n",
       "152       News  0.546491  0.720572  0.552791                       all  \n",
       "153       News  0.559399  0.653689  0.558512                       all  \n",
       "154       News  0.438972  0.392720  0.497573                       all  \n",
       "155       News  0.472989  0.645349  0.513003                       all  "
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_eval_data_lr = create_eval_df_from_results_macro(results, False)\n",
    "feature_eval_data_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.548718</td>\n",
       "      <td>0.593496</td>\n",
       "      <td>0.544402</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.610060</td>\n",
       "      <td>0.763145</td>\n",
       "      <td>0.595007</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.603376</td>\n",
       "      <td>0.713663</td>\n",
       "      <td>0.592803</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                      agg  \\\n",
       "1                                                                                                                     max   \n",
       "11                                                                                                          weighted_mean   \n",
       "125  [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "\n",
       "       dataset        f1      prec       rec               zc  \n",
       "1    Wikipedia  0.548718  0.593496  0.544402       linguistic  \n",
       "11        News  0.610060  0.763145  0.595007       linguistic  \n",
       "125   WikiNews  0.603376  0.713663  0.592803  corpus+semantic  "
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = feature_eval_data_lr.groupby(['dataset'])['f1'].transform(max) == feature_eval_data_lr['f1']\n",
    "feature_eval_data_lr[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "results = [Result(fs, fs.fc, fs.agg,\n",
    "        precision_recall_fscore_support(*logistic_regression(*transform_feat_to_num(remove_labels_for_binary_df(fs.train), \n",
    "        remove_labels_for_binary_df(fs.test))), average='macro')) for fs in all_fc_datasets_dswp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.616493</td>\n",
       "      <td>0.733735</td>\n",
       "      <td>0.593629</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.557086</td>\n",
       "      <td>0.564935</td>\n",
       "      <td>0.553089</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.576767</td>\n",
       "      <td>0.762745</td>\n",
       "      <td>0.564672</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.614268</td>\n",
       "      <td>0.614268</td>\n",
       "      <td>0.614268</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.634810</td>\n",
       "      <td>0.632884</td>\n",
       "      <td>0.636995</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.614268</td>\n",
       "      <td>0.614268</td>\n",
       "      <td>0.614268</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.654970</td>\n",
       "      <td>0.647595</td>\n",
       "      <td>0.668561</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.447734</td>\n",
       "      <td>0.443615</td>\n",
       "      <td>0.454837</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.458075</td>\n",
       "      <td>0.456405</td>\n",
       "      <td>0.460558</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.472837</td>\n",
       "      <td>0.471970</td>\n",
       "      <td>0.477549</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.444164</td>\n",
       "      <td>0.442075</td>\n",
       "      <td>0.446775</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.477523</td>\n",
       "      <td>0.501698</td>\n",
       "      <td>0.502896</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.397619</td>\n",
       "      <td>0.461698</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.469570</td>\n",
       "      <td>0.480602</td>\n",
       "      <td>0.472008</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.441270</td>\n",
       "      <td>0.482906</td>\n",
       "      <td>0.469112</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.381793</td>\n",
       "      <td>0.458838</td>\n",
       "      <td>0.446338</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.228454</td>\n",
       "      <td>0.343750</td>\n",
       "      <td>0.389520</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.502486</td>\n",
       "      <td>0.502609</td>\n",
       "      <td>0.502525</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.426525</td>\n",
       "      <td>0.445402</td>\n",
       "      <td>0.428030</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.435738</td>\n",
       "      <td>0.474791</td>\n",
       "      <td>0.462639</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.477082</td>\n",
       "      <td>0.495265</td>\n",
       "      <td>0.493325</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.450176</td>\n",
       "      <td>0.447551</td>\n",
       "      <td>0.454057</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.553484</td>\n",
       "      <td>0.551663</td>\n",
       "      <td>0.559639</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.392117</td>\n",
       "      <td>0.506061</td>\n",
       "      <td>0.510618</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.378512</td>\n",
       "      <td>0.442398</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.332890</td>\n",
       "      <td>0.464348</td>\n",
       "      <td>0.441120</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.400871</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.517375</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.337727</td>\n",
       "      <td>0.528045</td>\n",
       "      <td>0.522096</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.382699</td>\n",
       "      <td>0.517262</td>\n",
       "      <td>0.518308</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.390236</td>\n",
       "      <td>0.494695</td>\n",
       "      <td>0.493687</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.339229</td>\n",
       "      <td>0.507310</td>\n",
       "      <td>0.506313</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.377128</td>\n",
       "      <td>0.529598</td>\n",
       "      <td>0.532854</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.441943</td>\n",
       "      <td>0.511842</td>\n",
       "      <td>0.517164</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.426072</td>\n",
       "      <td>0.547473</td>\n",
       "      <td>0.560333</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.360499</td>\n",
       "      <td>0.492036</td>\n",
       "      <td>0.490638</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.477523</td>\n",
       "      <td>0.501698</td>\n",
       "      <td>0.502896</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.416188</td>\n",
       "      <td>0.480879</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.469570</td>\n",
       "      <td>0.480602</td>\n",
       "      <td>0.472008</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.441270</td>\n",
       "      <td>0.482906</td>\n",
       "      <td>0.469112</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.399605</td>\n",
       "      <td>0.470128</td>\n",
       "      <td>0.460227</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.211338</td>\n",
       "      <td>0.301170</td>\n",
       "      <td>0.328283</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.485316</td>\n",
       "      <td>0.485263</td>\n",
       "      <td>0.486742</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.426525</td>\n",
       "      <td>0.445402</td>\n",
       "      <td>0.428030</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.435738</td>\n",
       "      <td>0.474791</td>\n",
       "      <td>0.462639</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.385432</td>\n",
       "      <td>0.447740</td>\n",
       "      <td>0.423024</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.457476</td>\n",
       "      <td>0.454234</td>\n",
       "      <td>0.466973</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.494208</td>\n",
       "      <td>0.508586</td>\n",
       "      <td>0.511963</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.385458</td>\n",
       "      <td>0.489757</td>\n",
       "      <td>0.481660</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.469206</td>\n",
       "      <td>0.506410</td>\n",
       "      <td>0.511583</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.314997</td>\n",
       "      <td>0.505882</td>\n",
       "      <td>0.507722</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.408533</td>\n",
       "      <td>0.477273</td>\n",
       "      <td>0.457529</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.459273</td>\n",
       "      <td>0.545098</td>\n",
       "      <td>0.558081</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.613075</td>\n",
       "      <td>0.644022</td>\n",
       "      <td>0.700758</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.372269</td>\n",
       "      <td>0.511023</td>\n",
       "      <td>0.511364</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.468559</td>\n",
       "      <td>0.549879</td>\n",
       "      <td>0.565025</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.429449</td>\n",
       "      <td>0.520708</td>\n",
       "      <td>0.528606</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.511217</td>\n",
       "      <td>0.549076</td>\n",
       "      <td>0.572989</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.293883</td>\n",
       "      <td>0.434788</td>\n",
       "      <td>0.433946</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.451683</td>\n",
       "      <td>0.524291</td>\n",
       "      <td>0.535021</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.194139</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.527027</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.314997</td>\n",
       "      <td>0.505882</td>\n",
       "      <td>0.507722</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.421596</td>\n",
       "      <td>0.548104</td>\n",
       "      <td>0.582046</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.410714</td>\n",
       "      <td>0.501053</td>\n",
       "      <td>0.501931</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.175149</td>\n",
       "      <td>0.201124</td>\n",
       "      <td>0.416035</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.175149</td>\n",
       "      <td>0.201124</td>\n",
       "      <td>0.416035</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.510716</td>\n",
       "      <td>0.510714</td>\n",
       "      <td>0.511364</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.175149</td>\n",
       "      <td>0.201124</td>\n",
       "      <td>0.416035</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.199867</td>\n",
       "      <td>0.524089</td>\n",
       "      <td>0.503207</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.199867</td>\n",
       "      <td>0.524089</td>\n",
       "      <td>0.503207</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.292150</td>\n",
       "      <td>0.408989</td>\n",
       "      <td>0.394938</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.453895</td>\n",
       "      <td>0.508255</td>\n",
       "      <td>0.512223</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.516863</td>\n",
       "      <td>0.549285</td>\n",
       "      <td>0.589768</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.516863</td>\n",
       "      <td>0.549285</td>\n",
       "      <td>0.589768</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.516863</td>\n",
       "      <td>0.549285</td>\n",
       "      <td>0.589768</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.516863</td>\n",
       "      <td>0.549285</td>\n",
       "      <td>0.589768</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.189655</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.189655</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.189655</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.189655</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.176101</td>\n",
       "      <td>0.106870</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.176101</td>\n",
       "      <td>0.106870</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.176101</td>\n",
       "      <td>0.106870</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.176101</td>\n",
       "      <td>0.106870</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.207621</td>\n",
       "      <td>0.584337</td>\n",
       "      <td>0.533784</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.525079</td>\n",
       "      <td>0.553419</td>\n",
       "      <td>0.596525</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.355311</td>\n",
       "      <td>0.523438</td>\n",
       "      <td>0.534749</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.220833</td>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.430954</td>\n",
       "      <td>0.530210</td>\n",
       "      <td>0.537247</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.302113</td>\n",
       "      <td>0.501899</td>\n",
       "      <td>0.501263</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.307869</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.539773</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.459273</td>\n",
       "      <td>0.545098</td>\n",
       "      <td>0.558081</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.277347</td>\n",
       "      <td>0.487616</td>\n",
       "      <td>0.491678</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.324181</td>\n",
       "      <td>0.468355</td>\n",
       "      <td>0.466366</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.176101</td>\n",
       "      <td>0.106870</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.402798</td>\n",
       "      <td>0.494264</td>\n",
       "      <td>0.492112</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.424837</td>\n",
       "      <td>0.533769</td>\n",
       "      <td>0.559846</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.345426</td>\n",
       "      <td>0.519398</td>\n",
       "      <td>0.527992</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.508685</td>\n",
       "      <td>0.545263</td>\n",
       "      <td>0.583012</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.519976</td>\n",
       "      <td>0.571835</td>\n",
       "      <td>0.634170</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.229953</td>\n",
       "      <td>0.383660</td>\n",
       "      <td>0.443813</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.440476</td>\n",
       "      <td>0.535282</td>\n",
       "      <td>0.544192</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.434336</td>\n",
       "      <td>0.491256</td>\n",
       "      <td>0.488005</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.370560</td>\n",
       "      <td>0.482311</td>\n",
       "      <td>0.479798</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.305508</td>\n",
       "      <td>0.530030</td>\n",
       "      <td>0.521671</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.345717</td>\n",
       "      <td>0.477848</td>\n",
       "      <td>0.474428</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.343473</td>\n",
       "      <td>0.517984</td>\n",
       "      <td>0.517510</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.397701</td>\n",
       "      <td>0.508360</td>\n",
       "      <td>0.510836</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.409558</td>\n",
       "      <td>0.513477</td>\n",
       "      <td>0.524131</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.318903</td>\n",
       "      <td>0.471429</td>\n",
       "      <td>0.456564</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.496503</td>\n",
       "      <td>0.499237</td>\n",
       "      <td>0.499035</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.459965</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.542471</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.363626</td>\n",
       "      <td>0.446846</td>\n",
       "      <td>0.432449</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.350990</td>\n",
       "      <td>0.497391</td>\n",
       "      <td>0.497475</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.502218</td>\n",
       "      <td>0.505305</td>\n",
       "      <td>0.506313</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.347222</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.402875</td>\n",
       "      <td>0.548363</td>\n",
       "      <td>0.556345</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.400545</td>\n",
       "      <td>0.575198</td>\n",
       "      <td>0.579924</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.463496</td>\n",
       "      <td>0.466705</td>\n",
       "      <td>0.462292</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.396670</td>\n",
       "      <td>0.503303</td>\n",
       "      <td>0.504334</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.500537</td>\n",
       "      <td>0.541340</td>\n",
       "      <td>0.576255</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.630816</td>\n",
       "      <td>0.619792</td>\n",
       "      <td>0.677606</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.528321</td>\n",
       "      <td>0.587339</td>\n",
       "      <td>0.663127</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.530021</td>\n",
       "      <td>0.555985</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.308432</td>\n",
       "      <td>0.436073</td>\n",
       "      <td>0.438131</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.443270</td>\n",
       "      <td>0.514699</td>\n",
       "      <td>0.519571</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.416294</td>\n",
       "      <td>0.450909</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.329711</td>\n",
       "      <td>0.453460</td>\n",
       "      <td>0.452020</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.458894</td>\n",
       "      <td>0.479153</td>\n",
       "      <td>0.470614</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.433100</td>\n",
       "      <td>0.463069</td>\n",
       "      <td>0.446342</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.338529</td>\n",
       "      <td>0.473134</td>\n",
       "      <td>0.469574</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.399583</td>\n",
       "      <td>0.492465</td>\n",
       "      <td>0.489684</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.435268</td>\n",
       "      <td>0.524211</td>\n",
       "      <td>0.544402</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.397988</td>\n",
       "      <td>0.523203</td>\n",
       "      <td>0.539575</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.477523</td>\n",
       "      <td>0.501698</td>\n",
       "      <td>0.502896</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.494029</td>\n",
       "      <td>0.549096</td>\n",
       "      <td>0.591699</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.335613</td>\n",
       "      <td>0.553571</td>\n",
       "      <td>0.537879</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.189655</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.541463</td>\n",
       "      <td>0.562927</td>\n",
       "      <td>0.587121</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.477778</td>\n",
       "      <td>0.554598</td>\n",
       "      <td>0.571970</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.192132</td>\n",
       "      <td>0.608108</td>\n",
       "      <td>0.507282</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.207789</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>0.514563</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.438719</td>\n",
       "      <td>0.479212</td>\n",
       "      <td>0.469140</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.389206</td>\n",
       "      <td>0.513408</td>\n",
       "      <td>0.516557</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.435495</td>\n",
       "      <td>0.511936</td>\n",
       "      <td>0.522201</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.558528</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.623552</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.469206</td>\n",
       "      <td>0.506410</td>\n",
       "      <td>0.511583</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.393747</td>\n",
       "      <td>0.538095</td>\n",
       "      <td>0.561776</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.297554</td>\n",
       "      <td>0.426351</td>\n",
       "      <td>0.431187</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.469925</td>\n",
       "      <td>0.529630</td>\n",
       "      <td>0.540404</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.414751</td>\n",
       "      <td>0.463636</td>\n",
       "      <td>0.449495</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.329104</td>\n",
       "      <td>0.441449</td>\n",
       "      <td>0.436237</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.435917</td>\n",
       "      <td>0.483100</td>\n",
       "      <td>0.474861</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.433100</td>\n",
       "      <td>0.463069</td>\n",
       "      <td>0.446342</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.461217</td>\n",
       "      <td>0.500525</td>\n",
       "      <td>0.500780</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.406002</td>\n",
       "      <td>0.496047</td>\n",
       "      <td>0.494539</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                agg  \\\n",
       "0                                                                                                                              mean   \n",
       "1                                                                                                                               max   \n",
       "2                                                                                                                               min   \n",
       "3                                                                                                                     weighted_mean   \n",
       "4                                                                                                                              mean   \n",
       "5                                                                                                                               max   \n",
       "6                                                                                                                               min   \n",
       "7                                                                                                                     weighted_mean   \n",
       "8                                                                                                                              mean   \n",
       "9                                                                                                                               max   \n",
       "10                                                                                                                              min   \n",
       "11                                                                                                                    weighted_mean   \n",
       "12                                                                                                                             mean   \n",
       "13                                                                                                                              max   \n",
       "14                                                                                                                              min   \n",
       "15                                                                                                                    weighted_mean   \n",
       "16                                                                                                                             mean   \n",
       "17                                                                                                                              max   \n",
       "18                                                                                                                              min   \n",
       "19                                                                                                                    weighted_mean   \n",
       "20                                                                                                                             mean   \n",
       "21                                                                                                                              max   \n",
       "22                                                                                                                              min   \n",
       "23                                                                                                                    weighted_mean   \n",
       "24                                                                                                                             mean   \n",
       "25                                                                                                                              max   \n",
       "26                                                                                                                              min   \n",
       "27                                                                                                                    weighted_mean   \n",
       "28                                                                                                                             mean   \n",
       "29                                                                                                                              max   \n",
       "30                                                                                                                              min   \n",
       "31                                                                                                                    weighted_mean   \n",
       "32                                                                                                                             mean   \n",
       "33                                                                                                                              max   \n",
       "34                                                                                                                              min   \n",
       "35                                                                                                                    weighted_mean   \n",
       "36                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "37                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "38                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "39                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "40                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "41                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "42                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "43                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "44                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "45                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "46                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "47                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "48                                                                                                                             mean   \n",
       "49                                                                                                                              max   \n",
       "50                                                                                                                              min   \n",
       "51                                                                                                                    weighted_mean   \n",
       "52                                                                                                                             mean   \n",
       "53                                                                                                                              max   \n",
       "54                                                                                                                              min   \n",
       "55                                                                                                                    weighted_mean   \n",
       "56                                                                                                                             mean   \n",
       "57                                                                                                                              max   \n",
       "58                                                                                                                              min   \n",
       "59                                                                                                                    weighted_mean   \n",
       "60                                                                                                                             mean   \n",
       "61                                                                                                                              max   \n",
       "62                                                                                                                              min   \n",
       "63                                                                                                                    weighted_mean   \n",
       "64                                                                                                                             mean   \n",
       "65                                                                                                                              max   \n",
       "66                                                                                                                              min   \n",
       "67                                                                                                                    weighted_mean   \n",
       "68                                                                                                                             mean   \n",
       "69                                                                                                                              max   \n",
       "70                                                                                                                              min   \n",
       "71                                                                                                                    weighted_mean   \n",
       "72                                                                                                                             mean   \n",
       "73                                                                                                                              max   \n",
       "74                                                                                                                              min   \n",
       "75                                                                                                                    weighted_mean   \n",
       "76                                                                                                                             mean   \n",
       "77                                                                                                                              max   \n",
       "78                                                                                                                              min   \n",
       "79                                                                                                                    weighted_mean   \n",
       "80                                                                                                                             mean   \n",
       "81                                                                                                                              max   \n",
       "82                                                                                                                              min   \n",
       "83                                                                                                                    weighted_mean   \n",
       "84                                                                                                                             mean   \n",
       "85                                                                                                                              max   \n",
       "86                                                                                                                              min   \n",
       "87                                                                                                                    weighted_mean   \n",
       "88                                                                                                                             mean   \n",
       "89                                                                                                                              max   \n",
       "90                                                                                                                              min   \n",
       "91                                                                                                                    weighted_mean   \n",
       "92                                                                                                                             mean   \n",
       "93                                                                                                                              max   \n",
       "94                                                                                                                              min   \n",
       "95                                                                                                                    weighted_mean   \n",
       "96                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "97                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "98                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "99                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "100                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "101                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "102                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "103                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "104                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "105                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "106                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "107                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "108                                                                                                                            mean   \n",
       "109                                                                                                                             max   \n",
       "110                                                                                                                             min   \n",
       "111                                                                                                                   weighted_mean   \n",
       "112                                                                                                                            mean   \n",
       "113                                                                                                                             max   \n",
       "114                                                                                                                             min   \n",
       "115                                                                                                                   weighted_mean   \n",
       "116                                                                                                                            mean   \n",
       "117                                                                                                                             max   \n",
       "118                                                                                                                             min   \n",
       "119                                                                                                                   weighted_mean   \n",
       "120  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "121            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "122            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "123        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "124  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "125            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "126            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "127        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "128  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "129            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "130            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "131        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "132                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "133                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "134                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "135                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "136                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "137                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "138                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "139                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "140                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "141                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "142                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "143                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "144                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "145                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "146                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "147                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "148                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "149                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "150                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "151                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "152                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "153                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "154                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "155                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "\n",
       "       dataset        f1      prec       rec                        zc  \n",
       "0    Wikipedia  0.616493  0.733735  0.593629                linguistic  \n",
       "1    Wikipedia  0.557086  0.564935  0.553089                linguistic  \n",
       "2    Wikipedia  0.456790  0.420455  0.500000                linguistic  \n",
       "3    Wikipedia  0.576767  0.762745  0.564672                linguistic  \n",
       "4     WikiNews  0.614268  0.614268  0.614268                linguistic  \n",
       "5     WikiNews  0.634810  0.632884  0.636995                linguistic  \n",
       "6     WikiNews  0.614268  0.614268  0.614268                linguistic  \n",
       "7     WikiNews  0.654970  0.647595  0.668561                linguistic  \n",
       "8         News  0.447734  0.443615  0.454837                linguistic  \n",
       "9         News  0.458075  0.456405  0.460558                linguistic  \n",
       "10        News  0.472837  0.471970  0.477549                linguistic  \n",
       "11        News  0.444164  0.442075  0.446775                linguistic  \n",
       "12   Wikipedia  0.477523  0.501698  0.502896                 frequency  \n",
       "13   Wikipedia  0.397619  0.461698  0.428571                 frequency  \n",
       "14   Wikipedia  0.469570  0.480602  0.472008                 frequency  \n",
       "15   Wikipedia  0.441270  0.482906  0.469112                 frequency  \n",
       "16    WikiNews  0.381793  0.458838  0.446338                 frequency  \n",
       "17    WikiNews  0.228454  0.343750  0.389520                 frequency  \n",
       "18    WikiNews  0.502486  0.502609  0.502525                 frequency  \n",
       "19    WikiNews  0.426525  0.445402  0.428030                 frequency  \n",
       "20        News  0.435738  0.474791  0.462639                 frequency  \n",
       "21        News  0.477082  0.495265  0.493325                 frequency  \n",
       "22        News  0.450176  0.447551  0.454057                 frequency  \n",
       "23        News  0.553484  0.551663  0.559639                 frequency  \n",
       "24   Wikipedia  0.392117  0.506061  0.510618            language_model  \n",
       "25   Wikipedia  0.378512  0.442398  0.392857            language_model  \n",
       "26   Wikipedia  0.332890  0.464348  0.441120            language_model  \n",
       "27   Wikipedia  0.400871  0.509804  0.517375            language_model  \n",
       "28    WikiNews  0.337727  0.528045  0.522096            language_model  \n",
       "29    WikiNews  0.382699  0.517262  0.518308            language_model  \n",
       "30    WikiNews  0.390236  0.494695  0.493687            language_model  \n",
       "31    WikiNews  0.339229  0.507310  0.506313            language_model  \n",
       "32        News  0.377128  0.529598  0.532854            language_model  \n",
       "33        News  0.441943  0.511842  0.517164            language_model  \n",
       "34        News  0.426072  0.547473  0.560333            language_model  \n",
       "35        News  0.360499  0.492036  0.490638            language_model  \n",
       "36   Wikipedia  0.477523  0.501698  0.502896                    corpus  \n",
       "37   Wikipedia  0.416188  0.480879  0.464286                    corpus  \n",
       "38   Wikipedia  0.469570  0.480602  0.472008                    corpus  \n",
       "39   Wikipedia  0.441270  0.482906  0.469112                    corpus  \n",
       "40    WikiNews  0.399605  0.470128  0.460227                    corpus  \n",
       "41    WikiNews  0.211338  0.301170  0.328283                    corpus  \n",
       "42    WikiNews  0.485316  0.485263  0.486742                    corpus  \n",
       "43    WikiNews  0.426525  0.445402  0.428030                    corpus  \n",
       "44        News  0.435738  0.474791  0.462639                    corpus  \n",
       "45        News  0.385432  0.447740  0.423024                    corpus  \n",
       "46        News  0.457476  0.454234  0.466973                    corpus  \n",
       "47        News  0.494208  0.508586  0.511963                    corpus  \n",
       "48   Wikipedia  0.385458  0.489757  0.481660          psycholinguistic  \n",
       "49   Wikipedia  0.469206  0.506410  0.511583          psycholinguistic  \n",
       "50   Wikipedia  0.314997  0.505882  0.507722          psycholinguistic  \n",
       "51   Wikipedia  0.408533  0.477273  0.457529          psycholinguistic  \n",
       "52    WikiNews  0.459273  0.545098  0.558081          psycholinguistic  \n",
       "53    WikiNews  0.613075  0.644022  0.700758          psycholinguistic  \n",
       "54    WikiNews  0.372269  0.511023  0.511364          psycholinguistic  \n",
       "55    WikiNews  0.468559  0.549879  0.565025          psycholinguistic  \n",
       "56        News  0.429449  0.520708  0.528606          psycholinguistic  \n",
       "57        News  0.511217  0.549076  0.572989          psycholinguistic  \n",
       "58        News  0.293883  0.434788  0.433946          psycholinguistic  \n",
       "59        News  0.451683  0.524291  0.535021          psycholinguistic  \n",
       "60   Wikipedia  0.194139  0.583333  0.527027                   wordnet  \n",
       "61   Wikipedia  0.314997  0.505882  0.507722                   wordnet  \n",
       "62   Wikipedia  0.421596  0.548104  0.582046                   wordnet  \n",
       "63   Wikipedia  0.410714  0.501053  0.501931                   wordnet  \n",
       "64    WikiNews  0.175149  0.201124  0.416035                   wordnet  \n",
       "65    WikiNews  0.175149  0.201124  0.416035                   wordnet  \n",
       "66    WikiNews  0.510716  0.510714  0.511364                   wordnet  \n",
       "67    WikiNews  0.175149  0.201124  0.416035                   wordnet  \n",
       "68        News  0.199867  0.524089  0.503207                   wordnet  \n",
       "69        News  0.199867  0.524089  0.503207                   wordnet  \n",
       "70        News  0.292150  0.408989  0.394938                   wordnet  \n",
       "71        News  0.453895  0.508255  0.512223                   wordnet  \n",
       "72   Wikipedia  0.516863  0.549285  0.589768                   dbpedia  \n",
       "73   Wikipedia  0.516863  0.549285  0.589768                   dbpedia  \n",
       "74   Wikipedia  0.516863  0.549285  0.589768                   dbpedia  \n",
       "75   Wikipedia  0.516863  0.549285  0.589768                   dbpedia  \n",
       "76    WikiNews  0.189655  0.117021  0.500000                   dbpedia  \n",
       "77    WikiNews  0.189655  0.117021  0.500000                   dbpedia  \n",
       "78    WikiNews  0.189655  0.117021  0.500000                   dbpedia  \n",
       "79    WikiNews  0.189655  0.117021  0.500000                   dbpedia  \n",
       "80        News  0.176101  0.106870  0.500000                   dbpedia  \n",
       "81        News  0.176101  0.106870  0.500000                   dbpedia  \n",
       "82        News  0.176101  0.106870  0.500000                   dbpedia  \n",
       "83        News  0.176101  0.106870  0.500000                   dbpedia  \n",
       "84   Wikipedia  0.207621  0.584337  0.533784          brown_clustering  \n",
       "85   Wikipedia  0.525079  0.553419  0.596525          brown_clustering  \n",
       "86   Wikipedia  0.355311  0.523438  0.534749          brown_clustering  \n",
       "87   Wikipedia  0.220833  0.585366  0.540541          brown_clustering  \n",
       "88    WikiNews  0.430954  0.530210  0.537247          brown_clustering  \n",
       "89    WikiNews  0.302113  0.501899  0.501263          brown_clustering  \n",
       "90    WikiNews  0.307869  0.575000  0.539773          brown_clustering  \n",
       "91    WikiNews  0.459273  0.545098  0.558081          brown_clustering  \n",
       "92        News  0.277347  0.487616  0.491678          brown_clustering  \n",
       "93        News  0.324181  0.468355  0.466366          brown_clustering  \n",
       "94        News  0.176101  0.106870  0.500000          brown_clustering  \n",
       "95        News  0.402798  0.494264  0.492112          brown_clustering  \n",
       "96   Wikipedia  0.424837  0.533769  0.559846                  semantic  \n",
       "97   Wikipedia  0.345426  0.519398  0.527992                  semantic  \n",
       "98   Wikipedia  0.508685  0.545263  0.583012                  semantic  \n",
       "99   Wikipedia  0.519976  0.571835  0.634170                  semantic  \n",
       "100   WikiNews  0.229953  0.383660  0.443813                  semantic  \n",
       "101   WikiNews  0.440476  0.535282  0.544192                  semantic  \n",
       "102   WikiNews  0.434336  0.491256  0.488005                  semantic  \n",
       "103   WikiNews  0.370560  0.482311  0.479798                  semantic  \n",
       "104       News  0.305508  0.530030  0.521671                  semantic  \n",
       "105       News  0.345717  0.477848  0.474428                  semantic  \n",
       "106       News  0.343473  0.517984  0.517510                  semantic  \n",
       "107       News  0.397701  0.508360  0.510836                  semantic  \n",
       "108  Wikipedia  0.409558  0.513477  0.524131                dictionary  \n",
       "109  Wikipedia  0.318903  0.471429  0.456564                dictionary  \n",
       "110  Wikipedia  0.496503  0.499237  0.499035                dictionary  \n",
       "111  Wikipedia  0.459965  0.522727  0.542471                dictionary  \n",
       "112   WikiNews  0.363626  0.446846  0.432449                dictionary  \n",
       "113   WikiNews  0.350990  0.497391  0.497475                dictionary  \n",
       "114   WikiNews  0.502218  0.505305  0.506313                dictionary  \n",
       "115   WikiNews  0.347222  0.419540  0.393939                dictionary  \n",
       "116       News  0.402875  0.548363  0.556345                dictionary  \n",
       "117       News  0.400545  0.575198  0.579924                dictionary  \n",
       "118       News  0.463496  0.466705  0.462292                dictionary  \n",
       "119       News  0.396670  0.503303  0.504334                dictionary  \n",
       "120  Wikipedia  0.500537  0.541340  0.576255           corpus+semantic  \n",
       "121  Wikipedia  0.630816  0.619792  0.677606           corpus+semantic  \n",
       "122  Wikipedia  0.528321  0.587339  0.663127           corpus+semantic  \n",
       "123  Wikipedia  0.476190  0.530021  0.555985           corpus+semantic  \n",
       "124   WikiNews  0.308432  0.436073  0.438131           corpus+semantic  \n",
       "125   WikiNews  0.443270  0.514699  0.519571           corpus+semantic  \n",
       "126   WikiNews  0.416294  0.450909  0.431818           corpus+semantic  \n",
       "127   WikiNews  0.329711  0.453460  0.452020           corpus+semantic  \n",
       "128       News  0.458894  0.479153  0.470614           corpus+semantic  \n",
       "129       News  0.433100  0.463069  0.446342           corpus+semantic  \n",
       "130       News  0.338529  0.473134  0.469574           corpus+semantic  \n",
       "131       News  0.399583  0.492465  0.489684           corpus+semantic  \n",
       "132  Wikipedia  0.435268  0.524211  0.544402  wordnet+psycholinguistic  \n",
       "133  Wikipedia  0.397988  0.523203  0.539575  wordnet+psycholinguistic  \n",
       "134  Wikipedia  0.477523  0.501698  0.502896  wordnet+psycholinguistic  \n",
       "135  Wikipedia  0.494029  0.549096  0.591699  wordnet+psycholinguistic  \n",
       "136   WikiNews  0.335613  0.553571  0.537879  wordnet+psycholinguistic  \n",
       "137   WikiNews  0.189655  0.117021  0.500000  wordnet+psycholinguistic  \n",
       "138   WikiNews  0.541463  0.562927  0.587121  wordnet+psycholinguistic  \n",
       "139   WikiNews  0.477778  0.554598  0.571970  wordnet+psycholinguistic  \n",
       "140       News  0.192132  0.608108  0.507282  wordnet+psycholinguistic  \n",
       "141       News  0.207789  0.609375  0.514563  wordnet+psycholinguistic  \n",
       "142       News  0.438719  0.479212  0.469140  wordnet+psycholinguistic  \n",
       "143       News  0.389206  0.513408  0.516557  wordnet+psycholinguistic  \n",
       "144  Wikipedia  0.435495  0.511936  0.522201                       all  \n",
       "145  Wikipedia  0.558528  0.571429  0.623552                       all  \n",
       "146  Wikipedia  0.469206  0.506410  0.511583                       all  \n",
       "147  Wikipedia  0.393747  0.538095  0.561776                       all  \n",
       "148   WikiNews  0.297554  0.426351  0.431187                       all  \n",
       "149   WikiNews  0.469925  0.529630  0.540404                       all  \n",
       "150   WikiNews  0.414751  0.463636  0.449495                       all  \n",
       "151   WikiNews  0.329104  0.441449  0.436237                       all  \n",
       "152       News  0.435917  0.483100  0.474861                       all  \n",
       "153       News  0.433100  0.463069  0.446342                       all  \n",
       "154       News  0.461217  0.500525  0.500780                       all  \n",
       "155       News  0.406002  0.496047  0.494539                       all  "
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_eval_data_lr_dswp = create_eval_df_from_results_macro(results, False)\n",
    "feature_eval_data_lr_dswp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.654970</td>\n",
       "      <td>0.647595</td>\n",
       "      <td>0.668561</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.553484</td>\n",
       "      <td>0.551663</td>\n",
       "      <td>0.559639</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.630816</td>\n",
       "      <td>0.619792</td>\n",
       "      <td>0.677606</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                      agg  \\\n",
       "7                                                                                                           weighted_mean   \n",
       "23                                                                                                          weighted_mean   \n",
       "121  [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "\n",
       "       dataset        f1      prec       rec               zc  \n",
       "7     WikiNews  0.654970  0.647595  0.668561       linguistic  \n",
       "23        News  0.553484  0.551663  0.559639        frequency  \n",
       "121  Wikipedia  0.630816  0.619792  0.677606  corpus+semantic  "
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = feature_eval_data_lr_dswp.groupby(['dataset'])['f1'].transform(max) == feature_eval_data_lr_dswp['f1']\n",
    "feature_eval_data_lr_dswp[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.2.7 SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "results = [Result(fs, fs.fc, fs.agg,\n",
    "        precision_recall_fscore_support(*svm(*transform_feat_to_num(remove_labels_for_binary_df(fs.train), \n",
    "        remove_labels_for_binary_df(fs.test))), average='macro')) for fs in all_fc_datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.498491</td>\n",
       "      <td>0.521687</td>\n",
       "      <td>0.508687</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.469298</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.474903</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.557086</td>\n",
       "      <td>0.564935</td>\n",
       "      <td>0.553089</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.532468</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.530888</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.627947</td>\n",
       "      <td>0.644385</td>\n",
       "      <td>0.619318</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.634810</td>\n",
       "      <td>0.632884</td>\n",
       "      <td>0.636995</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.422930</td>\n",
       "      <td>0.419298</td>\n",
       "      <td>0.427399</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.553363</td>\n",
       "      <td>0.572321</td>\n",
       "      <td>0.551136</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.579631</td>\n",
       "      <td>0.616604</td>\n",
       "      <td>0.572382</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.578100</td>\n",
       "      <td>0.601524</td>\n",
       "      <td>0.571602</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.597542</td>\n",
       "      <td>0.630676</td>\n",
       "      <td>0.587812</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.562969</td>\n",
       "      <td>0.591848</td>\n",
       "      <td>0.558599</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.430303</td>\n",
       "      <td>0.381720</td>\n",
       "      <td>0.493056</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.426829</td>\n",
       "      <td>0.380435</td>\n",
       "      <td>0.486111</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.483832</td>\n",
       "      <td>0.583169</td>\n",
       "      <td>0.514650</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.495150</td>\n",
       "      <td>0.578233</td>\n",
       "      <td>0.518724</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.483832</td>\n",
       "      <td>0.583169</td>\n",
       "      <td>0.514650</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.483832</td>\n",
       "      <td>0.583169</td>\n",
       "      <td>0.514650</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.479730</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.479730</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.479730</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.430303</td>\n",
       "      <td>0.381720</td>\n",
       "      <td>0.493056</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.469448</td>\n",
       "      <td>0.551282</td>\n",
       "      <td>0.508838</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.483832</td>\n",
       "      <td>0.583169</td>\n",
       "      <td>0.514650</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.495150</td>\n",
       "      <td>0.578233</td>\n",
       "      <td>0.518724</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.481884</td>\n",
       "      <td>0.561924</td>\n",
       "      <td>0.512223</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.483832</td>\n",
       "      <td>0.583169</td>\n",
       "      <td>0.514650</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.479730</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.518750</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.528958</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.424837</td>\n",
       "      <td>0.411392</td>\n",
       "      <td>0.439189</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.426829</td>\n",
       "      <td>0.380435</td>\n",
       "      <td>0.486111</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.553656</td>\n",
       "      <td>0.769444</td>\n",
       "      <td>0.561237</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.580357</td>\n",
       "      <td>0.731061</td>\n",
       "      <td>0.577020</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.469448</td>\n",
       "      <td>0.551282</td>\n",
       "      <td>0.508838</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.464320</td>\n",
       "      <td>0.504392</td>\n",
       "      <td>0.500867</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.507629</td>\n",
       "      <td>0.589898</td>\n",
       "      <td>0.525225</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.562312</td>\n",
       "      <td>0.668788</td>\n",
       "      <td>0.560940</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.535678</td>\n",
       "      <td>0.634143</td>\n",
       "      <td>0.543083</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.511654</td>\n",
       "      <td>0.590196</td>\n",
       "      <td>0.522201</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.479842</td>\n",
       "      <td>0.887097</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.481884</td>\n",
       "      <td>0.561924</td>\n",
       "      <td>0.512223</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.533063</td>\n",
       "      <td>0.619157</td>\n",
       "      <td>0.540655</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.483832</td>\n",
       "      <td>0.583169</td>\n",
       "      <td>0.514650</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.483832</td>\n",
       "      <td>0.583169</td>\n",
       "      <td>0.514650</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.502646</td>\n",
       "      <td>0.587640</td>\n",
       "      <td>0.524621</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.502646</td>\n",
       "      <td>0.587640</td>\n",
       "      <td>0.524621</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.502646</td>\n",
       "      <td>0.587640</td>\n",
       "      <td>0.524621</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.502646</td>\n",
       "      <td>0.587640</td>\n",
       "      <td>0.524621</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.531957</td>\n",
       "      <td>0.700794</td>\n",
       "      <td>0.543863</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.531957</td>\n",
       "      <td>0.700794</td>\n",
       "      <td>0.543863</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.531957</td>\n",
       "      <td>0.700794</td>\n",
       "      <td>0.543863</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.531957</td>\n",
       "      <td>0.700794</td>\n",
       "      <td>0.543863</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.518750</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.528958</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.430303</td>\n",
       "      <td>0.381720</td>\n",
       "      <td>0.493056</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.515583</td>\n",
       "      <td>0.723443</td>\n",
       "      <td>0.538510</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.476135</td>\n",
       "      <td>0.519000</td>\n",
       "      <td>0.504941</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.498648</td>\n",
       "      <td>0.542977</td>\n",
       "      <td>0.515517</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.522909</td>\n",
       "      <td>0.573760</td>\n",
       "      <td>0.530947</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.479952</td>\n",
       "      <td>0.544841</td>\n",
       "      <td>0.509795</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.479842</td>\n",
       "      <td>0.887097</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.479842</td>\n",
       "      <td>0.887097</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.522358</td>\n",
       "      <td>0.891304</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.479842</td>\n",
       "      <td>0.887097</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.489785</td>\n",
       "      <td>0.696887</td>\n",
       "      <td>0.521931</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.487781</td>\n",
       "      <td>0.646484</td>\n",
       "      <td>0.519504</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.483832</td>\n",
       "      <td>0.583169</td>\n",
       "      <td>0.514650</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.489785</td>\n",
       "      <td>0.696887</td>\n",
       "      <td>0.521931</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.479842</td>\n",
       "      <td>0.887097</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.489785</td>\n",
       "      <td>0.696887</td>\n",
       "      <td>0.521931</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.489785</td>\n",
       "      <td>0.696887</td>\n",
       "      <td>0.521931</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.489785</td>\n",
       "      <td>0.696887</td>\n",
       "      <td>0.521931</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.489785</td>\n",
       "      <td>0.696887</td>\n",
       "      <td>0.521931</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.522358</td>\n",
       "      <td>0.891304</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.481884</td>\n",
       "      <td>0.561924</td>\n",
       "      <td>0.512223</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.497287</td>\n",
       "      <td>0.596825</td>\n",
       "      <td>0.521151</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.507629</td>\n",
       "      <td>0.589898</td>\n",
       "      <td>0.525225</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.483832</td>\n",
       "      <td>0.583169</td>\n",
       "      <td>0.514650</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.485797</td>\n",
       "      <td>0.610364</td>\n",
       "      <td>0.517077</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.485797</td>\n",
       "      <td>0.610364</td>\n",
       "      <td>0.517077</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.485797</td>\n",
       "      <td>0.610364</td>\n",
       "      <td>0.517077</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.485797</td>\n",
       "      <td>0.610364</td>\n",
       "      <td>0.517077</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                agg  \\\n",
       "0                                                                                                                              mean   \n",
       "1                                                                                                                               max   \n",
       "2                                                                                                                               min   \n",
       "3                                                                                                                     weighted_mean   \n",
       "4                                                                                                                              mean   \n",
       "5                                                                                                                               max   \n",
       "6                                                                                                                               min   \n",
       "7                                                                                                                     weighted_mean   \n",
       "8                                                                                                                              mean   \n",
       "9                                                                                                                               max   \n",
       "10                                                                                                                              min   \n",
       "11                                                                                                                    weighted_mean   \n",
       "12                                                                                                                             mean   \n",
       "13                                                                                                                              max   \n",
       "14                                                                                                                              min   \n",
       "15                                                                                                                    weighted_mean   \n",
       "16                                                                                                                             mean   \n",
       "17                                                                                                                              max   \n",
       "18                                                                                                                              min   \n",
       "19                                                                                                                    weighted_mean   \n",
       "20                                                                                                                             mean   \n",
       "21                                                                                                                              max   \n",
       "22                                                                                                                              min   \n",
       "23                                                                                                                    weighted_mean   \n",
       "24                                                                                                                             mean   \n",
       "25                                                                                                                              max   \n",
       "26                                                                                                                              min   \n",
       "27                                                                                                                    weighted_mean   \n",
       "28                                                                                                                             mean   \n",
       "29                                                                                                                              max   \n",
       "30                                                                                                                              min   \n",
       "31                                                                                                                    weighted_mean   \n",
       "32                                                                                                                             mean   \n",
       "33                                                                                                                              max   \n",
       "34                                                                                                                              min   \n",
       "35                                                                                                                    weighted_mean   \n",
       "36                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "37                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "38                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "39                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "40                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "41                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "42                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "43                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "44                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "45                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "46                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "47                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "48                                                                                                                             mean   \n",
       "49                                                                                                                              max   \n",
       "50                                                                                                                              min   \n",
       "51                                                                                                                    weighted_mean   \n",
       "52                                                                                                                             mean   \n",
       "53                                                                                                                              max   \n",
       "54                                                                                                                              min   \n",
       "55                                                                                                                    weighted_mean   \n",
       "56                                                                                                                             mean   \n",
       "57                                                                                                                              max   \n",
       "58                                                                                                                              min   \n",
       "59                                                                                                                    weighted_mean   \n",
       "60                                                                                                                             mean   \n",
       "61                                                                                                                              max   \n",
       "62                                                                                                                              min   \n",
       "63                                                                                                                    weighted_mean   \n",
       "64                                                                                                                             mean   \n",
       "65                                                                                                                              max   \n",
       "66                                                                                                                              min   \n",
       "67                                                                                                                    weighted_mean   \n",
       "68                                                                                                                             mean   \n",
       "69                                                                                                                              max   \n",
       "70                                                                                                                              min   \n",
       "71                                                                                                                    weighted_mean   \n",
       "72                                                                                                                             mean   \n",
       "73                                                                                                                              max   \n",
       "74                                                                                                                              min   \n",
       "75                                                                                                                    weighted_mean   \n",
       "76                                                                                                                             mean   \n",
       "77                                                                                                                              max   \n",
       "78                                                                                                                              min   \n",
       "79                                                                                                                    weighted_mean   \n",
       "80                                                                                                                             mean   \n",
       "81                                                                                                                              max   \n",
       "82                                                                                                                              min   \n",
       "83                                                                                                                    weighted_mean   \n",
       "84                                                                                                                             mean   \n",
       "85                                                                                                                              max   \n",
       "86                                                                                                                              min   \n",
       "87                                                                                                                    weighted_mean   \n",
       "88                                                                                                                             mean   \n",
       "89                                                                                                                              max   \n",
       "90                                                                                                                              min   \n",
       "91                                                                                                                    weighted_mean   \n",
       "92                                                                                                                             mean   \n",
       "93                                                                                                                              max   \n",
       "94                                                                                                                              min   \n",
       "95                                                                                                                    weighted_mean   \n",
       "96                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "97                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "98                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "99                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "100                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "101                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "102                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "103                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "104                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "105                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "106                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "107                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "108                                                                                                                            mean   \n",
       "109                                                                                                                             max   \n",
       "110                                                                                                                             min   \n",
       "111                                                                                                                   weighted_mean   \n",
       "112                                                                                                                            mean   \n",
       "113                                                                                                                             max   \n",
       "114                                                                                                                             min   \n",
       "115                                                                                                                   weighted_mean   \n",
       "116                                                                                                                            mean   \n",
       "117                                                                                                                             max   \n",
       "118                                                                                                                             min   \n",
       "119                                                                                                                   weighted_mean   \n",
       "120  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "121            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "122            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "123        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "124  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "125            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "126            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "127        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "128  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "129            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "130            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "131        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "132                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "133                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "134                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "135                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "136                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "137                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "138                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "139                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "140                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "141                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "142                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "143                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "144                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "145                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "146                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "147                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "148                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "149                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "150                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "151                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "152                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "153                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "154                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "155                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "\n",
       "       dataset        f1      prec       rec                        zc  \n",
       "0    Wikipedia  0.498491  0.521687  0.508687                linguistic  \n",
       "1    Wikipedia  0.469298  0.466667  0.474903                linguistic  \n",
       "2    Wikipedia  0.557086  0.564935  0.553089                linguistic  \n",
       "3    Wikipedia  0.532468  0.550000  0.530888                linguistic  \n",
       "4     WikiNews  0.627947  0.644385  0.619318                linguistic  \n",
       "5     WikiNews  0.634810  0.632884  0.636995                linguistic  \n",
       "6     WikiNews  0.422930  0.419298  0.427399                linguistic  \n",
       "7     WikiNews  0.553363  0.572321  0.551136                linguistic  \n",
       "8         News  0.579631  0.616604  0.572382                linguistic  \n",
       "9         News  0.578100  0.601524  0.571602                linguistic  \n",
       "10        News  0.597542  0.630676  0.587812                linguistic  \n",
       "11        News  0.562969  0.591848  0.558599                linguistic  \n",
       "12   Wikipedia  0.456790  0.420455  0.500000                 frequency  \n",
       "13   Wikipedia  0.450000  0.418605  0.486486                 frequency  \n",
       "14   Wikipedia  0.450000  0.418605  0.486486                 frequency  \n",
       "15   Wikipedia  0.456790  0.420455  0.500000                 frequency  \n",
       "16    WikiNews  0.433735  0.382979  0.500000                 frequency  \n",
       "17    WikiNews  0.430303  0.381720  0.493056                 frequency  \n",
       "18    WikiNews  0.426829  0.380435  0.486111                 frequency  \n",
       "19    WikiNews  0.433735  0.382979  0.500000                 frequency  \n",
       "20        News  0.483832  0.583169  0.514650                 frequency  \n",
       "21        News  0.495150  0.578233  0.518724                 frequency  \n",
       "22        News  0.483832  0.583169  0.514650                 frequency  \n",
       "23        News  0.483832  0.583169  0.514650                 frequency  \n",
       "24   Wikipedia  0.446541  0.417647  0.479730            language_model  \n",
       "25   Wikipedia  0.446541  0.417647  0.479730            language_model  \n",
       "26   Wikipedia  0.456790  0.420455  0.500000            language_model  \n",
       "27   Wikipedia  0.446541  0.417647  0.479730            language_model  \n",
       "28    WikiNews  0.433735  0.382979  0.500000            language_model  \n",
       "29    WikiNews  0.433735  0.382979  0.500000            language_model  \n",
       "30    WikiNews  0.433735  0.382979  0.500000            language_model  \n",
       "31    WikiNews  0.433735  0.382979  0.500000            language_model  \n",
       "32        News  0.440171  0.393130  0.500000            language_model  \n",
       "33        News  0.440171  0.393130  0.500000            language_model  \n",
       "34        News  0.440171  0.393130  0.500000            language_model  \n",
       "35        News  0.440171  0.393130  0.500000            language_model  \n",
       "36   Wikipedia  0.456790  0.420455  0.500000                    corpus  \n",
       "37   Wikipedia  0.450000  0.418605  0.486486                    corpus  \n",
       "38   Wikipedia  0.450000  0.418605  0.486486                    corpus  \n",
       "39   Wikipedia  0.456790  0.420455  0.500000                    corpus  \n",
       "40    WikiNews  0.433735  0.382979  0.500000                    corpus  \n",
       "41    WikiNews  0.430303  0.381720  0.493056                    corpus  \n",
       "42    WikiNews  0.469448  0.551282  0.508838                    corpus  \n",
       "43    WikiNews  0.433735  0.382979  0.500000                    corpus  \n",
       "44        News  0.483832  0.583169  0.514650                    corpus  \n",
       "45        News  0.495150  0.578233  0.518724                    corpus  \n",
       "46        News  0.481884  0.561924  0.512223                    corpus  \n",
       "47        News  0.483832  0.583169  0.514650                    corpus  \n",
       "48   Wikipedia  0.450000  0.418605  0.486486          psycholinguistic  \n",
       "49   Wikipedia  0.446541  0.417647  0.479730          psycholinguistic  \n",
       "50   Wikipedia  0.518750  0.674419  0.528958          psycholinguistic  \n",
       "51   Wikipedia  0.424837  0.411392  0.439189          psycholinguistic  \n",
       "52    WikiNews  0.426829  0.380435  0.486111          psycholinguistic  \n",
       "53    WikiNews  0.553656  0.769444  0.561237          psycholinguistic  \n",
       "54    WikiNews  0.580357  0.731061  0.577020          psycholinguistic  \n",
       "55    WikiNews  0.469448  0.551282  0.508838          psycholinguistic  \n",
       "56        News  0.464320  0.504392  0.500867          psycholinguistic  \n",
       "57        News  0.507629  0.589898  0.525225          psycholinguistic  \n",
       "58        News  0.562312  0.668788  0.560940          psycholinguistic  \n",
       "59        News  0.535678  0.634143  0.543083          psycholinguistic  \n",
       "60   Wikipedia  0.456790  0.420455  0.500000                   wordnet  \n",
       "61   Wikipedia  0.511654  0.590196  0.522201                   wordnet  \n",
       "62   Wikipedia  0.456790  0.420455  0.500000                   wordnet  \n",
       "63   Wikipedia  0.456790  0.420455  0.500000                   wordnet  \n",
       "64    WikiNews  0.433735  0.382979  0.500000                   wordnet  \n",
       "65    WikiNews  0.479842  0.887097  0.522727                   wordnet  \n",
       "66    WikiNews  0.433735  0.382979  0.500000                   wordnet  \n",
       "67    WikiNews  0.433735  0.382979  0.500000                   wordnet  \n",
       "68        News  0.481884  0.561924  0.512223                   wordnet  \n",
       "69        News  0.533063  0.619157  0.540655                   wordnet  \n",
       "70        News  0.483832  0.583169  0.514650                   wordnet  \n",
       "71        News  0.483832  0.583169  0.514650                   wordnet  \n",
       "72   Wikipedia  0.453416  0.419540  0.493243                   dbpedia  \n",
       "73   Wikipedia  0.453416  0.419540  0.493243                   dbpedia  \n",
       "74   Wikipedia  0.453416  0.419540  0.493243                   dbpedia  \n",
       "75   Wikipedia  0.453416  0.419540  0.493243                   dbpedia  \n",
       "76    WikiNews  0.502646  0.587640  0.524621                   dbpedia  \n",
       "77    WikiNews  0.502646  0.587640  0.524621                   dbpedia  \n",
       "78    WikiNews  0.502646  0.587640  0.524621                   dbpedia  \n",
       "79    WikiNews  0.502646  0.587640  0.524621                   dbpedia  \n",
       "80        News  0.531957  0.700794  0.543863                   dbpedia  \n",
       "81        News  0.531957  0.700794  0.543863                   dbpedia  \n",
       "82        News  0.531957  0.700794  0.543863                   dbpedia  \n",
       "83        News  0.531957  0.700794  0.543863                   dbpedia  \n",
       "84   Wikipedia  0.456790  0.420455  0.500000          brown_clustering  \n",
       "85   Wikipedia  0.456790  0.420455  0.500000          brown_clustering  \n",
       "86   Wikipedia  0.518750  0.674419  0.528958          brown_clustering  \n",
       "87   Wikipedia  0.456790  0.420455  0.500000          brown_clustering  \n",
       "88    WikiNews  0.433735  0.382979  0.500000          brown_clustering  \n",
       "89    WikiNews  0.430303  0.381720  0.493056          brown_clustering  \n",
       "90    WikiNews  0.515583  0.723443  0.538510          brown_clustering  \n",
       "91    WikiNews  0.433735  0.382979  0.500000          brown_clustering  \n",
       "92        News  0.476135  0.519000  0.504941          brown_clustering  \n",
       "93        News  0.498648  0.542977  0.515517          brown_clustering  \n",
       "94        News  0.522909  0.573760  0.530947          brown_clustering  \n",
       "95        News  0.479952  0.544841  0.509795          brown_clustering  \n",
       "96   Wikipedia  0.456790  0.420455  0.500000                  semantic  \n",
       "97   Wikipedia  0.456790  0.420455  0.500000                  semantic  \n",
       "98   Wikipedia  0.456790  0.420455  0.500000                  semantic  \n",
       "99   Wikipedia  0.456790  0.420455  0.500000                  semantic  \n",
       "100   WikiNews  0.479842  0.887097  0.522727                  semantic  \n",
       "101   WikiNews  0.479842  0.887097  0.522727                  semantic  \n",
       "102   WikiNews  0.522358  0.891304  0.545455                  semantic  \n",
       "103   WikiNews  0.479842  0.887097  0.522727                  semantic  \n",
       "104       News  0.489785  0.696887  0.521931                  semantic  \n",
       "105       News  0.487781  0.646484  0.519504                  semantic  \n",
       "106       News  0.483832  0.583169  0.514650                  semantic  \n",
       "107       News  0.489785  0.696887  0.521931                  semantic  \n",
       "108  Wikipedia  0.456790  0.420455  0.500000                dictionary  \n",
       "109  Wikipedia  0.456790  0.420455  0.500000                dictionary  \n",
       "110  Wikipedia  0.456790  0.420455  0.500000                dictionary  \n",
       "111  Wikipedia  0.456790  0.420455  0.500000                dictionary  \n",
       "112   WikiNews  0.433735  0.382979  0.500000                dictionary  \n",
       "113   WikiNews  0.433735  0.382979  0.500000                dictionary  \n",
       "114   WikiNews  0.433735  0.382979  0.500000                dictionary  \n",
       "115   WikiNews  0.433735  0.382979  0.500000                dictionary  \n",
       "116       News  0.440171  0.393130  0.500000                dictionary  \n",
       "117       News  0.440171  0.393130  0.500000                dictionary  \n",
       "118       News  0.440171  0.393130  0.500000                dictionary  \n",
       "119       News  0.440171  0.393130  0.500000                dictionary  \n",
       "120  Wikipedia  0.456790  0.420455  0.500000           corpus+semantic  \n",
       "121  Wikipedia  0.456790  0.420455  0.500000           corpus+semantic  \n",
       "122  Wikipedia  0.456790  0.420455  0.500000           corpus+semantic  \n",
       "123  Wikipedia  0.456790  0.420455  0.500000           corpus+semantic  \n",
       "124   WikiNews  0.433735  0.382979  0.500000           corpus+semantic  \n",
       "125   WikiNews  0.433735  0.382979  0.500000           corpus+semantic  \n",
       "126   WikiNews  0.479842  0.887097  0.522727           corpus+semantic  \n",
       "127   WikiNews  0.433735  0.382979  0.500000           corpus+semantic  \n",
       "128       News  0.489785  0.696887  0.521931           corpus+semantic  \n",
       "129       News  0.489785  0.696887  0.521931           corpus+semantic  \n",
       "130       News  0.489785  0.696887  0.521931           corpus+semantic  \n",
       "131       News  0.489785  0.696887  0.521931           corpus+semantic  \n",
       "132  Wikipedia  0.456790  0.420455  0.500000  wordnet+psycholinguistic  \n",
       "133  Wikipedia  0.456790  0.420455  0.500000  wordnet+psycholinguistic  \n",
       "134  Wikipedia  0.456790  0.420455  0.500000  wordnet+psycholinguistic  \n",
       "135  Wikipedia  0.456790  0.420455  0.500000  wordnet+psycholinguistic  \n",
       "136   WikiNews  0.433735  0.382979  0.500000  wordnet+psycholinguistic  \n",
       "137   WikiNews  0.433735  0.382979  0.500000  wordnet+psycholinguistic  \n",
       "138   WikiNews  0.522358  0.891304  0.545455  wordnet+psycholinguistic  \n",
       "139   WikiNews  0.433735  0.382979  0.500000  wordnet+psycholinguistic  \n",
       "140       News  0.481884  0.561924  0.512223  wordnet+psycholinguistic  \n",
       "141       News  0.497287  0.596825  0.521151  wordnet+psycholinguistic  \n",
       "142       News  0.507629  0.589898  0.525225  wordnet+psycholinguistic  \n",
       "143       News  0.483832  0.583169  0.514650  wordnet+psycholinguistic  \n",
       "144  Wikipedia  0.456790  0.420455  0.500000                       all  \n",
       "145  Wikipedia  0.456790  0.420455  0.500000                       all  \n",
       "146  Wikipedia  0.456790  0.420455  0.500000                       all  \n",
       "147  Wikipedia  0.456790  0.420455  0.500000                       all  \n",
       "148   WikiNews  0.433735  0.382979  0.500000                       all  \n",
       "149   WikiNews  0.433735  0.382979  0.500000                       all  \n",
       "150   WikiNews  0.433735  0.382979  0.500000                       all  \n",
       "151   WikiNews  0.433735  0.382979  0.500000                       all  \n",
       "152       News  0.485797  0.610364  0.517077                       all  \n",
       "153       News  0.485797  0.610364  0.517077                       all  \n",
       "154       News  0.485797  0.610364  0.517077                       all  \n",
       "155       News  0.485797  0.610364  0.517077                       all  "
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_eval_data_svm = create_eval_df_from_results_macro(results, False)\n",
    "feature_eval_data_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.557086</td>\n",
       "      <td>0.564935</td>\n",
       "      <td>0.553089</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.634810</td>\n",
       "      <td>0.632884</td>\n",
       "      <td>0.636995</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.597542</td>\n",
       "      <td>0.630676</td>\n",
       "      <td>0.587812</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    agg    dataset        f1      prec       rec          zc\n",
       "2   min  Wikipedia  0.557086  0.564935  0.553089  linguistic\n",
       "5   max   WikiNews  0.634810  0.632884  0.636995  linguistic\n",
       "10  min       News  0.597542  0.630676  0.587812  linguistic"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = feature_eval_data_svm.groupby(['dataset'])['f1'].transform(max) == feature_eval_data_svm['f1']\n",
    "feature_eval_data_svm[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [Result(fs, fs.fc, fs.agg,\n",
    "        precision_recall_fscore_support(*svm(*transform_feat_to_num(remove_labels_for_binary_df(fs.train), \n",
    "        remove_labels_for_binary_df(fs.test))), average='macro')) for fs in all_fc_datasets_dswp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_eval_data_svm_dswp = create_eval_df_from_results_macro(results)\n",
    "feature_eval_data_svm_dswp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = feature_eval_data_svm_dswp.groupby(['dataset'])['f1'].transform(max) == feature_eval_data_svm_dswp['f1']\n",
    "feature_eval_data_svm_dswp[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.2.8 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "results = [Result(fs, fs.fc, fs.agg,\n",
    "        precision_recall_fscore_support(*naive_bayes(*transform_feat_to_num(remove_labels_for_binary_df(fs.train), \n",
    "        remove_labels_for_binary_df(fs.test))), average='macro')) for fs in all_fc_datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.233788</td>\n",
       "      <td>0.586420</td>\n",
       "      <td>0.547297</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.194139</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.527027</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.365079</td>\n",
       "      <td>0.527302</td>\n",
       "      <td>0.541506</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.207621</td>\n",
       "      <td>0.584337</td>\n",
       "      <td>0.533784</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.197724</td>\n",
       "      <td>0.364130</td>\n",
       "      <td>0.484217</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.212507</td>\n",
       "      <td>0.448718</td>\n",
       "      <td>0.491162</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.219141</td>\n",
       "      <td>0.412360</td>\n",
       "      <td>0.475379</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.220009</td>\n",
       "      <td>0.619565</td>\n",
       "      <td>0.513889</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.176101</td>\n",
       "      <td>0.106870</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.176101</td>\n",
       "      <td>0.106870</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.176101</td>\n",
       "      <td>0.106870</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.176101</td>\n",
       "      <td>0.106870</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.490347</td>\n",
       "      <td>0.490347</td>\n",
       "      <td>0.490347</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.490347</td>\n",
       "      <td>0.490347</td>\n",
       "      <td>0.490347</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.194139</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.527027</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.594470</td>\n",
       "      <td>0.646384</td>\n",
       "      <td>0.580116</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.579381</td>\n",
       "      <td>0.598734</td>\n",
       "      <td>0.573864</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.561158</td>\n",
       "      <td>0.587369</td>\n",
       "      <td>0.558081</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.286507</td>\n",
       "      <td>0.386957</td>\n",
       "      <td>0.376894</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.624373</td>\n",
       "      <td>0.664474</td>\n",
       "      <td>0.610524</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.601277</td>\n",
       "      <td>0.627327</td>\n",
       "      <td>0.591886</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.459236</td>\n",
       "      <td>0.475333</td>\n",
       "      <td>0.493585</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.575196</td>\n",
       "      <td>0.644988</td>\n",
       "      <td>0.569088</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.483932</td>\n",
       "      <td>0.484475</td>\n",
       "      <td>0.483591</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.458462</td>\n",
       "      <td>0.456140</td>\n",
       "      <td>0.461390</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.458067</td>\n",
       "      <td>0.443050</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.524865</td>\n",
       "      <td>0.535162</td>\n",
       "      <td>0.524131</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.464387</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.501894</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.490349</td>\n",
       "      <td>0.527915</td>\n",
       "      <td>0.510732</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.467058</td>\n",
       "      <td>0.470427</td>\n",
       "      <td>0.482955</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.423313</td>\n",
       "      <td>0.379121</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.617208</td>\n",
       "      <td>0.680250</td>\n",
       "      <td>0.602375</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.631255</td>\n",
       "      <td>0.680842</td>\n",
       "      <td>0.615378</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.548986</td>\n",
       "      <td>0.564384</td>\n",
       "      <td>0.546463</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.526832</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.539008</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.490347</td>\n",
       "      <td>0.490347</td>\n",
       "      <td>0.490347</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.490347</td>\n",
       "      <td>0.490347</td>\n",
       "      <td>0.490347</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.194139</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.527027</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.594470</td>\n",
       "      <td>0.646384</td>\n",
       "      <td>0.580116</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.579381</td>\n",
       "      <td>0.598734</td>\n",
       "      <td>0.573864</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.561158</td>\n",
       "      <td>0.587369</td>\n",
       "      <td>0.558081</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.286507</td>\n",
       "      <td>0.386957</td>\n",
       "      <td>0.376894</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.624373</td>\n",
       "      <td>0.664474</td>\n",
       "      <td>0.610524</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.601277</td>\n",
       "      <td>0.627327</td>\n",
       "      <td>0.591886</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.459236</td>\n",
       "      <td>0.475333</td>\n",
       "      <td>0.493585</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.575196</td>\n",
       "      <td>0.644988</td>\n",
       "      <td>0.569088</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.503590</td>\n",
       "      <td>0.504386</td>\n",
       "      <td>0.503861</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.566807</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.557915</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.137255</td>\n",
       "      <td>0.079545</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.565789</td>\n",
       "      <td>0.579487</td>\n",
       "      <td>0.559846</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.673759</td>\n",
       "      <td>0.667536</td>\n",
       "      <td>0.682449</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.640947</td>\n",
       "      <td>0.700203</td>\n",
       "      <td>0.624369</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.276923</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.653248</td>\n",
       "      <td>0.655903</td>\n",
       "      <td>0.650884</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.610445</td>\n",
       "      <td>0.620874</td>\n",
       "      <td>0.604109</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.644597</td>\n",
       "      <td>0.643688</td>\n",
       "      <td>0.645544</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.599786</td>\n",
       "      <td>0.651251</td>\n",
       "      <td>0.588592</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.151947</td>\n",
       "      <td>0.580460</td>\n",
       "      <td>0.506757</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.469448</td>\n",
       "      <td>0.551282</td>\n",
       "      <td>0.508838</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>0.613043</td>\n",
       "      <td>0.623106</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.457156</td>\n",
       "      <td>0.644231</td>\n",
       "      <td>0.506501</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.472989</td>\n",
       "      <td>0.645349</td>\n",
       "      <td>0.513003</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.587786</td>\n",
       "      <td>0.583779</td>\n",
       "      <td>0.601855</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.434125</td>\n",
       "      <td>0.391051</td>\n",
       "      <td>0.487864</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.424710</td>\n",
       "      <td>0.424710</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.424710</td>\n",
       "      <td>0.424710</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.424710</td>\n",
       "      <td>0.424710</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.424710</td>\n",
       "      <td>0.424710</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.531121</td>\n",
       "      <td>0.605090</td>\n",
       "      <td>0.540404</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.531121</td>\n",
       "      <td>0.605090</td>\n",
       "      <td>0.540404</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.531121</td>\n",
       "      <td>0.605090</td>\n",
       "      <td>0.540404</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.531121</td>\n",
       "      <td>0.605090</td>\n",
       "      <td>0.540404</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.491810</td>\n",
       "      <td>0.772287</td>\n",
       "      <td>0.524359</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.491810</td>\n",
       "      <td>0.772287</td>\n",
       "      <td>0.524359</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.491810</td>\n",
       "      <td>0.772287</td>\n",
       "      <td>0.524359</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.491810</td>\n",
       "      <td>0.772287</td>\n",
       "      <td>0.524359</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.414634</td>\n",
       "      <td>0.459459</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.472973</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.238538</td>\n",
       "      <td>0.457949</td>\n",
       "      <td>0.460425</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.432258</td>\n",
       "      <td>0.413580</td>\n",
       "      <td>0.452703</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.404255</td>\n",
       "      <td>0.563763</td>\n",
       "      <td>0.563763</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.449712</td>\n",
       "      <td>0.463585</td>\n",
       "      <td>0.494365</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.448234</td>\n",
       "      <td>0.454232</td>\n",
       "      <td>0.491938</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.448234</td>\n",
       "      <td>0.454232</td>\n",
       "      <td>0.491938</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.238538</td>\n",
       "      <td>0.435160</td>\n",
       "      <td>0.431467</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.414634</td>\n",
       "      <td>0.459459</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.283998</td>\n",
       "      <td>0.515525</td>\n",
       "      <td>0.516409</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.188044</td>\n",
       "      <td>0.431217</td>\n",
       "      <td>0.462355</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.285212</td>\n",
       "      <td>0.427426</td>\n",
       "      <td>0.440025</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.469448</td>\n",
       "      <td>0.551282</td>\n",
       "      <td>0.508838</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.508768</td>\n",
       "      <td>0.581555</td>\n",
       "      <td>0.608586</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.444545</td>\n",
       "      <td>0.564935</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.483832</td>\n",
       "      <td>0.583169</td>\n",
       "      <td>0.514650</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.493034</td>\n",
       "      <td>0.562667</td>\n",
       "      <td>0.516297</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.517010</td>\n",
       "      <td>0.676987</td>\n",
       "      <td>0.534934</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.481884</td>\n",
       "      <td>0.561924</td>\n",
       "      <td>0.512223</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.166316</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.166316</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.166316</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.166316</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.189655</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.189655</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.189655</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.189655</td>\n",
       "      <td>0.117021</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.181487</td>\n",
       "      <td>0.607280</td>\n",
       "      <td>0.502427</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.176101</td>\n",
       "      <td>0.106870</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.176101</td>\n",
       "      <td>0.106870</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.176101</td>\n",
       "      <td>0.106870</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.442646</td>\n",
       "      <td>0.444292</td>\n",
       "      <td>0.441120</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.442646</td>\n",
       "      <td>0.444292</td>\n",
       "      <td>0.441120</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.237750</td>\n",
       "      <td>0.487013</td>\n",
       "      <td>0.489382</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.213370</td>\n",
       "      <td>0.464838</td>\n",
       "      <td>0.475869</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.587719</td>\n",
       "      <td>0.614286</td>\n",
       "      <td>0.580808</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.579381</td>\n",
       "      <td>0.598734</td>\n",
       "      <td>0.573864</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.306549</td>\n",
       "      <td>0.463331</td>\n",
       "      <td>0.469697</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.564058</td>\n",
       "      <td>0.562229</td>\n",
       "      <td>0.572601</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.563183</td>\n",
       "      <td>0.631439</td>\n",
       "      <td>0.560160</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.590101</td>\n",
       "      <td>0.668819</td>\n",
       "      <td>0.580444</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.460923</td>\n",
       "      <td>0.483339</td>\n",
       "      <td>0.496012</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.550888</td>\n",
       "      <td>0.616775</td>\n",
       "      <td>0.551231</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.462972</td>\n",
       "      <td>0.493939</td>\n",
       "      <td>0.489382</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.151947</td>\n",
       "      <td>0.580460</td>\n",
       "      <td>0.506757</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.673611</td>\n",
       "      <td>0.673611</td>\n",
       "      <td>0.673611</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.650831</td>\n",
       "      <td>0.727820</td>\n",
       "      <td>0.631313</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.549899</td>\n",
       "      <td>0.568223</td>\n",
       "      <td>0.594066</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.653248</td>\n",
       "      <td>0.655903</td>\n",
       "      <td>0.650884</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.455659</td>\n",
       "      <td>0.560489</td>\n",
       "      <td>0.504074</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.472989</td>\n",
       "      <td>0.645349</td>\n",
       "      <td>0.513003</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.626428</td>\n",
       "      <td>0.620439</td>\n",
       "      <td>0.657074</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.434125</td>\n",
       "      <td>0.391051</td>\n",
       "      <td>0.487864</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.442646</td>\n",
       "      <td>0.444292</td>\n",
       "      <td>0.441120</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.442646</td>\n",
       "      <td>0.444292</td>\n",
       "      <td>0.441120</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.237750</td>\n",
       "      <td>0.487013</td>\n",
       "      <td>0.489382</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.200830</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.469112</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.587719</td>\n",
       "      <td>0.614286</td>\n",
       "      <td>0.580808</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.579381</td>\n",
       "      <td>0.598734</td>\n",
       "      <td>0.573864</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.393548</td>\n",
       "      <td>0.539804</td>\n",
       "      <td>0.541035</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.550239</td>\n",
       "      <td>0.553154</td>\n",
       "      <td>0.567551</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.563183</td>\n",
       "      <td>0.631439</td>\n",
       "      <td>0.560160</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.590101</td>\n",
       "      <td>0.668819</td>\n",
       "      <td>0.580444</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.460923</td>\n",
       "      <td>0.483339</td>\n",
       "      <td>0.496012</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.553689</td>\n",
       "      <td>0.627893</td>\n",
       "      <td>0.553658</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                agg  \\\n",
       "0                                                                                                                              mean   \n",
       "1                                                                                                                               max   \n",
       "2                                                                                                                               min   \n",
       "3                                                                                                                     weighted_mean   \n",
       "4                                                                                                                              mean   \n",
       "5                                                                                                                               max   \n",
       "6                                                                                                                               min   \n",
       "7                                                                                                                     weighted_mean   \n",
       "8                                                                                                                              mean   \n",
       "9                                                                                                                               max   \n",
       "10                                                                                                                              min   \n",
       "11                                                                                                                    weighted_mean   \n",
       "12                                                                                                                             mean   \n",
       "13                                                                                                                              max   \n",
       "14                                                                                                                              min   \n",
       "15                                                                                                                    weighted_mean   \n",
       "16                                                                                                                             mean   \n",
       "17                                                                                                                              max   \n",
       "18                                                                                                                              min   \n",
       "19                                                                                                                    weighted_mean   \n",
       "20                                                                                                                             mean   \n",
       "21                                                                                                                              max   \n",
       "22                                                                                                                              min   \n",
       "23                                                                                                                    weighted_mean   \n",
       "24                                                                                                                             mean   \n",
       "25                                                                                                                              max   \n",
       "26                                                                                                                              min   \n",
       "27                                                                                                                    weighted_mean   \n",
       "28                                                                                                                             mean   \n",
       "29                                                                                                                              max   \n",
       "30                                                                                                                              min   \n",
       "31                                                                                                                    weighted_mean   \n",
       "32                                                                                                                             mean   \n",
       "33                                                                                                                              max   \n",
       "34                                                                                                                              min   \n",
       "35                                                                                                                    weighted_mean   \n",
       "36                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "37                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "38                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "39                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "40                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "41                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "42                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "43                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "44                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "45                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "46                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "47                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "48                                                                                                                             mean   \n",
       "49                                                                                                                              max   \n",
       "50                                                                                                                              min   \n",
       "51                                                                                                                    weighted_mean   \n",
       "52                                                                                                                             mean   \n",
       "53                                                                                                                              max   \n",
       "54                                                                                                                              min   \n",
       "55                                                                                                                    weighted_mean   \n",
       "56                                                                                                                             mean   \n",
       "57                                                                                                                              max   \n",
       "58                                                                                                                              min   \n",
       "59                                                                                                                    weighted_mean   \n",
       "60                                                                                                                             mean   \n",
       "61                                                                                                                              max   \n",
       "62                                                                                                                              min   \n",
       "63                                                                                                                    weighted_mean   \n",
       "64                                                                                                                             mean   \n",
       "65                                                                                                                              max   \n",
       "66                                                                                                                              min   \n",
       "67                                                                                                                    weighted_mean   \n",
       "68                                                                                                                             mean   \n",
       "69                                                                                                                              max   \n",
       "70                                                                                                                              min   \n",
       "71                                                                                                                    weighted_mean   \n",
       "72                                                                                                                             mean   \n",
       "73                                                                                                                              max   \n",
       "74                                                                                                                              min   \n",
       "75                                                                                                                    weighted_mean   \n",
       "76                                                                                                                             mean   \n",
       "77                                                                                                                              max   \n",
       "78                                                                                                                              min   \n",
       "79                                                                                                                    weighted_mean   \n",
       "80                                                                                                                             mean   \n",
       "81                                                                                                                              max   \n",
       "82                                                                                                                              min   \n",
       "83                                                                                                                    weighted_mean   \n",
       "84                                                                                                                             mean   \n",
       "85                                                                                                                              max   \n",
       "86                                                                                                                              min   \n",
       "87                                                                                                                    weighted_mean   \n",
       "88                                                                                                                             mean   \n",
       "89                                                                                                                              max   \n",
       "90                                                                                                                              min   \n",
       "91                                                                                                                    weighted_mean   \n",
       "92                                                                                                                             mean   \n",
       "93                                                                                                                              max   \n",
       "94                                                                                                                              min   \n",
       "95                                                                                                                    weighted_mean   \n",
       "96                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "97                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "98                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "99                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "100                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "101                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "102                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "103                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "104                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "105                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "106                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "107                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "108                                                                                                                            mean   \n",
       "109                                                                                                                             max   \n",
       "110                                                                                                                             min   \n",
       "111                                                                                                                   weighted_mean   \n",
       "112                                                                                                                            mean   \n",
       "113                                                                                                                             max   \n",
       "114                                                                                                                             min   \n",
       "115                                                                                                                   weighted_mean   \n",
       "116                                                                                                                            mean   \n",
       "117                                                                                                                             max   \n",
       "118                                                                                                                             min   \n",
       "119                                                                                                                   weighted_mean   \n",
       "120  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "121            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "122            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "123        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "124  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "125            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "126            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "127        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "128  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "129            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "130            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "131        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "132                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "133                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "134                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "135                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "136                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "137                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "138                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "139                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "140                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "141                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "142                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "143                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "144                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "145                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "146                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "147                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "148                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "149                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "150                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "151                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "152                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "153                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "154                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "155                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "\n",
       "       dataset        f1      prec       rec                        zc  \n",
       "0    Wikipedia  0.233788  0.586420  0.547297                linguistic  \n",
       "1    Wikipedia  0.194139  0.583333  0.527027                linguistic  \n",
       "2    Wikipedia  0.365079  0.527302  0.541506                linguistic  \n",
       "3    Wikipedia  0.207621  0.584337  0.533784                linguistic  \n",
       "4     WikiNews  0.197724  0.364130  0.484217                linguistic  \n",
       "5     WikiNews  0.212507  0.448718  0.491162                linguistic  \n",
       "6     WikiNews  0.219141  0.412360  0.475379                linguistic  \n",
       "7     WikiNews  0.220009  0.619565  0.513889                linguistic  \n",
       "8         News  0.176101  0.106870  0.500000                linguistic  \n",
       "9         News  0.176101  0.106870  0.500000                linguistic  \n",
       "10        News  0.176101  0.106870  0.500000                linguistic  \n",
       "11        News  0.176101  0.106870  0.500000                linguistic  \n",
       "12   Wikipedia  0.490347  0.490347  0.490347                 frequency  \n",
       "13   Wikipedia  0.490347  0.490347  0.490347                 frequency  \n",
       "14   Wikipedia  0.194139  0.583333  0.527027                 frequency  \n",
       "15   Wikipedia  0.594470  0.646384  0.580116                 frequency  \n",
       "16    WikiNews  0.579381  0.598734  0.573864                 frequency  \n",
       "17    WikiNews  0.561158  0.587369  0.558081                 frequency  \n",
       "18    WikiNews  0.286507  0.386957  0.376894                 frequency  \n",
       "19    WikiNews  0.433735  0.382979  0.500000                 frequency  \n",
       "20        News  0.624373  0.664474  0.610524                 frequency  \n",
       "21        News  0.601277  0.627327  0.591886                 frequency  \n",
       "22        News  0.459236  0.475333  0.493585                 frequency  \n",
       "23        News  0.575196  0.644988  0.569088                 frequency  \n",
       "24   Wikipedia  0.483932  0.484475  0.483591            language_model  \n",
       "25   Wikipedia  0.458462  0.456140  0.461390            language_model  \n",
       "26   Wikipedia  0.447214  0.458067  0.443050            language_model  \n",
       "27   Wikipedia  0.524865  0.535162  0.524131            language_model  \n",
       "28    WikiNews  0.464387  0.508333  0.501894            language_model  \n",
       "29    WikiNews  0.490349  0.527915  0.510732            language_model  \n",
       "30    WikiNews  0.467058  0.470427  0.482955            language_model  \n",
       "31    WikiNews  0.423313  0.379121  0.479167            language_model  \n",
       "32        News  0.617208  0.680250  0.602375            language_model  \n",
       "33        News  0.631255  0.680842  0.615378            language_model  \n",
       "34        News  0.548986  0.564384  0.546463            language_model  \n",
       "35        News  0.526832  0.650000  0.539008            language_model  \n",
       "36   Wikipedia  0.490347  0.490347  0.490347                    corpus  \n",
       "37   Wikipedia  0.490347  0.490347  0.490347                    corpus  \n",
       "38   Wikipedia  0.194139  0.583333  0.527027                    corpus  \n",
       "39   Wikipedia  0.594470  0.646384  0.580116                    corpus  \n",
       "40    WikiNews  0.579381  0.598734  0.573864                    corpus  \n",
       "41    WikiNews  0.561158  0.587369  0.558081                    corpus  \n",
       "42    WikiNews  0.286507  0.386957  0.376894                    corpus  \n",
       "43    WikiNews  0.433735  0.382979  0.500000                    corpus  \n",
       "44        News  0.624373  0.664474  0.610524                    corpus  \n",
       "45        News  0.601277  0.627327  0.591886                    corpus  \n",
       "46        News  0.459236  0.475333  0.493585                    corpus  \n",
       "47        News  0.575196  0.644988  0.569088                    corpus  \n",
       "48   Wikipedia  0.503590  0.504386  0.503861          psycholinguistic  \n",
       "49   Wikipedia  0.566807  0.678571  0.557915          psycholinguistic  \n",
       "50   Wikipedia  0.137255  0.079545  0.500000          psycholinguistic  \n",
       "51   Wikipedia  0.565789  0.579487  0.559846          psycholinguistic  \n",
       "52    WikiNews  0.673759  0.667536  0.682449          psycholinguistic  \n",
       "53    WikiNews  0.640947  0.700203  0.624369          psycholinguistic  \n",
       "54    WikiNews  0.276923  0.625000  0.541667          psycholinguistic  \n",
       "55    WikiNews  0.653248  0.655903  0.650884          psycholinguistic  \n",
       "56        News  0.610445  0.620874  0.604109          psycholinguistic  \n",
       "57        News  0.440171  0.393130  0.500000          psycholinguistic  \n",
       "58        News  0.644597  0.643688  0.645544          psycholinguistic  \n",
       "59        News  0.599786  0.651251  0.588592          psycholinguistic  \n",
       "60   Wikipedia  0.453416  0.419540  0.493243                   wordnet  \n",
       "61   Wikipedia  0.453416  0.419540  0.493243                   wordnet  \n",
       "62   Wikipedia  0.453416  0.419540  0.493243                   wordnet  \n",
       "63   Wikipedia  0.151947  0.580460  0.506757                   wordnet  \n",
       "64    WikiNews  0.433735  0.382979  0.500000                   wordnet  \n",
       "65    WikiNews  0.469448  0.551282  0.508838                   wordnet  \n",
       "66    WikiNews  0.617021  0.613043  0.623106                   wordnet  \n",
       "67    WikiNews  0.433735  0.382979  0.500000                   wordnet  \n",
       "68        News  0.457156  0.644231  0.506501                   wordnet  \n",
       "69        News  0.472989  0.645349  0.513003                   wordnet  \n",
       "70        News  0.587786  0.583779  0.601855                   wordnet  \n",
       "71        News  0.434125  0.391051  0.487864                   wordnet  \n",
       "72   Wikipedia  0.227273  0.424710  0.424710                   dbpedia  \n",
       "73   Wikipedia  0.227273  0.424710  0.424710                   dbpedia  \n",
       "74   Wikipedia  0.227273  0.424710  0.424710                   dbpedia  \n",
       "75   Wikipedia  0.227273  0.424710  0.424710                   dbpedia  \n",
       "76    WikiNews  0.531121  0.605090  0.540404                   dbpedia  \n",
       "77    WikiNews  0.531121  0.605090  0.540404                   dbpedia  \n",
       "78    WikiNews  0.531121  0.605090  0.540404                   dbpedia  \n",
       "79    WikiNews  0.531121  0.605090  0.540404                   dbpedia  \n",
       "80        News  0.491810  0.772287  0.524359                   dbpedia  \n",
       "81        News  0.491810  0.772287  0.524359                   dbpedia  \n",
       "82        News  0.491810  0.772287  0.524359                   dbpedia  \n",
       "83        News  0.491810  0.772287  0.524359                   dbpedia  \n",
       "84   Wikipedia  0.435897  0.414634  0.459459          brown_clustering  \n",
       "85   Wikipedia  0.443038  0.416667  0.472973          brown_clustering  \n",
       "86   Wikipedia  0.238538  0.457949  0.460425          brown_clustering  \n",
       "87   Wikipedia  0.432258  0.413580  0.452703          brown_clustering  \n",
       "88    WikiNews  0.433735  0.382979  0.500000          brown_clustering  \n",
       "89    WikiNews  0.433735  0.382979  0.500000          brown_clustering  \n",
       "90    WikiNews  0.404255  0.563763  0.563763          brown_clustering  \n",
       "91    WikiNews  0.433735  0.382979  0.500000          brown_clustering  \n",
       "92        News  0.449712  0.463585  0.494365          brown_clustering  \n",
       "93        News  0.448234  0.454232  0.491938          brown_clustering  \n",
       "94        News  0.440171  0.393130  0.500000          brown_clustering  \n",
       "95        News  0.448234  0.454232  0.491938          brown_clustering  \n",
       "96   Wikipedia  0.238538  0.435160  0.431467                  semantic  \n",
       "97   Wikipedia  0.435897  0.414634  0.459459                  semantic  \n",
       "98   Wikipedia  0.283998  0.515525  0.516409                  semantic  \n",
       "99   Wikipedia  0.188044  0.431217  0.462355                  semantic  \n",
       "100   WikiNews  0.285212  0.427426  0.440025                  semantic  \n",
       "101   WikiNews  0.469448  0.551282  0.508838                  semantic  \n",
       "102   WikiNews  0.508768  0.581555  0.608586                  semantic  \n",
       "103   WikiNews  0.444545  0.564935  0.575758                  semantic  \n",
       "104       News  0.483832  0.583169  0.514650                  semantic  \n",
       "105       News  0.493034  0.562667  0.516297                  semantic  \n",
       "106       News  0.517010  0.676987  0.534934                  semantic  \n",
       "107       News  0.481884  0.561924  0.512223                  semantic  \n",
       "108  Wikipedia  0.166316  0.581395  0.513514                dictionary  \n",
       "109  Wikipedia  0.166316  0.581395  0.513514                dictionary  \n",
       "110  Wikipedia  0.166316  0.581395  0.513514                dictionary  \n",
       "111  Wikipedia  0.166316  0.581395  0.513514                dictionary  \n",
       "112   WikiNews  0.189655  0.117021  0.500000                dictionary  \n",
       "113   WikiNews  0.189655  0.117021  0.500000                dictionary  \n",
       "114   WikiNews  0.189655  0.117021  0.500000                dictionary  \n",
       "115   WikiNews  0.189655  0.117021  0.500000                dictionary  \n",
       "116       News  0.181487  0.607280  0.502427                dictionary  \n",
       "117       News  0.176101  0.106870  0.500000                dictionary  \n",
       "118       News  0.176101  0.106870  0.500000                dictionary  \n",
       "119       News  0.176101  0.106870  0.500000                dictionary  \n",
       "120  Wikipedia  0.442646  0.444292  0.441120           corpus+semantic  \n",
       "121  Wikipedia  0.442646  0.444292  0.441120           corpus+semantic  \n",
       "122  Wikipedia  0.237750  0.487013  0.489382           corpus+semantic  \n",
       "123  Wikipedia  0.213370  0.464838  0.475869           corpus+semantic  \n",
       "124   WikiNews  0.587719  0.614286  0.580808           corpus+semantic  \n",
       "125   WikiNews  0.579381  0.598734  0.573864           corpus+semantic  \n",
       "126   WikiNews  0.306549  0.463331  0.469697           corpus+semantic  \n",
       "127   WikiNews  0.564058  0.562229  0.572601           corpus+semantic  \n",
       "128       News  0.563183  0.631439  0.560160           corpus+semantic  \n",
       "129       News  0.590101  0.668819  0.580444           corpus+semantic  \n",
       "130       News  0.460923  0.483339  0.496012           corpus+semantic  \n",
       "131       News  0.550888  0.616775  0.551231           corpus+semantic  \n",
       "132  Wikipedia  0.453416  0.419540  0.493243  wordnet+psycholinguistic  \n",
       "133  Wikipedia  0.453416  0.419540  0.493243  wordnet+psycholinguistic  \n",
       "134  Wikipedia  0.462972  0.493939  0.489382  wordnet+psycholinguistic  \n",
       "135  Wikipedia  0.151947  0.580460  0.506757  wordnet+psycholinguistic  \n",
       "136   WikiNews  0.673611  0.673611  0.673611  wordnet+psycholinguistic  \n",
       "137   WikiNews  0.650831  0.727820  0.631313  wordnet+psycholinguistic  \n",
       "138   WikiNews  0.549899  0.568223  0.594066  wordnet+psycholinguistic  \n",
       "139   WikiNews  0.653248  0.655903  0.650884  wordnet+psycholinguistic  \n",
       "140       News  0.455659  0.560489  0.504074  wordnet+psycholinguistic  \n",
       "141       News  0.472989  0.645349  0.513003  wordnet+psycholinguistic  \n",
       "142       News  0.626428  0.620439  0.657074  wordnet+psycholinguistic  \n",
       "143       News  0.434125  0.391051  0.487864  wordnet+psycholinguistic  \n",
       "144  Wikipedia  0.442646  0.444292  0.441120                       all  \n",
       "145  Wikipedia  0.442646  0.444292  0.441120                       all  \n",
       "146  Wikipedia  0.237750  0.487013  0.489382                       all  \n",
       "147  Wikipedia  0.200830  0.450000  0.469112                       all  \n",
       "148   WikiNews  0.587719  0.614286  0.580808                       all  \n",
       "149   WikiNews  0.579381  0.598734  0.573864                       all  \n",
       "150   WikiNews  0.393548  0.539804  0.541035                       all  \n",
       "151   WikiNews  0.550239  0.553154  0.567551                       all  \n",
       "152       News  0.563183  0.631439  0.560160                       all  \n",
       "153       News  0.590101  0.668819  0.580444                       all  \n",
       "154       News  0.460923  0.483339  0.496012                       all  \n",
       "155       News  0.553689  0.627893  0.553658                       all  "
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_eval_data_nb = create_eval_df_from_results_macro(results, False)\n",
    "feature_eval_data_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.594470</td>\n",
       "      <td>0.646384</td>\n",
       "      <td>0.580116</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.594470</td>\n",
       "      <td>0.646384</td>\n",
       "      <td>0.580116</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.673759</td>\n",
       "      <td>0.667536</td>\n",
       "      <td>0.682449</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.644597</td>\n",
       "      <td>0.643688</td>\n",
       "      <td>0.645544</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           agg    dataset  \\\n",
       "15                                               weighted_mean  Wikipedia   \n",
       "39  (weighted_mean, <function <lambda> at 0x000000FF52232E18>)  Wikipedia   \n",
       "52                                                        mean   WikiNews   \n",
       "58                                                         min       News   \n",
       "\n",
       "          f1      prec       rec                zc  \n",
       "15  0.594470  0.646384  0.580116         frequency  \n",
       "39  0.594470  0.646384  0.580116            corpus  \n",
       "52  0.673759  0.667536  0.682449  psycholinguistic  \n",
       "58  0.644597  0.643688  0.645544  psycholinguistic  "
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = feature_eval_data_nb.groupby(['dataset'])['f1'].transform(max) == feature_eval_data_nb['f1']\n",
    "feature_eval_data_nb[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "results = [Result(fs, fs.fc, fs.agg,\n",
    "        precision_recall_fscore_support(*naive_bayes(*transform_feat_to_num(remove_labels_for_binary_df(fs.train), \n",
    "        remove_labels_for_binary_df(fs.test))), average='macro')) for fs in all_fc_datasets_dswp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.498491</td>\n",
       "      <td>0.521687</td>\n",
       "      <td>0.508687</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.479842</td>\n",
       "      <td>0.887097</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.474593</td>\n",
       "      <td>0.635870</td>\n",
       "      <td>0.515783</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.479842</td>\n",
       "      <td>0.887097</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.437768</td>\n",
       "      <td>0.392308</td>\n",
       "      <td>0.495146</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.436559</td>\n",
       "      <td>0.391892</td>\n",
       "      <td>0.492718</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.438972</td>\n",
       "      <td>0.392720</td>\n",
       "      <td>0.497573</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.436559</td>\n",
       "      <td>0.391892</td>\n",
       "      <td>0.492718</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.539683</td>\n",
       "      <td>0.548263</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.306012</td>\n",
       "      <td>0.525684</td>\n",
       "      <td>0.529923</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.518750</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.528958</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.624893</td>\n",
       "      <td>0.658931</td>\n",
       "      <td>0.609073</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.562791</td>\n",
       "      <td>0.563927</td>\n",
       "      <td>0.561869</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.544560</td>\n",
       "      <td>0.543671</td>\n",
       "      <td>0.549874</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.469448</td>\n",
       "      <td>0.551282</td>\n",
       "      <td>0.508838</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.484388</td>\n",
       "      <td>0.508721</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.532977</td>\n",
       "      <td>0.539078</td>\n",
       "      <td>0.531900</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.470139</td>\n",
       "      <td>0.484348</td>\n",
       "      <td>0.478675</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.451194</td>\n",
       "      <td>0.475911</td>\n",
       "      <td>0.496793</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.502932</td>\n",
       "      <td>0.532197</td>\n",
       "      <td>0.514736</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.530130</td>\n",
       "      <td>0.533854</td>\n",
       "      <td>0.550193</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.533344</td>\n",
       "      <td>0.557682</td>\n",
       "      <td>0.603282</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.475339</td>\n",
       "      <td>0.492107</td>\n",
       "      <td>0.487452</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.562552</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.577220</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.484649</td>\n",
       "      <td>0.488393</td>\n",
       "      <td>0.491793</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.568279</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.570707</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.484649</td>\n",
       "      <td>0.488393</td>\n",
       "      <td>0.491793</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.512082</td>\n",
       "      <td>0.519409</td>\n",
       "      <td>0.514520</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.494891</td>\n",
       "      <td>0.531104</td>\n",
       "      <td>0.546203</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.466777</td>\n",
       "      <td>0.501055</td>\n",
       "      <td>0.501560</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.566394</td>\n",
       "      <td>0.580405</td>\n",
       "      <td>0.561893</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.521797</td>\n",
       "      <td>0.533039</td>\n",
       "      <td>0.546030</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.539683</td>\n",
       "      <td>0.548263</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.306012</td>\n",
       "      <td>0.525684</td>\n",
       "      <td>0.529923</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.518750</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.528958</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.624893</td>\n",
       "      <td>0.658931</td>\n",
       "      <td>0.609073</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.562791</td>\n",
       "      <td>0.563927</td>\n",
       "      <td>0.561869</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.544560</td>\n",
       "      <td>0.543671</td>\n",
       "      <td>0.549874</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.469448</td>\n",
       "      <td>0.551282</td>\n",
       "      <td>0.508838</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.484388</td>\n",
       "      <td>0.508721</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.532977</td>\n",
       "      <td>0.539078</td>\n",
       "      <td>0.531900</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.470139</td>\n",
       "      <td>0.484348</td>\n",
       "      <td>0.478675</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.451194</td>\n",
       "      <td>0.475911</td>\n",
       "      <td>0.496793</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.502932</td>\n",
       "      <td>0.532197</td>\n",
       "      <td>0.514736</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.461444</td>\n",
       "      <td>0.484220</td>\n",
       "      <td>0.473938</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.563692</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.592664</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.430670</td>\n",
       "      <td>0.458504</td>\n",
       "      <td>0.431467</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.549722</td>\n",
       "      <td>0.547225</td>\n",
       "      <td>0.555019</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.447405</td>\n",
       "      <td>0.477200</td>\n",
       "      <td>0.468434</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.391238</td>\n",
       "      <td>0.422227</td>\n",
       "      <td>0.393308</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.542208</td>\n",
       "      <td>0.547059</td>\n",
       "      <td>0.560606</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.427826</td>\n",
       "      <td>0.435065</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.547496</td>\n",
       "      <td>0.548577</td>\n",
       "      <td>0.562153</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.480679</td>\n",
       "      <td>0.484779</td>\n",
       "      <td>0.481796</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.609113</td>\n",
       "      <td>0.603927</td>\n",
       "      <td>0.618845</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.512929</td>\n",
       "      <td>0.517879</td>\n",
       "      <td>0.514043</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.498491</td>\n",
       "      <td>0.521687</td>\n",
       "      <td>0.508687</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.442424</td>\n",
       "      <td>0.540598</td>\n",
       "      <td>0.573359</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.151947</td>\n",
       "      <td>0.580460</td>\n",
       "      <td>0.506757</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.457156</td>\n",
       "      <td>0.644231</td>\n",
       "      <td>0.506501</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.438972</td>\n",
       "      <td>0.392720</td>\n",
       "      <td>0.497573</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.472989</td>\n",
       "      <td>0.645349</td>\n",
       "      <td>0.513003</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.468658</td>\n",
       "      <td>0.486026</td>\n",
       "      <td>0.495232</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.524865</td>\n",
       "      <td>0.535162</td>\n",
       "      <td>0.524131</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.524865</td>\n",
       "      <td>0.535162</td>\n",
       "      <td>0.524131</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.524865</td>\n",
       "      <td>0.535162</td>\n",
       "      <td>0.524131</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.524865</td>\n",
       "      <td>0.535162</td>\n",
       "      <td>0.524131</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.553656</td>\n",
       "      <td>0.769444</td>\n",
       "      <td>0.561237</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.553656</td>\n",
       "      <td>0.769444</td>\n",
       "      <td>0.561237</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.553656</td>\n",
       "      <td>0.769444</td>\n",
       "      <td>0.561237</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.553656</td>\n",
       "      <td>0.769444</td>\n",
       "      <td>0.561237</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.505349</td>\n",
       "      <td>0.575749</td>\n",
       "      <td>0.522798</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.505349</td>\n",
       "      <td>0.575749</td>\n",
       "      <td>0.522798</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.505349</td>\n",
       "      <td>0.575749</td>\n",
       "      <td>0.522798</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.505349</td>\n",
       "      <td>0.575749</td>\n",
       "      <td>0.522798</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.369697</td>\n",
       "      <td>0.470085</td>\n",
       "      <td>0.445946</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.246497</td>\n",
       "      <td>0.587500</td>\n",
       "      <td>0.554054</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.478764</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.487435</td>\n",
       "      <td>0.539377</td>\n",
       "      <td>0.554293</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.339229</td>\n",
       "      <td>0.507310</td>\n",
       "      <td>0.506313</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.474593</td>\n",
       "      <td>0.635870</td>\n",
       "      <td>0.515783</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.411631</td>\n",
       "      <td>0.519629</td>\n",
       "      <td>0.523359</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.498648</td>\n",
       "      <td>0.542977</td>\n",
       "      <td>0.515517</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.355020</td>\n",
       "      <td>0.474462</td>\n",
       "      <td>0.468707</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.458657</td>\n",
       "      <td>0.894636</td>\n",
       "      <td>0.508929</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.510799</td>\n",
       "      <td>0.536624</td>\n",
       "      <td>0.518811</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.540399</td>\n",
       "      <td>0.568783</td>\n",
       "      <td>0.537645</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.540399</td>\n",
       "      <td>0.568783</td>\n",
       "      <td>0.537645</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.557492</td>\n",
       "      <td>0.627711</td>\n",
       "      <td>0.551158</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.606455</td>\n",
       "      <td>0.598328</td>\n",
       "      <td>0.641892</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.479952</td>\n",
       "      <td>0.544841</td>\n",
       "      <td>0.509795</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.478037</td>\n",
       "      <td>0.530786</td>\n",
       "      <td>0.507368</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.474759</td>\n",
       "      <td>0.729086</td>\n",
       "      <td>0.515430</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.484759</td>\n",
       "      <td>0.519309</td>\n",
       "      <td>0.506588</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.530021</td>\n",
       "      <td>0.555985</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.410714</td>\n",
       "      <td>0.501053</td>\n",
       "      <td>0.501931</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.496503</td>\n",
       "      <td>0.499237</td>\n",
       "      <td>0.499035</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.461444</td>\n",
       "      <td>0.484220</td>\n",
       "      <td>0.473938</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.462857</td>\n",
       "      <td>0.487115</td>\n",
       "      <td>0.482323</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.389971</td>\n",
       "      <td>0.447998</td>\n",
       "      <td>0.428662</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.516958</td>\n",
       "      <td>0.517689</td>\n",
       "      <td>0.520202</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.426525</td>\n",
       "      <td>0.445402</td>\n",
       "      <td>0.428030</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.495232</td>\n",
       "      <td>0.525763</td>\n",
       "      <td>0.538055</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.425439</td>\n",
       "      <td>0.527390</td>\n",
       "      <td>0.536755</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.465786</td>\n",
       "      <td>0.468537</td>\n",
       "      <td>0.464719</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.478428</td>\n",
       "      <td>0.494002</td>\n",
       "      <td>0.491678</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.584390</td>\n",
       "      <td>0.587179</td>\n",
       "      <td>0.582046</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.580952</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>0.606178</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.511654</td>\n",
       "      <td>0.590196</td>\n",
       "      <td>0.522201</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.628138</td>\n",
       "      <td>0.632308</td>\n",
       "      <td>0.624517</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.585737</td>\n",
       "      <td>0.648810</td>\n",
       "      <td>0.578914</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.555784</td>\n",
       "      <td>0.616340</td>\n",
       "      <td>0.556187</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.469448</td>\n",
       "      <td>0.551282</td>\n",
       "      <td>0.508838</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.464387</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.501894</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.527928</td>\n",
       "      <td>0.594035</td>\n",
       "      <td>0.535801</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.501630</td>\n",
       "      <td>0.647638</td>\n",
       "      <td>0.526006</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.469488</td>\n",
       "      <td>0.561198</td>\n",
       "      <td>0.508148</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.488861</td>\n",
       "      <td>0.538018</td>\n",
       "      <td>0.511442</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.489425</td>\n",
       "      <td>0.500635</td>\n",
       "      <td>0.500965</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.529097</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>0.581081</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.471413</td>\n",
       "      <td>0.474316</td>\n",
       "      <td>0.470077</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.492219</td>\n",
       "      <td>0.509936</td>\n",
       "      <td>0.516409</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.426829</td>\n",
       "      <td>0.380435</td>\n",
       "      <td>0.486111</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.408805</td>\n",
       "      <td>0.373563</td>\n",
       "      <td>0.451389</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.459397</td>\n",
       "      <td>0.482022</td>\n",
       "      <td>0.494949</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.423313</td>\n",
       "      <td>0.379121</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.452557</td>\n",
       "      <td>0.452744</td>\n",
       "      <td>0.483877</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.449712</td>\n",
       "      <td>0.463585</td>\n",
       "      <td>0.494365</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.505980</td>\n",
       "      <td>0.516409</td>\n",
       "      <td>0.510749</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>0.520299</td>\n",
       "      <td>0.511529</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.584390</td>\n",
       "      <td>0.587179</td>\n",
       "      <td>0.582046</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.580952</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>0.606178</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.511654</td>\n",
       "      <td>0.590196</td>\n",
       "      <td>0.522201</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.628138</td>\n",
       "      <td>0.632308</td>\n",
       "      <td>0.624517</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.585737</td>\n",
       "      <td>0.648810</td>\n",
       "      <td>0.578914</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.555784</td>\n",
       "      <td>0.616340</td>\n",
       "      <td>0.556187</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.469448</td>\n",
       "      <td>0.551282</td>\n",
       "      <td>0.508838</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.464387</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.501894</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.530480</td>\n",
       "      <td>0.605882</td>\n",
       "      <td>0.538228</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.501630</td>\n",
       "      <td>0.647638</td>\n",
       "      <td>0.526006</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.469488</td>\n",
       "      <td>0.561198</td>\n",
       "      <td>0.508148</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.488861</td>\n",
       "      <td>0.538018</td>\n",
       "      <td>0.511442</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                agg  \\\n",
       "0                                                                                                                              mean   \n",
       "1                                                                                                                               max   \n",
       "2                                                                                                                               min   \n",
       "3                                                                                                                     weighted_mean   \n",
       "4                                                                                                                              mean   \n",
       "5                                                                                                                               max   \n",
       "6                                                                                                                               min   \n",
       "7                                                                                                                     weighted_mean   \n",
       "8                                                                                                                              mean   \n",
       "9                                                                                                                               max   \n",
       "10                                                                                                                              min   \n",
       "11                                                                                                                    weighted_mean   \n",
       "12                                                                                                                             mean   \n",
       "13                                                                                                                              max   \n",
       "14                                                                                                                              min   \n",
       "15                                                                                                                    weighted_mean   \n",
       "16                                                                                                                             mean   \n",
       "17                                                                                                                              max   \n",
       "18                                                                                                                              min   \n",
       "19                                                                                                                    weighted_mean   \n",
       "20                                                                                                                             mean   \n",
       "21                                                                                                                              max   \n",
       "22                                                                                                                              min   \n",
       "23                                                                                                                    weighted_mean   \n",
       "24                                                                                                                             mean   \n",
       "25                                                                                                                              max   \n",
       "26                                                                                                                              min   \n",
       "27                                                                                                                    weighted_mean   \n",
       "28                                                                                                                             mean   \n",
       "29                                                                                                                              max   \n",
       "30                                                                                                                              min   \n",
       "31                                                                                                                    weighted_mean   \n",
       "32                                                                                                                             mean   \n",
       "33                                                                                                                              max   \n",
       "34                                                                                                                              min   \n",
       "35                                                                                                                    weighted_mean   \n",
       "36                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "37                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "38                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "39                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "40                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "41                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "42                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "43                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "44                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "45                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "46                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "47                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "48                                                                                                                             mean   \n",
       "49                                                                                                                              max   \n",
       "50                                                                                                                              min   \n",
       "51                                                                                                                    weighted_mean   \n",
       "52                                                                                                                             mean   \n",
       "53                                                                                                                              max   \n",
       "54                                                                                                                              min   \n",
       "55                                                                                                                    weighted_mean   \n",
       "56                                                                                                                             mean   \n",
       "57                                                                                                                              max   \n",
       "58                                                                                                                              min   \n",
       "59                                                                                                                    weighted_mean   \n",
       "60                                                                                                                             mean   \n",
       "61                                                                                                                              max   \n",
       "62                                                                                                                              min   \n",
       "63                                                                                                                    weighted_mean   \n",
       "64                                                                                                                             mean   \n",
       "65                                                                                                                              max   \n",
       "66                                                                                                                              min   \n",
       "67                                                                                                                    weighted_mean   \n",
       "68                                                                                                                             mean   \n",
       "69                                                                                                                              max   \n",
       "70                                                                                                                              min   \n",
       "71                                                                                                                    weighted_mean   \n",
       "72                                                                                                                             mean   \n",
       "73                                                                                                                              max   \n",
       "74                                                                                                                              min   \n",
       "75                                                                                                                    weighted_mean   \n",
       "76                                                                                                                             mean   \n",
       "77                                                                                                                              max   \n",
       "78                                                                                                                              min   \n",
       "79                                                                                                                    weighted_mean   \n",
       "80                                                                                                                             mean   \n",
       "81                                                                                                                              max   \n",
       "82                                                                                                                              min   \n",
       "83                                                                                                                    weighted_mean   \n",
       "84                                                                                                                             mean   \n",
       "85                                                                                                                              max   \n",
       "86                                                                                                                              min   \n",
       "87                                                                                                                    weighted_mean   \n",
       "88                                                                                                                             mean   \n",
       "89                                                                                                                              max   \n",
       "90                                                                                                                              min   \n",
       "91                                                                                                                    weighted_mean   \n",
       "92                                                                                                                             mean   \n",
       "93                                                                                                                              max   \n",
       "94                                                                                                                              min   \n",
       "95                                                                                                                    weighted_mean   \n",
       "96                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "97                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "98                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "99                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "100                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "101                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "102                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "103                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "104                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "105                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "106                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "107                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "108                                                                                                                            mean   \n",
       "109                                                                                                                             max   \n",
       "110                                                                                                                             min   \n",
       "111                                                                                                                   weighted_mean   \n",
       "112                                                                                                                            mean   \n",
       "113                                                                                                                             max   \n",
       "114                                                                                                                             min   \n",
       "115                                                                                                                   weighted_mean   \n",
       "116                                                                                                                            mean   \n",
       "117                                                                                                                             max   \n",
       "118                                                                                                                             min   \n",
       "119                                                                                                                   weighted_mean   \n",
       "120  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "121            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "122            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "123        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "124  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "125            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "126            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "127        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "128  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "129            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "130            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "131        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "132                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "133                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "134                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "135                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "136                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "137                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "138                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "139                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "140                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "141                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "142                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "143                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "144                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "145                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "146                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "147                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "148                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "149                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "150                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "151                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "152                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "153                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "154                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "155                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "\n",
       "       dataset        f1      prec       rec                        zc  \n",
       "0    Wikipedia  0.453416  0.419540  0.493243                linguistic  \n",
       "1    Wikipedia  0.498491  0.521687  0.508687                linguistic  \n",
       "2    Wikipedia  0.456790  0.420455  0.500000                linguistic  \n",
       "3    Wikipedia  0.450000  0.418605  0.486486                linguistic  \n",
       "4     WikiNews  0.479842  0.887097  0.522727                linguistic  \n",
       "5     WikiNews  0.474593  0.635870  0.515783                linguistic  \n",
       "6     WikiNews  0.433735  0.382979  0.500000                linguistic  \n",
       "7     WikiNews  0.479842  0.887097  0.522727                linguistic  \n",
       "8         News  0.437768  0.392308  0.495146                linguistic  \n",
       "9         News  0.436559  0.391892  0.492718                linguistic  \n",
       "10        News  0.438972  0.392720  0.497573                linguistic  \n",
       "11        News  0.436559  0.391892  0.492718                linguistic  \n",
       "12   Wikipedia  0.541667  0.539683  0.548263                 frequency  \n",
       "13   Wikipedia  0.306012  0.525684  0.529923                 frequency  \n",
       "14   Wikipedia  0.518750  0.674419  0.528958                 frequency  \n",
       "15   Wikipedia  0.624893  0.658931  0.609073                 frequency  \n",
       "16    WikiNews  0.562791  0.563927  0.561869                 frequency  \n",
       "17    WikiNews  0.544560  0.543671  0.549874                 frequency  \n",
       "18    WikiNews  0.469448  0.551282  0.508838                 frequency  \n",
       "19    WikiNews  0.484388  0.508721  0.503788                 frequency  \n",
       "20        News  0.532977  0.539078  0.531900                 frequency  \n",
       "21        News  0.470139  0.484348  0.478675                 frequency  \n",
       "22        News  0.451194  0.475911  0.496793                 frequency  \n",
       "23        News  0.502932  0.532197  0.514736                 frequency  \n",
       "24   Wikipedia  0.530130  0.533854  0.550193            language_model  \n",
       "25   Wikipedia  0.533344  0.557682  0.603282            language_model  \n",
       "26   Wikipedia  0.475339  0.492107  0.487452            language_model  \n",
       "27   Wikipedia  0.562552  0.558824  0.577220            language_model  \n",
       "28    WikiNews  0.484649  0.488393  0.491793            language_model  \n",
       "29    WikiNews  0.568279  0.566667  0.570707            language_model  \n",
       "30    WikiNews  0.484649  0.488393  0.491793            language_model  \n",
       "31    WikiNews  0.512082  0.519409  0.514520            language_model  \n",
       "32        News  0.494891  0.531104  0.546203            language_model  \n",
       "33        News  0.466777  0.501055  0.501560            language_model  \n",
       "34        News  0.566394  0.580405  0.561893            language_model  \n",
       "35        News  0.521797  0.533039  0.546030            language_model  \n",
       "36   Wikipedia  0.541667  0.539683  0.548263                    corpus  \n",
       "37   Wikipedia  0.306012  0.525684  0.529923                    corpus  \n",
       "38   Wikipedia  0.518750  0.674419  0.528958                    corpus  \n",
       "39   Wikipedia  0.624893  0.658931  0.609073                    corpus  \n",
       "40    WikiNews  0.562791  0.563927  0.561869                    corpus  \n",
       "41    WikiNews  0.544560  0.543671  0.549874                    corpus  \n",
       "42    WikiNews  0.469448  0.551282  0.508838                    corpus  \n",
       "43    WikiNews  0.484388  0.508721  0.503788                    corpus  \n",
       "44        News  0.532977  0.539078  0.531900                    corpus  \n",
       "45        News  0.470139  0.484348  0.478675                    corpus  \n",
       "46        News  0.451194  0.475911  0.496793                    corpus  \n",
       "47        News  0.502932  0.532197  0.514736                    corpus  \n",
       "48   Wikipedia  0.461444  0.484220  0.473938          psycholinguistic  \n",
       "49   Wikipedia  0.563692  0.562500  0.592664          psycholinguistic  \n",
       "50   Wikipedia  0.430670  0.458504  0.431467          psycholinguistic  \n",
       "51   Wikipedia  0.549722  0.547225  0.555019          psycholinguistic  \n",
       "52    WikiNews  0.447405  0.477200  0.468434          psycholinguistic  \n",
       "53    WikiNews  0.391238  0.422227  0.393308          psycholinguistic  \n",
       "54    WikiNews  0.542208  0.547059  0.560606          psycholinguistic  \n",
       "55    WikiNews  0.427826  0.435065  0.424242          psycholinguistic  \n",
       "56        News  0.547496  0.548577  0.562153          psycholinguistic  \n",
       "57        News  0.480679  0.484779  0.481796          psycholinguistic  \n",
       "58        News  0.609113  0.603927  0.618845          psycholinguistic  \n",
       "59        News  0.512929  0.517879  0.514043          psycholinguistic  \n",
       "60   Wikipedia  0.498491  0.521687  0.508687                   wordnet  \n",
       "61   Wikipedia  0.442424  0.540598  0.573359                   wordnet  \n",
       "62   Wikipedia  0.456790  0.420455  0.500000                   wordnet  \n",
       "63   Wikipedia  0.151947  0.580460  0.506757                   wordnet  \n",
       "64    WikiNews  0.433735  0.382979  0.500000                   wordnet  \n",
       "65    WikiNews  0.433735  0.382979  0.500000                   wordnet  \n",
       "66    WikiNews  0.433735  0.382979  0.500000                   wordnet  \n",
       "67    WikiNews  0.433735  0.382979  0.500000                   wordnet  \n",
       "68        News  0.457156  0.644231  0.506501                   wordnet  \n",
       "69        News  0.438972  0.392720  0.497573                   wordnet  \n",
       "70        News  0.472989  0.645349  0.513003                   wordnet  \n",
       "71        News  0.468658  0.486026  0.495232                   wordnet  \n",
       "72   Wikipedia  0.524865  0.535162  0.524131                   dbpedia  \n",
       "73   Wikipedia  0.524865  0.535162  0.524131                   dbpedia  \n",
       "74   Wikipedia  0.524865  0.535162  0.524131                   dbpedia  \n",
       "75   Wikipedia  0.524865  0.535162  0.524131                   dbpedia  \n",
       "76    WikiNews  0.553656  0.769444  0.561237                   dbpedia  \n",
       "77    WikiNews  0.553656  0.769444  0.561237                   dbpedia  \n",
       "78    WikiNews  0.553656  0.769444  0.561237                   dbpedia  \n",
       "79    WikiNews  0.553656  0.769444  0.561237                   dbpedia  \n",
       "80        News  0.505349  0.575749  0.522798                   dbpedia  \n",
       "81        News  0.505349  0.575749  0.522798                   dbpedia  \n",
       "82        News  0.505349  0.575749  0.522798                   dbpedia  \n",
       "83        News  0.505349  0.575749  0.522798                   dbpedia  \n",
       "84   Wikipedia  0.369697  0.470085  0.445946          brown_clustering  \n",
       "85   Wikipedia  0.246497  0.587500  0.554054          brown_clustering  \n",
       "86   Wikipedia  0.453416  0.419540  0.493243          brown_clustering  \n",
       "87   Wikipedia  0.312500  0.484848  0.478764          brown_clustering  \n",
       "88    WikiNews  0.487435  0.539377  0.554293          brown_clustering  \n",
       "89    WikiNews  0.339229  0.507310  0.506313          brown_clustering  \n",
       "90    WikiNews  0.474593  0.635870  0.515783          brown_clustering  \n",
       "91    WikiNews  0.411631  0.519629  0.523359          brown_clustering  \n",
       "92        News  0.498648  0.542977  0.515517          brown_clustering  \n",
       "93        News  0.355020  0.474462  0.468707          brown_clustering  \n",
       "94        News  0.458657  0.894636  0.508929          brown_clustering  \n",
       "95        News  0.510799  0.536624  0.518811          brown_clustering  \n",
       "96   Wikipedia  0.540399  0.568783  0.537645                  semantic  \n",
       "97   Wikipedia  0.540399  0.568783  0.537645                  semantic  \n",
       "98   Wikipedia  0.557492  0.627711  0.551158                  semantic  \n",
       "99   Wikipedia  0.606455  0.598328  0.641892                  semantic  \n",
       "100   WikiNews  0.433735  0.382979  0.500000                  semantic  \n",
       "101   WikiNews  0.433735  0.382979  0.500000                  semantic  \n",
       "102   WikiNews  0.433735  0.382979  0.500000                  semantic  \n",
       "103   WikiNews  0.433735  0.382979  0.500000                  semantic  \n",
       "104       News  0.479952  0.544841  0.509795                  semantic  \n",
       "105       News  0.478037  0.530786  0.507368                  semantic  \n",
       "106       News  0.474759  0.729086  0.515430                  semantic  \n",
       "107       News  0.484759  0.519309  0.506588                  semantic  \n",
       "108  Wikipedia  0.476190  0.530021  0.555985                dictionary  \n",
       "109  Wikipedia  0.410714  0.501053  0.501931                dictionary  \n",
       "110  Wikipedia  0.496503  0.499237  0.499035                dictionary  \n",
       "111  Wikipedia  0.461444  0.484220  0.473938                dictionary  \n",
       "112   WikiNews  0.462857  0.487115  0.482323                dictionary  \n",
       "113   WikiNews  0.389971  0.447998  0.428662                dictionary  \n",
       "114   WikiNews  0.516958  0.517689  0.520202                dictionary  \n",
       "115   WikiNews  0.426525  0.445402  0.428030                dictionary  \n",
       "116       News  0.495232  0.525763  0.538055                dictionary  \n",
       "117       News  0.425439  0.527390  0.536755                dictionary  \n",
       "118       News  0.465786  0.468537  0.464719                dictionary  \n",
       "119       News  0.478428  0.494002  0.491678                dictionary  \n",
       "120  Wikipedia  0.584390  0.587179  0.582046           corpus+semantic  \n",
       "121  Wikipedia  0.580952  0.575758  0.606178           corpus+semantic  \n",
       "122  Wikipedia  0.511654  0.590196  0.522201           corpus+semantic  \n",
       "123  Wikipedia  0.628138  0.632308  0.624517           corpus+semantic  \n",
       "124   WikiNews  0.585737  0.648810  0.578914           corpus+semantic  \n",
       "125   WikiNews  0.555784  0.616340  0.556187           corpus+semantic  \n",
       "126   WikiNews  0.469448  0.551282  0.508838           corpus+semantic  \n",
       "127   WikiNews  0.464387  0.508333  0.501894           corpus+semantic  \n",
       "128       News  0.527928  0.594035  0.535801           corpus+semantic  \n",
       "129       News  0.501630  0.647638  0.526006           corpus+semantic  \n",
       "130       News  0.469488  0.561198  0.508148           corpus+semantic  \n",
       "131       News  0.488861  0.538018  0.511442           corpus+semantic  \n",
       "132  Wikipedia  0.489425  0.500635  0.500965  wordnet+psycholinguistic  \n",
       "133  Wikipedia  0.529097  0.546875  0.581081  wordnet+psycholinguistic  \n",
       "134  Wikipedia  0.471413  0.474316  0.470077  wordnet+psycholinguistic  \n",
       "135  Wikipedia  0.492219  0.509936  0.516409  wordnet+psycholinguistic  \n",
       "136   WikiNews  0.426829  0.380435  0.486111  wordnet+psycholinguistic  \n",
       "137   WikiNews  0.408805  0.373563  0.451389  wordnet+psycholinguistic  \n",
       "138   WikiNews  0.459397  0.482022  0.494949  wordnet+psycholinguistic  \n",
       "139   WikiNews  0.423313  0.379121  0.479167  wordnet+psycholinguistic  \n",
       "140       News  0.452557  0.452744  0.483877  wordnet+psycholinguistic  \n",
       "141       News  0.449712  0.463585  0.494365  wordnet+psycholinguistic  \n",
       "142       News  0.505980  0.516409  0.510749  wordnet+psycholinguistic  \n",
       "143       News  0.503788  0.520299  0.511529  wordnet+psycholinguistic  \n",
       "144  Wikipedia  0.584390  0.587179  0.582046                       all  \n",
       "145  Wikipedia  0.580952  0.575758  0.606178                       all  \n",
       "146  Wikipedia  0.511654  0.590196  0.522201                       all  \n",
       "147  Wikipedia  0.628138  0.632308  0.624517                       all  \n",
       "148   WikiNews  0.585737  0.648810  0.578914                       all  \n",
       "149   WikiNews  0.555784  0.616340  0.556187                       all  \n",
       "150   WikiNews  0.469448  0.551282  0.508838                       all  \n",
       "151   WikiNews  0.464387  0.508333  0.501894                       all  \n",
       "152       News  0.530480  0.605882  0.538228                       all  \n",
       "153       News  0.501630  0.647638  0.526006                       all  \n",
       "154       News  0.469488  0.561198  0.508148                       all  \n",
       "155       News  0.488861  0.538018  0.511442                       all  "
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_eval_data_nb_dswp = create_eval_df_from_results_macro(results, False)\n",
    "feature_eval_data_nb_dswp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.609113</td>\n",
       "      <td>0.603927</td>\n",
       "      <td>0.618845</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.628138</td>\n",
       "      <td>0.632308</td>\n",
       "      <td>0.624517</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.585737</td>\n",
       "      <td>0.648810</td>\n",
       "      <td>0.578914</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.628138</td>\n",
       "      <td>0.632308</td>\n",
       "      <td>0.624517</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.585737</td>\n",
       "      <td>0.648810</td>\n",
       "      <td>0.578914</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                agg  \\\n",
       "58                                                                                                                              min   \n",
       "123        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "124  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "147                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "148                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "\n",
       "       dataset        f1      prec       rec                zc  \n",
       "58        News  0.609113  0.603927  0.618845  psycholinguistic  \n",
       "123  Wikipedia  0.628138  0.632308  0.624517   corpus+semantic  \n",
       "124   WikiNews  0.585737  0.648810  0.578914   corpus+semantic  \n",
       "147  Wikipedia  0.628138  0.632308  0.624517               all  \n",
       "148   WikiNews  0.585737  0.648810  0.578914               all  "
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = feature_eval_data_nb_dswp.groupby(['dataset'])['f1'].transform(max) == feature_eval_data_nb_dswp['f1']\n",
    "feature_eval_data_nb_dswp[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.2.9 kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "results = [Result(fs, fs.fc, fs.agg,\n",
    "        precision_recall_fscore_support(*knn(*transform_feat_to_num(remove_labels_for_binary_df(fs.train), \n",
    "        remove_labels_for_binary_df(fs.test))), average='macro')) for fs in all_fc_datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.511654</td>\n",
       "      <td>0.590196</td>\n",
       "      <td>0.522201</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.511654</td>\n",
       "      <td>0.590196</td>\n",
       "      <td>0.522201</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.526294</td>\n",
       "      <td>0.925287</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.479730</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.439901</td>\n",
       "      <td>0.432026</td>\n",
       "      <td>0.467172</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.472756</td>\n",
       "      <td>0.480952</td>\n",
       "      <td>0.489899</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.415936</td>\n",
       "      <td>0.404464</td>\n",
       "      <td>0.432449</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.484388</td>\n",
       "      <td>0.508721</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.533063</td>\n",
       "      <td>0.619157</td>\n",
       "      <td>0.540655</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.516962</td>\n",
       "      <td>0.585874</td>\n",
       "      <td>0.529300</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.620632</td>\n",
       "      <td>0.690544</td>\n",
       "      <td>0.604802</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.493034</td>\n",
       "      <td>0.562667</td>\n",
       "      <td>0.516297</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.504923</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.515444</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.504923</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.515444</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.505263</td>\n",
       "      <td>0.509615</td>\n",
       "      <td>0.507576</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.484388</td>\n",
       "      <td>0.508721</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.461421</td>\n",
       "      <td>0.461382</td>\n",
       "      <td>0.476010</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.622076</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.610480</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.595188</td>\n",
       "      <td>0.658898</td>\n",
       "      <td>0.584518</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.585743</td>\n",
       "      <td>0.631863</td>\n",
       "      <td>0.577236</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.520439</td>\n",
       "      <td>0.565007</td>\n",
       "      <td>0.528519</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.596560</td>\n",
       "      <td>0.694428</td>\n",
       "      <td>0.585298</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.480519</td>\n",
       "      <td>0.481250</td>\n",
       "      <td>0.488417</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.469298</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.474903</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.486329</td>\n",
       "      <td>0.491182</td>\n",
       "      <td>0.495174</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.463841</td>\n",
       "      <td>0.461039</td>\n",
       "      <td>0.468147</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.561158</td>\n",
       "      <td>0.587369</td>\n",
       "      <td>0.558081</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.533231</td>\n",
       "      <td>0.556911</td>\n",
       "      <td>0.535354</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.555784</td>\n",
       "      <td>0.616340</td>\n",
       "      <td>0.556187</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.533231</td>\n",
       "      <td>0.556911</td>\n",
       "      <td>0.535354</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.560290</td>\n",
       "      <td>0.621157</td>\n",
       "      <td>0.557732</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.585743</td>\n",
       "      <td>0.631863</td>\n",
       "      <td>0.577236</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.599859</td>\n",
       "      <td>0.709091</td>\n",
       "      <td>0.587725</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.566112</td>\n",
       "      <td>0.642660</td>\n",
       "      <td>0.562587</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.504923</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.515444</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.504923</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.515444</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.505263</td>\n",
       "      <td>0.509615</td>\n",
       "      <td>0.507576</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.484388</td>\n",
       "      <td>0.508721</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.467058</td>\n",
       "      <td>0.470427</td>\n",
       "      <td>0.482955</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.622076</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.610480</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.595188</td>\n",
       "      <td>0.658898</td>\n",
       "      <td>0.584518</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.596581</td>\n",
       "      <td>0.642816</td>\n",
       "      <td>0.586165</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.533057</td>\n",
       "      <td>0.581818</td>\n",
       "      <td>0.537448</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.596560</td>\n",
       "      <td>0.694428</td>\n",
       "      <td>0.585298</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.439490</td>\n",
       "      <td>0.415663</td>\n",
       "      <td>0.466216</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.439490</td>\n",
       "      <td>0.415663</td>\n",
       "      <td>0.466216</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.518750</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.528958</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.498491</td>\n",
       "      <td>0.521687</td>\n",
       "      <td>0.508687</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.664025</td>\n",
       "      <td>0.661666</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.622076</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.610480</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.545894</td>\n",
       "      <td>0.693258</td>\n",
       "      <td>0.554293</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.490349</td>\n",
       "      <td>0.527915</td>\n",
       "      <td>0.510732</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.522909</td>\n",
       "      <td>0.573760</td>\n",
       "      <td>0.530947</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.530479</td>\n",
       "      <td>0.573495</td>\n",
       "      <td>0.535021</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.535663</td>\n",
       "      <td>0.590891</td>\n",
       "      <td>0.539875</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.521838</td>\n",
       "      <td>0.613479</td>\n",
       "      <td>0.534154</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.472973</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.479730</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.498491</td>\n",
       "      <td>0.521687</td>\n",
       "      <td>0.508687</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.498491</td>\n",
       "      <td>0.521687</td>\n",
       "      <td>0.508687</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.577322</td>\n",
       "      <td>0.624863</td>\n",
       "      <td>0.571970</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.618919</td>\n",
       "      <td>0.630117</td>\n",
       "      <td>0.612374</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.426829</td>\n",
       "      <td>0.380435</td>\n",
       "      <td>0.486111</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.579381</td>\n",
       "      <td>0.598734</td>\n",
       "      <td>0.573864</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.538299</td>\n",
       "      <td>0.600826</td>\n",
       "      <td>0.542302</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.586937</td>\n",
       "      <td>0.657563</td>\n",
       "      <td>0.578017</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.549164</td>\n",
       "      <td>0.652439</td>\n",
       "      <td>0.552011</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.460923</td>\n",
       "      <td>0.483339</td>\n",
       "      <td>0.496012</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.518750</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.528958</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.518750</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.528958</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.518750</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.528958</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.518750</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.528958</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.555784</td>\n",
       "      <td>0.616340</td>\n",
       "      <td>0.556187</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.555784</td>\n",
       "      <td>0.616340</td>\n",
       "      <td>0.556187</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.555784</td>\n",
       "      <td>0.616340</td>\n",
       "      <td>0.556187</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.555784</td>\n",
       "      <td>0.616340</td>\n",
       "      <td>0.556187</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.488861</td>\n",
       "      <td>0.538018</td>\n",
       "      <td>0.511442</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.488861</td>\n",
       "      <td>0.538018</td>\n",
       "      <td>0.511442</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.488861</td>\n",
       "      <td>0.538018</td>\n",
       "      <td>0.511442</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.488861</td>\n",
       "      <td>0.538018</td>\n",
       "      <td>0.511442</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.439490</td>\n",
       "      <td>0.415663</td>\n",
       "      <td>0.466216</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.605128</td>\n",
       "      <td>0.682927</td>\n",
       "      <td>0.586873</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.518750</td>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.528958</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.605042</td>\n",
       "      <td>0.652439</td>\n",
       "      <td>0.594697</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.660299</td>\n",
       "      <td>0.680290</td>\n",
       "      <td>0.648990</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.703158</td>\n",
       "      <td>0.735577</td>\n",
       "      <td>0.685606</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.713979</td>\n",
       "      <td>0.757384</td>\n",
       "      <td>0.692551</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.494284</td>\n",
       "      <td>0.526641</td>\n",
       "      <td>0.510662</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.494683</td>\n",
       "      <td>0.502853</td>\n",
       "      <td>0.501820</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.549053</td>\n",
       "      <td>0.587313</td>\n",
       "      <td>0.548024</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.500668</td>\n",
       "      <td>0.525832</td>\n",
       "      <td>0.512309</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.472973</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.548718</td>\n",
       "      <td>0.593496</td>\n",
       "      <td>0.544402</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.524865</td>\n",
       "      <td>0.535162</td>\n",
       "      <td>0.524131</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.585737</td>\n",
       "      <td>0.648810</td>\n",
       "      <td>0.578914</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.587719</td>\n",
       "      <td>0.614286</td>\n",
       "      <td>0.580808</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.533231</td>\n",
       "      <td>0.556911</td>\n",
       "      <td>0.535354</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.545731</td>\n",
       "      <td>0.559072</td>\n",
       "      <td>0.544192</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.568262</td>\n",
       "      <td>0.704858</td>\n",
       "      <td>0.565794</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.569211</td>\n",
       "      <td>0.625063</td>\n",
       "      <td>0.564234</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.576626</td>\n",
       "      <td>0.609647</td>\n",
       "      <td>0.569955</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.478037</td>\n",
       "      <td>0.530786</td>\n",
       "      <td>0.507368</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.492308</td>\n",
       "      <td>0.504065</td>\n",
       "      <td>0.501931</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.426799</td>\n",
       "      <td>0.475789</td>\n",
       "      <td>0.455598</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.498491</td>\n",
       "      <td>0.521687</td>\n",
       "      <td>0.508687</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.545731</td>\n",
       "      <td>0.559072</td>\n",
       "      <td>0.544192</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.531121</td>\n",
       "      <td>0.605090</td>\n",
       "      <td>0.540404</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.517949</td>\n",
       "      <td>0.541496</td>\n",
       "      <td>0.557449</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.490349</td>\n",
       "      <td>0.527915</td>\n",
       "      <td>0.510732</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.529377</td>\n",
       "      <td>0.673126</td>\n",
       "      <td>0.541436</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.476135</td>\n",
       "      <td>0.519000</td>\n",
       "      <td>0.504941</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.514148</td>\n",
       "      <td>0.530063</td>\n",
       "      <td>0.542822</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.535913</td>\n",
       "      <td>0.548649</td>\n",
       "      <td>0.535107</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.504923</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.515444</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.548718</td>\n",
       "      <td>0.593496</td>\n",
       "      <td>0.544402</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.490943</td>\n",
       "      <td>0.498101</td>\n",
       "      <td>0.498737</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.540567</td>\n",
       "      <td>0.573384</td>\n",
       "      <td>0.542298</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.533231</td>\n",
       "      <td>0.556911</td>\n",
       "      <td>0.535354</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.478529</td>\n",
       "      <td>0.493464</td>\n",
       "      <td>0.496843</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.598417</td>\n",
       "      <td>0.669283</td>\n",
       "      <td>0.586945</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.627793</td>\n",
       "      <td>0.672423</td>\n",
       "      <td>0.612951</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.577685</td>\n",
       "      <td>0.628605</td>\n",
       "      <td>0.570735</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.522909</td>\n",
       "      <td>0.573760</td>\n",
       "      <td>0.530947</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.479730</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.479730</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.479730</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.498491</td>\n",
       "      <td>0.521687</td>\n",
       "      <td>0.508687</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.605042</td>\n",
       "      <td>0.652439</td>\n",
       "      <td>0.594697</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.587719</td>\n",
       "      <td>0.614286</td>\n",
       "      <td>0.580808</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.656433</td>\n",
       "      <td>0.698214</td>\n",
       "      <td>0.640152</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.614076</td>\n",
       "      <td>0.676342</td>\n",
       "      <td>0.601641</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.533057</td>\n",
       "      <td>0.581818</td>\n",
       "      <td>0.537448</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.599786</td>\n",
       "      <td>0.651251</td>\n",
       "      <td>0.588592</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.522909</td>\n",
       "      <td>0.573760</td>\n",
       "      <td>0.530947</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.472373</td>\n",
       "      <td>0.500288</td>\n",
       "      <td>0.500087</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.504923</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.515444</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.479730</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.548718</td>\n",
       "      <td>0.593496</td>\n",
       "      <td>0.544402</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.490943</td>\n",
       "      <td>0.498101</td>\n",
       "      <td>0.498737</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.540567</td>\n",
       "      <td>0.573384</td>\n",
       "      <td>0.542298</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.533231</td>\n",
       "      <td>0.556911</td>\n",
       "      <td>0.535354</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.478529</td>\n",
       "      <td>0.493464</td>\n",
       "      <td>0.496843</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.595188</td>\n",
       "      <td>0.658898</td>\n",
       "      <td>0.584518</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.627793</td>\n",
       "      <td>0.672423</td>\n",
       "      <td>0.612951</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.571700</td>\n",
       "      <td>0.612476</td>\n",
       "      <td>0.565881</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.520439</td>\n",
       "      <td>0.565007</td>\n",
       "      <td>0.528519</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                agg  \\\n",
       "0                                                                                                                              mean   \n",
       "1                                                                                                                               max   \n",
       "2                                                                                                                               min   \n",
       "3                                                                                                                     weighted_mean   \n",
       "4                                                                                                                              mean   \n",
       "5                                                                                                                               max   \n",
       "6                                                                                                                               min   \n",
       "7                                                                                                                     weighted_mean   \n",
       "8                                                                                                                              mean   \n",
       "9                                                                                                                               max   \n",
       "10                                                                                                                              min   \n",
       "11                                                                                                                    weighted_mean   \n",
       "12                                                                                                                             mean   \n",
       "13                                                                                                                              max   \n",
       "14                                                                                                                              min   \n",
       "15                                                                                                                    weighted_mean   \n",
       "16                                                                                                                             mean   \n",
       "17                                                                                                                              max   \n",
       "18                                                                                                                              min   \n",
       "19                                                                                                                    weighted_mean   \n",
       "20                                                                                                                             mean   \n",
       "21                                                                                                                              max   \n",
       "22                                                                                                                              min   \n",
       "23                                                                                                                    weighted_mean   \n",
       "24                                                                                                                             mean   \n",
       "25                                                                                                                              max   \n",
       "26                                                                                                                              min   \n",
       "27                                                                                                                    weighted_mean   \n",
       "28                                                                                                                             mean   \n",
       "29                                                                                                                              max   \n",
       "30                                                                                                                              min   \n",
       "31                                                                                                                    weighted_mean   \n",
       "32                                                                                                                             mean   \n",
       "33                                                                                                                              max   \n",
       "34                                                                                                                              min   \n",
       "35                                                                                                                    weighted_mean   \n",
       "36                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "37                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "38                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "39                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "40                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "41                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "42                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "43                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "44                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "45                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "46                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "47                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "48                                                                                                                             mean   \n",
       "49                                                                                                                              max   \n",
       "50                                                                                                                              min   \n",
       "51                                                                                                                    weighted_mean   \n",
       "52                                                                                                                             mean   \n",
       "53                                                                                                                              max   \n",
       "54                                                                                                                              min   \n",
       "55                                                                                                                    weighted_mean   \n",
       "56                                                                                                                             mean   \n",
       "57                                                                                                                              max   \n",
       "58                                                                                                                              min   \n",
       "59                                                                                                                    weighted_mean   \n",
       "60                                                                                                                             mean   \n",
       "61                                                                                                                              max   \n",
       "62                                                                                                                              min   \n",
       "63                                                                                                                    weighted_mean   \n",
       "64                                                                                                                             mean   \n",
       "65                                                                                                                              max   \n",
       "66                                                                                                                              min   \n",
       "67                                                                                                                    weighted_mean   \n",
       "68                                                                                                                             mean   \n",
       "69                                                                                                                              max   \n",
       "70                                                                                                                              min   \n",
       "71                                                                                                                    weighted_mean   \n",
       "72                                                                                                                             mean   \n",
       "73                                                                                                                              max   \n",
       "74                                                                                                                              min   \n",
       "75                                                                                                                    weighted_mean   \n",
       "76                                                                                                                             mean   \n",
       "77                                                                                                                              max   \n",
       "78                                                                                                                              min   \n",
       "79                                                                                                                    weighted_mean   \n",
       "80                                                                                                                             mean   \n",
       "81                                                                                                                              max   \n",
       "82                                                                                                                              min   \n",
       "83                                                                                                                    weighted_mean   \n",
       "84                                                                                                                             mean   \n",
       "85                                                                                                                              max   \n",
       "86                                                                                                                              min   \n",
       "87                                                                                                                    weighted_mean   \n",
       "88                                                                                                                             mean   \n",
       "89                                                                                                                              max   \n",
       "90                                                                                                                              min   \n",
       "91                                                                                                                    weighted_mean   \n",
       "92                                                                                                                             mean   \n",
       "93                                                                                                                              max   \n",
       "94                                                                                                                              min   \n",
       "95                                                                                                                    weighted_mean   \n",
       "96                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "97                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "98                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "99                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "100                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "101                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "102                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "103                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "104                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "105                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "106                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "107                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "108                                                                                                                            mean   \n",
       "109                                                                                                                             max   \n",
       "110                                                                                                                             min   \n",
       "111                                                                                                                   weighted_mean   \n",
       "112                                                                                                                            mean   \n",
       "113                                                                                                                             max   \n",
       "114                                                                                                                             min   \n",
       "115                                                                                                                   weighted_mean   \n",
       "116                                                                                                                            mean   \n",
       "117                                                                                                                             max   \n",
       "118                                                                                                                             min   \n",
       "119                                                                                                                   weighted_mean   \n",
       "120  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "121            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "122            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "123        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "124  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "125            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "126            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "127        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "128  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "129            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "130            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "131        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "132                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "133                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "134                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "135                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "136                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "137                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "138                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "139                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "140                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "141                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "142                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "143                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "144                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "145                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "146                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "147                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "148                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "149                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "150                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "151                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "152                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "153                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "154                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "155                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "\n",
       "       dataset        f1      prec       rec                        zc  \n",
       "0    Wikipedia  0.511654  0.590196  0.522201                linguistic  \n",
       "1    Wikipedia  0.511654  0.590196  0.522201                linguistic  \n",
       "2    Wikipedia  0.526294  0.925287  0.535714                linguistic  \n",
       "3    Wikipedia  0.446541  0.417647  0.479730                linguistic  \n",
       "4     WikiNews  0.439901  0.432026  0.467172                linguistic  \n",
       "5     WikiNews  0.472756  0.480952  0.489899                linguistic  \n",
       "6     WikiNews  0.415936  0.404464  0.432449                linguistic  \n",
       "7     WikiNews  0.484388  0.508721  0.503788                linguistic  \n",
       "8         News  0.533063  0.619157  0.540655                linguistic  \n",
       "9         News  0.516962  0.585874  0.529300                linguistic  \n",
       "10        News  0.620632  0.690544  0.604802                linguistic  \n",
       "11        News  0.493034  0.562667  0.516297                linguistic  \n",
       "12   Wikipedia  0.504923  0.547619  0.515444                 frequency  \n",
       "13   Wikipedia  0.450000  0.418605  0.486486                 frequency  \n",
       "14   Wikipedia  0.450000  0.418605  0.486486                 frequency  \n",
       "15   Wikipedia  0.504923  0.547619  0.515444                 frequency  \n",
       "16    WikiNews  0.505263  0.509615  0.507576                 frequency  \n",
       "17    WikiNews  0.484388  0.508721  0.503788                 frequency  \n",
       "18    WikiNews  0.461421  0.461382  0.476010                 frequency  \n",
       "19    WikiNews  0.622076  0.656250  0.610480                 frequency  \n",
       "20        News  0.595188  0.658898  0.584518                 frequency  \n",
       "21        News  0.585743  0.631863  0.577236                 frequency  \n",
       "22        News  0.520439  0.565007  0.528519                 frequency  \n",
       "23        News  0.596560  0.694428  0.585298                 frequency  \n",
       "24   Wikipedia  0.480519  0.481250  0.488417            language_model  \n",
       "25   Wikipedia  0.469298  0.466667  0.474903            language_model  \n",
       "26   Wikipedia  0.486329  0.491182  0.495174            language_model  \n",
       "27   Wikipedia  0.463841  0.461039  0.468147            language_model  \n",
       "28    WikiNews  0.561158  0.587369  0.558081            language_model  \n",
       "29    WikiNews  0.533231  0.556911  0.535354            language_model  \n",
       "30    WikiNews  0.555784  0.616340  0.556187            language_model  \n",
       "31    WikiNews  0.533231  0.556911  0.535354            language_model  \n",
       "32        News  0.560290  0.621157  0.557732            language_model  \n",
       "33        News  0.585743  0.631863  0.577236            language_model  \n",
       "34        News  0.599859  0.709091  0.587725            language_model  \n",
       "35        News  0.566112  0.642660  0.562587            language_model  \n",
       "36   Wikipedia  0.504923  0.547619  0.515444                    corpus  \n",
       "37   Wikipedia  0.450000  0.418605  0.486486                    corpus  \n",
       "38   Wikipedia  0.450000  0.418605  0.486486                    corpus  \n",
       "39   Wikipedia  0.504923  0.547619  0.515444                    corpus  \n",
       "40    WikiNews  0.505263  0.509615  0.507576                    corpus  \n",
       "41    WikiNews  0.484388  0.508721  0.503788                    corpus  \n",
       "42    WikiNews  0.467058  0.470427  0.482955                    corpus  \n",
       "43    WikiNews  0.622076  0.656250  0.610480                    corpus  \n",
       "44        News  0.595188  0.658898  0.584518                    corpus  \n",
       "45        News  0.596581  0.642816  0.586165                    corpus  \n",
       "46        News  0.533057  0.581818  0.537448                    corpus  \n",
       "47        News  0.596560  0.694428  0.585298                    corpus  \n",
       "48   Wikipedia  0.439490  0.415663  0.466216          psycholinguistic  \n",
       "49   Wikipedia  0.439490  0.415663  0.466216          psycholinguistic  \n",
       "50   Wikipedia  0.518750  0.674419  0.528958          psycholinguistic  \n",
       "51   Wikipedia  0.498491  0.521687  0.508687          psycholinguistic  \n",
       "52    WikiNews  0.664025  0.661666  0.666667          psycholinguistic  \n",
       "53    WikiNews  0.622076  0.656250  0.610480          psycholinguistic  \n",
       "54    WikiNews  0.545894  0.693258  0.554293          psycholinguistic  \n",
       "55    WikiNews  0.490349  0.527915  0.510732          psycholinguistic  \n",
       "56        News  0.522909  0.573760  0.530947          psycholinguistic  \n",
       "57        News  0.530479  0.573495  0.535021          psycholinguistic  \n",
       "58        News  0.535663  0.590891  0.539875          psycholinguistic  \n",
       "59        News  0.521838  0.613479  0.534154          psycholinguistic  \n",
       "60   Wikipedia  0.443038  0.416667  0.472973                   wordnet  \n",
       "61   Wikipedia  0.446541  0.417647  0.479730                   wordnet  \n",
       "62   Wikipedia  0.498491  0.521687  0.508687                   wordnet  \n",
       "63   Wikipedia  0.498491  0.521687  0.508687                   wordnet  \n",
       "64    WikiNews  0.577322  0.624863  0.571970                   wordnet  \n",
       "65    WikiNews  0.618919  0.630117  0.612374                   wordnet  \n",
       "66    WikiNews  0.426829  0.380435  0.486111                   wordnet  \n",
       "67    WikiNews  0.579381  0.598734  0.573864                   wordnet  \n",
       "68        News  0.538299  0.600826  0.542302                   wordnet  \n",
       "69        News  0.586937  0.657563  0.578017                   wordnet  \n",
       "70        News  0.549164  0.652439  0.552011                   wordnet  \n",
       "71        News  0.460923  0.483339  0.496012                   wordnet  \n",
       "72   Wikipedia  0.518750  0.674419  0.528958                   dbpedia  \n",
       "73   Wikipedia  0.518750  0.674419  0.528958                   dbpedia  \n",
       "74   Wikipedia  0.518750  0.674419  0.528958                   dbpedia  \n",
       "75   Wikipedia  0.518750  0.674419  0.528958                   dbpedia  \n",
       "76    WikiNews  0.555784  0.616340  0.556187                   dbpedia  \n",
       "77    WikiNews  0.555784  0.616340  0.556187                   dbpedia  \n",
       "78    WikiNews  0.555784  0.616340  0.556187                   dbpedia  \n",
       "79    WikiNews  0.555784  0.616340  0.556187                   dbpedia  \n",
       "80        News  0.488861  0.538018  0.511442                   dbpedia  \n",
       "81        News  0.488861  0.538018  0.511442                   dbpedia  \n",
       "82        News  0.488861  0.538018  0.511442                   dbpedia  \n",
       "83        News  0.488861  0.538018  0.511442                   dbpedia  \n",
       "84   Wikipedia  0.439490  0.415663  0.466216          brown_clustering  \n",
       "85   Wikipedia  0.450000  0.418605  0.486486          brown_clustering  \n",
       "86   Wikipedia  0.605128  0.682927  0.586873          brown_clustering  \n",
       "87   Wikipedia  0.518750  0.674419  0.528958          brown_clustering  \n",
       "88    WikiNews  0.605042  0.652439  0.594697          brown_clustering  \n",
       "89    WikiNews  0.660299  0.680290  0.648990          brown_clustering  \n",
       "90    WikiNews  0.703158  0.735577  0.685606          brown_clustering  \n",
       "91    WikiNews  0.713979  0.757384  0.692551          brown_clustering  \n",
       "92        News  0.494284  0.526641  0.510662          brown_clustering  \n",
       "93        News  0.494683  0.502853  0.501820          brown_clustering  \n",
       "94        News  0.549053  0.587313  0.548024          brown_clustering  \n",
       "95        News  0.500668  0.525832  0.512309          brown_clustering  \n",
       "96   Wikipedia  0.450000  0.418605  0.486486                  semantic  \n",
       "97   Wikipedia  0.443038  0.416667  0.472973                  semantic  \n",
       "98   Wikipedia  0.548718  0.593496  0.544402                  semantic  \n",
       "99   Wikipedia  0.524865  0.535162  0.524131                  semantic  \n",
       "100   WikiNews  0.585737  0.648810  0.578914                  semantic  \n",
       "101   WikiNews  0.587719  0.614286  0.580808                  semantic  \n",
       "102   WikiNews  0.533231  0.556911  0.535354                  semantic  \n",
       "103   WikiNews  0.545731  0.559072  0.544192                  semantic  \n",
       "104       News  0.568262  0.704858  0.565794                  semantic  \n",
       "105       News  0.569211  0.625063  0.564234                  semantic  \n",
       "106       News  0.576626  0.609647  0.569955                  semantic  \n",
       "107       News  0.478037  0.530786  0.507368                  semantic  \n",
       "108  Wikipedia  0.492308  0.504065  0.501931                dictionary  \n",
       "109  Wikipedia  0.426799  0.475789  0.455598                dictionary  \n",
       "110  Wikipedia  0.456790  0.420455  0.500000                dictionary  \n",
       "111  Wikipedia  0.498491  0.521687  0.508687                dictionary  \n",
       "112   WikiNews  0.545731  0.559072  0.544192                dictionary  \n",
       "113   WikiNews  0.531121  0.605090  0.540404                dictionary  \n",
       "114   WikiNews  0.517949  0.541496  0.557449                dictionary  \n",
       "115   WikiNews  0.490349  0.527915  0.510732                dictionary  \n",
       "116       News  0.529377  0.673126  0.541436                dictionary  \n",
       "117       News  0.476135  0.519000  0.504941                dictionary  \n",
       "118       News  0.514148  0.530063  0.542822                dictionary  \n",
       "119       News  0.535913  0.548649  0.535107                dictionary  \n",
       "120  Wikipedia  0.453416  0.419540  0.493243           corpus+semantic  \n",
       "121  Wikipedia  0.504923  0.547619  0.515444           corpus+semantic  \n",
       "122  Wikipedia  0.450000  0.418605  0.486486           corpus+semantic  \n",
       "123  Wikipedia  0.548718  0.593496  0.544402           corpus+semantic  \n",
       "124   WikiNews  0.490943  0.498101  0.498737           corpus+semantic  \n",
       "125   WikiNews  0.540567  0.573384  0.542298           corpus+semantic  \n",
       "126   WikiNews  0.533231  0.556911  0.535354           corpus+semantic  \n",
       "127   WikiNews  0.478529  0.493464  0.496843           corpus+semantic  \n",
       "128       News  0.598417  0.669283  0.586945           corpus+semantic  \n",
       "129       News  0.627793  0.672423  0.612951           corpus+semantic  \n",
       "130       News  0.577685  0.628605  0.570735           corpus+semantic  \n",
       "131       News  0.522909  0.573760  0.530947           corpus+semantic  \n",
       "132  Wikipedia  0.446541  0.417647  0.479730  wordnet+psycholinguistic  \n",
       "133  Wikipedia  0.446541  0.417647  0.479730  wordnet+psycholinguistic  \n",
       "134  Wikipedia  0.446541  0.417647  0.479730  wordnet+psycholinguistic  \n",
       "135  Wikipedia  0.498491  0.521687  0.508687  wordnet+psycholinguistic  \n",
       "136   WikiNews  0.605042  0.652439  0.594697  wordnet+psycholinguistic  \n",
       "137   WikiNews  0.587719  0.614286  0.580808  wordnet+psycholinguistic  \n",
       "138   WikiNews  0.656433  0.698214  0.640152  wordnet+psycholinguistic  \n",
       "139   WikiNews  0.614076  0.676342  0.601641  wordnet+psycholinguistic  \n",
       "140       News  0.533057  0.581818  0.537448  wordnet+psycholinguistic  \n",
       "141       News  0.599786  0.651251  0.588592  wordnet+psycholinguistic  \n",
       "142       News  0.522909  0.573760  0.530947  wordnet+psycholinguistic  \n",
       "143       News  0.472373  0.500288  0.500087  wordnet+psycholinguistic  \n",
       "144  Wikipedia  0.453416  0.419540  0.493243                       all  \n",
       "145  Wikipedia  0.504923  0.547619  0.515444                       all  \n",
       "146  Wikipedia  0.446541  0.417647  0.479730                       all  \n",
       "147  Wikipedia  0.548718  0.593496  0.544402                       all  \n",
       "148   WikiNews  0.490943  0.498101  0.498737                       all  \n",
       "149   WikiNews  0.540567  0.573384  0.542298                       all  \n",
       "150   WikiNews  0.533231  0.556911  0.535354                       all  \n",
       "151   WikiNews  0.478529  0.493464  0.496843                       all  \n",
       "152       News  0.595188  0.658898  0.584518                       all  \n",
       "153       News  0.627793  0.672423  0.612951                       all  \n",
       "154       News  0.571700  0.612476  0.565881                       all  \n",
       "155       News  0.520439  0.565007  0.528519                       all  "
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_eval_data_kn = create_eval_df_from_results_macro(results, False)\n",
    "feature_eval_data_kn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.605128</td>\n",
       "      <td>0.682927</td>\n",
       "      <td>0.586873</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.713979</td>\n",
       "      <td>0.757384</td>\n",
       "      <td>0.692551</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.627793</td>\n",
       "      <td>0.672423</td>\n",
       "      <td>0.612951</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.627793</td>\n",
       "      <td>0.672423</td>\n",
       "      <td>0.612951</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                      agg  \\\n",
       "86                                                                                                                    min   \n",
       "91                                                                                                          weighted_mean   \n",
       "129  [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "153                                                              (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "\n",
       "       dataset        f1      prec       rec                zc  \n",
       "86   Wikipedia  0.605128  0.682927  0.586873  brown_clustering  \n",
       "91    WikiNews  0.713979  0.757384  0.692551  brown_clustering  \n",
       "129       News  0.627793  0.672423  0.612951   corpus+semantic  \n",
       "153       News  0.627793  0.672423  0.612951               all  "
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = feature_eval_data_kn.groupby(['dataset'])['f1'].transform(max) == feature_eval_data_kn['f1']\n",
    "feature_eval_data_kn[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [Result(fs, fs.fc, fs.agg,\n",
    "        precision_recall_fscore_support(*knn(*transform_feat_to_num(remove_labels_for_binary_df(fs.train), \n",
    "        remove_labels_for_binary_df(fs.test))), average='macro')) for fs in all_fc_datasets_dswp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.492308</td>\n",
       "      <td>0.504065</td>\n",
       "      <td>0.501931</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.498491</td>\n",
       "      <td>0.521687</td>\n",
       "      <td>0.508687</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.511654</td>\n",
       "      <td>0.590196</td>\n",
       "      <td>0.522201</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.414634</td>\n",
       "      <td>0.459459</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.435096</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.460227</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.519006</td>\n",
       "      <td>0.530357</td>\n",
       "      <td>0.521465</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.411133</td>\n",
       "      <td>0.400422</td>\n",
       "      <td>0.425505</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.510417</td>\n",
       "      <td>0.536905</td>\n",
       "      <td>0.519571</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.530480</td>\n",
       "      <td>0.605882</td>\n",
       "      <td>0.538228</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.502932</td>\n",
       "      <td>0.532197</td>\n",
       "      <td>0.514736</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.617208</td>\n",
       "      <td>0.680250</td>\n",
       "      <td>0.602375</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.500860</td>\n",
       "      <td>0.552591</td>\n",
       "      <td>0.517944</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.538013</td>\n",
       "      <td>0.539465</td>\n",
       "      <td>0.556950</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.563692</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.592664</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.537235</td>\n",
       "      <td>0.551500</td>\n",
       "      <td>0.587838</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.570312</td>\n",
       "      <td>0.586057</td>\n",
       "      <td>0.652510</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.518329</td>\n",
       "      <td>0.529872</td>\n",
       "      <td>0.539773</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.582940</td>\n",
       "      <td>0.580106</td>\n",
       "      <td>0.595328</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.584301</td>\n",
       "      <td>0.583165</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.601640</td>\n",
       "      <td>0.599851</td>\n",
       "      <td>0.626894</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.600354</td>\n",
       "      <td>0.602555</td>\n",
       "      <td>0.598474</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.598354</td>\n",
       "      <td>0.595751</td>\n",
       "      <td>0.601768</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.623833</td>\n",
       "      <td>0.617336</td>\n",
       "      <td>0.636702</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.657516</td>\n",
       "      <td>0.678188</td>\n",
       "      <td>0.645458</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.521029</td>\n",
       "      <td>0.542424</td>\n",
       "      <td>0.574324</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.544471</td>\n",
       "      <td>0.572602</td>\n",
       "      <td>0.632239</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.499666</td>\n",
       "      <td>0.522321</td>\n",
       "      <td>0.538610</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.473539</td>\n",
       "      <td>0.518315</td>\n",
       "      <td>0.533784</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.559395</td>\n",
       "      <td>0.563923</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.526260</td>\n",
       "      <td>0.535441</td>\n",
       "      <td>0.546717</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.564978</td>\n",
       "      <td>0.584359</td>\n",
       "      <td>0.616793</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.627828</td>\n",
       "      <td>0.621875</td>\n",
       "      <td>0.647727</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.548487</td>\n",
       "      <td>0.546919</td>\n",
       "      <td>0.553138</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.578655</td>\n",
       "      <td>0.578631</td>\n",
       "      <td>0.604369</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.570324</td>\n",
       "      <td>0.567639</td>\n",
       "      <td>0.581571</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.582226</td>\n",
       "      <td>0.580822</td>\n",
       "      <td>0.583911</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.538013</td>\n",
       "      <td>0.539465</td>\n",
       "      <td>0.556950</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.555306</td>\n",
       "      <td>0.556508</td>\n",
       "      <td>0.585907</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.521029</td>\n",
       "      <td>0.542424</td>\n",
       "      <td>0.574324</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.570312</td>\n",
       "      <td>0.586057</td>\n",
       "      <td>0.652510</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.518329</td>\n",
       "      <td>0.529872</td>\n",
       "      <td>0.539773</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.582940</td>\n",
       "      <td>0.580106</td>\n",
       "      <td>0.595328</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.601244</td>\n",
       "      <td>0.597396</td>\n",
       "      <td>0.618056</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.601640</td>\n",
       "      <td>0.599851</td>\n",
       "      <td>0.626894</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.600354</td>\n",
       "      <td>0.602555</td>\n",
       "      <td>0.598474</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.606245</td>\n",
       "      <td>0.602984</td>\n",
       "      <td>0.610697</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.627841</td>\n",
       "      <td>0.620760</td>\n",
       "      <td>0.643204</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.657516</td>\n",
       "      <td>0.678188</td>\n",
       "      <td>0.645458</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.475339</td>\n",
       "      <td>0.492107</td>\n",
       "      <td>0.487452</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.393103</td>\n",
       "      <td>0.469979</td>\n",
       "      <td>0.444015</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.589868</td>\n",
       "      <td>0.583156</td>\n",
       "      <td>0.612934</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.496503</td>\n",
       "      <td>0.499237</td>\n",
       "      <td>0.499035</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.567731</td>\n",
       "      <td>0.574443</td>\n",
       "      <td>0.599116</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.402432</td>\n",
       "      <td>0.417339</td>\n",
       "      <td>0.396465</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.653042</td>\n",
       "      <td>0.646547</td>\n",
       "      <td>0.686237</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.574661</td>\n",
       "      <td>0.572917</td>\n",
       "      <td>0.588384</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.520147</td>\n",
       "      <td>0.522196</td>\n",
       "      <td>0.519764</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.521374</td>\n",
       "      <td>0.523377</td>\n",
       "      <td>0.528866</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.519172</td>\n",
       "      <td>0.520558</td>\n",
       "      <td>0.524792</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.532977</td>\n",
       "      <td>0.539078</td>\n",
       "      <td>0.531900</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.457851</td>\n",
       "      <td>0.510898</td>\n",
       "      <td>0.520270</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.495290</td>\n",
       "      <td>0.491313</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.562552</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.577220</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.553016</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.638996</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.478250</td>\n",
       "      <td>0.497203</td>\n",
       "      <td>0.496212</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.451340</td>\n",
       "      <td>0.501368</td>\n",
       "      <td>0.501894</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.564058</td>\n",
       "      <td>0.562229</td>\n",
       "      <td>0.572601</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.446493</td>\n",
       "      <td>0.470370</td>\n",
       "      <td>0.459596</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.535366</td>\n",
       "      <td>0.550254</td>\n",
       "      <td>0.572035</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.567664</td>\n",
       "      <td>0.584397</td>\n",
       "      <td>0.623180</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.512847</td>\n",
       "      <td>0.512706</td>\n",
       "      <td>0.513350</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.497979</td>\n",
       "      <td>0.513148</td>\n",
       "      <td>0.518464</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.442135</td>\n",
       "      <td>0.503618</td>\n",
       "      <td>0.506757</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.442135</td>\n",
       "      <td>0.503618</td>\n",
       "      <td>0.506757</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.442135</td>\n",
       "      <td>0.503618</td>\n",
       "      <td>0.506757</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.442135</td>\n",
       "      <td>0.503618</td>\n",
       "      <td>0.506757</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.500885</td>\n",
       "      <td>0.556144</td>\n",
       "      <td>0.577020</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.500885</td>\n",
       "      <td>0.556144</td>\n",
       "      <td>0.577020</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.500885</td>\n",
       "      <td>0.556144</td>\n",
       "      <td>0.577020</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.500885</td>\n",
       "      <td>0.556144</td>\n",
       "      <td>0.577020</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.568105</td>\n",
       "      <td>0.580012</td>\n",
       "      <td>0.615031</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.568105</td>\n",
       "      <td>0.580012</td>\n",
       "      <td>0.615031</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.568105</td>\n",
       "      <td>0.580012</td>\n",
       "      <td>0.615031</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.568105</td>\n",
       "      <td>0.580012</td>\n",
       "      <td>0.615031</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>0.581609</td>\n",
       "      <td>0.637066</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.484375</td>\n",
       "      <td>0.514161</td>\n",
       "      <td>0.525097</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>0.581609</td>\n",
       "      <td>0.637066</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.580952</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>0.606178</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.548077</td>\n",
       "      <td>0.548438</td>\n",
       "      <td>0.558712</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.573589</td>\n",
       "      <td>0.589744</td>\n",
       "      <td>0.623737</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.599318</td>\n",
       "      <td>0.627660</td>\n",
       "      <td>0.678030</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.534476</td>\n",
       "      <td>0.552002</td>\n",
       "      <td>0.571338</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.487996</td>\n",
       "      <td>0.490625</td>\n",
       "      <td>0.489078</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.510779</td>\n",
       "      <td>0.515074</td>\n",
       "      <td>0.519157</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.452456</td>\n",
       "      <td>0.487324</td>\n",
       "      <td>0.481276</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.495367</td>\n",
       "      <td>0.496785</td>\n",
       "      <td>0.496359</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.497143</td>\n",
       "      <td>0.529915</td>\n",
       "      <td>0.554054</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.457851</td>\n",
       "      <td>0.510898</td>\n",
       "      <td>0.520270</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.632184</td>\n",
       "      <td>0.722008</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.553016</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.638996</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.540917</td>\n",
       "      <td>0.587095</td>\n",
       "      <td>0.620581</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.455141</td>\n",
       "      <td>0.482143</td>\n",
       "      <td>0.475379</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.639017</td>\n",
       "      <td>0.644444</td>\n",
       "      <td>0.696970</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.585294</td>\n",
       "      <td>0.611413</td>\n",
       "      <td>0.655303</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.553236</td>\n",
       "      <td>0.566098</td>\n",
       "      <td>0.594747</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.568132</td>\n",
       "      <td>0.577897</td>\n",
       "      <td>0.610957</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.558582</td>\n",
       "      <td>0.565642</td>\n",
       "      <td>0.591453</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.496775</td>\n",
       "      <td>0.506460</td>\n",
       "      <td>0.508669</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.557991</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.561776</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.335417</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>0.521236</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.570732</td>\n",
       "      <td>0.572253</td>\n",
       "      <td>0.614865</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>0.521236</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.532638</td>\n",
       "      <td>0.533268</td>\n",
       "      <td>0.532197</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.339229</td>\n",
       "      <td>0.507310</td>\n",
       "      <td>0.506313</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.568279</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.570707</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.466185</td>\n",
       "      <td>0.464859</td>\n",
       "      <td>0.470960</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.492445</td>\n",
       "      <td>0.499074</td>\n",
       "      <td>0.499393</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.513417</td>\n",
       "      <td>0.517102</td>\n",
       "      <td>0.521585</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.397648</td>\n",
       "      <td>0.495373</td>\n",
       "      <td>0.493759</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.507682</td>\n",
       "      <td>0.507944</td>\n",
       "      <td>0.507628</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.525217</td>\n",
       "      <td>0.524658</td>\n",
       "      <td>0.526062</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.477626</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>0.476834</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.530917</td>\n",
       "      <td>0.540476</td>\n",
       "      <td>0.565637</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.522994</td>\n",
       "      <td>0.535652</td>\n",
       "      <td>0.558880</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.559375</td>\n",
       "      <td>0.568609</td>\n",
       "      <td>0.592172</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.574661</td>\n",
       "      <td>0.572917</td>\n",
       "      <td>0.588384</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.574661</td>\n",
       "      <td>0.572917</td>\n",
       "      <td>0.588384</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.548077</td>\n",
       "      <td>0.548438</td>\n",
       "      <td>0.558712</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.592170</td>\n",
       "      <td>0.603732</td>\n",
       "      <td>0.586252</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.635106</td>\n",
       "      <td>0.638034</td>\n",
       "      <td>0.632542</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.575539</td>\n",
       "      <td>0.572706</td>\n",
       "      <td>0.580704</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.580128</td>\n",
       "      <td>0.585962</td>\n",
       "      <td>0.576543</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.500537</td>\n",
       "      <td>0.541340</td>\n",
       "      <td>0.576255</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.431405</td>\n",
       "      <td>0.488064</td>\n",
       "      <td>0.477799</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.526098</td>\n",
       "      <td>0.526471</td>\n",
       "      <td>0.534749</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.516863</td>\n",
       "      <td>0.549285</td>\n",
       "      <td>0.589768</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.486339</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.520833</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.453753</td>\n",
       "      <td>0.495018</td>\n",
       "      <td>0.493056</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.688226</td>\n",
       "      <td>0.677923</td>\n",
       "      <td>0.722854</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.540182</td>\n",
       "      <td>0.541987</td>\n",
       "      <td>0.551768</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.578655</td>\n",
       "      <td>0.578631</td>\n",
       "      <td>0.604369</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.572847</td>\n",
       "      <td>0.592602</td>\n",
       "      <td>0.636182</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.490446</td>\n",
       "      <td>0.492641</td>\n",
       "      <td>0.491505</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.512649</td>\n",
       "      <td>0.517924</td>\n",
       "      <td>0.523232</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.517808</td>\n",
       "      <td>0.517361</td>\n",
       "      <td>0.519305</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.477626</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>0.476834</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.507379</td>\n",
       "      <td>0.526599</td>\n",
       "      <td>0.545367</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>0.531034</td>\n",
       "      <td>0.552124</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.559375</td>\n",
       "      <td>0.568609</td>\n",
       "      <td>0.592172</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.574661</td>\n",
       "      <td>0.572917</td>\n",
       "      <td>0.588384</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.574661</td>\n",
       "      <td>0.572917</td>\n",
       "      <td>0.588384</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.548077</td>\n",
       "      <td>0.548438</td>\n",
       "      <td>0.558712</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.586098</td>\n",
       "      <td>0.594505</td>\n",
       "      <td>0.581397</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.628515</td>\n",
       "      <td>0.629381</td>\n",
       "      <td>0.627687</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.559110</td>\n",
       "      <td>0.556946</td>\n",
       "      <td>0.564494</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.577179</td>\n",
       "      <td>0.581920</td>\n",
       "      <td>0.574116</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                agg  \\\n",
       "0                                                                                                                              mean   \n",
       "1                                                                                                                               max   \n",
       "2                                                                                                                               min   \n",
       "3                                                                                                                     weighted_mean   \n",
       "4                                                                                                                              mean   \n",
       "5                                                                                                                               max   \n",
       "6                                                                                                                               min   \n",
       "7                                                                                                                     weighted_mean   \n",
       "8                                                                                                                              mean   \n",
       "9                                                                                                                               max   \n",
       "10                                                                                                                              min   \n",
       "11                                                                                                                    weighted_mean   \n",
       "12                                                                                                                             mean   \n",
       "13                                                                                                                              max   \n",
       "14                                                                                                                              min   \n",
       "15                                                                                                                    weighted_mean   \n",
       "16                                                                                                                             mean   \n",
       "17                                                                                                                              max   \n",
       "18                                                                                                                              min   \n",
       "19                                                                                                                    weighted_mean   \n",
       "20                                                                                                                             mean   \n",
       "21                                                                                                                              max   \n",
       "22                                                                                                                              min   \n",
       "23                                                                                                                    weighted_mean   \n",
       "24                                                                                                                             mean   \n",
       "25                                                                                                                              max   \n",
       "26                                                                                                                              min   \n",
       "27                                                                                                                    weighted_mean   \n",
       "28                                                                                                                             mean   \n",
       "29                                                                                                                              max   \n",
       "30                                                                                                                              min   \n",
       "31                                                                                                                    weighted_mean   \n",
       "32                                                                                                                             mean   \n",
       "33                                                                                                                              max   \n",
       "34                                                                                                                              min   \n",
       "35                                                                                                                    weighted_mean   \n",
       "36                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "37                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "38                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "39                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "40                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "41                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "42                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "43                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "44                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "45                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "46                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "47                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "48                                                                                                                             mean   \n",
       "49                                                                                                                              max   \n",
       "50                                                                                                                              min   \n",
       "51                                                                                                                    weighted_mean   \n",
       "52                                                                                                                             mean   \n",
       "53                                                                                                                              max   \n",
       "54                                                                                                                              min   \n",
       "55                                                                                                                    weighted_mean   \n",
       "56                                                                                                                             mean   \n",
       "57                                                                                                                              max   \n",
       "58                                                                                                                              min   \n",
       "59                                                                                                                    weighted_mean   \n",
       "60                                                                                                                             mean   \n",
       "61                                                                                                                              max   \n",
       "62                                                                                                                              min   \n",
       "63                                                                                                                    weighted_mean   \n",
       "64                                                                                                                             mean   \n",
       "65                                                                                                                              max   \n",
       "66                                                                                                                              min   \n",
       "67                                                                                                                    weighted_mean   \n",
       "68                                                                                                                             mean   \n",
       "69                                                                                                                              max   \n",
       "70                                                                                                                              min   \n",
       "71                                                                                                                    weighted_mean   \n",
       "72                                                                                                                             mean   \n",
       "73                                                                                                                              max   \n",
       "74                                                                                                                              min   \n",
       "75                                                                                                                    weighted_mean   \n",
       "76                                                                                                                             mean   \n",
       "77                                                                                                                              max   \n",
       "78                                                                                                                              min   \n",
       "79                                                                                                                    weighted_mean   \n",
       "80                                                                                                                             mean   \n",
       "81                                                                                                                              max   \n",
       "82                                                                                                                              min   \n",
       "83                                                                                                                    weighted_mean   \n",
       "84                                                                                                                             mean   \n",
       "85                                                                                                                              max   \n",
       "86                                                                                                                              min   \n",
       "87                                                                                                                    weighted_mean   \n",
       "88                                                                                                                             mean   \n",
       "89                                                                                                                              max   \n",
       "90                                                                                                                              min   \n",
       "91                                                                                                                    weighted_mean   \n",
       "92                                                                                                                             mean   \n",
       "93                                                                                                                              max   \n",
       "94                                                                                                                              min   \n",
       "95                                                                                                                    weighted_mean   \n",
       "96                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "97                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "98                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "99                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "100                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "101                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "102                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "103                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "104                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "105                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "106                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "107                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "108                                                                                                                            mean   \n",
       "109                                                                                                                             max   \n",
       "110                                                                                                                             min   \n",
       "111                                                                                                                   weighted_mean   \n",
       "112                                                                                                                            mean   \n",
       "113                                                                                                                             max   \n",
       "114                                                                                                                             min   \n",
       "115                                                                                                                   weighted_mean   \n",
       "116                                                                                                                            mean   \n",
       "117                                                                                                                             max   \n",
       "118                                                                                                                             min   \n",
       "119                                                                                                                   weighted_mean   \n",
       "120  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "121            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "122            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "123        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "124  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "125            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "126            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "127        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "128  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "129            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "130            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "131        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "132                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "133                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "134                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "135                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "136                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "137                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "138                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "139                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "140                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "141                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "142                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "143                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "144                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "145                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "146                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "147                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "148                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "149                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "150                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "151                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "152                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "153                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "154                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "155                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "\n",
       "       dataset        f1      prec       rec                        zc  \n",
       "0    Wikipedia  0.492308  0.504065  0.501931                linguistic  \n",
       "1    Wikipedia  0.498491  0.521687  0.508687                linguistic  \n",
       "2    Wikipedia  0.511654  0.590196  0.522201                linguistic  \n",
       "3    Wikipedia  0.435897  0.414634  0.459459                linguistic  \n",
       "4     WikiNews  0.435096  0.425000  0.460227                linguistic  \n",
       "5     WikiNews  0.519006  0.530357  0.521465                linguistic  \n",
       "6     WikiNews  0.411133  0.400422  0.425505                linguistic  \n",
       "7     WikiNews  0.510417  0.536905  0.519571                linguistic  \n",
       "8         News  0.530480  0.605882  0.538228                linguistic  \n",
       "9         News  0.502932  0.532197  0.514736                linguistic  \n",
       "10        News  0.617208  0.680250  0.602375                linguistic  \n",
       "11        News  0.500860  0.552591  0.517944                linguistic  \n",
       "12   Wikipedia  0.538013  0.539465  0.556950                 frequency  \n",
       "13   Wikipedia  0.563692  0.562500  0.592664                 frequency  \n",
       "14   Wikipedia  0.537235  0.551500  0.587838                 frequency  \n",
       "15   Wikipedia  0.570312  0.586057  0.652510                 frequency  \n",
       "16    WikiNews  0.518329  0.529872  0.539773                 frequency  \n",
       "17    WikiNews  0.582940  0.580106  0.595328                 frequency  \n",
       "18    WikiNews  0.584301  0.583165  0.604167                 frequency  \n",
       "19    WikiNews  0.601640  0.599851  0.626894                 frequency  \n",
       "20        News  0.600354  0.602555  0.598474                 frequency  \n",
       "21        News  0.598354  0.595751  0.601768                 frequency  \n",
       "22        News  0.623833  0.617336  0.636702                 frequency  \n",
       "23        News  0.657516  0.678188  0.645458                 frequency  \n",
       "24   Wikipedia  0.521029  0.542424  0.574324            language_model  \n",
       "25   Wikipedia  0.544471  0.572602  0.632239            language_model  \n",
       "26   Wikipedia  0.499666  0.522321  0.538610            language_model  \n",
       "27   Wikipedia  0.473539  0.518315  0.533784            language_model  \n",
       "28    WikiNews  0.559395  0.563923  0.583333            language_model  \n",
       "29    WikiNews  0.526260  0.535441  0.546717            language_model  \n",
       "30    WikiNews  0.564978  0.584359  0.616793            language_model  \n",
       "31    WikiNews  0.627828  0.621875  0.647727            language_model  \n",
       "32        News  0.548487  0.546919  0.553138            language_model  \n",
       "33        News  0.578655  0.578631  0.604369            language_model  \n",
       "34        News  0.570324  0.567639  0.581571            language_model  \n",
       "35        News  0.582226  0.580822  0.583911            language_model  \n",
       "36   Wikipedia  0.538013  0.539465  0.556950                    corpus  \n",
       "37   Wikipedia  0.555306  0.556508  0.585907                    corpus  \n",
       "38   Wikipedia  0.521029  0.542424  0.574324                    corpus  \n",
       "39   Wikipedia  0.570312  0.586057  0.652510                    corpus  \n",
       "40    WikiNews  0.518329  0.529872  0.539773                    corpus  \n",
       "41    WikiNews  0.582940  0.580106  0.595328                    corpus  \n",
       "42    WikiNews  0.601244  0.597396  0.618056                    corpus  \n",
       "43    WikiNews  0.601640  0.599851  0.626894                    corpus  \n",
       "44        News  0.600354  0.602555  0.598474                    corpus  \n",
       "45        News  0.606245  0.602984  0.610697                    corpus  \n",
       "46        News  0.627841  0.620760  0.643204                    corpus  \n",
       "47        News  0.657516  0.678188  0.645458                    corpus  \n",
       "48   Wikipedia  0.475339  0.492107  0.487452          psycholinguistic  \n",
       "49   Wikipedia  0.393103  0.469979  0.444015          psycholinguistic  \n",
       "50   Wikipedia  0.589868  0.583156  0.612934          psycholinguistic  \n",
       "51   Wikipedia  0.496503  0.499237  0.499035          psycholinguistic  \n",
       "52    WikiNews  0.567731  0.574443  0.599116          psycholinguistic  \n",
       "53    WikiNews  0.402432  0.417339  0.396465          psycholinguistic  \n",
       "54    WikiNews  0.653042  0.646547  0.686237          psycholinguistic  \n",
       "55    WikiNews  0.574661  0.572917  0.588384          psycholinguistic  \n",
       "56        News  0.520147  0.522196  0.519764          psycholinguistic  \n",
       "57        News  0.521374  0.523377  0.528866          psycholinguistic  \n",
       "58        News  0.519172  0.520558  0.524792          psycholinguistic  \n",
       "59        News  0.532977  0.539078  0.531900          psycholinguistic  \n",
       "60   Wikipedia  0.457851  0.510898  0.520270                   wordnet  \n",
       "61   Wikipedia  0.446541  0.495290  0.491313                   wordnet  \n",
       "62   Wikipedia  0.562552  0.558824  0.577220                   wordnet  \n",
       "63   Wikipedia  0.553016  0.576923  0.638996                   wordnet  \n",
       "64    WikiNews  0.478250  0.497203  0.496212                   wordnet  \n",
       "65    WikiNews  0.451340  0.501368  0.501894                   wordnet  \n",
       "66    WikiNews  0.564058  0.562229  0.572601                   wordnet  \n",
       "67    WikiNews  0.446493  0.470370  0.459596                   wordnet  \n",
       "68        News  0.535366  0.550254  0.572035                   wordnet  \n",
       "69        News  0.567664  0.584397  0.623180                   wordnet  \n",
       "70        News  0.512847  0.512706  0.513350                   wordnet  \n",
       "71        News  0.497979  0.513148  0.518464                   wordnet  \n",
       "72   Wikipedia  0.442135  0.503618  0.506757                   dbpedia  \n",
       "73   Wikipedia  0.442135  0.503618  0.506757                   dbpedia  \n",
       "74   Wikipedia  0.442135  0.503618  0.506757                   dbpedia  \n",
       "75   Wikipedia  0.442135  0.503618  0.506757                   dbpedia  \n",
       "76    WikiNews  0.500885  0.556144  0.577020                   dbpedia  \n",
       "77    WikiNews  0.500885  0.556144  0.577020                   dbpedia  \n",
       "78    WikiNews  0.500885  0.556144  0.577020                   dbpedia  \n",
       "79    WikiNews  0.500885  0.556144  0.577020                   dbpedia  \n",
       "80        News  0.568105  0.580012  0.615031                   dbpedia  \n",
       "81        News  0.568105  0.580012  0.615031                   dbpedia  \n",
       "82        News  0.568105  0.580012  0.615031                   dbpedia  \n",
       "83        News  0.568105  0.580012  0.615031                   dbpedia  \n",
       "84   Wikipedia  0.575758  0.581609  0.637066          brown_clustering  \n",
       "85   Wikipedia  0.484375  0.514161  0.525097          brown_clustering  \n",
       "86   Wikipedia  0.575758  0.581609  0.637066          brown_clustering  \n",
       "87   Wikipedia  0.580952  0.575758  0.606178          brown_clustering  \n",
       "88    WikiNews  0.548077  0.548438  0.558712          brown_clustering  \n",
       "89    WikiNews  0.573589  0.589744  0.623737          brown_clustering  \n",
       "90    WikiNews  0.599318  0.627660  0.678030          brown_clustering  \n",
       "91    WikiNews  0.534476  0.552002  0.571338          brown_clustering  \n",
       "92        News  0.487996  0.490625  0.489078          brown_clustering  \n",
       "93        News  0.510779  0.515074  0.519157          brown_clustering  \n",
       "94        News  0.452456  0.487324  0.481276          brown_clustering  \n",
       "95        News  0.495367  0.496785  0.496359          brown_clustering  \n",
       "96   Wikipedia  0.497143  0.529915  0.554054                  semantic  \n",
       "97   Wikipedia  0.457851  0.510898  0.520270                  semantic  \n",
       "98   Wikipedia  0.636364  0.632184  0.722008                  semantic  \n",
       "99   Wikipedia  0.553016  0.576923  0.638996                  semantic  \n",
       "100   WikiNews  0.540917  0.587095  0.620581                  semantic  \n",
       "101   WikiNews  0.455141  0.482143  0.475379                  semantic  \n",
       "102   WikiNews  0.639017  0.644444  0.696970                  semantic  \n",
       "103   WikiNews  0.585294  0.611413  0.655303                  semantic  \n",
       "104       News  0.553236  0.566098  0.594747                  semantic  \n",
       "105       News  0.568132  0.577897  0.610957                  semantic  \n",
       "106       News  0.558582  0.565642  0.591453                  semantic  \n",
       "107       News  0.496775  0.506460  0.508669                  semantic  \n",
       "108  Wikipedia  0.557991  0.555556  0.561776                dictionary  \n",
       "109  Wikipedia  0.335417  0.515152  0.521236                dictionary  \n",
       "110  Wikipedia  0.570732  0.572253  0.614865                dictionary  \n",
       "111  Wikipedia  0.511111  0.515152  0.521236                dictionary  \n",
       "112   WikiNews  0.532638  0.533268  0.532197                dictionary  \n",
       "113   WikiNews  0.339229  0.507310  0.506313                dictionary  \n",
       "114   WikiNews  0.568279  0.566667  0.570707                dictionary  \n",
       "115   WikiNews  0.466185  0.464859  0.470960                dictionary  \n",
       "116       News  0.492445  0.499074  0.499393                dictionary  \n",
       "117       News  0.513417  0.517102  0.521585                dictionary  \n",
       "118       News  0.397648  0.495373  0.493759                dictionary  \n",
       "119       News  0.507682  0.507944  0.507628                dictionary  \n",
       "120  Wikipedia  0.525217  0.524658  0.526062           corpus+semantic  \n",
       "121  Wikipedia  0.477626  0.479167  0.476834           corpus+semantic  \n",
       "122  Wikipedia  0.530917  0.540476  0.565637           corpus+semantic  \n",
       "123  Wikipedia  0.522994  0.535652  0.558880           corpus+semantic  \n",
       "124   WikiNews  0.559375  0.568609  0.592172           corpus+semantic  \n",
       "125   WikiNews  0.574661  0.572917  0.588384           corpus+semantic  \n",
       "126   WikiNews  0.574661  0.572917  0.588384           corpus+semantic  \n",
       "127   WikiNews  0.548077  0.548438  0.558712           corpus+semantic  \n",
       "128       News  0.592170  0.603732  0.586252           corpus+semantic  \n",
       "129       News  0.635106  0.638034  0.632542           corpus+semantic  \n",
       "130       News  0.575539  0.572706  0.580704           corpus+semantic  \n",
       "131       News  0.580128  0.585962  0.576543           corpus+semantic  \n",
       "132  Wikipedia  0.500537  0.541340  0.576255  wordnet+psycholinguistic  \n",
       "133  Wikipedia  0.431405  0.488064  0.477799  wordnet+psycholinguistic  \n",
       "134  Wikipedia  0.526098  0.526471  0.534749  wordnet+psycholinguistic  \n",
       "135  Wikipedia  0.516863  0.549285  0.589768  wordnet+psycholinguistic  \n",
       "136   WikiNews  0.486339  0.515000  0.520833  wordnet+psycholinguistic  \n",
       "137   WikiNews  0.453753  0.495018  0.493056  wordnet+psycholinguistic  \n",
       "138   WikiNews  0.688226  0.677923  0.722854  wordnet+psycholinguistic  \n",
       "139   WikiNews  0.540182  0.541987  0.551768  wordnet+psycholinguistic  \n",
       "140       News  0.578655  0.578631  0.604369  wordnet+psycholinguistic  \n",
       "141       News  0.572847  0.592602  0.636182  wordnet+psycholinguistic  \n",
       "142       News  0.490446  0.492641  0.491505  wordnet+psycholinguistic  \n",
       "143       News  0.512649  0.517924  0.523232  wordnet+psycholinguistic  \n",
       "144  Wikipedia  0.517808  0.517361  0.519305                       all  \n",
       "145  Wikipedia  0.477626  0.479167  0.476834                       all  \n",
       "146  Wikipedia  0.507379  0.526599  0.545367                       all  \n",
       "147  Wikipedia  0.515152  0.531034  0.552124                       all  \n",
       "148   WikiNews  0.559375  0.568609  0.592172                       all  \n",
       "149   WikiNews  0.574661  0.572917  0.588384                       all  \n",
       "150   WikiNews  0.574661  0.572917  0.588384                       all  \n",
       "151   WikiNews  0.548077  0.548438  0.558712                       all  \n",
       "152       News  0.586098  0.594505  0.581397                       all  \n",
       "153       News  0.628515  0.629381  0.627687                       all  \n",
       "154       News  0.559110  0.556946  0.564494                       all  \n",
       "155       News  0.577179  0.581920  0.574116                       all  "
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_eval_data_kn_dswp = create_eval_df_from_results_macro(results, False)\n",
    "feature_eval_data_kn_dswp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.657516</td>\n",
       "      <td>0.678188</td>\n",
       "      <td>0.645458</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.657516</td>\n",
       "      <td>0.678188</td>\n",
       "      <td>0.645458</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.632184</td>\n",
       "      <td>0.722008</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.688226</td>\n",
       "      <td>0.677923</td>\n",
       "      <td>0.722854</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            agg    dataset  \\\n",
       "23                                                weighted_mean       News   \n",
       "47   (weighted_mean, <function <lambda> at 0x000000FF52232E18>)       News   \n",
       "98     (min, <function agg_feat_num_min at 0x000000FF52232950>)  Wikipedia   \n",
       "138    (min, <function agg_feat_num_min at 0x000000FF52232950>)   WikiNews   \n",
       "\n",
       "           f1      prec       rec                        zc  \n",
       "23   0.657516  0.678188  0.645458                 frequency  \n",
       "47   0.657516  0.678188  0.645458                    corpus  \n",
       "98   0.636364  0.632184  0.722008                  semantic  \n",
       "138  0.688226  0.677923  0.722854  wordnet+psycholinguistic  "
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = feature_eval_data_kn_dswp.groupby(['dataset'])['f1'].transform(max) == feature_eval_data_kn_dswp['f1']\n",
    "feature_eval_data_kn_dswp[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X.2.10 MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "results = [Result(fs, fs.fc, fs.agg,\n",
    "        precision_recall_fscore_support(*mlp(*transform_feat_to_num(remove_labels_for_binary_df(fs.train), \n",
    "        remove_labels_for_binary_df(fs.test))), average='macro')) for fs in all_fc_datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.439490</td>\n",
       "      <td>0.415663</td>\n",
       "      <td>0.466216</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.439490</td>\n",
       "      <td>0.415663</td>\n",
       "      <td>0.466216</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.538393</td>\n",
       "      <td>0.642045</td>\n",
       "      <td>0.547348</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.603376</td>\n",
       "      <td>0.713663</td>\n",
       "      <td>0.592803</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.496429</td>\n",
       "      <td>0.553030</td>\n",
       "      <td>0.517677</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.496429</td>\n",
       "      <td>0.553030</td>\n",
       "      <td>0.517677</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.517010</td>\n",
       "      <td>0.676987</td>\n",
       "      <td>0.534934</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.557700</td>\n",
       "      <td>0.711307</td>\n",
       "      <td>0.559293</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.571303</td>\n",
       "      <td>0.726671</td>\n",
       "      <td>0.568221</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.521881</td>\n",
       "      <td>0.757143</td>\n",
       "      <td>0.539788</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.444726</td>\n",
       "      <td>0.440407</td>\n",
       "      <td>0.474116</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.478529</td>\n",
       "      <td>0.493464</td>\n",
       "      <td>0.496843</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.430303</td>\n",
       "      <td>0.381720</td>\n",
       "      <td>0.493056</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.559503</td>\n",
       "      <td>0.561520</td>\n",
       "      <td>0.581657</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.588204</td>\n",
       "      <td>0.611592</td>\n",
       "      <td>0.580531</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.537072</td>\n",
       "      <td>0.537330</td>\n",
       "      <td>0.536841</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.479528</td>\n",
       "      <td>0.488103</td>\n",
       "      <td>0.493672</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.438972</td>\n",
       "      <td>0.392720</td>\n",
       "      <td>0.497573</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.438972</td>\n",
       "      <td>0.392720</td>\n",
       "      <td>0.497573</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.446541</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.479730</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.498491</td>\n",
       "      <td>0.521687</td>\n",
       "      <td>0.508687</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.469298</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.474903</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.480519</td>\n",
       "      <td>0.481250</td>\n",
       "      <td>0.488417</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.526050</td>\n",
       "      <td>0.542735</td>\n",
       "      <td>0.528409</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.478250</td>\n",
       "      <td>0.497203</td>\n",
       "      <td>0.496212</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.483862</td>\n",
       "      <td>0.495642</td>\n",
       "      <td>0.494318</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.594411</td>\n",
       "      <td>0.677778</td>\n",
       "      <td>0.585859</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.464320</td>\n",
       "      <td>0.504392</td>\n",
       "      <td>0.500867</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.662398</td>\n",
       "      <td>0.652386</td>\n",
       "      <td>0.683773</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.498889</td>\n",
       "      <td>0.515876</td>\n",
       "      <td>0.522538</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.570712</td>\n",
       "      <td>0.596878</td>\n",
       "      <td>0.565101</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.555306</td>\n",
       "      <td>0.556508</td>\n",
       "      <td>0.585907</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.418605</td>\n",
       "      <td>0.486486</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.511654</td>\n",
       "      <td>0.590196</td>\n",
       "      <td>0.522201</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.430303</td>\n",
       "      <td>0.381720</td>\n",
       "      <td>0.493056</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.567554</td>\n",
       "      <td>0.601449</td>\n",
       "      <td>0.641414</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.430303</td>\n",
       "      <td>0.381720</td>\n",
       "      <td>0.493056</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.621476</td>\n",
       "      <td>0.614721</td>\n",
       "      <td>0.638350</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.600083</td>\n",
       "      <td>0.596354</td>\n",
       "      <td>0.605843</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.612230</td>\n",
       "      <td>0.607080</td>\n",
       "      <td>0.621273</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.548791</td>\n",
       "      <td>0.547380</td>\n",
       "      <td>0.551491</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.194139</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.527027</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.175149</td>\n",
       "      <td>0.201124</td>\n",
       "      <td>0.416035</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.206632</td>\n",
       "      <td>0.437333</td>\n",
       "      <td>0.483703</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.438972</td>\n",
       "      <td>0.392720</td>\n",
       "      <td>0.497573</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.438972</td>\n",
       "      <td>0.392720</td>\n",
       "      <td>0.497573</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.439490</td>\n",
       "      <td>0.415663</td>\n",
       "      <td>0.466216</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.439490</td>\n",
       "      <td>0.415663</td>\n",
       "      <td>0.466216</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.439490</td>\n",
       "      <td>0.415663</td>\n",
       "      <td>0.466216</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.439490</td>\n",
       "      <td>0.415663</td>\n",
       "      <td>0.466216</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.561718</td>\n",
       "      <td>0.895604</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.561718</td>\n",
       "      <td>0.895604</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.561718</td>\n",
       "      <td>0.895604</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.561718</td>\n",
       "      <td>0.895604</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.273962</td>\n",
       "      <td>0.474953</td>\n",
       "      <td>0.482750</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.273962</td>\n",
       "      <td>0.474953</td>\n",
       "      <td>0.482750</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.273962</td>\n",
       "      <td>0.474953</td>\n",
       "      <td>0.482750</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.273962</td>\n",
       "      <td>0.474953</td>\n",
       "      <td>0.482750</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.194139</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.527027</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.194139</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.527027</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.432258</td>\n",
       "      <td>0.413580</td>\n",
       "      <td>0.452703</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.207621</td>\n",
       "      <td>0.584337</td>\n",
       "      <td>0.533784</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.159447</td>\n",
       "      <td>0.163383</td>\n",
       "      <td>0.370581</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.430303</td>\n",
       "      <td>0.381720</td>\n",
       "      <td>0.493056</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.449249</td>\n",
       "      <td>0.444900</td>\n",
       "      <td>0.479022</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.439414</td>\n",
       "      <td>0.428221</td>\n",
       "      <td>0.464459</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.457156</td>\n",
       "      <td>0.644231</td>\n",
       "      <td>0.506501</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.454167</td>\n",
       "      <td>0.451155</td>\n",
       "      <td>0.475815</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.421911</td>\n",
       "      <td>0.432113</td>\n",
       "      <td>0.414093</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.617761</td>\n",
       "      <td>0.617761</td>\n",
       "      <td>0.617761</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.458462</td>\n",
       "      <td>0.456140</td>\n",
       "      <td>0.461390</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.463841</td>\n",
       "      <td>0.461039</td>\n",
       "      <td>0.468147</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.526050</td>\n",
       "      <td>0.542735</td>\n",
       "      <td>0.528409</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.548077</td>\n",
       "      <td>0.592857</td>\n",
       "      <td>0.549242</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.588652</td>\n",
       "      <td>0.585797</td>\n",
       "      <td>0.593434</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.487631</td>\n",
       "      <td>0.493856</td>\n",
       "      <td>0.492424</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.477475</td>\n",
       "      <td>0.484082</td>\n",
       "      <td>0.491245</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.507651</td>\n",
       "      <td>0.517688</td>\n",
       "      <td>0.524098</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.417148</td>\n",
       "      <td>0.498407</td>\n",
       "      <td>0.497746</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.456319</td>\n",
       "      <td>0.477512</td>\n",
       "      <td>0.468187</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.430303</td>\n",
       "      <td>0.381720</td>\n",
       "      <td>0.493056</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.524051</td>\n",
       "      <td>0.577035</td>\n",
       "      <td>0.533460</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.472756</td>\n",
       "      <td>0.480952</td>\n",
       "      <td>0.489899</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.596265</td>\n",
       "      <td>0.632004</td>\n",
       "      <td>0.587753</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.570988</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.632576</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.472354</td>\n",
       "      <td>0.504349</td>\n",
       "      <td>0.506415</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.501626</td>\n",
       "      <td>0.517647</td>\n",
       "      <td>0.524965</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.530479</td>\n",
       "      <td>0.573495</td>\n",
       "      <td>0.535021</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.545431</td>\n",
       "      <td>0.552989</td>\n",
       "      <td>0.543256</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.599006</td>\n",
       "      <td>0.591176</td>\n",
       "      <td>0.619691</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.306729</td>\n",
       "      <td>0.555708</td>\n",
       "      <td>0.558880</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.464387</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.501894</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.440171</td>\n",
       "      <td>0.393130</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.534738</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.536668</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.458657</td>\n",
       "      <td>0.894636</td>\n",
       "      <td>0.508929</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.492410</td>\n",
       "      <td>0.537500</td>\n",
       "      <td>0.569498</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.530021</td>\n",
       "      <td>0.555985</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.605128</td>\n",
       "      <td>0.682927</td>\n",
       "      <td>0.586873</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.549722</td>\n",
       "      <td>0.547225</td>\n",
       "      <td>0.555019</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.416149</td>\n",
       "      <td>0.376404</td>\n",
       "      <td>0.465278</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.526050</td>\n",
       "      <td>0.542735</td>\n",
       "      <td>0.528409</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.560284</td>\n",
       "      <td>0.558551</td>\n",
       "      <td>0.563763</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.584596</td>\n",
       "      <td>0.584596</td>\n",
       "      <td>0.584596</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.554848</td>\n",
       "      <td>0.586351</td>\n",
       "      <td>0.552098</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.470510</td>\n",
       "      <td>0.492713</td>\n",
       "      <td>0.497660</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.598548</td>\n",
       "      <td>0.593996</td>\n",
       "      <td>0.607490</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.563069</td>\n",
       "      <td>0.562088</td>\n",
       "      <td>0.578363</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                agg  \\\n",
       "0                                                                                                                              mean   \n",
       "1                                                                                                                               max   \n",
       "2                                                                                                                               min   \n",
       "3                                                                                                                     weighted_mean   \n",
       "4                                                                                                                              mean   \n",
       "5                                                                                                                               max   \n",
       "6                                                                                                                               min   \n",
       "7                                                                                                                     weighted_mean   \n",
       "8                                                                                                                              mean   \n",
       "9                                                                                                                               max   \n",
       "10                                                                                                                              min   \n",
       "11                                                                                                                    weighted_mean   \n",
       "12                                                                                                                             mean   \n",
       "13                                                                                                                              max   \n",
       "14                                                                                                                              min   \n",
       "15                                                                                                                    weighted_mean   \n",
       "16                                                                                                                             mean   \n",
       "17                                                                                                                              max   \n",
       "18                                                                                                                              min   \n",
       "19                                                                                                                    weighted_mean   \n",
       "20                                                                                                                             mean   \n",
       "21                                                                                                                              max   \n",
       "22                                                                                                                              min   \n",
       "23                                                                                                                    weighted_mean   \n",
       "24                                                                                                                             mean   \n",
       "25                                                                                                                              max   \n",
       "26                                                                                                                              min   \n",
       "27                                                                                                                    weighted_mean   \n",
       "28                                                                                                                             mean   \n",
       "29                                                                                                                              max   \n",
       "30                                                                                                                              min   \n",
       "31                                                                                                                    weighted_mean   \n",
       "32                                                                                                                             mean   \n",
       "33                                                                                                                              max   \n",
       "34                                                                                                                              min   \n",
       "35                                                                                                                    weighted_mean   \n",
       "36                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "37                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "38                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "39                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "40                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "41                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "42                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "43                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "44                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "45                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "46                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "47                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "48                                                                                                                             mean   \n",
       "49                                                                                                                              max   \n",
       "50                                                                                                                              min   \n",
       "51                                                                                                                    weighted_mean   \n",
       "52                                                                                                                             mean   \n",
       "53                                                                                                                              max   \n",
       "54                                                                                                                              min   \n",
       "55                                                                                                                    weighted_mean   \n",
       "56                                                                                                                             mean   \n",
       "57                                                                                                                              max   \n",
       "58                                                                                                                              min   \n",
       "59                                                                                                                    weighted_mean   \n",
       "60                                                                                                                             mean   \n",
       "61                                                                                                                              max   \n",
       "62                                                                                                                              min   \n",
       "63                                                                                                                    weighted_mean   \n",
       "64                                                                                                                             mean   \n",
       "65                                                                                                                              max   \n",
       "66                                                                                                                              min   \n",
       "67                                                                                                                    weighted_mean   \n",
       "68                                                                                                                             mean   \n",
       "69                                                                                                                              max   \n",
       "70                                                                                                                              min   \n",
       "71                                                                                                                    weighted_mean   \n",
       "72                                                                                                                             mean   \n",
       "73                                                                                                                              max   \n",
       "74                                                                                                                              min   \n",
       "75                                                                                                                    weighted_mean   \n",
       "76                                                                                                                             mean   \n",
       "77                                                                                                                              max   \n",
       "78                                                                                                                              min   \n",
       "79                                                                                                                    weighted_mean   \n",
       "80                                                                                                                             mean   \n",
       "81                                                                                                                              max   \n",
       "82                                                                                                                              min   \n",
       "83                                                                                                                    weighted_mean   \n",
       "84                                                                                                                             mean   \n",
       "85                                                                                                                              max   \n",
       "86                                                                                                                              min   \n",
       "87                                                                                                                    weighted_mean   \n",
       "88                                                                                                                             mean   \n",
       "89                                                                                                                              max   \n",
       "90                                                                                                                              min   \n",
       "91                                                                                                                    weighted_mean   \n",
       "92                                                                                                                             mean   \n",
       "93                                                                                                                              max   \n",
       "94                                                                                                                              min   \n",
       "95                                                                                                                    weighted_mean   \n",
       "96                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "97                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "98                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "99                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "100                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "101                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "102                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "103                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "104                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "105                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "106                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "107                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "108                                                                                                                            mean   \n",
       "109                                                                                                                             max   \n",
       "110                                                                                                                             min   \n",
       "111                                                                                                                   weighted_mean   \n",
       "112                                                                                                                            mean   \n",
       "113                                                                                                                             max   \n",
       "114                                                                                                                             min   \n",
       "115                                                                                                                   weighted_mean   \n",
       "116                                                                                                                            mean   \n",
       "117                                                                                                                             max   \n",
       "118                                                                                                                             min   \n",
       "119                                                                                                                   weighted_mean   \n",
       "120  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "121            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "122            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "123        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "124  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "125            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "126            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "127        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "128  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "129            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "130            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "131        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "132                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "133                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "134                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "135                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "136                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "137                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "138                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "139                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "140                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "141                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "142                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "143                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "144                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "145                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "146                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "147                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "148                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "149                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "150                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "151                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "152                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "153                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "154                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "155                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "\n",
       "       dataset        f1      prec       rec                        zc  \n",
       "0    Wikipedia  0.456790  0.420455  0.500000                linguistic  \n",
       "1    Wikipedia  0.439490  0.415663  0.466216                linguistic  \n",
       "2    Wikipedia  0.439490  0.415663  0.466216                linguistic  \n",
       "3    Wikipedia  0.456790  0.420455  0.500000                linguistic  \n",
       "4     WikiNews  0.538393  0.642045  0.547348                linguistic  \n",
       "5     WikiNews  0.603376  0.713663  0.592803                linguistic  \n",
       "6     WikiNews  0.496429  0.553030  0.517677                linguistic  \n",
       "7     WikiNews  0.496429  0.553030  0.517677                linguistic  \n",
       "8         News  0.517010  0.676987  0.534934                linguistic  \n",
       "9         News  0.557700  0.711307  0.559293                linguistic  \n",
       "10        News  0.571303  0.726671  0.568221                linguistic  \n",
       "11        News  0.521881  0.757143  0.539788                linguistic  \n",
       "12   Wikipedia  0.456790  0.420455  0.500000                 frequency  \n",
       "13   Wikipedia  0.456790  0.420455  0.500000                 frequency  \n",
       "14   Wikipedia  0.456790  0.420455  0.500000                 frequency  \n",
       "15   Wikipedia  0.456790  0.420455  0.500000                 frequency  \n",
       "16    WikiNews  0.433735  0.382979  0.500000                 frequency  \n",
       "17    WikiNews  0.444726  0.440407  0.474116                 frequency  \n",
       "18    WikiNews  0.478529  0.493464  0.496843                 frequency  \n",
       "19    WikiNews  0.430303  0.381720  0.493056                 frequency  \n",
       "20        News  0.559503  0.561520  0.581657                 frequency  \n",
       "21        News  0.588204  0.611592  0.580531                 frequency  \n",
       "22        News  0.537072  0.537330  0.536841                 frequency  \n",
       "23        News  0.479528  0.488103  0.493672                 frequency  \n",
       "24   Wikipedia  0.456790  0.420455  0.500000            language_model  \n",
       "25   Wikipedia  0.456790  0.420455  0.500000            language_model  \n",
       "26   Wikipedia  0.456790  0.420455  0.500000            language_model  \n",
       "27   Wikipedia  0.456790  0.420455  0.500000            language_model  \n",
       "28    WikiNews  0.433735  0.382979  0.500000            language_model  \n",
       "29    WikiNews  0.433735  0.382979  0.500000            language_model  \n",
       "30    WikiNews  0.433735  0.382979  0.500000            language_model  \n",
       "31    WikiNews  0.433735  0.382979  0.500000            language_model  \n",
       "32        News  0.438972  0.392720  0.497573            language_model  \n",
       "33        News  0.438972  0.392720  0.497573            language_model  \n",
       "34        News  0.440171  0.393130  0.500000            language_model  \n",
       "35        News  0.440171  0.393130  0.500000            language_model  \n",
       "36   Wikipedia  0.446541  0.417647  0.479730                    corpus  \n",
       "37   Wikipedia  0.498491  0.521687  0.508687                    corpus  \n",
       "38   Wikipedia  0.469298  0.466667  0.474903                    corpus  \n",
       "39   Wikipedia  0.480519  0.481250  0.488417                    corpus  \n",
       "40    WikiNews  0.526050  0.542735  0.528409                    corpus  \n",
       "41    WikiNews  0.478250  0.497203  0.496212                    corpus  \n",
       "42    WikiNews  0.483862  0.495642  0.494318                    corpus  \n",
       "43    WikiNews  0.594411  0.677778  0.585859                    corpus  \n",
       "44        News  0.464320  0.504392  0.500867                    corpus  \n",
       "45        News  0.662398  0.652386  0.683773                    corpus  \n",
       "46        News  0.498889  0.515876  0.522538                    corpus  \n",
       "47        News  0.570712  0.596878  0.565101                    corpus  \n",
       "48   Wikipedia  0.456790  0.420455  0.500000          psycholinguistic  \n",
       "49   Wikipedia  0.555306  0.556508  0.585907          psycholinguistic  \n",
       "50   Wikipedia  0.450000  0.418605  0.486486          psycholinguistic  \n",
       "51   Wikipedia  0.511654  0.590196  0.522201          psycholinguistic  \n",
       "52    WikiNews  0.433735  0.382979  0.500000          psycholinguistic  \n",
       "53    WikiNews  0.430303  0.381720  0.493056          psycholinguistic  \n",
       "54    WikiNews  0.567554  0.601449  0.641414          psycholinguistic  \n",
       "55    WikiNews  0.430303  0.381720  0.493056          psycholinguistic  \n",
       "56        News  0.621476  0.614721  0.638350          psycholinguistic  \n",
       "57        News  0.600083  0.596354  0.605843          psycholinguistic  \n",
       "58        News  0.612230  0.607080  0.621273          psycholinguistic  \n",
       "59        News  0.548791  0.547380  0.551491          psycholinguistic  \n",
       "60   Wikipedia  0.453416  0.419540  0.493243                   wordnet  \n",
       "61   Wikipedia  0.194139  0.583333  0.527027                   wordnet  \n",
       "62   Wikipedia  0.456790  0.420455  0.500000                   wordnet  \n",
       "63   Wikipedia  0.456790  0.420455  0.500000                   wordnet  \n",
       "64    WikiNews  0.433735  0.382979  0.500000                   wordnet  \n",
       "65    WikiNews  0.175149  0.201124  0.416035                   wordnet  \n",
       "66    WikiNews  0.433735  0.382979  0.500000                   wordnet  \n",
       "67    WikiNews  0.433735  0.382979  0.500000                   wordnet  \n",
       "68        News  0.206632  0.437333  0.483703                   wordnet  \n",
       "69        News  0.438972  0.392720  0.497573                   wordnet  \n",
       "70        News  0.438972  0.392720  0.497573                   wordnet  \n",
       "71        News  0.440171  0.393130  0.500000                   wordnet  \n",
       "72   Wikipedia  0.439490  0.415663  0.466216                   dbpedia  \n",
       "73   Wikipedia  0.439490  0.415663  0.466216                   dbpedia  \n",
       "74   Wikipedia  0.439490  0.415663  0.466216                   dbpedia  \n",
       "75   Wikipedia  0.439490  0.415663  0.466216                   dbpedia  \n",
       "76    WikiNews  0.561718  0.895604  0.568182                   dbpedia  \n",
       "77    WikiNews  0.561718  0.895604  0.568182                   dbpedia  \n",
       "78    WikiNews  0.561718  0.895604  0.568182                   dbpedia  \n",
       "79    WikiNews  0.561718  0.895604  0.568182                   dbpedia  \n",
       "80        News  0.273962  0.474953  0.482750                   dbpedia  \n",
       "81        News  0.273962  0.474953  0.482750                   dbpedia  \n",
       "82        News  0.273962  0.474953  0.482750                   dbpedia  \n",
       "83        News  0.273962  0.474953  0.482750                   dbpedia  \n",
       "84   Wikipedia  0.194139  0.583333  0.527027          brown_clustering  \n",
       "85   Wikipedia  0.194139  0.583333  0.527027          brown_clustering  \n",
       "86   Wikipedia  0.432258  0.413580  0.452703          brown_clustering  \n",
       "87   Wikipedia  0.207621  0.584337  0.533784          brown_clustering  \n",
       "88    WikiNews  0.159447  0.163383  0.370581          brown_clustering  \n",
       "89    WikiNews  0.433735  0.382979  0.500000          brown_clustering  \n",
       "90    WikiNews  0.433735  0.382979  0.500000          brown_clustering  \n",
       "91    WikiNews  0.430303  0.381720  0.493056          brown_clustering  \n",
       "92        News  0.449249  0.444900  0.479022          brown_clustering  \n",
       "93        News  0.439414  0.428221  0.464459          brown_clustering  \n",
       "94        News  0.457156  0.644231  0.506501          brown_clustering  \n",
       "95        News  0.454167  0.451155  0.475815          brown_clustering  \n",
       "96   Wikipedia  0.421911  0.432113  0.414093                  semantic  \n",
       "97   Wikipedia  0.617761  0.617761  0.617761                  semantic  \n",
       "98   Wikipedia  0.458462  0.456140  0.461390                  semantic  \n",
       "99   Wikipedia  0.463841  0.461039  0.468147                  semantic  \n",
       "100   WikiNews  0.526050  0.542735  0.528409                  semantic  \n",
       "101   WikiNews  0.548077  0.592857  0.549242                  semantic  \n",
       "102   WikiNews  0.588652  0.585797  0.593434                  semantic  \n",
       "103   WikiNews  0.487631  0.493856  0.492424                  semantic  \n",
       "104       News  0.477475  0.484082  0.491245                  semantic  \n",
       "105       News  0.507651  0.517688  0.524098                  semantic  \n",
       "106       News  0.417148  0.498407  0.497746                  semantic  \n",
       "107       News  0.456319  0.477512  0.468187                  semantic  \n",
       "108  Wikipedia  0.456790  0.420455  0.500000                dictionary  \n",
       "109  Wikipedia  0.456790  0.420455  0.500000                dictionary  \n",
       "110  Wikipedia  0.456790  0.420455  0.500000                dictionary  \n",
       "111  Wikipedia  0.456790  0.420455  0.500000                dictionary  \n",
       "112   WikiNews  0.433735  0.382979  0.500000                dictionary  \n",
       "113   WikiNews  0.433735  0.382979  0.500000                dictionary  \n",
       "114   WikiNews  0.433735  0.382979  0.500000                dictionary  \n",
       "115   WikiNews  0.430303  0.381720  0.493056                dictionary  \n",
       "116       News  0.440171  0.393130  0.500000                dictionary  \n",
       "117       News  0.440171  0.393130  0.500000                dictionary  \n",
       "118       News  0.440171  0.393130  0.500000                dictionary  \n",
       "119       News  0.440171  0.393130  0.500000                dictionary  \n",
       "120  Wikipedia  0.456790  0.420455  0.500000           corpus+semantic  \n",
       "121  Wikipedia  0.456790  0.420455  0.500000           corpus+semantic  \n",
       "122  Wikipedia  0.456790  0.420455  0.500000           corpus+semantic  \n",
       "123  Wikipedia  0.456790  0.420455  0.500000           corpus+semantic  \n",
       "124   WikiNews  0.524051  0.577035  0.533460           corpus+semantic  \n",
       "125   WikiNews  0.472756  0.480952  0.489899           corpus+semantic  \n",
       "126   WikiNews  0.596265  0.632004  0.587753           corpus+semantic  \n",
       "127   WikiNews  0.570988  0.595238  0.632576           corpus+semantic  \n",
       "128       News  0.472354  0.504349  0.506415           corpus+semantic  \n",
       "129       News  0.501626  0.517647  0.524965           corpus+semantic  \n",
       "130       News  0.530479  0.573495  0.535021           corpus+semantic  \n",
       "131       News  0.545431  0.552989  0.543256           corpus+semantic  \n",
       "132  Wikipedia  0.599006  0.591176  0.619691  wordnet+psycholinguistic  \n",
       "133  Wikipedia  0.306729  0.555708  0.558880  wordnet+psycholinguistic  \n",
       "134  Wikipedia  0.456790  0.420455  0.500000  wordnet+psycholinguistic  \n",
       "135  Wikipedia  0.456790  0.420455  0.500000  wordnet+psycholinguistic  \n",
       "136   WikiNews  0.433735  0.382979  0.500000  wordnet+psycholinguistic  \n",
       "137   WikiNews  0.433735  0.382979  0.500000  wordnet+psycholinguistic  \n",
       "138   WikiNews  0.464387  0.508333  0.501894  wordnet+psycholinguistic  \n",
       "139   WikiNews  0.433735  0.382979  0.500000  wordnet+psycholinguistic  \n",
       "140       News  0.440171  0.393130  0.500000  wordnet+psycholinguistic  \n",
       "141       News  0.440171  0.393130  0.500000  wordnet+psycholinguistic  \n",
       "142       News  0.534738  0.566667  0.536668  wordnet+psycholinguistic  \n",
       "143       News  0.458657  0.894636  0.508929  wordnet+psycholinguistic  \n",
       "144  Wikipedia  0.492410  0.537500  0.569498                       all  \n",
       "145  Wikipedia  0.476190  0.530021  0.555985                       all  \n",
       "146  Wikipedia  0.605128  0.682927  0.586873                       all  \n",
       "147  Wikipedia  0.549722  0.547225  0.555019                       all  \n",
       "148   WikiNews  0.416149  0.376404  0.465278                       all  \n",
       "149   WikiNews  0.526050  0.542735  0.528409                       all  \n",
       "150   WikiNews  0.560284  0.558551  0.563763                       all  \n",
       "151   WikiNews  0.584596  0.584596  0.584596                       all  \n",
       "152       News  0.554848  0.586351  0.552098                       all  \n",
       "153       News  0.470510  0.492713  0.497660                       all  \n",
       "154       News  0.598548  0.593996  0.607490                       all  \n",
       "155       News  0.563069  0.562088  0.578363                       all  "
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_eval_data_mlp = create_eval_df_from_results_macro(results, False)\n",
    "feature_eval_data_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.603376</td>\n",
       "      <td>0.713663</td>\n",
       "      <td>0.592803</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.662398</td>\n",
       "      <td>0.652386</td>\n",
       "      <td>0.683773</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.617761</td>\n",
       "      <td>0.617761</td>\n",
       "      <td>0.617761</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         agg    dataset  \\\n",
       "5                                                        max   WikiNews   \n",
       "45  (max, <function agg_feat_num_max at 0x000000FF522328C8>)       News   \n",
       "97  (max, <function agg_feat_num_max at 0x000000FF522328C8>)  Wikipedia   \n",
       "\n",
       "          f1      prec       rec          zc  \n",
       "5   0.603376  0.713663  0.592803  linguistic  \n",
       "45  0.662398  0.652386  0.683773      corpus  \n",
       "97  0.617761  0.617761  0.617761    semantic  "
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = feature_eval_data_mlp.groupby(['dataset'])['f1'].transform(max) == feature_eval_data_mlp['f1']\n",
    "feature_eval_data_mlp[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "results = [Result(fs, fs.fc, fs.agg,\n",
    "        precision_recall_fscore_support(*mlp(*transform_feat_to_num(remove_labels_for_binary_df(fs.train), \n",
    "        remove_labels_for_binary_df(fs.test))), average='macro')) for fs in all_fc_datasets_dswp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.545894</td>\n",
       "      <td>0.693258</td>\n",
       "      <td>0.554293</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.563713</td>\n",
       "      <td>0.645349</td>\n",
       "      <td>0.563131</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.515583</td>\n",
       "      <td>0.723443</td>\n",
       "      <td>0.538510</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.580357</td>\n",
       "      <td>0.731061</td>\n",
       "      <td>0.577020</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.537228</td>\n",
       "      <td>0.776575</td>\n",
       "      <td>0.548717</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.531957</td>\n",
       "      <td>0.700794</td>\n",
       "      <td>0.543863</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.552137</td>\n",
       "      <td>0.792051</td>\n",
       "      <td>0.557646</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.501630</td>\n",
       "      <td>0.647638</td>\n",
       "      <td>0.526006</td>\n",
       "      <td>linguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.439490</td>\n",
       "      <td>0.415663</td>\n",
       "      <td>0.466216</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.547059</td>\n",
       "      <td>0.550868</td>\n",
       "      <td>0.579151</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.451143</td>\n",
       "      <td>0.543985</td>\n",
       "      <td>0.580116</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.410256</td>\n",
       "      <td>0.432432</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.490943</td>\n",
       "      <td>0.498101</td>\n",
       "      <td>0.498737</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.455836</td>\n",
       "      <td>0.453466</td>\n",
       "      <td>0.469066</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.552381</td>\n",
       "      <td>0.550905</td>\n",
       "      <td>0.556818</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.531121</td>\n",
       "      <td>0.605090</td>\n",
       "      <td>0.540404</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.436509</td>\n",
       "      <td>0.537465</td>\n",
       "      <td>0.550537</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.549927</td>\n",
       "      <td>0.552965</td>\n",
       "      <td>0.570302</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.413192</td>\n",
       "      <td>0.512210</td>\n",
       "      <td>0.516470</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.581631</td>\n",
       "      <td>0.580920</td>\n",
       "      <td>0.606796</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.499666</td>\n",
       "      <td>0.522321</td>\n",
       "      <td>0.538610</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.469206</td>\n",
       "      <td>0.506410</td>\n",
       "      <td>0.511583</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.447876</td>\n",
       "      <td>0.447876</td>\n",
       "      <td>0.447876</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.497143</td>\n",
       "      <td>0.529915</td>\n",
       "      <td>0.554054</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.533035</td>\n",
       "      <td>0.557727</td>\n",
       "      <td>0.580177</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.533035</td>\n",
       "      <td>0.557727</td>\n",
       "      <td>0.580177</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.527299</td>\n",
       "      <td>0.570455</td>\n",
       "      <td>0.597854</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.549899</td>\n",
       "      <td>0.568223</td>\n",
       "      <td>0.594066</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.476934</td>\n",
       "      <td>0.490945</td>\n",
       "      <td>0.487604</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.533543</td>\n",
       "      <td>0.537898</td>\n",
       "      <td>0.550017</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.413066</td>\n",
       "      <td>0.482221</td>\n",
       "      <td>0.474168</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.521965</td>\n",
       "      <td>0.522152</td>\n",
       "      <td>0.525572</td>\n",
       "      <td>language_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.316770</td>\n",
       "      <td>0.530159</td>\n",
       "      <td>0.536680</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.478764</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.427278</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.515444</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.461444</td>\n",
       "      <td>0.484220</td>\n",
       "      <td>0.473938</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.459397</td>\n",
       "      <td>0.482022</td>\n",
       "      <td>0.494949</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.444726</td>\n",
       "      <td>0.440407</td>\n",
       "      <td>0.474116</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.509040</td>\n",
       "      <td>0.518402</td>\n",
       "      <td>0.523990</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.468927</td>\n",
       "      <td>0.485301</td>\n",
       "      <td>0.480429</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.617653</td>\n",
       "      <td>0.649828</td>\n",
       "      <td>0.605669</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.237513</td>\n",
       "      <td>0.480372</td>\n",
       "      <td>0.491765</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.559110</td>\n",
       "      <td>0.556946</td>\n",
       "      <td>0.564494</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.676067</td>\n",
       "      <td>0.665181</td>\n",
       "      <td>0.697555</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.468966</td>\n",
       "      <td>0.538302</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.461647</td>\n",
       "      <td>0.502650</td>\n",
       "      <td>0.504826</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>0.521236</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.530021</td>\n",
       "      <td>0.555985</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.558271</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.659091</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.541463</td>\n",
       "      <td>0.562927</td>\n",
       "      <td>0.587121</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.502486</td>\n",
       "      <td>0.502609</td>\n",
       "      <td>0.502525</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.461421</td>\n",
       "      <td>0.461382</td>\n",
       "      <td>0.476010</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.435345</td>\n",
       "      <td>0.391473</td>\n",
       "      <td>0.490291</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.176101</td>\n",
       "      <td>0.106870</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.475185</td>\n",
       "      <td>0.514173</td>\n",
       "      <td>0.521064</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.438972</td>\n",
       "      <td>0.392720</td>\n",
       "      <td>0.497573</td>\n",
       "      <td>psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.260504</td>\n",
       "      <td>0.538961</td>\n",
       "      <td>0.531853</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.283998</td>\n",
       "      <td>0.548205</td>\n",
       "      <td>0.545367</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.562189</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.608108</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.420455</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.205003</td>\n",
       "      <td>0.618280</td>\n",
       "      <td>0.506944</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.190324</td>\n",
       "      <td>0.276557</td>\n",
       "      <td>0.461490</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.623397</td>\n",
       "      <td>0.704762</td>\n",
       "      <td>0.608586</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.205003</td>\n",
       "      <td>0.618280</td>\n",
       "      <td>0.506944</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.181498</td>\n",
       "      <td>0.354651</td>\n",
       "      <td>0.486997</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.176101</td>\n",
       "      <td>0.106870</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.448762</td>\n",
       "      <td>0.451626</td>\n",
       "      <td>0.446862</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.178858</td>\n",
       "      <td>0.355769</td>\n",
       "      <td>0.493499</td>\n",
       "      <td>wordnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.525217</td>\n",
       "      <td>0.524658</td>\n",
       "      <td>0.526062</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.525217</td>\n",
       "      <td>0.524658</td>\n",
       "      <td>0.526062</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.525217</td>\n",
       "      <td>0.524658</td>\n",
       "      <td>0.526062</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.525217</td>\n",
       "      <td>0.524658</td>\n",
       "      <td>0.526062</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.318841</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.429293</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.318841</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.429293</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.318841</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.429293</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.318841</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.429293</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.339454</td>\n",
       "      <td>0.489724</td>\n",
       "      <td>0.489078</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.339454</td>\n",
       "      <td>0.489724</td>\n",
       "      <td>0.489078</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.339454</td>\n",
       "      <td>0.489724</td>\n",
       "      <td>0.489078</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.339454</td>\n",
       "      <td>0.489724</td>\n",
       "      <td>0.489078</td>\n",
       "      <td>dbpedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.414634</td>\n",
       "      <td>0.459459</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.137255</td>\n",
       "      <td>0.079545</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.429443</td>\n",
       "      <td>0.447917</td>\n",
       "      <td>0.422780</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.474851</td>\n",
       "      <td>0.473277</td>\n",
       "      <td>0.481660</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.603376</td>\n",
       "      <td>0.713663</td>\n",
       "      <td>0.592803</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.612665</td>\n",
       "      <td>0.759442</td>\n",
       "      <td>0.599747</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.540182</td>\n",
       "      <td>0.541987</td>\n",
       "      <td>0.551768</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.622321</td>\n",
       "      <td>0.820076</td>\n",
       "      <td>0.606692</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.496456</td>\n",
       "      <td>0.534381</td>\n",
       "      <td>0.513089</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.461412</td>\n",
       "      <td>0.459094</td>\n",
       "      <td>0.471827</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.347090</td>\n",
       "      <td>0.535313</td>\n",
       "      <td>0.532940</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.181487</td>\n",
       "      <td>0.607280</td>\n",
       "      <td>0.502427</td>\n",
       "      <td>brown_clustering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.379705</td>\n",
       "      <td>0.515780</td>\n",
       "      <td>0.526062</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.148090</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.442085</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.306729</td>\n",
       "      <td>0.555708</td>\n",
       "      <td>0.558880</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.530130</td>\n",
       "      <td>0.533854</td>\n",
       "      <td>0.550193</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.511942</td>\n",
       "      <td>0.595400</td>\n",
       "      <td>0.624369</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.570776</td>\n",
       "      <td>0.573649</td>\n",
       "      <td>0.568813</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.641281</td>\n",
       "      <td>0.650175</td>\n",
       "      <td>0.635101</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.443270</td>\n",
       "      <td>0.514699</td>\n",
       "      <td>0.519571</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.387814</td>\n",
       "      <td>0.489812</td>\n",
       "      <td>0.486477</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.442337</td>\n",
       "      <td>0.431667</td>\n",
       "      <td>0.482230</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.278237</td>\n",
       "      <td>0.478982</td>\n",
       "      <td>0.485177</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.356942</td>\n",
       "      <td>0.489881</td>\n",
       "      <td>0.488211</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.563692</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.592664</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>max</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.249612</td>\n",
       "      <td>0.495614</td>\n",
       "      <td>0.496139</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>min</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.517808</td>\n",
       "      <td>0.517361</td>\n",
       "      <td>0.519305</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.517808</td>\n",
       "      <td>0.517361</td>\n",
       "      <td>0.519305</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.601244</td>\n",
       "      <td>0.597396</td>\n",
       "      <td>0.618056</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>max</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.382699</td>\n",
       "      <td>0.517262</td>\n",
       "      <td>0.518308</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>min</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433026</td>\n",
       "      <td>0.544562</td>\n",
       "      <td>0.553030</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.551052</td>\n",
       "      <td>0.562937</td>\n",
       "      <td>0.585227</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.489572</td>\n",
       "      <td>0.490863</td>\n",
       "      <td>0.489858</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>max</td>\n",
       "      <td>News</td>\n",
       "      <td>0.374009</td>\n",
       "      <td>0.564637</td>\n",
       "      <td>0.562933</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>min</td>\n",
       "      <td>News</td>\n",
       "      <td>0.385602</td>\n",
       "      <td>0.492755</td>\n",
       "      <td>0.490551</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>weighted_mean</td>\n",
       "      <td>News</td>\n",
       "      <td>0.487996</td>\n",
       "      <td>0.490625</td>\n",
       "      <td>0.489078</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.452179</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>0.557915</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.468966</td>\n",
       "      <td>0.538302</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.557991</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.561776</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.526098</td>\n",
       "      <td>0.526471</td>\n",
       "      <td>0.534749</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.401818</td>\n",
       "      <td>0.514069</td>\n",
       "      <td>0.516414</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.584301</td>\n",
       "      <td>0.583165</td>\n",
       "      <td>0.604167</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.608333</td>\n",
       "      <td>0.612782</td>\n",
       "      <td>0.651515</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.525253</td>\n",
       "      <td>0.525253</td>\n",
       "      <td>0.525253</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>[(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;), (mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.615118</td>\n",
       "      <td>0.618590</td>\n",
       "      <td>0.612257</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>[(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;), (max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.450839</td>\n",
       "      <td>0.459003</td>\n",
       "      <td>0.449376</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>[(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;), (min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.527377</td>\n",
       "      <td>0.536835</td>\n",
       "      <td>0.550884</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>[(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;), (weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)]</td>\n",
       "      <td>News</td>\n",
       "      <td>0.543618</td>\n",
       "      <td>0.553580</td>\n",
       "      <td>0.575243</td>\n",
       "      <td>corpus+semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.402356</td>\n",
       "      <td>0.497350</td>\n",
       "      <td>0.495174</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.327374</td>\n",
       "      <td>0.534325</td>\n",
       "      <td>0.543436</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.435495</td>\n",
       "      <td>0.511936</td>\n",
       "      <td>0.522201</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.453416</td>\n",
       "      <td>0.419540</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.226974</td>\n",
       "      <td>0.491667</td>\n",
       "      <td>0.498106</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.433735</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.430954</td>\n",
       "      <td>0.530210</td>\n",
       "      <td>0.537247</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.340127</td>\n",
       "      <td>0.489865</td>\n",
       "      <td>0.490530</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.479952</td>\n",
       "      <td>0.544841</td>\n",
       "      <td>0.509795</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.194676</td>\n",
       "      <td>0.507004</td>\n",
       "      <td>0.500780</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.211489</td>\n",
       "      <td>0.329545</td>\n",
       "      <td>0.363471</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.459965</td>\n",
       "      <td>0.477632</td>\n",
       "      <td>0.468967</td>\n",
       "      <td>wordnet+psycholinguistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.525079</td>\n",
       "      <td>0.553419</td>\n",
       "      <td>0.596525</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.615830</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.537190</td>\n",
       "      <td>0.579398</td>\n",
       "      <td>0.647683</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.511397</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>0.627413</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.523068</td>\n",
       "      <td>0.577773</td>\n",
       "      <td>0.606692</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.623397</td>\n",
       "      <td>0.704762</td>\n",
       "      <td>0.608586</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.534476</td>\n",
       "      <td>0.552002</td>\n",
       "      <td>0.571338</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.397436</td>\n",
       "      <td>0.487903</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000FF52232D90&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.365488</td>\n",
       "      <td>0.511209</td>\n",
       "      <td>0.512569</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000FF522328C8&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.510718</td>\n",
       "      <td>0.525136</td>\n",
       "      <td>0.515603</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.596681</td>\n",
       "      <td>0.592954</td>\n",
       "      <td>0.618932</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.556619</td>\n",
       "      <td>0.559362</td>\n",
       "      <td>0.579230</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                agg  \\\n",
       "0                                                                                                                              mean   \n",
       "1                                                                                                                               max   \n",
       "2                                                                                                                               min   \n",
       "3                                                                                                                     weighted_mean   \n",
       "4                                                                                                                              mean   \n",
       "5                                                                                                                               max   \n",
       "6                                                                                                                               min   \n",
       "7                                                                                                                     weighted_mean   \n",
       "8                                                                                                                              mean   \n",
       "9                                                                                                                               max   \n",
       "10                                                                                                                              min   \n",
       "11                                                                                                                    weighted_mean   \n",
       "12                                                                                                                             mean   \n",
       "13                                                                                                                              max   \n",
       "14                                                                                                                              min   \n",
       "15                                                                                                                    weighted_mean   \n",
       "16                                                                                                                             mean   \n",
       "17                                                                                                                              max   \n",
       "18                                                                                                                              min   \n",
       "19                                                                                                                    weighted_mean   \n",
       "20                                                                                                                             mean   \n",
       "21                                                                                                                              max   \n",
       "22                                                                                                                              min   \n",
       "23                                                                                                                    weighted_mean   \n",
       "24                                                                                                                             mean   \n",
       "25                                                                                                                              max   \n",
       "26                                                                                                                              min   \n",
       "27                                                                                                                    weighted_mean   \n",
       "28                                                                                                                             mean   \n",
       "29                                                                                                                              max   \n",
       "30                                                                                                                              min   \n",
       "31                                                                                                                    weighted_mean   \n",
       "32                                                                                                                             mean   \n",
       "33                                                                                                                              max   \n",
       "34                                                                                                                              min   \n",
       "35                                                                                                                    weighted_mean   \n",
       "36                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "37                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "38                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "39                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "40                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "41                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "42                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "43                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "44                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "45                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "46                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "47                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "48                                                                                                                             mean   \n",
       "49                                                                                                                              max   \n",
       "50                                                                                                                              min   \n",
       "51                                                                                                                    weighted_mean   \n",
       "52                                                                                                                             mean   \n",
       "53                                                                                                                              max   \n",
       "54                                                                                                                              min   \n",
       "55                                                                                                                    weighted_mean   \n",
       "56                                                                                                                             mean   \n",
       "57                                                                                                                              max   \n",
       "58                                                                                                                              min   \n",
       "59                                                                                                                    weighted_mean   \n",
       "60                                                                                                                             mean   \n",
       "61                                                                                                                              max   \n",
       "62                                                                                                                              min   \n",
       "63                                                                                                                    weighted_mean   \n",
       "64                                                                                                                             mean   \n",
       "65                                                                                                                              max   \n",
       "66                                                                                                                              min   \n",
       "67                                                                                                                    weighted_mean   \n",
       "68                                                                                                                             mean   \n",
       "69                                                                                                                              max   \n",
       "70                                                                                                                              min   \n",
       "71                                                                                                                    weighted_mean   \n",
       "72                                                                                                                             mean   \n",
       "73                                                                                                                              max   \n",
       "74                                                                                                                              min   \n",
       "75                                                                                                                    weighted_mean   \n",
       "76                                                                                                                             mean   \n",
       "77                                                                                                                              max   \n",
       "78                                                                                                                              min   \n",
       "79                                                                                                                    weighted_mean   \n",
       "80                                                                                                                             mean   \n",
       "81                                                                                                                              max   \n",
       "82                                                                                                                              min   \n",
       "83                                                                                                                    weighted_mean   \n",
       "84                                                                                                                             mean   \n",
       "85                                                                                                                              max   \n",
       "86                                                                                                                              min   \n",
       "87                                                                                                                    weighted_mean   \n",
       "88                                                                                                                             mean   \n",
       "89                                                                                                                              max   \n",
       "90                                                                                                                              min   \n",
       "91                                                                                                                    weighted_mean   \n",
       "92                                                                                                                             mean   \n",
       "93                                                                                                                              max   \n",
       "94                                                                                                                              min   \n",
       "95                                                                                                                    weighted_mean   \n",
       "96                                                                    (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "97                                                                         (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "98                                                                         (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "99                                                                       (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "100                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "101                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "102                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "103                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "104                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "105                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "106                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "107                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "108                                                                                                                            mean   \n",
       "109                                                                                                                             max   \n",
       "110                                                                                                                             min   \n",
       "111                                                                                                                   weighted_mean   \n",
       "112                                                                                                                            mean   \n",
       "113                                                                                                                             max   \n",
       "114                                                                                                                             min   \n",
       "115                                                                                                                   weighted_mean   \n",
       "116                                                                                                                            mean   \n",
       "117                                                                                                                             max   \n",
       "118                                                                                                                             min   \n",
       "119                                                                                                                   weighted_mean   \n",
       "120  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "121            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "122            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "123        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "124  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "125            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "126            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "127        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "128  [(mean, <function agg_feat_num_average at 0x000000FF52232D90>), (mean, <function agg_feat_num_average at 0x000000FF52232D90>)]   \n",
       "129            [(max, <function agg_feat_num_max at 0x000000FF522328C8>), (max, <function agg_feat_num_max at 0x000000FF522328C8>)]   \n",
       "130            [(min, <function agg_feat_num_min at 0x000000FF52232950>), (min, <function agg_feat_num_min at 0x000000FF52232950>)]   \n",
       "131        [(weighted_mean, <function <lambda> at 0x000000FF52232E18>), (weighted_mean, <function <lambda> at 0x000000FF52232E18>)]   \n",
       "132                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "133                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "134                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "135                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "136                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "137                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "138                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "139                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "140                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "141                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "142                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "143                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "144                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "145                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "146                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "147                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "148                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "149                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "150                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "151                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "152                                                                   (mean, <function agg_feat_num_average at 0x000000FF52232D90>)   \n",
       "153                                                                        (max, <function agg_feat_num_max at 0x000000FF522328C8>)   \n",
       "154                                                                        (min, <function agg_feat_num_min at 0x000000FF52232950>)   \n",
       "155                                                                      (weighted_mean, <function <lambda> at 0x000000FF52232E18>)   \n",
       "\n",
       "       dataset        f1      prec       rec                        zc  \n",
       "0    Wikipedia  0.456790  0.420455  0.500000                linguistic  \n",
       "1    Wikipedia  0.456790  0.420455  0.500000                linguistic  \n",
       "2    Wikipedia  0.456790  0.420455  0.500000                linguistic  \n",
       "3    Wikipedia  0.456790  0.420455  0.500000                linguistic  \n",
       "4     WikiNews  0.545894  0.693258  0.554293                linguistic  \n",
       "5     WikiNews  0.563713  0.645349  0.563131                linguistic  \n",
       "6     WikiNews  0.515583  0.723443  0.538510                linguistic  \n",
       "7     WikiNews  0.580357  0.731061  0.577020                linguistic  \n",
       "8         News  0.537228  0.776575  0.548717                linguistic  \n",
       "9         News  0.531957  0.700794  0.543863                linguistic  \n",
       "10        News  0.552137  0.792051  0.557646                linguistic  \n",
       "11        News  0.501630  0.647638  0.526006                linguistic  \n",
       "12   Wikipedia  0.439490  0.415663  0.466216                 frequency  \n",
       "13   Wikipedia  0.547059  0.550868  0.579151                 frequency  \n",
       "14   Wikipedia  0.451143  0.543985  0.580116                 frequency  \n",
       "15   Wikipedia  0.421053  0.410256  0.432432                 frequency  \n",
       "16    WikiNews  0.490943  0.498101  0.498737                 frequency  \n",
       "17    WikiNews  0.455836  0.453466  0.469066                 frequency  \n",
       "18    WikiNews  0.552381  0.550905  0.556818                 frequency  \n",
       "19    WikiNews  0.531121  0.605090  0.540404                 frequency  \n",
       "20        News  0.436509  0.537465  0.550537                 frequency  \n",
       "21        News  0.549927  0.552965  0.570302                 frequency  \n",
       "22        News  0.413192  0.512210  0.516470                 frequency  \n",
       "23        News  0.581631  0.580920  0.606796                 frequency  \n",
       "24   Wikipedia  0.499666  0.522321  0.538610            language_model  \n",
       "25   Wikipedia  0.469206  0.506410  0.511583            language_model  \n",
       "26   Wikipedia  0.447876  0.447876  0.447876            language_model  \n",
       "27   Wikipedia  0.497143  0.529915  0.554054            language_model  \n",
       "28    WikiNews  0.533035  0.557727  0.580177            language_model  \n",
       "29    WikiNews  0.533035  0.557727  0.580177            language_model  \n",
       "30    WikiNews  0.527299  0.570455  0.597854            language_model  \n",
       "31    WikiNews  0.549899  0.568223  0.594066            language_model  \n",
       "32        News  0.476934  0.490945  0.487604            language_model  \n",
       "33        News  0.533543  0.537898  0.550017            language_model  \n",
       "34        News  0.413066  0.482221  0.474168            language_model  \n",
       "35        News  0.521965  0.522152  0.525572            language_model  \n",
       "36   Wikipedia  0.316770  0.530159  0.536680                    corpus  \n",
       "37   Wikipedia  0.476190  0.484848  0.478764                    corpus  \n",
       "38   Wikipedia  0.427278  0.508333  0.515444                    corpus  \n",
       "39   Wikipedia  0.461444  0.484220  0.473938                    corpus  \n",
       "40    WikiNews  0.459397  0.482022  0.494949                    corpus  \n",
       "41    WikiNews  0.444726  0.440407  0.474116                    corpus  \n",
       "42    WikiNews  0.509040  0.518402  0.523990                    corpus  \n",
       "43    WikiNews  0.468927  0.485301  0.480429                    corpus  \n",
       "44        News  0.617653  0.649828  0.605669                    corpus  \n",
       "45        News  0.237513  0.480372  0.491765                    corpus  \n",
       "46        News  0.559110  0.556946  0.564494                    corpus  \n",
       "47        News  0.676067  0.665181  0.697555                    corpus  \n",
       "48   Wikipedia  0.468966  0.538302  0.571429          psycholinguistic  \n",
       "49   Wikipedia  0.461647  0.502650  0.504826          psycholinguistic  \n",
       "50   Wikipedia  0.511111  0.515152  0.521236          psycholinguistic  \n",
       "51   Wikipedia  0.476190  0.530021  0.555985          psycholinguistic  \n",
       "52    WikiNews  0.558271  0.616667  0.659091          psycholinguistic  \n",
       "53    WikiNews  0.541463  0.562927  0.587121          psycholinguistic  \n",
       "54    WikiNews  0.502486  0.502609  0.502525          psycholinguistic  \n",
       "55    WikiNews  0.461421  0.461382  0.476010          psycholinguistic  \n",
       "56        News  0.435345  0.391473  0.490291          psycholinguistic  \n",
       "57        News  0.176101  0.106870  0.500000          psycholinguistic  \n",
       "58        News  0.475185  0.514173  0.521064          psycholinguistic  \n",
       "59        News  0.438972  0.392720  0.497573          psycholinguistic  \n",
       "60   Wikipedia  0.260504  0.538961  0.531853                   wordnet  \n",
       "61   Wikipedia  0.283998  0.548205  0.545367                   wordnet  \n",
       "62   Wikipedia  0.562189  0.566667  0.608108                   wordnet  \n",
       "63   Wikipedia  0.456790  0.420455  0.500000                   wordnet  \n",
       "64    WikiNews  0.205003  0.618280  0.506944                   wordnet  \n",
       "65    WikiNews  0.190324  0.276557  0.461490                   wordnet  \n",
       "66    WikiNews  0.623397  0.704762  0.608586                   wordnet  \n",
       "67    WikiNews  0.205003  0.618280  0.506944                   wordnet  \n",
       "68        News  0.181498  0.354651  0.486997                   wordnet  \n",
       "69        News  0.176101  0.106870  0.500000                   wordnet  \n",
       "70        News  0.448762  0.451626  0.446862                   wordnet  \n",
       "71        News  0.178858  0.355769  0.493499                   wordnet  \n",
       "72   Wikipedia  0.525217  0.524658  0.526062                   dbpedia  \n",
       "73   Wikipedia  0.525217  0.524658  0.526062                   dbpedia  \n",
       "74   Wikipedia  0.525217  0.524658  0.526062                   dbpedia  \n",
       "75   Wikipedia  0.525217  0.524658  0.526062                   dbpedia  \n",
       "76    WikiNews  0.318841  0.433333  0.429293                   dbpedia  \n",
       "77    WikiNews  0.318841  0.433333  0.429293                   dbpedia  \n",
       "78    WikiNews  0.318841  0.433333  0.429293                   dbpedia  \n",
       "79    WikiNews  0.318841  0.433333  0.429293                   dbpedia  \n",
       "80        News  0.339454  0.489724  0.489078                   dbpedia  \n",
       "81        News  0.339454  0.489724  0.489078                   dbpedia  \n",
       "82        News  0.339454  0.489724  0.489078                   dbpedia  \n",
       "83        News  0.339454  0.489724  0.489078                   dbpedia  \n",
       "84   Wikipedia  0.435897  0.414634  0.459459          brown_clustering  \n",
       "85   Wikipedia  0.137255  0.079545  0.500000          brown_clustering  \n",
       "86   Wikipedia  0.429443  0.447917  0.422780          brown_clustering  \n",
       "87   Wikipedia  0.474851  0.473277  0.481660          brown_clustering  \n",
       "88    WikiNews  0.603376  0.713663  0.592803          brown_clustering  \n",
       "89    WikiNews  0.612665  0.759442  0.599747          brown_clustering  \n",
       "90    WikiNews  0.540182  0.541987  0.551768          brown_clustering  \n",
       "91    WikiNews  0.622321  0.820076  0.606692          brown_clustering  \n",
       "92        News  0.496456  0.534381  0.513089          brown_clustering  \n",
       "93        News  0.461412  0.459094  0.471827          brown_clustering  \n",
       "94        News  0.347090  0.535313  0.532940          brown_clustering  \n",
       "95        News  0.181487  0.607280  0.502427          brown_clustering  \n",
       "96   Wikipedia  0.379705  0.515780  0.526062                  semantic  \n",
       "97   Wikipedia  0.148090  0.321429  0.442085                  semantic  \n",
       "98   Wikipedia  0.306729  0.555708  0.558880                  semantic  \n",
       "99   Wikipedia  0.530130  0.533854  0.550193                  semantic  \n",
       "100   WikiNews  0.511942  0.595400  0.624369                  semantic  \n",
       "101   WikiNews  0.570776  0.573649  0.568813                  semantic  \n",
       "102   WikiNews  0.641281  0.650175  0.635101                  semantic  \n",
       "103   WikiNews  0.443270  0.514699  0.519571                  semantic  \n",
       "104       News  0.387814  0.489812  0.486477                  semantic  \n",
       "105       News  0.442337  0.431667  0.482230                  semantic  \n",
       "106       News  0.278237  0.478982  0.485177                  semantic  \n",
       "107       News  0.356942  0.489881  0.488211                  semantic  \n",
       "108  Wikipedia  0.563692  0.562500  0.592664                dictionary  \n",
       "109  Wikipedia  0.249612  0.495614  0.496139                dictionary  \n",
       "110  Wikipedia  0.517808  0.517361  0.519305                dictionary  \n",
       "111  Wikipedia  0.517808  0.517361  0.519305                dictionary  \n",
       "112   WikiNews  0.601244  0.597396  0.618056                dictionary  \n",
       "113   WikiNews  0.382699  0.517262  0.518308                dictionary  \n",
       "114   WikiNews  0.433026  0.544562  0.553030                dictionary  \n",
       "115   WikiNews  0.551052  0.562937  0.585227                dictionary  \n",
       "116       News  0.489572  0.490863  0.489858                dictionary  \n",
       "117       News  0.374009  0.564637  0.562933                dictionary  \n",
       "118       News  0.385602  0.492755  0.490551                dictionary  \n",
       "119       News  0.487996  0.490625  0.489078                dictionary  \n",
       "120  Wikipedia  0.452179  0.531250  0.557915           corpus+semantic  \n",
       "121  Wikipedia  0.468966  0.538302  0.571429           corpus+semantic  \n",
       "122  Wikipedia  0.557991  0.555556  0.561776           corpus+semantic  \n",
       "123  Wikipedia  0.526098  0.526471  0.534749           corpus+semantic  \n",
       "124   WikiNews  0.401818  0.514069  0.516414           corpus+semantic  \n",
       "125   WikiNews  0.584301  0.583165  0.604167           corpus+semantic  \n",
       "126   WikiNews  0.608333  0.612782  0.651515           corpus+semantic  \n",
       "127   WikiNews  0.525253  0.525253  0.525253           corpus+semantic  \n",
       "128       News  0.615118  0.618590  0.612257           corpus+semantic  \n",
       "129       News  0.450839  0.459003  0.449376           corpus+semantic  \n",
       "130       News  0.527377  0.536835  0.550884           corpus+semantic  \n",
       "131       News  0.543618  0.553580  0.575243           corpus+semantic  \n",
       "132  Wikipedia  0.402356  0.497350  0.495174  wordnet+psycholinguistic  \n",
       "133  Wikipedia  0.327374  0.534325  0.543436  wordnet+psycholinguistic  \n",
       "134  Wikipedia  0.435495  0.511936  0.522201  wordnet+psycholinguistic  \n",
       "135  Wikipedia  0.453416  0.419540  0.493243  wordnet+psycholinguistic  \n",
       "136   WikiNews  0.226974  0.491667  0.498106  wordnet+psycholinguistic  \n",
       "137   WikiNews  0.433735  0.382979  0.500000  wordnet+psycholinguistic  \n",
       "138   WikiNews  0.430954  0.530210  0.537247  wordnet+psycholinguistic  \n",
       "139   WikiNews  0.340127  0.489865  0.490530  wordnet+psycholinguistic  \n",
       "140       News  0.479952  0.544841  0.509795  wordnet+psycholinguistic  \n",
       "141       News  0.194676  0.507004  0.500780  wordnet+psycholinguistic  \n",
       "142       News  0.211489  0.329545  0.363471  wordnet+psycholinguistic  \n",
       "143       News  0.459965  0.477632  0.468967  wordnet+psycholinguistic  \n",
       "144  Wikipedia  0.525079  0.553419  0.596525                       all  \n",
       "145  Wikipedia  0.466667  0.564103  0.615830                       all  \n",
       "146  Wikipedia  0.537190  0.579398  0.647683                       all  \n",
       "147  Wikipedia  0.511397  0.568182  0.627413                       all  \n",
       "148   WikiNews  0.523068  0.577773  0.606692                       all  \n",
       "149   WikiNews  0.623397  0.704762  0.608586                       all  \n",
       "150   WikiNews  0.534476  0.552002  0.571338                       all  \n",
       "151   WikiNews  0.397436  0.487903  0.484848                       all  \n",
       "152       News  0.365488  0.511209  0.512569                       all  \n",
       "153       News  0.510718  0.525136  0.515603                       all  \n",
       "154       News  0.596681  0.592954  0.618932                       all  \n",
       "155       News  0.556619  0.559362  0.579230                       all  "
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_eval_data_mlp_dswp = create_eval_df_from_results_macro(results, False)\n",
    "feature_eval_data_mlp_dswp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000FF52232E18&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.676067</td>\n",
       "      <td>0.665181</td>\n",
       "      <td>0.697555</td>\n",
       "      <td>corpus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>(min, &lt;function agg_feat_num_min at 0x000000FF52232950&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.641281</td>\n",
       "      <td>0.650175</td>\n",
       "      <td>0.635101</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>mean</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.563692</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.592664</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            agg    dataset  \\\n",
       "47   (weighted_mean, <function <lambda> at 0x000000FF52232E18>)       News   \n",
       "102    (min, <function agg_feat_num_min at 0x000000FF52232950>)   WikiNews   \n",
       "108                                                        mean  Wikipedia   \n",
       "\n",
       "           f1      prec       rec          zc  \n",
       "47   0.676067  0.665181  0.697555      corpus  \n",
       "102  0.641281  0.650175  0.635101    semantic  \n",
       "108  0.563692  0.562500  0.592664  dictionary  "
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = feature_eval_data_mlp_dswp.groupby(['dataset'])['f1'].transform(max) == feature_eval_data_mlp_dswp['f1']\n",
    "feature_eval_data_mlp_dswp[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X. CPI Model for final CWPI System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we first load the DS-P datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets and preprocess data\n",
    "datasets_original_phrases = load_datasets(['Wikipedia', 'WikiNews', 'News'], 'TrainDev', 'Test', \\\n",
    "                         type_train='phrase', type_test='phrase', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_predictions(dataset, model):\n",
    "    test, preds = model(*transform_feat_to_num(\n",
    "                remove_labels_for_binary_df(dataset.train), \n",
    "                remove_labels_for_binary_df(dataset.test)))\n",
    "    return preds\n",
    "\n",
    "def concatenate_preds_and_dataframe(dataframe, preds):\n",
    "    df = dataframe.copy()\n",
    "    df['prediction'] = preds\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X.1 Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Studio\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\core\\fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: Mean of empty slice\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: RuntimeWarning: Mean of empty slice\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:166: RuntimeWarning: invalid value encountered in double_scalars\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:202: RuntimeWarning: invalid value encountered in long_scalars\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:226: RuntimeWarning: divide by zero encountered in long_scalars\n",
      "C:\\Users\\Studio\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "datasets = load_datasets(['Wikipedia'], 'TrainDev', 'Test', type_train='both', type_test='phrase')\n",
    "datasets = preprocess_datasets(datasets)\n",
    "datasets_fc_wordnet_wp = compute_features_wordnet(datasets, aggs=agg_weighted)\n",
    "datasets_fc_dbpedia_wp = compute_features_dbpedia(datasets, aggs=agg_weighted)\n",
    "datasets_fc_brown_clustering_wp = compute_features_brown_clustering(datasets, aggs=agg_weighted)\n",
    "datasets_fc_semantic_wp = compute_features_semantic([datasets_fc_wordnet_wp, datasets_fc_dbpedia_wp, \n",
    "                                            datasets_fc_brown_clustering_wp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_wiki = [Result(fs, fs.fc, fs.agg,\n",
    "                precision_recall_fscore_support(*xgboost(*transform_feat_to_num(remove_labels_for_binary_df(fs.train), \n",
    "                remove_labels_for_binary_df(fs.test))), average='macro')) for fs in datasets_fc_semantic_wp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(weighted_mean, &lt;function &lt;lambda&gt; at 0x000000AF2B768AE8&gt;)</td>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.531006</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.529685</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          agg    dataset  \\\n",
       "0  (weighted_mean, <function <lambda> at 0x000000AF2B768AE8>)  Wikipedia   \n",
       "\n",
       "         f1      prec       rec        zc  \n",
       "0  0.531006  0.538462  0.529685  semantic  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_eval_df_from_results_macro(results_wiki, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_wiki = compute_predictions(datasets_fc_semantic_wp[0], xgboost)\n",
    "dataframe_preds_wiki_phrases = concatenate_preds_and_dataframe(datasets_original_phrases[0].test, predictions_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH_DATASET = \"../cwishareddataset/traindevset/english/\"\n",
    "# Save best Wikipedia features\n",
    "dataframe_preds_wiki_phrases.to_csv(MAIN_PATH_DATASET+'WikipediaPhrase_Test.tsv', sep='\\t', \\\n",
    "                                encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X.2 WikiNews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Studio\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\core\\fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: Mean of empty slice\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: RuntimeWarning: Mean of empty slice\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:166: RuntimeWarning: invalid value encountered in double_scalars\n",
      "C:\\Users\\Studio\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\core\\_methods.py:26: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_maximum(a, axis, None, out, keepdims)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:202: RuntimeWarning: invalid value encountered in long_scalars\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:226: RuntimeWarning: divide by zero encountered in long_scalars\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:202: RuntimeWarning: invalid value encountered in double_scalars\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:226: RuntimeWarning: invalid value encountered in long_scalars\n",
      "C:\\Users\\Studio\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "datasets = load_datasets(['WikiNews'], 'TrainDev', 'Test', type_train='phrase', type_test='phrase')\n",
    "datasets = preprocess_datasets(datasets)\n",
    "datasets_fc_wordnet_wikinews = compute_features_wordnet(datasets, aggs=agg_max)\n",
    "datasets_fc_dbpedia_wikinews = compute_features_dbpedia(datasets, aggs=agg_max)\n",
    "datasets_fc_brown_clustering_wikinews = compute_features_brown_clustering(datasets, aggs=agg_max)\n",
    "datasets_fc_semantic_wikinews = compute_features_semantic([datasets_fc_wordnet_wikinews, \n",
    "                                    datasets_fc_dbpedia_wikinews, datasets_fc_brown_clustering_wikinews])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_wikinews = [Result(fs, fs.fc, fs.agg,\n",
    "                precision_recall_fscore_support(*xgboost(*transform_feat_to_num(remove_labels_for_binary_df(fs.train), \n",
    "                remove_labels_for_binary_df(fs.test))), average='macro')) for fs in datasets_fc_semantic_wikinews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(max, &lt;function agg_feat_num_max at 0x000000AF2B768A60&gt;)</td>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.604191</td>\n",
       "      <td>0.632785</td>\n",
       "      <td>0.594885</td>\n",
       "      <td>semantic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        agg   dataset  \\\n",
       "0  (max, <function agg_feat_num_max at 0x000000AF2B768A60>)  WikiNews   \n",
       "\n",
       "         f1      prec       rec        zc  \n",
       "0  0.604191  0.632785  0.594885  semantic  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_eval_df_from_results_macro(results_wikinews, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_wikinews = compute_predictions(datasets_fc_semantic_wikinews[0], xgboost)\n",
    "dataframe_preds_wikinews_phrases = concatenate_preds_and_dataframe(datasets_original_phrases[1].test, predictions_wikinews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH_DATASET = \"../cwishareddataset/traindevset/english/\"\n",
    "# Save best Wikipedia features\n",
    "dataframe_preds_wikinews_phrases.to_csv(MAIN_PATH_DATASET+'WikiNewsPhrase_Test.tsv', sep='\\t', \\\n",
    "                                encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X.3 News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Studio\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\core\\fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\Studio\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: Mean of empty slice\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: RuntimeWarning: Mean of empty slice\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:166: RuntimeWarning: invalid value encountered in double_scalars\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:202: RuntimeWarning: invalid value encountered in double_scalars\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:202: RuntimeWarning: invalid value encountered in long_scalars\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:226: RuntimeWarning: invalid value encountered in long_scalars\n",
      "C:\\Users\\Studio\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:226: RuntimeWarning: divide by zero encountered in long_scalars\n"
     ]
    }
   ],
   "source": [
    "datasets = load_datasets(['News'], 'TrainDev', 'Test', type_train='phrase', type_test='phrase')\n",
    "datasets = preprocess_datasets(datasets)\n",
    "# 1. Linguistic Features\n",
    "datasets_fc_linguistic = compute_features_linguistic(datasets, aggs=agg_mean)\n",
    "# 2. Corpus Features\n",
    "datasets_fc_frequency = compute_features_frequency(datasets, aggs=agg_mean)\n",
    "datasets_fc_language_model = compute_features_language_model(datasets, aggs=agg_mean)\n",
    "datasets_fc_corpus = compute_features_corpus([datasets_fc_frequency, datasets_fc_language_model])\n",
    "# 3. Psycholinguistic\n",
    "datasets_fc_psycholinguistic = compute_features_psycholinguistic(datasets, aggs=agg_mean)\n",
    "# 4. Semantic Features\n",
    "datasets_fc_wordnet = compute_features_wordnet(datasets, aggs=agg_mean)\n",
    "datasets_fc_dbpedia = compute_features_dbpedia(datasets, aggs=agg_mean)\n",
    "datasets_fc_brown_clustering = compute_features_brown_clustering(datasets, aggs=agg_mean)\n",
    "datasets_fc_semantic = compute_features_semantic([datasets_fc_wordnet, datasets_fc_dbpedia, datasets_fc_brown_clustering])\n",
    "# 5. Dictionary Features\n",
    "datasets_fc_dictionary = compute_features_dictionary(datasets, aggs=agg_mean)\n",
    "#(3) All categories\n",
    "datasets_fc_all = concat_feature_datasets(datasets_fc_linguistic, datasets_fc_psycholinguistic, \\\n",
    "                            datasets_fc_semantic, datasets_fc_corpus, datasets_fc_dictionary, name='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_news = [Result(fs, fs.fc, fs.agg,\n",
    "                precision_recall_fscore_support(*adaboost(*transform_feat_to_num(remove_labels_for_binary_df(fs.train), \n",
    "                remove_labels_for_binary_df(fs.test))), average='macro')) for fs in datasets_fc_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(mean, &lt;function agg_feat_num_average at 0x000000AF2B768D08&gt;)</td>\n",
       "      <td>News</td>\n",
       "      <td>0.519678</td>\n",
       "      <td>0.520635</td>\n",
       "      <td>0.525341</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             agg dataset  \\\n",
       "0  (mean, <function agg_feat_num_average at 0x000000AF2B768D08>)    News   \n",
       "\n",
       "         f1      prec       rec   zc  \n",
       "0  0.519678  0.520635  0.525341  all  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_eval_df_from_results_macro(results_news, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_news = compute_predictions(datasets_fc_all[0], adaboost)\n",
    "dataframe_preds_news_phrases = concatenate_preds_and_dataframe(datasets_original_phrases[2].test, predictions_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH_DATASET = \"../cwishareddataset/traindevset/english/\"\n",
    "# Save best Wikipedia features\n",
    "dataframe_preds_news_phrases.to_csv(MAIN_PATH_DATASET+'NewsPhrase_Test.tsv', sep='\\t', \\\n",
    "                                encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix = datasets_fc_all[1].train.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.80) and column not in columns]\n",
    "# Drop features \n",
    "#df.drop(df.columns[to_drop], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_fc_all_corr_feats = [FeatureDataset(ds.name, ds.fc, ds.agg, ds.train.drop(to_drop, axis=1),\n",
    "                 ds.test.drop(to_drop, axis=1)) for ds in datasets_fc_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [Result(fs, fs.fc, fs.agg,\n",
    "        precision_recall_fscore_support(*xgboost(*transform_feat_to_num(remove_labels_for_binary_df(fs.train), \n",
    "        remove_labels_for_binary_df(fs.test))))) for fs in datasets_fc_all_corr_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wikipedia</td>\n",
       "      <td>0.718929</td>\n",
       "      <td>0.728682</td>\n",
       "      <td>0.709434</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WikiNews</td>\n",
       "      <td>0.773619</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.772242</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>News</td>\n",
       "      <td>0.806551</td>\n",
       "      <td>0.809035</td>\n",
       "      <td>0.804082</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset        f1      prec       rec   zc\n",
       "0  Wikipedia  0.718929  0.728682  0.709434  all\n",
       "1   WikiNews  0.773619  0.775000  0.772242  all\n",
       "2       News  0.806551  0.809035  0.804082  all"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_eval_data = create_eval_df_from_results(results)\n",
    "feature_eval_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
